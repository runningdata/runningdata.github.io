<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[hue兼容slider部署的presto]]></title>
    <url>%2F2017%2F12%2F26%2Fhue%E5%85%BC%E5%AE%B9slider%E9%83%A8%E7%BD%B2%E7%9A%84presto%2F</url>
    <content type="text"><![CDATA[通常对于应用部署，尤其是presto这种占用资源比较大，而且可能需要动态扩缩容的场景，我们会考虑使用资源管理框架来进行控制。首先想到的就是yarn和mesos。 我们可以通过slider将presot部署到yarn上，或者使用docker部署persto到mesos上实现资源管控。对于前者slider的服务发现与注册功能稍有欠缺，而后者的话，通过marathon-lb很容易实现服务发现功能，只需要把coordinator注册过去就可了。我们目前暂时使用第一种方案。 前面我们开发了自己的presto connector，但是我们还要面临一个跟jdbc相关的presto的问题，就是当我们使用slider部署presto到yarn的时候，presto的coordinator是变化的。为解决这个问题，我们需要修改一下presto建立数据库连接的位置的代码，使用动态获取yarn应用的方式进行url更新。 两种方案 从yarn爬取原来的presto connector是从jdbc复制过来，需要的option参数如下：1234[[[presto]]] name=Presto interface=presto options='&#123;"password": "***empty***", "url": "jdbc:presto://datanode13.will.com:8099/hive/", "driver": "com.facebook.presto.jdbc.PrestoDriver"&#125;' 我们修改为如下：1234[[[presto]]] name=Presto interface=presto options=&apos;&#123;&quot;password&quot;: &quot;***empty***&quot;, &quot;url&quot;: &quot;jdbc:presto://&#123;coordinator_host&#125;:8099/hive/&quot;, &quot;driver&quot;: &quot;com.facebook.presto.jdbc.PrestoDriver&quot;, &quot;app_name&quot;: &quot;presto_will_online&quot;, &quot;resource_manager&quot;: &quot;http://datanode02.will.com:8088&quot;&#125;&apos; 然后presto.py原代码：12345678910111213141516class PrestoApi(Api): def __init__(self, user, interpreter=None): global API_CACHE Api.__init__(self, user, interpreter=interpreter) self.db = None self.options = interpreter['options'] # 爬取yarn上的presto地址，清洗url参数 if self.cache_key in API_CACHE: self.db = API_CACHE[self.cache_key] elif 'password' in self.options: username = self.options.get('user') or user.username self.db = API_CACHE[self.cache_key] = Jdbc(self.options['driver'], self.options['url'], username, self.options['password']) 参考： https://hadoop.apache.org/docs/r2.7.3/hadoop-yarn/hadoop-yarn-site/ResourceManagerRest.html#Cluster_Application_Statistics_API presto coordinator启动后自动注册当前主机找到presto-yarn项目解压后的脚本package/scripts/presto_coordinator.py内容123456from presto_server import PrestoServerif __name__ == "__main__": PrestoServer('COORDINATOR').execute() # 添加向指定位置注册当前主机名的代码, 可以注册到redis、mysql、hdfs、zk等 最终选用的方案是方案一，考虑是已经对hue有了一些二次开发了。就直接把修改放在hue上吧，后续我们还是有可能把presto迁移到docker上的。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto与impala的初步比较]]></title>
    <url>%2F2017%2F12%2F26%2Fpresto%E4%B8%8Eimpala%E7%9A%84%E5%88%9D%E6%AD%A5%E6%AF%94%E8%BE%83%2F</url>
    <content type="text"><![CDATA[特性 presto impala sql兼容 兼容mysql标准函数与类型，分析师只需掌握mysql语法函数即可 高度兼容hive 部署难度 较低 高 规模 动态调整 每个DataNode上都要一个impala实例 容错 多个worker挂掉不影响 多个impalad实例挂掉会有较大影响 性能 高，秒级 很高，秒级 语言 java c++ 容量 易于扩缩容，不影响既有应用 完全取决于datanode 问题解决 基于java,，自主patch或者熟悉原理的成本低 基于c++，自主patch或者熟悉原理的成本较高 社区 国内各种技术会议议题点击率较高【美团、头条、滴滴、链家等等】 国内使用的目前耳闻几乎没有 hive兼容性 对于基于异构存储的hive表不能兼容 自己维护额外的元数据表，所有的hive表变更都需要额外的命令操作，极不方便 数据源兼容性 高，自定义connector 低，只兼容HDFS之上 我们的现状 人力资源少，可投入专项维护的人员几乎没有 需求方接受查询在分钟级别的体验，期望更快的体验 需尽快响应使用中出现的问题 基于以上，两者的性能差距对我们的影响较小，执行时间差别感知较小。而其他维护性的工作【扩缩容、问题排查等】对于当前数据组的有限精力来说更为看重。所以选型暂定为persto。 其实最主要的就是： 出问题的话，是几度懵逼！！ 参考 : https://www.quora.com/How-does-Cloudera-Impala-compare-to-Facebooks-Presto http://blog.cloudera.com/blog/2014/09/new-benchmarks-for-sql-on-hadoop-impala-1-4-widens-the-performance-gap/ 【这个是impala的爸爸CDH的文章，准确性有待探测】 https://groups.google.com/forum/#!topic/presto-users/NA42x4B0H-o]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GIAC-全球互联网架构大会]]></title>
    <url>%2F2017%2F12%2F26%2FGIAC-%E5%85%A8%E7%90%83%E4%BA%92%E8%81%94%E7%BD%91%E6%9E%B6%E6%9E%84%E5%A4%A7%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[摩拜新上的火爆活动，需要后端扩容 —— 使用swarm支持动态扩缩容 国际化带来的问题：欧盟数据保护规范：GDPR 使用腾讯云在全球主要位置部署了多数据中心。 多集群、同城多活，也是通过腾讯云给的解决方案搞定 多集群、异地多活。可以缓存的全部引入CDN dbproxy. mysql分表分库-&gt;TiDB DRC 数据复制中心 Mysql -&gt; kafka -&gt; xxx 随着开发人员增多，代码合并后的质量很成问题。 -&gt; 拆分spring cloud微服务 + netflix nginx + lua [路由控制、负载均衡、服务发现] -&gt; spring zuul [目前的问题：同步IO，占用CPU内存较高] spring config 、 consul + vault进行加密控制，后面接git、svn、文件 feign client + consul 内部通信使用grpc 容器问题【端口协调、依赖ansible、】 docker swarm的overlay网络开销比较大，使用了腾讯的swarm服务，基于docker swarm有了一些改进。 mongo是有可能有安全漏洞的，摩拜只是使用mongo来存储地理位置信息，因为对安全不太相关，最多就车的信息被泄露出去。同时也有网络隔离等安全措施 所有的服务都应该是无状态的，可以飘移到任何地方，这在最开始的时候就已经有了这个考虑。 服务化扩展使用docker swarm实现，持久化数据的扩展使用TiDB、codis等易于扩展的数据库【分表分库对于一些很复杂的连接sql，外键关联等业务不能支持很好】。 不可能所有服务都满足CAP，所以先需要对事务进行服务定级。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slider服务发现]]></title>
    <url>%2F2017%2F12%2F22%2Fslider%E6%9C%8D%E5%8A%A1%E5%8F%91%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[问题 client可能是运行在cluster之外的 部署在yarn里的各个component的位置都是会变的 服务端口可以是固定的、或者可预测的 比如开始在client端指定了service的位置，但是如果service遇到失败自己重启了，这个位置可能就在yarn集群中变了。client自然就访问不到了。这种事情的发生几率跟集群稳定性有很大关系。 其他限制 降低已经存在app的变化次数，最好是一次都不要变。。这显然不现实 防止恶意注册service endpoint app和client都可以任意扩缩容 可能的解决方案zookeeperclient用zk来发现service，hbase就是这样的 DNS变化的IP地址LDAP等等我的场景就是使用slider在yarn上部署了presto，presto的coordinator的位置是会变化的，但是端口是固定的。在不修改hue的配置的前提下，怎样才能跟随presto变化呢？ 个人认为官网给的点子都把问题复杂化了。 前面提到其实我们已经在hue的源码里自己创建presto的jdbc类，那么我们在建立session的时候，那个url通过爬取yarn上运行的我们指定presto app名字的位置的接口不就可以获取到当前在yarn里运行着的presto的coordinator了。聪明如我，嘎嘎。而且，hue本来就是要配置resourcemanager的地址，来抓取hive任务的执行进度的。 伪代码如下123def creat_sesssion(): url = request_yarn(options['presto_app_name']) session = JDBC(self.user, self.pwd, url..) 参考： https://slider.incubator.apache.org/design/registry/index.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pika笔记]]></title>
    <url>%2F2017%2F12%2F22%2Fpika%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[redis的角色从缓存，现在慢慢转向内存数据库了。 但是因为自身的内存限制上有限制，所以诞生了pika。借助了redis的很多机制和rocksdb的存储，基于rocksdb的接口实现pika的功能。 pika 1.0ttl功能通过在插入数据时，修改rocksdb时带上ttl时间戳字段。修改rocksdb的get接口，进行时间戳判断。 使用version实现了大量数据的的秒删，先把这个hash当前version递加，其实原理跟hbase是一样的原理，后面再在compact的过程中进行真正物理的删除。 问题： 同步的数据缓冲区容易写满，扫瞄binlog的进程比较快，传输比较慢 存活检测与数据同步相互影响，存活检测迟迟得不到响应 全同步bgsave，主库dump，然后发给从库。很慢，而且数据量有很大 pika 2.0解决1.0的问题 线程合并与拆分，进行重构优化 存活检测与数据同步隔离 快照式备份，百G秒级相应。使用rocksdb的checkpoint执行，阻塞一下，确保数据一致，创建文件硬链 解耦rocksdbrocksdb更新速度非常快，使用新版特性的时候，要修改很多代码。 在rocksdb之上实现一个adaptor，也就是nemo-rocksdb。 回收metakey，metakey其实是hash表的唯一key。主要是比如hash1被删除了，但是还没有compact的时候，然后又马上重建了hash1，那么这个metakey的version就又成了0了。期望version是绝对的增长，后来把新建version改成当前时间戳了，删的时候就直接删，再新建的时候再弄这个当前时间戳。 pika 2.3.0 pika双主，高可用。但是还需要切流量。当原主恢复后，可以先全同步数据，再增量同步，因为可能挂掉的时间比较长，binlog会很多。 还有：不支持双主写同一个key的value，可能会出现问题。推荐写只走一个实例，另一个作为standby。 跨机房同步。pika_hub以slave的身份加入多个机房集群，相互同步。注意pika_hub也要多实例高可用，借用外部的高可用一致性raft组件存储元数据、竞锁选主。如果有多个对于同一个key的操作，就以时间戳的形式兼容。pika_hub之间不同步log，只发送raft记录断点，主节点定时记录checkpoint到raft集群。 支持订阅。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slider配置相关]]></title>
    <url>%2F2017%2F12%2F22%2Fslider%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[配置component的失败处理文档中并没有明确说这回事儿，需要自己去理解，理解了好半天。就是需要下面两个参数配置起来才行的。先配置一个window，指定一个时段长度，然后在单个时段长度内达到阈值上限才会认为app失败，否则就会一直重启。 参数 作用 默认 yarn.container.failure.threshold 窗口内失败次数的阈值 默认貌似是5次 yarn.container.failure.window.day、yarn.container.failure.window.hours、yarn.container.failure.window.minutes 窗口时间段 默认是yarn.container.failure.window.hours=6 放置策略其实就是把app分配到哪些node上的策略。有三个值 默认的：就是重启的时候满集群找合适的节点，除了不可用的node都有可能被选中，如果没有满足的，就触发escalation strict： 每个node都要有一个component，不管有没有失败历史。这就不会发生escalation了。 anywhere： 不顾历史，到处请求 anti affinity required(无穷远)：目前不支持。 例子123456&quot;HBASE_REST&quot;: &#123; &quot;yarn.role.priority&quot;: &quot;3&quot;, &quot;yarn.component.instances&quot;: &quot;1&quot;, &quot;yarn.component.placement.policy&quot;: &quot;1&quot;, &quot;yarn.memory&quot;: &quot;556&quot;&#125;, 这里指定了为1，那么每次启动的时候都会找到上次运行的同一个节点。 yarn.node.failure.threshold | 在某个node上执行component失败的阈值，到达这个阈值后，就不会接受component了。 一个hbase的实例配置：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566&#123; "schema": "http://example.org/specification/v2.0.0", "metadata": &#123; &#125;, "global": &#123; "yarn.log.include.patterns": "hbase.*", "yarn.log.exclude.patterns": "hbase.*out", // 30分钟一个window "yarn.container.failure.window.hours": "0", "yarn.container.failure.window.minutes": "30", // 选择带有development label的yarnnode分配component "yarn.label.expression":"development" &#125;, "components": &#123; "slider-appmaster": &#123; "yarn.memory": "1024", "yarn.vcores": "1" // 可以随意分配到default label的node "yarn.label.expression":"" &#125;, "HBASE_MASTER": &#123; // 数字越低，优先级越高，通常用来控制启动次序 "yarn.role.priority": "1", "yarn.component.instances": "1", "yarn.placement.escalate.seconds": "10", "yarn.vcores": "1", "yarn.memory": "1500" &#125;, "HBASE_REGIONSERVER": &#123; "yarn.role.priority": "2", "yarn.component.instances": "1", "yarn.vcores": "1", "yarn.memory": "1500", // 30分钟内失败15次的话，就标记这个component部署失败 "yarn.container.failure.threshold": "15", "yarn.placement.escalate.seconds": "60" &#125;, "HBASE_REST": &#123; "yarn.role.priority": "3", "yarn.component.instances": "1", "yarn.component.placement.policy": "1", // 30分钟内失败3次的话，就标记这个component部署失败 "yarn.container.failure.threshold": "3", "yarn.vcores": "1", "yarn.memory": "556" &#125;, "HBASE_THRIFT": &#123; "yarn.role.priority": "4", "yarn.component.instances": "0", "yarn.component.placement.policy": "1", "yarn.vcores": "1", "yarn.memory": "556", // 选择带有stable label的yarnnode分配component "yarn.label.expression":"stable" &#125;, "HBASE_THRIFT2": &#123; "yarn.role.priority": "5", "yarn.component.instances": "1", "yarn.component.placement.policy": "1", "yarn.vcores": "1", "yarn.memory": "556"， // 选择带有stable label的yarnnode分配component "yarn.label.expression":"stable" &#125; &#125;&#125;]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[整合presto与hue]]></title>
    <url>%2F2017%2F12%2F19%2F%E6%95%B4%E5%90%88presto%E4%B8%8Ehue%2F</url>
    <content type="text"><![CDATA[传说是因为hue里没有直接对于presto的支持，所以大家都通过RDBMS间接的在hue上执行presto的查询。 可行方案一共找到两种方案： 借助postgresql创建中间的字典表，然后转化SQL进行执行。具体参考 presto jdbc 第一种方案部署相对复杂很多，选用了第二种方案。不过过程中发现了一些不错的项目，回头来看其实都是presto官网推荐的，可以看一下这些resource 实际方案最后确认的方案，是使用官方提供的jdbc连接方式。这种相对简单，架构也更清晰一些。 主要包含以下几个步骤 1. 下载对应presto版本的jdbc包2. 修改本地java环境变量CLASSPATH否则会报classNotFound1export CLASSPATH=$CLASSPATH:/server/todo/presto-jdbc-0.184.jar 其实个人感觉不太方便，所以在hue的启动脚本build/env/bin/hue里添加了一下环境变量的设置，算是高内聚一下hue相关123456789if __name__ == '__main__': # 添加环境变量设置 os.putenv('CLASSPATH', '/server/todo/presto-jdbc-0.184.jar:' + os.getenv('CLASSPATH', '')) print os.getenv('CLASSPATH') sys.argv[0] = re.sub(r'(-script\.pyw?|\.exe)?$', '', sys.argv[0]) sys.exit( load_entry_point('desktop', 'console_scripts', 'hue')() ) 3.在hue.ini里配置interpreter123456[[interpreters]] # Define the name and how to connect and execute the language. [[[presto]]] name=Presto JDBC interface=jdbc options=&apos;&#123;&quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;***empty***&quot;, &quot;url&quot;: &quot;jdbc:presto://datanode36.will.com:8099/hive/&quot;, &quot;driver&quot;: &quot;com.facebook.presto.jdbc.PrestoDriver&quot;&#125;&apos; 这里有一个比较坑的地方，就是我们一般不会在presto server的接口做https，甚至不需要用户名密码验证。最开始的时候我配置成了1options=&apos;&#123;&quot;user&quot;: &quot;root&quot;, &quot;password&quot;: &quot;&quot;, &quot;url&quot;: &quot;jdbc:presto://datanode36.will.com:8099/hive/&quot;, &quot;driver&quot;: &quot;com.facebook.presto.jdbc.PrestoDriver&quot;&#125;&apos; 或者1options=&apos;&#123; &quot;url&quot;: &quot;jdbc:presto://datanode36.will.com:8099/hive/&quot;, &quot;driver&quot;: &quot;com.facebook.presto.jdbc.PrestoDriver&quot;&#125;&apos; 就都会报不一样的错误, 看了一下presto jdbc的相关源码：1234567891011121314151617181920... // TODO: fix Tempto to allow empty passwordsString password = PASSWORD.getValue(properties).orElse("");// 如果密码不为空，就走这里if (!password.isEmpty() &amp;&amp; !password.equals("***empty***")) &#123; if (!useSecureConnection) &#123; throw new SQLException("Authentication using username/password requires SSL to be enabled"); &#125; builder.addInterceptor(basicAuth(getUser(), password));&#125;if (useSecureConnection) &#123; setupSsl( builder, SSL_KEY_STORE_PATH.getValue(properties), SSL_KEY_STORE_PASSWORD.getValue(properties), SSL_TRUST_STORE_PATH.getValue(properties), SSL_TRUST_STORE_PASSWORD.getValue(properties));&#125;... 如果密码不为空的话，java.sql.DriverManager会报错，没有提供密码… 还有就是url里一定要执行catalog，不然报错Catalog must be specified when session catalog 4. 改jdbc.py源码最后一个是相对最麻烦的，就是hue的通用的获取数据库元数据的方式，对于presto来说并不适用，需要自己修改hue的python代码。 可以看到下面这个类里的各种获取元数据的sql，其实都是不适用于我们的presto 0.184的，所以需要针对性的修改。 123456789101112131415161718192021222324252627282930313233...class Assist(): def __init__(self, db): self.db = db def get_databases(self): #dbs, description = query_and_fetch(self.db, 'SELECT DatabaseName FROM DBC.Databases') dbs, description = query_and_fetch(self.db, 'show schemas') return [db[0] and db[0].strip() for db in dbs] def get_tables(self, database, table_names=[]): #tables, description = query_and_fetch(self.db, "SELECT * FROM dbc.tables WHERE tablekind = 'T' and databasename='%s'" % database) tables, description = query_and_fetch(self.db, "show tables from %s" % database) return [&#123;"comment": "will ff", "type": "Table", "name": table[0] and table[0].strip()&#125; for table in tables] def get_columns(self, database, table): #columns, description = query_and_fetch(self.db, "SELECT ColumnName, ColumnType, CommentString FROM DBC.Columns WHERE DatabaseName='%s' AND TableName='%s'" % (database, table)) columns, description = query_and_fetch(self.db, "desc %s.%s" % (database, table)) return [[col[0] and col[0].strip(), self._type_converter(col[1]), '', '', col[3], ''] for col in columns] def get_sample_data(self, database, table, column=None): column = column or '*' return query_and_fetch(self.db, 'SELECT %s FROM %s.%s' % (column, database, table)) def _type_converter(self, name): return &#123; "I": "INT_TYPE", "I2": "SMALLINT_TYPE", "CF": "STRING_TYPE", "CV": "CHAR_TYPE", "DA": "DATE_TYPE", &#125;.get(name, 'STRING_TYPE') 当然直接修改这个通用接口类对于架构来说肯定是不好的，甚至是破坏性的。我测试了一下，这个类只对hue不能支持的(也就是mysql、oracle、postgresql、hive等)之外的自定义的jdbc起作用，所以并不会影响到其他数据库和引擎的使用，我们应该也没有需要其他自定义数据源了。所以，暂时这样，后面要TODO优化。 另外，我发现还有一个问题。presto的执行user是固定的，并不像hiveserver那样，执行时使用真正的用户名去hive提交。这样就不能保证ACL的很多特性了。这方面其实可以找到hue相关部分的源码（应该也在jdbc.py）里进行改进。123456789101112131415161718192021222324252627282930class JdbcApi(Api): def __init__(self, user, interpreter=None): global API_CACHE Api.__init__(self, user, interpreter=interpreter) self.db = None self.options = interpreter['options'] if self.cache_key in API_CACHE: self.db = API_CACHE[self.cache_key] elif 'password' in self.options: # 主要在于这个地方，看代码可以发现，实际建立JDBC连接的时候 # 会先去获取option里的user配置项，如果没有这个配置项的话， # 就会使用当前的用户名去创建JDBC连接了。这样也就实现了hiveserver的 # doAs类似的功能，即以真实执行用户的身份执行最终查询 username = self.options.get('user') or user.username #username = self.user print('-------------------got username --------------------------------- %s' % username) self.db = API_CACHE[self.cache_key] = Jdbc(self.options['driver'], self.options['url'], username, self.options['password']) def create_session(self, lang=None, properties=None): ... if self.db is None: if 'password' in properties: user = properties.get('user') or self.options.get('user') props['properties'] = &#123;'user': user&#125; self.db = API_CACHE[self.cache_key] = Jdbc(self.options['driver'], self.options['url'], user, properties.pop('password')) ... return props 从上面的代码分析可得，我们只要在option里不配置user信息，就可以了。。。（结果让我相对崩溃…..） 升级从3.10到4.1，压根儿不知道应该怎样升级数据库里的数据。搜了很多文档都没有找到，最后到github上找到cdh/hue的源码，看到有个commit的comment是关于upgrade的，点进去发现了一篇文档：https://github.com/cloudera/hue/tree/master/dist。里面提到以下三个步骤: 备份原有数据库 ./build/env/bin/hue syncdb ./build/env/bin/hue migrate 其实在第二个步骤的时候，就会进行数据库比对。而第三个数据库则会把所有4.1多余3.10的migrations进行执行。因为hue是基于django的，所以利用了django的migrate的特性。 不知道为什么一直没有搜到这个hue的文档，可能还是关键字有问题吧。 进一步思考其实现在presto是可用了，但是依照规范的话，我们是可以自己依照hive、mysql的数据查询类实现presto查询类的。而且难度也基本是很低的，hue这么长时间都没有presto的兼容，不知道是不是官网别有用心[impala]。 参考 https://github.com/skame/docker-prestogres https://github.com/treasure-data/prestogres https://medium.com/@ilkkaturunen/integrating-presto-with-hue-61702b244839 https://prestodb.io/docs/0.184/installation/jdbc.html https://groups.google.com/forum/#!topic/presto-users/24MMrfLKdu0 https://github.com/cloudera/hue/blob/release-4.1.0/desktop/libs/notebook/src/notebook/connectors/jdbc.py]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto手札]]></title>
    <url>%2F2017%2F12%2F19%2Fpresto%E6%89%8B%E6%9C%AD%2F</url>
    <content type="text"><![CDATA[outputFormat should not be accessed from a null StorageFormat对于hive的一些外部表，presto不能支持，比如基于hbase、或者ES的hive外部表。 1234567presto:default&gt; select table_schema, table_name, column_name, is_nullable, data_type from information_schema.columns;Query 20171219_073016_00015_4zvkw, FAILED, 2 nodesSplits: 17 total, 0 done (0.00%)0:00 [0 rows, 0B] [0 rows/s, 0B/s]Query 20171219_073016_00015_4zvkw failed: outputFormat should not be accessed from a null StorageFormat 所以略过这种特殊表的才能成功：1select table_schema, table_name, column_name, is_nullable, data_type from information_schema.columns where table_name not in ('hive_table_based_on_hbase'); 参考： https://github.com/prestodb/presto/issues/7721]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slider-部署presto实战]]></title>
    <url>%2F2017%2F12%2F15%2Fslider-%E9%83%A8%E7%BD%B2presto%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[首先，使用ambari么有成功，不是特别清楚问题出在哪里，主要是因为对slider还不太熟悉。索性自己手动搞一下slider部署presto，就能相对清楚些，学的也稍微透彻些。 版本构建 slider-0.90.0-incubating(从github上下载了打包，只所以没用现成的，是因为自己打了一个小patch)。注释掉不关注的子项目，比如slider-funtest，注释掉。 找一个跟自己hadoop版本相对应的分支进行打包，我的是2.7，所以就找的0.90.0的tag分支。(有很多人会通过指定hadoop.version的属性来指定依赖的hadoop版本，窃以为有些冒险，因为有可能有依赖，自己的版本高还好些) presto-yarn-1.2.1(这个没有现成的包，只能github上弄下来，自己打包)。这个要加上presto server的版本号，自己想用哪个版本就加哪个就成了。 presto 0.184要求JDK 8U92+, 我安装了8U1521mvn clean package -Dpresto.version=0.184 slider相关配置slider-site.xml1234&lt;property&gt; &lt;name&gt;hadoop.registry.zk.quorum&lt;/name&gt; &lt;value&gt;datanode01.will.com:2181&lt;/value&gt; &lt;/property&gt; slider-env.sh12export JAVA_HOME=/server/java/jdk1.8.0_152/export HADOOP_CONF_DIR=/usr/hdp/current/hadoop-client/conf/ app相关配置把上面构建好的slider和presto-yarn的tar包copy到指定机器上解压。 12cp appConfig-default.json appConfig.jsoncp resources-default.json resources.json 配置appConfig.json这个文件是slider针对不同app的个性配置的文件：123456789101112131415161718192021222324252627282930313233&#123; "schema" : "http://example.org/specification/v2.0.0", "metadata" : &#123; &#125;, "global" : &#123; "site.global.catalog" : "&#123;'hive': ['hive.config.resources=/usr/hdp/2.4.2.0-258/hadoop/etc/hadoop/core-site.xml,/usr/hdp/2.4.2.0-258/hadoop/etc/hadoop/hdfs-site.xml', 'connector.name=hive-hadoop2', 'hive.metastore.uri=thrift://datanode03.will.com:9083'],'tpch': ['connector.name=tpch']&#125;", "java_home" : "/server/java/jdk1.8.0_152/", "zookeeper.quorum" : "datanode01.will.com:2181", "env.MALLOC_ARENA_MAX" : "4", "site.global.config_dir" : "/server/presto/etc", "application.def" : ".slider/package/presto/presto-yarn-package-1.2.1-0.184.zip", "zookeeper.hosts" : "datanode01.will.com", "site.global.app_name" : "presto-server-0.184", "site.global.coordinator_host" : "$&#123;COORDINATOR_HOST&#125;", "zookeeper.path" : "/services/slider/users/yarn/presto-will", "site.global.app_user" : "yarn", "site.global.app_pkg_plugin" : "$&#123;AGENT_WORK_ROOT&#125;/app/definition/package/plugins/", "site.global.user_group" : "hadoop", "site.global.data_dir" : "/server/presto/data", "site.global.presto_query_max_memory" : "8GB", "site.global.jvm_args" : "['-server', '-Xmx1024M', '-XX:+UseG1GC', '-XX:G1HeapRegionSize=32M', '-XX:+UseGCOverheadLimit', '-XX:+ExplicitGCInvokesConcurrent', '-XX:+HeapDumpOnOutOfMemoryError', '-XX:OnOutOfMemoryError=kill -9 %p']", "site.global.presto_query_max_memory_per_node" : "512MB", "site.fs.defaultFS" : "hdfs://dd", "site.global.presto_server_port" : "8099", "site.global.singlenode" : "true", "site.fs.default.name" : "hdfs://dd" &#125;, "credentials" : &#123; &#125;, "components" : &#123; "slider-appmaster" : &#123; "jvm.heapsize" : "128M" &#125; &#125;&#125; 配置resources.json这个文件主要是针对yarn资源申请的。123456789101112131415161718192021222324&#123; "schema": "http://example.org/specification/v2.0.0", "metadata": &#123; &#125;, "global": &#123; "yarn.vcores": "1" &#125;, "components": &#123; "slider-appmaster": &#123; &#125;, "COORDINATOR": &#123; "yarn.role.priority": "1", "yarn.component.instances": "1", "yarn.component.placement.policy": "1", "yarn.memory": "1500" &#125;, "WORKER": &#123; "yarn.role.priority": "2", "yarn.component.instances": "3", "yarn.component.placement.policy": "1", "yarn.memory": "1500" &#125; &#125;&#125; 默认的resource申请中其实是可以为WORKER或者COORDINATOR指定yarn label的节点的，因为我们的集群暂时没有启用这个，测试阶段先不用这特性了。 参考：https://prestodb.io/presto-yarn/installation-yarn-configuration-options.html 文件夹准备 HDFS中要为运行slider应用，也就是presto的用户创建家目录：/user/yarn，而且拥有此目录所有权限。 1hdfs dfs -mkdr /user/yarn &amp;&amp; hdfs dfs -chwon -R yarn /user/yarn 所有可能运行presto节点的机器都要有appConfig.json中配置的presto数据目录，而且拥有此目录所有权限。 1mkdir -p /server/presto/data &amp;&amp; chown -R yarn /server/presto/data 创建package到slider目录下，使用yarn用户创建presto的package1./bin/slider package --install --name presto --package ../presto-yarn-package-1.2.1-0.184.zip 到HDFS中看一下文件：123[root@schedule metamap_django]#hdfs dfs -ls /user/yarn/.slider/package/prestoFound 1 items-rw-r--r-- 3 yarn hdfs 449926122 2017-12-14 11:04 /user/yarn/.slider/package/presto/presto-yarn-package-1.2.1-0.184.zip 创建app集群1./bin/slider create presto-will --template appConfig.json --resources resources.json 这个命令会创建presto-will的集群，并启动之。 可以看到HDFS目录里多了一些内容：1234567891011[root@schedule metamap_django]#hdfs dfs -ls /user/yarn/.slider/cluster/presto-willFound 9 items-rw-r--r-- 3 root hdfs 1740 2017-12-15 15:16 /user/yarn/.slider/cluster/presto-will/app_config.jsondrwxrwxrwx - yarn hdfs 0 2017-12-15 15:16 /user/yarn/.slider/cluster/presto-will/confdirdrwxr-x--- - yarn hdfs 0 2017-12-15 15:16 /user/yarn/.slider/cluster/presto-will/databasedrwxr-x--- - yarn hdfs 0 2017-12-15 14:43 /user/yarn/.slider/cluster/presto-will/generateddrwxr-x--- - yarn hdfs 0 2017-12-15 15:17 /user/yarn/.slider/cluster/presto-will/history-rw-r--r-- 3 yarn hdfs 1436 2017-12-15 14:43 /user/yarn/.slider/cluster/presto-will/internal.json-rw-r--r-- 3 yarn hdfs 651 2017-12-15 14:43 /user/yarn/.slider/cluster/presto-will/resources.jsondrwxr-x--- - yarn hdfs 0 2017-12-15 14:43 /user/yarn/.slider/cluster/presto-will/snapshotdrwxr-xr-x - yarn hdfs 0 2017-12-15 15:16 /user/yarn/.slider/cluster/presto-will/tmp 然后到yarn上观察就可以了，名字为presto-will的application，点击开，观察所有的component的Desired和Actual一致了，就代表启动完成了。下面还会有各个组件所在的nodemanager及其container的细腻些，可以由此定位到log位置与coodinator的web界面位置等。 Hive配置与验证配置主要包括两方面： connector的配置主要是指定hive的thrift位置，以查询hive元数据。具体参考上面的配置文件。 hive的HDFS的配置这个主要是为了解决presto使用hive connector的时候对于HDFS HA的识别，需要在hive connector的配置文件中添加hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml属性。对于yarn-presto，就加在appConfig.json中的hive connector(site.global.catalog)中了。 验证主要就是验证一下Hive Connector的正确性吧。下载一个presto的客户端presto-cli-0.184-executable.jar，重命名为presto，然后给个可执行权限。 12345678910111213141516171819202122[root@datanode21 presto]# ./presto --server datanode29.will.com:8099 --catalog hive --schema defaultpresto:default&gt; show tables;Query 20171215_071741_00000_pxck3 failed: Presto server is still initializingpresto:default&gt; show tables; Table --------------------------------------------------------------------------------------- ast_loan_asset batting batting2 customers h5_test kylin_intermediate_app_active_cube_4aef52f1_11ae_4dce_b548_f5d3a249331c ... ... presto:default&gt; select * from web_logs limit 100; _version_ | app | bytes | city | client_ip | code | country_code | country_code3 | country_name | device_family | extension |---------------------+------------+-------+---------------+---------------+------+--------------+---------------+---------------+---------------+-----------+ 1480895575619534853 | sqoop | 460 | Hyderabad | 49.206.186.56 | NULL | IN | IND | India | Other | | 1480895575619534854 | jobbrowser | 269 | Bangkok | 61.90.20.30 | NULL | TH | THA | Thailand | Other | | ..... ..... 也可以在coordinator的web界面看到相关的查询信息。 参考：https://prestodb.io/docs/current/installation/cli.html 参考 https://prestodb.io/presto-yarn/installation-yarn-configuration-options.html https://prestodb.io/docs/current/installation/cli.html TODO benchmark dirver测试一下性能 开启node label，让presto集群跑在计算能力比较强的node上 探测合理的配置信息(主要是内存方面) 坑slider不能识别HA的HDFS在slider-site.xml中不配置的dfs.defaultFS的时候slider会自己去判断HDFS全路径，代码如下：123public Path getHomeDirectory() &#123; return fileSystem.getHomeDirectory();&#125; 然后生成的启动SliderAppMaster的launcher脚本中就会成为：1234567$JAVA_HOME/bin/java -Djava.net.preferIPv4Stack=true -Djava.awt.headless=true -Xmx128M -ea -esa -Dlog4j.configuration=log4j-server.properties -DLOG_DIR=/yk_data/hadoop/yarn/local/application_1511064572746_35970/container_e51_1511064572746_35970_01_000001 org.apache.slider.server.appmaster.SliderAppMaster create presto-will -cluster-uri hdfs://namenode01.will.com:8020/user/yarn/.slider/cluster/presto-will --rm datanode02.will.com:8030 -D hadoop.registry.zk.root=/registry -D hadoop.registry.zk.quorum=datanode01.will.com:2181 1&gt;/yk_data/hadoop/yarn/local/application_1511064572746_35970/container_e51_1511064572746_35970_01_000001/slider-out.txt 2&gt;/yk_data/hadoop/yarn/local/application_1511064572746_35970/container_e51_1511064572746_35970_01_000001/slider-err.txt 然后SliderAppMaster验证HDFS文件的时候就会报错：1Wrong FS: hdfs://namenode01.will.com:8020/user/yarn, expected: hfds://dd/ 我们的集群配置了HA，集群namespace是dd。 考虑了一下，源头在于slider获取路径不准确，就直接修改了源码，也就是上面提到的patch：123public Path getHomeDirectory() &#123; return new Path("hdfs://dd/user/"+System.getProperty("user.name"));&#125; 其实可以把dd弄成一个slider-client.xml的配置项，暂时图测试方便。 重新build，覆盖既有slider就可以了。 找不到满足条件的node主要是最开始的时候resources.json中带有coordinator和worker的yarn.label.expression选项，而我们的yarn集群没有开启node label功能，当然node就都不满足条件了。 去掉这个配置，或者开启node label功能，并指定一些计算型节点的node为特定label就可以了。 hive配置appConfig.json不生效就是在create prest-will cluster的时候不能生效，每次集群正常启动之后，使用./bin/slider status presto-will都发现相关配置site.global.catalog没有更新，但是其他的配置项又是跟本地的appConfig.json是一致的。 解决方案：通过直接hdfs dfs -get /user/yarn/.slider/cluster/presto-will/app_config.json .，编辑之后再上传，最后重启slider的presto-will app搞定。 具体为什么有待研究。 hive找不到connector Factory开始以为connector.name是可以自己命名的，自己改成hive-will了。官方给的例子是hive-hadoop2，presto-yarn中给的是hive-cdh5,应该也有hdp版本的。虽然我使用的是ambari的hive，属于hdp的，但应该无大碍暂时使用官网的（没有搜到hdp版本的connecor.name对应名字）。 同node多presto worker实例发现yarn上会出现一个问题，就是当flex的worker数比较多的时候，会出现同一个node上出现多个worker的情况，这时候就会出现失败。1234567891011http:// http://datanode02.will.com:8088/proxy/application_1511064572746_40916/classpath:org.apache.slider.client.rest http://datanode02.will.com:8088/proxy/application_1511064572746_40916/classpath:org.apache.slider.management http://datanode02.will.com:8088/proxy/application_1511064572746_40916/ws/v1/slider/mgmtclasspath:org.apache.slider.publisher http://datanode02.will.com:8088/proxy/application_1511064572746_40916/ws/v1/slider/publisherclasspath:org.apache.slider.registry http://datanode02.will.com:8088/proxy/application_1511064572746_40916/ws/v1/slider/registryclasspath:org.apache.slider.publisher.configurations http://datanode02.will.com:8088/proxy/application_1511064572746_40916/ws/v1/slider/publisher/sliderclasspath:org.apache.slider.publisher.exports http://datanode02.will.com:8088/proxy/application_1511064572746_40916/ws/v1/slider/publisher/exportsCOORDINATOR Host(s)/Container(s): [datanode16.will.com/container_e51_1511064572746_40916_01_000009]slider-appmaster Host(s)/Container(s): [datanode23.will.com/container_e51_1511064572746_40916_01_000001]WORKER Host(s)/Container(s): [datanode21.will.com/container_e51_1511064572746_40916_01_000008, datanode13.will.com/container_e51_1511064572746_40916_01_000007, datanode32.will.com/container_e51_1511064572746_40916_01_000010, datanode11.will.com/container_e51_1511064572746_40916_01_000002, datanode29.will.com/container_e51_1511064572746_40916_01_000004, datanode11.will.com/container_e51_1511064572746_40916_01_000003, datanode12.will.com/container_e51_1511064572746_40916_01_000006, datanode20.will.com/container_e51_1511064572746_40916_01_000005] 这里就是datanode11出现重复了，一会儿就变成了别的，因为重试的原因。直到不再重复位置，如果yarn资源池有些问题，比如只有2个node有足够的资源，但是我们需要8个worker，这2个node的总资源又大于8个worker申请的资源，就可能出现这种情况。 首先想到的是slider应该支持有单个node上只能部署app的一种component的一个实例。 failed: Failed to list directory: hdfs://xxxx/premium_record/log_day=20171212hive里可以查询，但是presto查询单表报错。一般是因为presto server不能找到这个目录，或者没有读取这个目录的权限导致。到HDFS上确认了目录确实存在，因为hive里确实能查的到，所以应该是第二个问题。 从上面步骤中也可以知道我们是使用yarn用户启动的presto集群，找到这个目录，使用HDFS dfs命令，让yarn用户能够访问这个目录就可以了。 query.max-memory-per-node set to but only of useable heap available一般是jvm的内存参数太小，而worker的max-memory-per-node又比较大的原因。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto之hive-connector]]></title>
    <url>%2F2017%2F12%2F15%2Fpresto%E4%B9%8Bhive-connector%2F</url>
    <content type="text"><![CDATA[概览hive是由三个部分组成的 特定格式的HDFS文件 metadata数据库，通常是mysql HQL及其执行引擎 presto会用到前面两者。 支持的文件类型ORC、Parquet、Avro、RCFile、SequenceFile、JSON、Text 配置创建etc/catalog/hive.properties文件，挂载hive-hadoop2 connector作为hive catalog，替换掉你的hive metastore的thrift host和port： 12connector.name=hive-hadoop2hive.metastore.uri=thrift://example.net:9083 多个hive集群随便你使用多少catalog，添加其他的hive 集群就是了，新增properties文件到etc/catalog就可以了。 HDFS配置对于基本设置，presto自动配置了HDFS client，不需要额外的配置文件。但是对于hdfs联邦和NN HA的情况，需要额外指定一下参数才能正常访问HDFS cluster。这个要添加hive.config.resources引用到你的hdfs配置文件中：1hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml 官方建议尽量少添加配置项，多余的配置项可能更容易引起问题。 还有就是这些配置文件必须在每个presto节点上都是有效存在的。 HDFS用户在没有整合Kerberos的HDFS中，presto会使用presto进程的运行用户访问HDFS。我们可以通过配置presto的JVM参数指定访问HDFS的用户。1-DHADOOP_USER_NAME=hdfs_user 访问带有kerberos认证的HDFSkerberos认证对于HDFS和hive metastore是都支持的。但是，通过ticket cache进行认证的还没有支持。 hive配置项 Property Name Description Default hive.metastore.uri Hive metastore 的thrift URI. 如果多个的话，默认使用第一个，后面的作为备用，逗号隔开。Example: thrift://192.0.2.3:9083 or thrift://192.0.2.3:9083,thrift://192.0.2.4:9083 hive.config.resources 逗号分隔的HDFS配置文件, 每个presto server所在的节点都要有效存在. Example: /etc/hdfs-site.xml hive.storage-format 创建新表的默认文件格式 RCBINARY hive.compression-codec 写文件的压缩格式 GZIP hive.force-local-scheduling Force splits to be scheduled on the same node as the Hadoop DataNode process serving the split data. This is useful for installations where Presto is collocated with every DataNode. false hive.respect-table-format Should new partitions be written using the existing table format or the default Presto format? true hive.immutable-partitions 新数据能不能insert到已经存在的partitions? false hive.max-partitions-per-writers 每个writer的最大partition数. 100 hive.max-partitions-per-scan 单个table scan可扫描的最大partition数 100,000 hive.metastore.authentication.type Hive metastore认证类型,可是是 NONE 或者 KERBEROS. NONE hive.metastore.service.principal The Kerberos principal of the Hive metastore service. hive.metastore.client.principal The Kerberos principal that Presto will use when connecting to the Hive metastore service. hive.metastore.client.keytab Hive metastore client keytab 位置. hive.hdfs.authentication.type HDFS 认证类型，可以是 NONE 或者 KERBEROS. NONE hive.hdfs.impersonation.enabled 启用 HDFS终端user可以假装. false hive.hdfs.presto.principal The Kerberos principal that Presto will use when connecting to HDFS. hive.hdfs.presto.keytab HDFS client keytab 位置. hive.security 参考Hive Security Configuration. security.config-file Path of config file to use when hive.security=file. See File Based Authorization for details. hive.non-managed-table-writes-enabled Enable writes to non-managed (external) Hive tables. false schema演化hive允许同一个table的不同partition的数据有不同的schema。有的时候字段类型也有变化，Hive connector 也支持。 tinyint, smallint, integer，bigint与varchar 之间的相互转化 real转为double int类型的扩展转换，比如tinyint、smallint。 和hive一样，转化失败就会返回null，比如把’foo’转为int 例子hive connector支持查询与操作hive表和库。然而一些特殊的操作还是需要通过hive执行。 下面创建一个指定位置的hive库：12CREATE SCHEMA hive.webWITH (location = 's3://my-bucket/') 创建一个ORC格式存储的page_views 表，使用date和conuntry进行分区，根据user进行分桶，共计50个桶(注意分区字段必须是最后的字段) 12345678910111213CREATE TABLE hive.web.page_views ( view_time timestamp, user_id bigint, page_url varchar, ds date, country varchar)WITH ( format = 'ORC', partitioned_by = ARRAY['ds', 'country'], bucketed_by = ARRAY['user_id'], bucket_count = 50) 删除分区123DELETE FROM hive.web.page_viewsWHERE ds = DATE '2016-08-09' AND country = 'US' 查询：1SELECT * FROM hive.web.page_views 创建外部表：12345678910CREATE TABLE hive.web.request_logs ( request_time timestamp, url varchar, ip varchar, user_agent varchar)WITH ( format = 'TEXTFILE', external_location = 's3://my-bucket/data/logs/') 删除外部表，只会删除metadata，不会删除真实数据。 1DROP TABLE hive.web.request_logs 不足delete只支持where语句能满足整个分区的情况，也就是说只能一个分区为单位进行delete。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn的节点label]]></title>
    <url>%2F2017%2F12%2F15%2Fyarn%E7%9A%84%E8%8A%82%E7%82%B9label%2F</url>
    <content type="text"><![CDATA[Overviewnodel label是用来把一些共同特征的节点进行归组的手段，然后任务可以指定label要求，运行到指定label的节点上。 现在只支持node partition： 一个node只有一个node partition，所以一个集群被分为多个子集，这些子集互不相交。默认情况下，node是属于DEFAULT partition的。用户可以配置每个partition对于queue中可用多少资源。具体需要看下一节。 有两种不同的node partition： exclusive： container会分配在node label完全匹配的node上。(例如指定了partition=”x”，那么就被分配到partition=“x”的node上，默认的就分配到默认的node上) non-exclusive: 如果一个partition是non-exclusive的，它就可以把空闲资源分配给请求DEFAULT partition的。 用户可以为每个queue指定可以访问的多个node label，app只能使用这个queue能使用的node label的node。 Features Partition cluster - 每个node都可以有一个label，这样cluster就会被划分为包含多个不相交的子集partition. ACL of node-labels on queues - 用户可以指定每个queue可以访问的label的node。 指定一个queue可以访问某个label的比例 - 例如queue A可以使用label=hbase的30%的node resource。 在resource request中指定nodel label的要求, 那么就只会分配这个label属性的节点资源。如果什么都没有指定的话，就默认为DEFAULT partition。 运维相关 nodel label和node label的映射关系可以通过RM restart进行恢复 更新node label - admin可在RM运行态更新node和queue的label Configuration设置ResourceManager启用Node Labels:yarn-site.xml Property Value yarn.node-labels.fs-store.root-dir hdfs://namenode:port/path/to/store/node-labels/ yarn.node-labels.enabled true 注意: 确保yarn.node-labels.fs-store.root-dir已经存在，而且RM有权限访问(一般是yarn用户访问) 如果用户想把node label存储在RM的本地文件系统，就是用file:///home/yarn/node-label 添加/修改 node labels list和node-to-labels mapping to YARN这是两个连续的步骤。 添加Node labels池到cluster中，后面给node添加的label只能是这里面的label，并不是给集群所有节点默认label，这个要注意一下: 执行yarn rmadmin -addToClusterNodeLabels &quot;label_1(exclusive=true/false),label_2(exclusive=true/false) 添加 node label. 如果用户没有指定 “(exclusive=…)”, execlusive默认为true. 运行yarn cluster --list-node-labels，检查新加的node label是否已生效 给node添加label 执行yarn rmadmin -replaceLabelsOnNode “node1[:port]=label1 node2=label2”. 给node1添加label1,给node2添加label2. 如果用户没有指定port, 就给这个node上运行的所有的NodeManagers都添加. 给node labels配置SchedulersCapacity Scheduler 配置 Property Value yarn.scheduler.capacity..capacity Set the percentage of the queue can access to nodes belong to DEFAULT partition. The sum of DEFAULT capacities for direct children under each parent, must be equal to 100. yarn.scheduler.capacity..accessible-node-labels Admin need specify labels can be accessible by each queue, split by comma, like “hbase,storm” means queue can access label hbase and storm. All queues can access to nodes without label, user don’t have to specify that. If user don’t specify this field, it will inherit from its parent. If user want to explicitly specify a queue can only access nodes without labels, just put a space as the value. yarn.scheduler.capacity..accessible-node-labels..capacity Set the percentage of the queue can access to nodes belong to partition . The sum of capacities for direct children under each parent, must be equal to 100. By default, it’s 0. yarn.scheduler.capacity..accessible-node-labels..maximum-capacity Similar to yarn.scheduler.capacity..maximum-capacity, it is for maximum-capacity for labels of each queue. By default, it’s 100. yarn.scheduler.capacity..default-node-label-expression Value like “hbase”, which means: if applications submitted to the queue without specifying node label in their resource requests, it will use “hbase” as default-node-label-expression. By default, this is empty, so application will get containers from nodes without label. node label配置示例: 假设我们的queue是这样的 root / | \ engineer sales marketing We have 5 nodes (hostname=h1..h5) in the cluster, each of them has 24G memory, 24 vcores. 1 among the 5 nodes has GPU (assume it’s h5). So admin added GPU label to h5. 我们集群中有5个node，每个node有24G内存，24个vcore。其中一个node h5有GPU，那么admin想给很h5添加一个GPU的label。 用户就可以这样配置：123456789101112yarn.scheduler.capacity.root.queues=engineering,marketing,salesyarn.scheduler.capacity.root.engineering.capacity=33yarn.scheduler.capacity.root.marketing.capacity=34yarn.scheduler.capacity.root.sales.capacity=33yarn.scheduler.capacity.root.engineering.accessible-node-labels=GPUyarn.scheduler.capacity.root.marketing.accessible-node-labels=GPUyarn.scheduler.capacity.root.engineering.accessible-node-labels.GPU.capacity=50yarn.scheduler.capacity.root.marketing.accessible-node-labels.GPU.capacity=50yarn.scheduler.capacity.root.engineering.default-node-label-expression=GPU 我们设置了root.engineering/marketing/sales.capacity=33, 这样每个队列都保证有33%的资源,也就是 24 4 (1/3) = (32G mem, 32 v-cores). 只有engineering/marketing queue 有权限使用GPU partition (因为 root..accessible-node-labels). engineering/marketing queue 都可以使用partition=GPU的 1/2资源，也就是h5的1/2的资源，也就是24 * 0.5 = (12G mem, 12 v-cores). 注意: 配置完后，要执行yarn rmadmin -refreshQueues来使配置生效, 然后去RM的web UI检查一下。 为APP指定node label可以通过java API指定node label的要求。 ApplicationSubmissionContext.setNodeLabelExpression(..) 设置node label表达式 ResourceRequest.setNodeLabelExpression(..) 设置单独resource request的node label表达式, 这个可以overwrite ApplicationSubmissionContext的表达式设置 在ApplicationSubmissionContext中的setAMContainerResourceRequest.setNodeLabelExpressio用来定位app master container要求的node label 监控通过UI监控web UI提供的指标: Nodes page: http://RM-Address:port/cluster/nodesNode labels page: http://RM-Address:port/cluster/nodelabels,可以获取到类型 (exclusive/non-exclusive), active node managers的数量, 每个partition的总资源Scheduler page: http://RM-Address:port/cluster/scheduler, 每个queue关于label的配置 通过命令行监控 yarn cluster –list-node-labels yarn node -status 其他YARN Capacity Scheduler, if you need more understanding about how to configure Capacity SchedulerWrite YARN application using node labels, you can see following two links as examples: YARN distributed shell, Hadoop MapReduce]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive-on-spark配置]]></title>
    <url>%2F2017%2F12%2F12%2Fhive-on-spark%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[安装sparkhive on spark默认也是支持spark on yarn的模式的。 配置YARN不能使用capacity scheduler，必须使用fair scheduler【….看到这里就想放弃了】 yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler 配置Hive添加spark依赖： hive2.2.0之前，把spark-assembly jar链接到HIVE_HOME/lib中 2.2.0之后，hive on spark需要运行在spark 2.0.0以上的版本，没有assembly jar包 要运行YARN模式的话，添加下面的包到HIVE_HOME/lib scala-library spark-core spark-network-common 运行local模式的话，添加下面包的链接到hive_home/lib中 chill-java chill jackson-module-paranamer jackson-module-scala jersey-container-servlet-core jersey-server json4s-ast kryo-shaded minlog scala-xml spark-launcher spark-network-shuffle spark-unsafe xbean-asm5-shaded 配置hive的执行引擎1set hive.execution.engine=spark; 配置spark相关的参数12345set spark.master=&lt;Spark Master URL&gt;set spark.eventLog.enabled=true;set spark.eventLog.dir=&lt;Spark event log folder (must exist)&gt;set spark.executor.memory=512m; set spark.serializer=org.apache.spark.serializer.KryoSerializer; hive 2.2.0以前，要把spark-assembly jar到HDS中1234&lt;property&gt; &lt;name&gt;spark.yarn.jar&lt;/name&gt; &lt;value&gt;hdfs://xxxx:8020/spark-assembly.jar&lt;/value&gt;&lt;/property&gt; 2.2.0以上版本后, 把$SPARK_HOME/jars中的所有jar都上传到hdfs指定文件夹中：12345&lt;property&gt; &lt;name&gt;spark.yarn.jars&lt;/name&gt; &lt;value&gt;hdfs://xxxx:8020/spark-jars/*&lt;/value&gt;&lt;/property&gt; 配置spark 属性 推荐设置 spark.executor.cores Between 5-7, See tuning details section spark.executor.memory yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores) spark.yarn.executor.memoryOverhead 15-20% of spark.executor.memory spark.executor.instances Depends on spark.executor.memory + spark.yarn.executor.memoryOverhead, see tuning details section. 推荐配置123456789101112131415161718192021222324252627282930313233343536mapreduce.input.fileinputformat.split.maxsize=750000000hive.vectorized.execution.enabled=truehive.cbo.enable=truehive.optimize.reducededuplication.min.reducer=4hive.optimize.reducededuplication=truehive.orc.splits.include.file.footer=falsehive.merge.mapfiles=truehive.merge.sparkfiles=falsehive.merge.smallfiles.avgsize=16000000hive.merge.size.per.task=256000000hive.merge.orcfile.stripe.level=truehive.auto.convert.join=truehive.auto.convert.join.noconditionaltask=truehive.auto.convert.join.noconditionaltask.size=894435328hive.optimize.bucketmapjoin.sortedmerge=falsehive.map.aggr.hash.percentmemory=0.5hive.map.aggr=truehive.optimize.sort.dynamic.partition=falsehive.stats.autogather=truehive.stats.fetch.column.stats=truehive.vectorized.execution.reduce.enabled=falsehive.vectorized.groupby.checkinterval=4096hive.vectorized.groupby.flush.percent=0.1hive.compute.query.using.stats=truehive.limit.pushdown.memory.usage=0.4hive.optimize.index.filter=truehive.exec.reducers.bytes.per.reducer=67108864hive.smbjoin.cache.rows=10000hive.exec.orc.default.stripe.size=67108864hive.fetch.task.conversion=morehive.fetch.task.conversion.threshold=1073741824hive.fetch.task.aggr=falsemapreduce.input.fileinputformat.list-status.num-threads=5spark.kryo.referenceTracking=falsespark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker容器的时间同步]]></title>
    <url>%2F2017%2F12%2F12%2Fdocker%E5%AE%B9%E5%99%A8%E7%9A%84%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%2F</url>
    <content type="text"><![CDATA[最近在测试环境上上了一个新的组件，发现docker容器内的时间是有问题的，就是总比真正的时间慢4分钟左右。然后就exec进去安装了ntpdate工具进行时间同步工作。 然后收到了如下信息：12root@1462539f1dc2:/# ntpdate -u cn.pool.ntp.org12 Dec 15:57:35 ntpdate[502]: step-systime: Operation not permitted 看到是这个操作被禁止了，而且在docker里是用root执行的，竟然被禁止了。所以考虑到应该是docker对于苏初级的保护。返回宿主机，查看时间…果然与docker容器内一致，然后到宿主机安装ntpdate，进行时间同步之后，docker内时间自然就正常了。 回想一下docker原理：Docker本质上是宿主机（Linux）上的进程，通过namespace实现资源隔离，通过cgroups实现资源限制，通过写时复用机制(copy-on-write)实现高效的文件操作。归根到底还是调用的宿主机的东西，并不像虚拟机自己有一套完整的机制。 https://kasheemlew.github.io/2017/05/18/docker-linux/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tez-UI配置]]></title>
    <url>%2F2017%2F12%2F11%2Ftez-UI%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[准备将tez根目录下的war包copy到tomcat的webapps目录中（0.8.5版本有两个ui包,但是没有感觉到有什么区别）1cp ~/apache-tez-0.8.5-bin/tez-ui-0.8.5.war /server/tomcat9/webapps 重启tomcat，war包会自动发布。修改配置文件里的timeline-service和resourcemanager的位置信息后，重启tomcat。12timeline: &quot;http://datanode04.will.com:8188&quot;,rm: &quot;http://datanode02.will.com:8088&quot;, 配置tez-site.xml123456789&lt;property&gt; &lt;name&gt;tez.tez-ui.history-url.base&lt;/name&gt; &lt;value&gt;http://schedule.will.com:8181/tez-ui-0.8.5&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;tez.history.logging.service.class&lt;/name&gt; &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt; &lt;/property&gt; 问题设置yarn-site.xml中timeline-service的cros属性为true。之后重启timeline-service 1yarn.timeline-service.http-cross-origin.enabled = true 其实tez ui就是把所有tez类型的app历史拉出来，按照tez的特性进行定制化的完美展示。 参考： https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=61331897 https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive2-on-tez问题]]></title>
    <url>%2F2017%2F12%2F11%2Fhive2-on-tez%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[hive cli端1234567hive&gt; set hive.execution.engine;hive.execution.engine=tezhive&gt; select count(1) as num, year(create_time) as time from money_record group by year(create_time);Query ID = root_20171211171347_57cea87e-f54a-457d-93cf-b312063a822eTotal jobs = 1Launching Job 1 out of 1FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask yarn app的log信息123456789101112131415161718192021222324252627282930........................# container文件内容7749233 1292 -r-xr-xr-x 1 yarn hadoop 1319613 Mar 8 2017 ./tezlib/apache-tez-0.8.5-bin/tez-dag-0.8.5.jar37749246 8 -r-xr-xr-x 1 yarn hadoop 7734 Mar 8 2017 ./tezlib/apache-tez-0.8.5-bin/tez-yarn-timeline-history-with-acls-0.8.5.jar37749242 144 -r-xr-xr-x 1 yarn hadoop 146572 Mar 8 2017 ./tezlib/apache-tez-0.8.5-bin/tez-tests-0.8.5.jar37749230 4 drwxr-xr-x 2 yarn hadoop 4096 Dec 11 17:13 ./tezlib/apache-tez-0.8.5-bin/share37749247 41916 -r-xr-xr-x 1 yarn hadoop 42921455 Mar 8 2017 ./tezlib/apache-tez-0.8.5-bin/share/tez.tar.gz......# 启动脚本LogType:launch_container.shLog Upload Time:Mon Dec 11 17:13:49 +0800 2017LogLength:5084Log Contents:#!/bin/bash........# 指定类路径export CLASSPATH=&quot;/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure:$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*:$HADOOP_CONF_DIR:&quot;........LogType:stderrLog Upload Time:Mon Dec 11 17:13:50 +0800 2017LogLength:77Log Contents:Error: Could not find or load main class org.apache.tez.dag.app.DAGAppMasterEnd of LogType:stderr 可以看到报错信息是找不到tez的主类，这个一般就是类路径的问题了。 找一个hive 1.2.1的tez的正常的程序，看一下container文件内容：123456789101112131415161718....12322304 164 -r-xr-xr-x 1 yarn hadoop 164267 Apr 25 2016 ./tezlib/tez-runtime-internals-0.7.0.2.4.2.0-258.jar12322432 64 -r-xr-xr-x 1 yarn hadoop 62943 Apr 25 2016 ./tezlib/tez-history-parser-0.7.0.2.4.2.0-258.jar12322283 68 -r-xr-xr-x 1 yarn hadoop 67195 Apr 25 2016 ./tezlib/tez-common-0.7.0.2.4.2.0-258.jar12322430 28 -r-xr-xr-x 1 yarn hadoop 25622 Apr 25 2016 ./tezlib/tez-yarn-timeline-history-0.7.0.2.4.2.0-258.jar12322306 532 -r-xr-xr-x 1 yarn hadoop 542434 Apr 25 2016 ./tezlib/tez-runtime-library-0.7.0.2.4.2.0-258.jar....LogType:launch_container.shLog Upload Time:Mon Dec 11 15:47:42 +0800 2017LogLength:5740Log Contents:#!/bin/bash....export CLASSPATH=&quot;/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure:$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*:$HADOOP_CONF_DIR:&quot;.... 可以看到都是用的默认的$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*, 上面的执行错误的container里的tez文件多一个文件夹路径。 对于tez.lib.uris的注释里也说到：会解压开，然后直接作为classpath。 tez对于classpath配置参考. 我们目前的配置是12345 &lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;/hdp/apps/$&#123;hdp.version&#125;/tez/apache-tez-0.8.5-bin.tar.gz&lt;/value&gt;&lt;/property&gt; 可以看出是这个包的问题，那么就有两个方式解决了。 把apache-tez-0.8.5-bin.tar.gz自己处理一下，把多余的文件夹去掉，重新上传 添加个classpath的前缀，也就是把文件夹名称加上。1234&lt;property&gt; &lt;name&gt;tez.lib.uris.classpath&lt;/name&gt; &lt;value&gt;....&lt;/value&gt;&lt;/property&gt; 看一下tez的目录，发现share目录下有个tez.tar.gz, 这个文件就是专门做这个事情的。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive命令行使用tez问题]]></title>
    <url>%2F2017%2F12%2F11%2Fhive%E5%91%BD%E4%BB%A4%E8%A1%8C%E4%BD%BF%E7%94%A8tez%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[背景 使用ambari2.2.2.0搭建的集群环境 使用了自定义的hive UDF包 问题通过hue在hiveserver2上使用tez进行查询，正常运行。 使用hive cli进行tez查询的时候，报错：12345678910111213141516172017-12-11 15:06:30,745 INFO [main]: ql.Context (Context.java:getMRScratchDir(330)) - New scratch dir is hdfs://dd/tmp/hive/root/c6354673-afc9-4515-8c44-ff7c7f75edcb/hive_2017-12-11_15-06-30_517_4074051706046309823-12017-12-11 15:06:30,750 INFO [main]: exec.Task (TezTask.java:updateSession(270)) - Tez session hasn&apos;t been created yet. Opening session2017-12-11 15:06:30,750 INFO [main]: tez.TezSessionState (TezSessionState.java:open(130)) - Opening the session with id 97240ae8-cce2-4000-83d6-f6729146ebab for thread main log trace id - query id - root_20171211150630_4fbc2d95-5613-438c-9499-126da1ce1f492017-12-11 15:06:30,750 INFO [main]: tez.TezSessionState (TezSessionState.java:open(146)) - User of session id 97240ae8-cce2-4000-83d6-f6729146ebab is root2017-12-11 15:06:30,763 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129745302702017-12-11 15:06:30,766 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129745304082017-12-11 15:06:30,770 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129745304532017-12-11 15:06:30,773 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129745304872017-12-11 15:06:30,775 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129745304082017-12-11 15:06:30,777 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/server/app/hive/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar2017-12-11 15:06:30,780 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(958)) - Looks like another thread is writing the same file will wait.2017-12-11 15:06:30,780 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(965)) - Number of wait attempts: 5. Wait interval: 50002017-12-11 15:06:55,790 ERROR [main]: tez.DagUtils (DagUtils.java:localizeResource(981)) - Could not find the jar that was being uploaded2017-12-11 15:06:55,790 ERROR [main]: exec.Task (TezTask.java:execute(212)) - Failed to execute tez graph.java.io.IOException: Previous writer likely failed to write hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar. Failing because I am unlikely to write too. at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982) at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862) 问题定位上传udf到hdfs中tez session目录的时候，出现错误。 排查过程一遍遍地检查HDFS上的目录，发现尽管报错，也已经有了这个UDF jar文件了。123456[root@servicenode02 livy]# hdfs dfs -ls /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebabFound 4 items-rw-r--r-- 3 root hdfs 258861 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/hive-hcatalog-core.jar-rw-r--r-- 3 root hdfs 3809 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_hive_udf.jar-rw-r--r-- 3 root hdfs 83977 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_json_serde_1.3.8.jar-rw-r--r-- 3 root hdfs 24630 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar 来来回回看了八百遍，到源码里也看了一下123456789101112131415161718192021222324252627282930313233343536try &#123; // 尝试将本地文件copy到远程的目录中 destFS.copyFromLocalFile(false, false, src, dest);&#125; catch (IOException e) &#123; // 如果copy失败，一般就是文件已经存在了，或者有其他人在进行同样的操作 LOG.info("Looks like another thread is writing the same file will wait."); int waitAttempts = conf.getInt(HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.varname, HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.defaultIntVal); long sleepInterval = HiveConf.getTimeVar( conf, HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_WAIT_INTERVAL, TimeUnit.MILLISECONDS); LOG.info("Number of wait attempts: " + waitAttempts + ". Wait interval: " + sleepInterval); // 隔一段时间检查一下，这个文件是不是已经有了 boolean found = false; for (int i = 0; i &lt; waitAttempts; i++) &#123; if (!checkPreExisting(src, dest, conf)) &#123; try &#123; Thread.sleep(sleepInterval); &#125; catch (InterruptedException interruptedException) &#123; throw new IOException(interruptedException); &#125; &#125; else &#123; found = true; break; &#125; &#125; // 如果一直都么有，就报上面的错误了 if (!found) &#123; LOG.error("Could not find the jar that was being uploaded"); throw new IOException("Previous writer likely failed to write " + dest + ". Failing because I am unlikely to write too."); &#125;&#125; 对于checkPreExisting方法，也要看一下：12345678910111213// 检查目标目录下是不是已经有了同名文件，// 如果文件已经有了，那么要检查文件是否一致，这里简单用len验证private boolean checkPreExisting(Path src, Path dest, Configuration conf) throws IOException &#123; FileSystem destFS = dest.getFileSystem(conf); FileSystem sourceFS = src.getFileSystem(conf); FileStatus destStatus = FileUtils.getFileStatusOrNull(destFS, dest); if (destStatus != null) &#123; // 我挂到了这一步，就是udf的版本不一致 return (sourceFS.getFileStatus(src).getLen() == destStatus.getLen()); &#125; return false;&#125; 好了。那么看一下我的问题。为了使用udf，我们的数据研发在hive cli的初始化文件bin/.hiverc中添加了下面的命令：12345678910111213141516171819add jar /server/app/hive/will_hive_udf.jar;add jar /usr/hdp/current/hive-client/auxlib/tinyv_hive_udf.jar;create temporary function strstart as &apos;com.will.common.hive.StrStart&apos;; create temporary function strDateFormat as &apos;com.will.common.hive.StrDateFormat&apos;;create temporary function strContain as &apos;com.will.common.hive.StrContain&apos;; create temporary function parse_uri as &apos;com.will.common.hive.ParseUri&apos;;create temporary function strEnd as &apos;com.will.common.hive.StrEnd&apos;;create temporary function split_by_index as &apos;com.will.common.hive.SplitByIndex&apos;;create temporary function birthday as &apos;com.will.common.hive.BirthdayByIdcard&apos;;create temporary function gender as &apos;com.will.common.hive.GenderByIdcard&apos;; create temporary function usernamesen as &apos;com.will.common.hive.UserNameSen&apos;;create temporary function strTrim as &apos;com.will.common.hive.StrTrim&apos;;create temporary function channel as &apos;com.will.common.hive.Channel&apos;;create temporary function iptonum as &apos;com.will.common.hive.IpToNumber&apos;;create temporary function geturl as &apos;com.will.common.hive.ParseUrls&apos;;create temporary function money_record_type as &apos;com.will.common.hive.MoneyRecordType&apos;;create temporary function xv_encode as &apos;com.will.tinyv.Encode&apos;;create temporary function xv_decode as &apos;com.will.tinyv.Decode&apos;;set hive.mapred.mode=nostrict; 重新启动hive cli，报错的日志，发现上传了来自两个不同地方的will_hive_udf.jar文件：一个来自hive/auxlib目录，另一个来自/server/app…..而只有第一次报错的时候才能看到第一个udf的上传过程，因为同一个session中，已经上传过的，验证没问题就跳过了。。。。只打印了一个时间戳，也没有指明是哪个文件…..导致定位问题的时候，有些恍惚，还以为是tez与UDF功能之间的问题12345678910111213141516171819202122232425262728293031323334352017-12-11 15:09:30,604 INFO [main]: ql.Context (Context.java:getMRScratchDir(330)) - New scratch dir is hdfs://dd/tmp/hive/root/c6354673-afc9-4515-8c44-ff7c7f75edcb/hive_2017-12-11_15-09-30_406_4902171368286077085-12017-12-11 15:09:30,608 INFO [main]: exec.Task (TezTask.java:updateSession(270)) - Tez session hasn&apos;t been created yet. Opening session2017-12-11 15:09:30,608 INFO [main]: tez.TezSessionState (TezSessionState.java:open(130)) - Opening the session with id 97240ae8-cce2-4000-83d6-f6729146ebab for thread main log trace id - query id - root_20171211150930_d928c00e-62da-4c0e-95c2-23a28594a7c02017-12-11 15:09:30,608 INFO [main]: tez.TezSessionState (TezSessionState.java:open(146)) - User of session id 97240ae8-cce2-4000-83d6-f6729146ebab is root2017-12-11 15:09:30,613 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/hive-hcatalog-core.jar2017-12-11 15:09:30,655 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129761706672017-12-11 15:09:30,656 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_hive_udf.jar2017-12-11 15:09:30,683 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129761706912017-12-11 15:09:30,684 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_json_serde_1.3.8.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_json_serde_1.3.8.jar2017-12-11 15:09:30,707 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129761707202017-12-11 15:09:30,709 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2.0-258/hive/auxlib/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar2017-12-11 15:09:30,729 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129761707422017-12-11 15:09:30,732 INFO [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 15129761706912017-12-11 15:09:30,733 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/server/app/hive/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar2017-12-11 15:09:30,735 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(958)) - Looks like another thread is writing the same file will wait.2017-12-11 15:09:30,736 INFO [main]: tez.DagUtils (DagUtils.java:localizeResource(965)) - Number of wait attempts: 5. Wait interval: 50002017-12-11 15:09:55,745 ERROR [main]: tez.DagUtils (DagUtils.java:localizeResource(981)) - Could not find the jar that was being uploaded2017-12-11 15:09:55,745 ERROR [main]: exec.Task (TezTask.java:execute(212)) - Failed to execute tez graph.java.io.IOException: Previous writer likely failed to write hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar. Failing because I am unlikely to write too. at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982) at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862) at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFilesFromConf(DagUtils.java:805) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.refreshLocalResourcesFromConf(TezSessionState.java:233) at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:158) at org.apache.hadoop.hive.ql.exec.tez.TezTask.updateSession(TezTask.java:271) at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:151) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89) 总结原因 udf版本不一致，导致后面jar包上传失败 解决把.hiverc中的命令暂时去掉，重试tez引擎。就能成功调用了。 其他思考为什么已经设定了hive.aux.jars.path了，还要在.hiverc额外处理呢。 先在hivecli中确认一下：123456[root@schedule ~]#hive -e &quot;set hive.aux.jars.path&quot;WARNING: Use &quot;yarn jar&quot; to launch YARN applications.Logging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.propertiesPutting the global hiverc in $HIVE_HOME/bin/.hiverc is deprecated. Please use $HIVE_CONF_DIR/.hiverc instead.hive.aux.jars.path=file:///usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_hive_udf.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_json_serde_1.3.8.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/will_hive_udf.jar 但是测试了一下，没有.hiverc里的初始化，就不能执行udf函数。12hive&gt; select iptonum(&apos;10.2.19.32&apos;);FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function &apos;iptonum&apos; 可是hue里，也就是hiveserver2里却可以执行udf函数。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[canal同步的TimeStamp问题]]></title>
    <url>%2F2017%2F12%2F11%2Fcanal%E5%90%8C%E6%AD%A5%E7%9A%84TimeStamp%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[部门接了新的实时方面的需求，选定使用canal进行mysql binlog方面的处理。 上线一段时间后都比较稳定，但是有一天，实时数据开发的同事找到我说有个别字段是有问题的。大概表结构如下：1234567CREATE TABLE `bb` ( `id` int(11) NOT NULL AUTO_INCREMENT, `lastUpdateTime` timestamp NULL DEFAULT CURRENT_TIMESTAMP, `firstTimeBindTime` datetime DEFAULT NULL, `name` varchar(255) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=MyISAM AUTO_INCREMENT=1526640 DEFAULT CHARSET=utf8 有问题的字段为lastUpdateTime，主库为2017-12-11 11:22:40的时候，canal client中消费到的却是2017-12-10 22:22:40，而且所有这个字段都是相差同样的13个小时。但是有一个地方比较值得注意，就是firstTimeBindTime的值都是正常的。 排查流程 查看canal server的系统时区，进行修正确认 12cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime date # 发现时间已经正确了 重启canal server 发现还是有问题，去mysql主机上翻看binlog，找到对应的记录。1mysqlbinlog -vv /server/mysql_data/mysql-bin.000141 发现这个字段对应的时间戳是正确的1512962560。那么问题看来还是在canal server这块了。 源码走读顺便看下源码吧。对于binlog进行解析的主类：MysqlEventParser.start() 12345...// event处理与解析CanalEntry.Entry entry = parseAndProfilingIfNecessary(event);... 进入parseAndProfilingIfNecessary方法 1234...// 使用parser解析CanalEntry.Entry event = binlogParser.parse(bod);... 找到对应的parser类为LogEventConvert，可以看到LogEventConvert是继承了BinlogParser的。12345protected BinlogParser buildParser() &#123; LogEventConvert convert = new LogEventConvert(); ...... return convert;&#125; 再去LogEventConvert的parse方法12345...// 对于修改数据的event的处理case LogEvent.WRITE_ROWS_EVENT: return parseRowsEvent((WriteRowsLogEvent) logEvent);... 继续找12345678910111213141516171819202122while (buffer.nextOneRow(columns)) &#123; // 处理row记录 RowData.Builder rowDataBuilder = RowData.newBuilder(); if (EventType.INSERT == eventType) &#123; // insert的记录放在before字段中 tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, true, tableMeta); &#125; else if (EventType.DELETE == eventType) &#123; // delete的记录放在before字段中 tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, false, tableMeta); &#125; else &#123; // update需要处理before/after tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, false, tableMeta); if (!buffer.nextOneRow(changeColumns)) &#123; rowChangeBuider.addRowDatas(rowDataBuilder.build()); break; &#125; tableError |= parseOneRow(rowDataBuilder, event, buffer, changeColumns, true, tableMeta); &#125; rowChangeBuider.addRowDatas(rowDataBuilder.build());&#125; 对于行数据的每个字段的处理，是先获取到value，然后再针对地做一些额外处理。123....final Serializable value = buffer.getValue();... 通过RowsLogBuffer进行value的获取，我们找到给value赋值的地方fetchValue1234567891011121314151617final Serializable fetchValue(int type, final int meta, boolean isBinary) &#123; ..... switch (type) &#123; case LogEvent.MYSQL_TYPE_TIMESTAMP: &#123; final long i32 = buffer.getUint32(); if (i32 == 0) &#123; value = "0000-00-00 00:00:00"; &#125; else &#123; String v = new Timestamp(i32 * 1000).toString(); value = v.substring(0, v.length() - 2); &#125; javaType = Types.TIMESTAMP; length = 4; break; &#125; .....&#125; 把这部分弄出去，使用上面binlog文件中的timestamp单独测试. 123456789101112131415161718192021import java.sql.Timestamp;/** * Created by root on 17-12-11. */public class Will &#123; public static void main(String[] p )&#123; String value = ""; final long i32 = 1512962560l; System.out.println(TimeZone.getDefault()); System.out.println(i32); if (i32 == 0) &#123; value = "0000-00-00 00:00:00"; &#125; else &#123; String v = new Timestamp(i32 * 1000).toString(); value = v.substring(0, v.length() - 2); &#125; System.out.println(value); &#125;&#125; 在测试机是正常的, 但是在线上环境是不正常的，最后在java里输出了一下Timezone.getDefault()，发现线上是”America/New_York”，正好就是差13个小时的。可是在系统里运行date命令，返回的是上海时区。上网搜了一下，有人建议使用下面命令确定时区12[root@etl01 ~]# ls -l /etc/localtimelrwxrwxrwx. 1 root root 38 Oct 21 00:25 /etc/localtime -&gt; ../usr/share/zoneinfo/America/New_York 这里看到虽然我前面使用cp已经覆盖了/etc/localtime文件，而且并没有报错。但是查看的时候，还是显示为到纽约的链接。 删掉这个文件，重新cp。12345678[root@etl01 ~]# rm -vf /etc/localtimeremoved \u2018/etc/localtime\u2019[root@etl01 ~]# cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime [root@etl01 ~]# ls -l /etc/localtime-rw-r--r--. 1 root root 405 Dec 11 13:14 /etc/localtime[root@etl01 ~]# java Willsun.util.calendar.ZoneInfo[id=&quot;America/New_York&quot;,offset=-18000000,dstSavings=3600000,useDaylight=true,transitions=235,lastRule=java.util.SimpleTimeZone[id=America/New_York,offset=-18000000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]America/New_York 还是不对，无奈，再次删掉。按照系统的样子，创建软链，成功。12345678[root@etl01 ~]# rm /etc/localtime rm: remove regular file \u2018/etc/localtime\u2019? y[root@etl01 ~]# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime[root@etl01 ~]# ls -l /etc/localtimelrwxrwxrwx. 1 root root 33 Dec 11 13:15 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai[root@etl01 ~]# java Willsun.util.calendar.ZoneInfo[id=&quot;Asia/Shanghai&quot;,offset=28800000,dstSavings=0,useDaylight=false,transitions=19,lastRule=null]Asia/Shanghai CentOS Linux release 7.0.1406 (Core)系统……….比较无语]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive-llap启动]]></title>
    <url>%2F2017%2F12%2F07%2Fhive-llap%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[前提 tez slider 配置conf/hive-env.sh12345# 到tez的配置目录里找tez-site.xmlexport TEZ_HOME=/usr/hdp/current/tez-clientexport TEZ_CONF_DIR=$TEZ_HOME/confHADOOP_HOME=/usr/hdp/current/hadoop-client/ hive-site.xml12345678&lt;property&gt; &lt;name&gt;hive.execution.mode&lt;/name&gt; &lt;value&gt;llap&lt;/value&gt; &lt;description&gt; Expects one of [container, llap]. Chooses whether query fragments will run in container or in llap &lt;/description&gt; &lt;/property&gt; tez-site.xml1234&lt;property&gt; &lt;name&gt;tez.lib.uris&lt;/name&gt; &lt;value&gt;/hdp/apps/$&#123;hdp.version&#125;/tez/apache-tez-0.8.5-bin.tar.gz&lt;/value&gt; &lt;/property&gt; slider查看12345[root@servicenode04 apache-hive-2.3.2-bin]# slider list2017-12-07 18:10:01,421 [main] INFO impl.TimelineClientImpl - Timeline service address: http://datanode04.will.com:8188/ws/v1/timeline/2017-12-07 18:10:02,387 [main] WARN shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.2017-12-07 18:10:02,403 [main] INFO client.RMProxy - Connecting to ResourceManager at datanode02.will.com/10.2.19.83:8050llap_service RUNNING application_1511064572746_27005 http://datanode02.will.com:8088/proxy/application_1511064572746_27005/ 报错container大小12345678910[root@servicenode04 apache-hive-2.3.2-bin]# ./bin/hive --service llap --instances 5 llapINFO cli.LlapServiceDriver: LLAP service driver invoked with arguments=-directoryINFO conf.HiveConf: Found configuration file file:/server/apache-hive-2.3.2-bin/conf/hive-site.xmlFailed: Container size (-1B) should be greater than minimum allocation(2.00GB)java.lang.IllegalArgumentException: Container size (-1B) should be greater than minimum allocation(2.00GB) at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92) at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.run(LlapServiceDriver.java:313) at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.main(LlapServiceDriver.java:116)INFO cli.LlapServiceDriver: LLAP service driver finished 提示是container大小还没有最小的大。加个参数就行了1./bin/hive --service llap --size 2147483648 --instances 3 配置漏项生成slider项目目录之后，启动llap失败。到yarn的日志里翻看，发现异常：1yarn logs --applicationId application_1511064572746_25101 |less 12345017-12-06T11:56:39,771 WARN [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exceptionjava.lang.NullPointerException at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.&lt;init&gt;(LlapDaemon.java:139) ~[hive-llap-server-2.3.2.jar:2.3.2] at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.main(LlapDaemon.java:521) [hive-llap-server-2.3.2.jar:2.3.2]End of LogType:llap-daemon-root-datanode02.will.com.log 找到对应版本的源码里，发现是少了配置项String hosts = HiveConf.getTrimmedVar(daemonConf, ConfVars.LLAP_DAEMON_SERVICE_HOSTS);,也就是hive.llap.daemon.service.hosts，应该配置为@llap_service. tez版本问题重新生产slider项目，再次，运行，还是异常退出了，继续看yarn的log。12345678910111213ue2017-12-07T17:12:20,582 WARN [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exceptionjava.lang.NoClassDefFoundError: org/apache/tez/hadoop/shim/HadoopShimsLoader at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.&lt;init&gt;(ContainerRunnerImpl.java:157) ~[hive-llap-server-2.3.2.jar:2.3.2] at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.&lt;init&gt;(LlapDaemon.java:283) ~[hive-llap-server-2.3.2.jar:2.3.2] at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.main(LlapDaemon.java:521) [hive-llap-server-2.3.2.jar:2.3.2]Caused by: java.lang.ClassNotFoundException: org.apache.tez.hadoop.shim.HadoopShimsLoader at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_60] at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_60] at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_60] at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_60] ... 3 moreEnd of LogType:llap-daemon-root-datanode05.will.com.log 查了一下，这个类是tez的。我当前使用的是0.7版本的，到github上试了一下，这个版本还没有这个类。那就自己下载个有这个类的tez，上传到hdfs，重新修改tez-site.xml，指定下位置。 log4j版本问题？123456789Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory at org.apache.hadoop.service.AbstractService.&lt;clinit&gt;(AbstractService.java:43)Caused by: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 1 moreEnd of LogType:llap-daemon-root-datanode11.will.com.out 这次是hadoop的AbstractService初始化的时候，找不到commons-logging的类了…好累 看这行日志上面输出了类路径之类的东西：12345678910+ exec /server/java/jdk1.8.0_60/bin/java -Dproc_llapdaemon -Xms4096m -Xmx4096m -XX:+UseG1GC -XX:+ResizeTLAB -XX:+UseNUMA -XX:-ResizePLAB -server -Djava.net.preferIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+PrintGCDetails -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=4 -XX:GCLogFileSize=100M -XX:+PrintGCDateStamps -Xloggc:/server/hadoop/yarn/log/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006//gc.log -Djava.io.tmpdir=/server/hadoop/yarn/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/tmp/ -Dlog4j.configurationFile=llap-daemon-log4j2.properties -Dllap.daemon.log.dir=/server/hadoop/yarn/log/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/ -Dllap.daemon.log.file=llap-daemon-root-datanode02.will.com.log -Dllap.daemon.root.logger=query-routing -Dllap.daemon.log.level=INFO -classpath &apos;/server/hadoop/yarn/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//conf/:/server/hadoop/yarn/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//lib/*:/server/hadoop/yarn/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//lib/tez/*:/server/hadoop/yarn/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//lib/udfs/*:.:&apos; 明显发现这个classpath是不包含hadoop相关的类的。一番查找，找到hive根目录下的scripts/llap/bin/runLlapDaemon.sh文件，添加一下：1CLASSPATH=$&#123;LLAP_DAEMON_CONF_DIR&#125;:$&#123;LLAP_DAEMON_HOME&#125;/lib/*:$&#123;LLAP_DAEMON_HOME&#125;/lib/tez/*:$&#123;LLAP_DAEMON_HOME&#125;/lib/udfs/*:.`hadoop classpath` 再重新生成slider项目。 参考： http://housong.github.io/2017/hive-llap/ http://blog.csdn.net/qingzhenli/article/details/72723018]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[开源年会]]></title>
    <url>%2F2017%2F12%2F06%2F%E5%BC%80%E6%BA%90%E5%B9%B4%E4%BC%9A%2F</url>
    <content type="text"><![CDATA[霍泰稳 本土社区运营的思考运营 社区开始需要灵魂人物，后面不应该需要，而应该是品牌效应 开始的时候，要频繁出去跑，与各种专家交往，成为交际花。当品牌已经有了的时候，就要重点突出品牌，而不是个人了。 架构 一线人员参与内容，因为内容专业性很强。使用架构组织内容的生产，总编辑 -&gt; 6个专题首席编辑(java, ruby，每个专题有一个首席编辑，负责组建对应主题的编辑运营团队) -&gt; 若干社区编辑。 付费 不要被人当成公益组织，不带道德光环。费用是按照翻译字数/写作字数，或者专家约稿收取。多多少少有些补贴。 大会门票理所当然也不会是免费的。 这样才能让人们知道自己的支持是有价值、有价格的。 开源和免费完全没关系 工具 raven。自动展示infoQ的英文站文章，然后社区编辑过来领取任务，进行翻译。有些众包平台的意思，以前都是专人整理Excel，每天发送一次，统计一次。 wiki。 stuQ. 流程 经验 运营公司的心态去做社区，与时俱进调整价值观 青出于蓝，找到可以帮助自己的黑马 坚持创新 kylin 成长之路teradata太昂贵，数据都放在HDFS，原来需要将HDFS的数据先load到teardata才能查询。调研之后，就决定自己做。luke负责这个东西，找到蒋旭(了解SQL分析与支持，了解Hadoop)。13年开始做，14年10月1日，ebay内部上了生产环境，同时放到github上进行开源，很快收获了很多的好的评价。 14年11月成为apache孵化器项目，接受了很多mentor的指导，代码迁移到apache代码库，成员加入apache，各种license的处理与剥离。解决大量用户的各种问题，快速升级。获得了国外知名技术社区的关注，并给以奖项。 kylin与apache建立私人关系，开始在国内美团、京东、bat等开始铺展，进一步推动孵化器向顶级项目转化。apache社区的人有人说：中国人喜欢用QQ群等工具尽享项目讨论，而不符合apache社区的邮件讨论的风格。 也有人投反对票，前端有google的不明确license前端字体。随后，kylin快速处理，发布新的release，以供审核。 后来成立了kylingence，专门搞kylin。开源版知识产权属于apache社区。 kylin2.0主要变化是支持雪花模型，支持spark进行构建工作，2.1支持查询下压(下压给hive或者spark sql进行查询)。 核心代码人员有5个，活跃的commiter不到10个，contributter有70个左右。 对用户友好，好用。kylin最开始就很完善，包括前端、后端、性能 构建生态。合适地利用其他技术，不要重复造轮子 轻量级构建。对管理员友好，易于扩展、容灾等 倾听社区用户。解决用户痛点，才能得民心 贡献到kylin 加入社区讨论，知道大家在做什么，难点在哪儿 贡献patch，或者github提PR，量大了就直接提为committer carbonData保证代码质量：充分的review，大牛多不能保证质量 把apache 文档研究了一个月，把项目关键路径规划了一下 去美国找了基金会的导师，这个比较重要 有人会说你的代码跟我们的代码相似，就懵了….就跟他说受了你的鼓舞~~~~然后成了导师 apache way 只要有人用起来，就会被鞭子抽着跑 apache weex使用HTML + css描述app结构、样式，逻辑写在script中。可以远程加载成为native 界面。 http://weex.apache.org/cn/ 内部事业部推销 外部邀请使用者参与进来 跟内部apache项目团队取经 开源的目的： 加大曝光率，增强内部竞争优势：阿里内部有很多类似weex的东西 国际化 去掉商业背景，业界共享 问题： 目前committer只有7个，而且都是阿里的。近一年内，只release了一次。 原因： 团队业务属性比较重，这种开发方式与apache这种社区的方式有些区别，JIRA上基本都是内部业务的，并没有跟apache社区的结合同步起来。文档更新不及时、版本划分不明晰等。 单元测试质量与覆盖率 RocketMQ所有成功的开源项目，背后都是一个商业化公司在运营。 社区：reader、user，还有writer，负责宣传工具的好处。 有很多同质的项目，比如kafka。 社区要大、要建生态、定规范(方便同质产品的互操作)、商业化解决方案。 捐献项目(宽进严出) 找导师，3位以上，其中一个是主席(activeMQ的相关人)，找对了champion，就成功了一半 找mentor 找proposer，项目的介绍，项目的承诺：不能孤立，人员多元化 apache协议 可以用或者改我的软件，告诉我改了哪儿？ 在license中说明 扩展协议要与apache协议兼容 用我的软件，出问题我不负责 apache way（价值主张） 社区大于产品 writer，就是社区运营 社区中每个人都是个体，不受任何人雇佣，没有任何上下级关系 社区的表彰是基于对社区的共享，持续关注并提交PR，就会被邀请成为commiter，然后负责更多的事情，一直到成为PMC 流程 apache license的扫描等 单元测试必须在3分钟之内做完。 集成测试，maven PR的模板 简单告诉改动了什么 怎样验证此次改动 阿里类docker工具: pouch隔离性docker容器内free -g，其实是宿主机的内存，但是java应用的内存分配是根据这个自动分配的，所以会有些问题。 基于磁盘和网络的隔离与限制 仓库的p2p分发降低各种客户端同时对registry进行pull或push等。P2P就是类迅雷的东西了，每个客户端也是一个服务端 #####]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slider对于app的要求]]></title>
    <url>%2F2017%2F12%2F05%2Fslider%E5%AF%B9%E4%BA%8Eapp%E7%9A%84%E8%A6%81%E6%B1%82%2F</url>
    <content type="text"><![CDATA[slider把app安装并运行在yarn上，但是app并不一定要是针对yarn定制的。 app应该是被slider发布，也就是yarn负责安装，slider负责配置，最后yarn负责执行。yarn会在销毁container的时候kill掉执行的进程，所以被部署的app应该知道这个，并能够在没有人工参与的前提下自动启动一个新的container运行自己的component。 app的component还应该能够自己动态发现其他的component，在启动和执行的时候都要能发现，因为后面server或者进程失败的话，也会导致component位置变化。 must 使用tar进行安装运行，而且运行用户不能是root 自包含或者所有的依赖都已经预装了 支持节点的动态发现，比如通过ZK 节点能够动态rebind自己，就是说如果即便节点被删除，app仍能继续运行 把kill作为通用机制处理 支持多个实例运行在同个集群中，不同app实例可以共享同一个server 当多个role实例被分配到同一个物理节点的时候，要能够正确操作。 安全可靠。yarn不会再sandbox中运行代码的 如果与HDFS交互，注意兼容性 持久化数据到HDFS，配置好文件位置。slider中每个app实例都需要有一个目录。 配置目录要可配置，最好不要是绝对路径 log输出不要是一个固定的位置 不明确让它停止，就一直跑下去。slider吧app的终止作为失败处理，会自动重启container。 must not 人工的启动和关闭 should 发布一个RPC或者HTTP端口，可以通过zk或者admin API。slider会提供一个发布机制 可通过标准的hadoop机制进行配置：text、xml文件。不然，就需要自定义的解析器 支持明确的参数来指定配置目录 允许通过-D类似方式指定运行参数 不要运行复杂脚本 提供给slider获取集群节点和状态的方式，这样slider才能探测失败的worker节点并处理 启动的时候能够感知位置。 支持简单的探活，可以是http get 退出的时候返回一个可读的内容，方便针对性处理 支持集群大小调整 支持ambari类似的平台管理 may 动态分配RPC或者web端口 包含一个单独的进程，运行在指定位置，这个进程一挂，就引起app终止。这样的进程也能运行在slider am的container中。如果一个活着的cluster不能处理这个进程的restart或者migration，这个slider app就不能处理slider am的restart 汇报load/cost may not yarn可写 纯java 支持动态配置]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slider上手]]></title>
    <url>%2F2017%2F12%2F05%2Fslider%E4%B8%8A%E6%89%8B%2F</url>
    <content type="text"><![CDATA[前提 hadoop2.6+ HDFS,YARN,ZKK JDK1.7 python 2.6 openssl 配置集群配置hadoop集群。 注意：debug设置为非0的能进行debug。如果使用一个vm或者一个sandbox，那么可以修改yarn配置，允许多个container在同一个host上。在yarn-site.xml中修改下面的配置 例子12345678&lt;property&gt; &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;/name&gt; &lt;value&gt;256&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.delete.debug-delay-sec&lt;/name&gt; &lt;value&gt;3600&lt;/value&gt;&lt;/property&gt; 下载slider配置slider进入目录slider-0.80.0-incubating/conf后，编辑slider-env.sh文件：12export JAVA_HOME=/usr/jdk64/jdk1.7.0_67export HADOOP_CONF_DIR=/etc/hadoop/conf 如果运行在一个没有安装hadoop的节点上，只需要把相关配置放到相应目录就可以了。也可以通过slider-clietn.xml配置hadoop配置路径：1234&lt;property&gt; &lt;name&gt;HADOOP_CONF_DIR&lt;/name&gt; &lt;value&gt;/etc/hadoop/conf&lt;/value&gt;&lt;/property&gt; 或者1234&lt;property&gt; &lt;name&gt;HADOOP_CONF_DIR&lt;/name&gt; &lt;value&gt;$&#123;SLIDER_CONF_DIR&#125;/../hadoop-conf&lt;/value&gt;&lt;/property&gt; 对于一个没有nn HA和 RM HA的集群，修改slider-client.xml配置如下：12345678910111213141516&lt;property&gt; &lt;name&gt;hadoop.registry.zk.quorum&lt;/name&gt; &lt;value&gt;yourZooKeeperHost:port&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;yourResourceManagerHost:8050&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;yourResourceManagerHost:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://yourNameNodeHost:8020&lt;/value&gt; &lt;/property&gt; 执行命令123$&#123;slider-install-dir&#125;/slider-0.80.0-incubating/bin/slider versionpython %slider-install-dir%/slider-0.80.0-incubating/bin/slider.py version 保证没有错误输出，那么slider就已经正确安装了。 发布slider resource确保所有的文件目录都可以被app实例的创建者使用，我们这里使用yarn作为app创建者。 确保HDFS home存在12345su hdfshdfs dfs -mkdir /user/yarnhdfs dfs -chown yarn:hdfs /user/yarn 创建app包有几个简单的例子： app-packages/memcached-win app-packages/hbase app-packages/accumulo app-packages/storm 根据各个里面的README，创建一个或多个slider app。 安装，配置，启动，验证 安装1slider install-package --name *package name* --package *sample-application-package* 安装包会被发布到HDFS的/.slider/package/。 创建。分两部分，一个是resource specification，另一个是app configuration resource specification。slider需要知道要部署多少component，需要多少CPU，内存。这些信息放在resources.json中。 application configuration。应用的配置信息，例如jvm堆大小等 启动。通过cli启动的话 12cd $&#123;slider-install-dir&#125;/slider-0.80.0-incubating/bin./slider create cl1 --template appConfig.json --resources resources.json 验证。到yarn里，打开appmaster，看到slider app master 获取client配置一个app发布几个用户的细节信息，用来管理app实例。 可以使用registry命令获取这些数据。 发布的数据。在app master的/ws/v1/slider/publisher可以看到，通过slider-client status app1 client配置。/ws/v1/slider/publisher/slider/ log位置。ws/v1/slider/publisher/slider/logfolders 一些监控UI, jmx终端等。/ws/v1/slider/publisher/slider/quicklinks app生命周期管理123./slider start/stop cl1./slider destroy./slider flex cl1 --component worker 5]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[slider架构概览]]></title>
    <url>%2F2017%2F12%2F05%2Fslider%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[概述slider是一个YARN application，用来部署非YARN的app到YARN集群中。 slider包含一个YARN application master，Slider AM，然后client application需要通过RPC或者HTTP与YARN和Slider AM进行通信。client application提供了命令行与low-level API，以供测试。 被部署的app应该是可以运行在YARN管理的server池中的程序，还要能动态定位到它自己的peer。slider并不负责配置这些peer server，只负责一些app指定的实例的配置初始化工作。 每个app实例都是一个或多个component，每个component可以有不同的程序或命令，不同的配置项或参数。 AM负责启动哪些具体的role，为每个component请求一个YARN container。然后监控app实例的状态，当一个远程执行的进程完成之后，YARN会通知到AM。然后AM就去部署这个component的下一个实例去了。 slider打包slider一个重要的目标就是支持已经存在的APP部署到yarn上去 AM架构AM包含： AM engine，负责处理所有的外部服务的整合，尤其是YARN和其他的slider client provider，指定部署app的class app的状态 app状态是app实例的模型，包含： app实例一些期望的状态，比如每种component的实例数量，他们的YARN container内存等 当前实例在yarn集群中每种component的数据，包括每个node上的可用资源 role history。记录每个node都被部署过哪种component，后面可以还这样部署。这是为了在需要读写本地磁盘的时候不出问题。 追踪消息队列：请求、发布、启动节点 app engine整合了所有外部的东西：YARN RM， 指定node的NM，接受来自RM的service，request，释放container的event，在分配的container上启动app。 集群中发生任何变化，都会发送通知，然后app engine把通知传递给App的Status类，Status类更新它的状态，返回一系列集群操作(请求不同类型的container， 可能会指定节点，或者请求释放container)，供提交。 有了这些之后，再加上分配消息，app engine就可以把app State分配给指定的组件了，然后出发provider构建app的启动context。 provider有带有用于启动provider支持的程序的文件关联、环境变量、命令，用以发布container。 core provider在目标container上部署一个minimal 的agent，然后这个agent会计入agent provider的REST API， 执行它的命令。 这个agnet要执行的命令主要是从HDFS下载归档文件，解压开，运行python脚本来执行真正的配置，最后就是目标的执行了。这里面会用到很多模板。 总结一下：slider不是一个典型的YARN analysis app，YARN analysis app是指在短期、中期生命的container中分配和调度工作，带有一个query或者analysis session的一个生命周期，slider的生命周期是长达几天或者几个月的。slider要是app集群保持某个状态，app也应该能在node失败后进行恢复，还有就是定位到自己的peer node，再有就是与HDFS文件系统的数据进行交互。 Samza是第一个被设计运行在yarn上的app，它作为一个平台或者long-live 服务存在。这些app对yarn的需求是不一样的，他们的application master的设计主要集中在维护分布式app在一个稳定的状态，而不是提交什么作业。 MVC的切分实现了mode与aid mock测试的隔离性，增强了slider能够在YARN上部署更大规模的信息。 失败模型app master被设计为一个 crash-only application, client是随时可以直接请求YARN来终止app实例的。 有一个RPC方法可以停掉app实例。这个是挺好的，会记录一条message在日志里，以后可能还会给provider警告一下这个app实例要被关掉了。这也有一定潜在的危险，以你为provider实现里可能开始期望这个方法被可靠调用。slider的设计是失败不会告警，而是基于配置进行重新构建，可以被人工停止。 RPC 接口RPC接口允许client查询当前app的状态，也能通过json请求进行更新。 主要操作有： getJSONClusterStatus(): 获取app实例的json状态输出 flexCluster(): 更新调整不同component的数量 stopCluster 还有一些其他更low-level的操作供我们进行诊断、测试，但是比较有限。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在yarn上安装presto]]></title>
    <url>%2F2017%2F12%2F04%2F%E5%9C%A8yarn%E4%B8%8A%E5%AE%89%E8%A3%85presto%2F</url>
    <content type="text"><![CDATA[前置条件 hdp 2.2+ 或者CDH5.4+ slider 0.80.0+ jdk 1.8 zookeeper openssl &gt;= 1.0.1e-16 ambari 2.1 presto安装目录结构使用ambari slider view安装基于yarn的presto集群的时候，安装目录与标准的是不一样的。 如果使用slider脚本或者ambari slider view来部署presto到yarn上的话，presto是会使用presto server的tar包的形式进行安装的(不是通过rpm)。当yarn app启动后，可发现presto server安装在yarn上的nodemanager的yarn.nodemanager.local-dirs。例如，配置yarn.nodemanager.local-dirs为/mnt/hadoop/nm-local-dirs，且app_user配置为yarn，那么就安装在/mnt/hadoop-hdfs/nm-local-dir/usercache/yarn/appcache/application_&lt;id&gt;/container_&lt;id&gt;/app/install/presto-server-&lt;version&gt;。container_id之前的部分在slider中叫做AGENT_WORK_ROOT，也就是说AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;。 通常，使用tar安装的presto，catalog、plugin、lib目录等都在presto-server主目录下。catalog目录在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/etc/catalog, plugin和lib目录在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/plugin和AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/lib.启动脚本在AGENT_WORK_ROOT/app/install/presto-server-&lt;version&gt;/bin. presto日志是基于数据目录的配置的。 参考：https://prestodb.io/presto-yarn/installation-yarn-directory-structure.html presto配置安装过程中，ambari slider view允许你进行配置。 如果使用mabri进行安装，可以通过UI配置，如果是手动安装的话，就自己编辑配置文件。 主要配置文件为appConfig.json和resources-[singlenode|multinode].json，需要在运行presto之前配置好。在下面提出的位置有样例配置：1presto-yarn-package/src/main/resources presto-yarn-package/src/main/resources/appConfig.json和presto-yarn-package/src/main/resources/resources-multinode.json是对应的默认配置。 appConfig.json site.global.app_user。 默认是yarn，启动presto的用户。确认app_user要有一个HDFS home目录。如果要访问hive的话，也要确认这个app_user有相应的权限 site.global.user_group。默认是hadoop site.global.data_dir。默认是/var/lib/presto/data，presto的数据目录，应该在启动之前就存在，并且是属于app_user，否则slider就会因权限问题不能启动了。 12mkdir -p /var/lib/presto/datachown -R yarn:hadoop /var/lib/presto/data site.global.config_dir,默认是/var/lib/presto/etc。presto配置文件所在的目录，包含node.properties, jvm.config, config.properties以及connector配置文件等。这些文件会从模板presto-yarn-package/package/templates/*.j2和相关appConfig.json的参数生成。 site.global.singlenode。默认true，当前node既做为coordinator也作为worker。 site.global.presto_query_max_memory。在config.properties文件中是query.max_memroy，默认50G. site.global.presto_query_max_memory_per_node，默认1G site.global.presto_server_port，默认8080 site.global.catalog.默认是tpch connector。这个是用来配置presto的connector的。应该对应于非基于yarn的presto集群的connector.properites。格式一般为：{‘connector1’ : [‘key1=value1’, ‘key2=value2’..], ‘connector2’ : [‘key1=value1’, ‘key2=value2’..]..}.。这个会创建connector1.properties, connector2.properties两个配置文件，带有不同的entry。看一个hive.properties的例子1&quot;site.global.catalog&quot;: &quot;&#123;&apos;hive&apos;: [&apos;connector.name=hive-cdh5&apos;, &apos;hive.metastore.uri=thrift://$&#123;NN_HOST&#125;:9083&apos;], &apos;tpch&apos;: [&apos;connector.name=tpch&apos;]&#125;&quot; 这里的NN_HOST运行时会被替换成NN的地址，如果hive metastore跟NN没在一起，需要自己修改一下。 site.global.jvm_args。这个是生成presto的jvm.properties文件的，默认是heapsize是1G. 1&quot;site.global.jvm_args&quot;: &quot;[&apos;-server&apos;, &apos;-Xmx1024M&apos;, &apos;-XX:+UseG1GC&apos;, &apos;-XX:G1HeapRegionSize=32M&apos;, &apos;-XX:+UseGCOverheadLimit&apos;, &apos;-XX:+ExplicitGCInvokesConcurrent&apos;, &apos;-XX:+HeapDumpOnOutOfMemoryError&apos;, &apos;-XX:OnOutOfMemoryError=kill -9 %p&apos;]&quot;, site.global.log_properties. 配置presto的日志级别默认是[‘com.facebook.presto=INFO’]。应该是一行一个表达式。例子 1&quot;site.global.log_properties&quot;: &quot;[&apos;com.facebook.presto.hive=WARN&apos;, &apos;com.facebook.presto.server=INFO&apos;]&quot; site.global.additional_node_properties和site.global.additional_config_properties。 site.global.plugin 1&quot;site.global.plugin&quot;: &quot;&#123;&apos;ml&apos;: [&apos;presto-ml-$&#123;presto.version&#125;.jar&apos;]&#125;&quot;, site.global.app_name application.def. 对于slider用户，安装presto的命令运行的时候，日志会打印出这个参数。如果不是自定义的presto包，不必理会这个参数。 java_home ， 默认是/usr/lib/jvm/java appConfig.json里的类似${COORDINATOR_HOST}, ${AGENT_WORK_ROOT}的变量是在运行时确认的。 resources.json这个配置可以对应于全局，也可以针对每个组件 yarn.vcores: 默认是全局的，-1 yarn.component.instances，默认coordinator是-1，worker是3.多节点模式下的例子配置中presto-yarn-package/src/main/resources/resources-multinode.json是1个coordinator，3个worker。他们的分布有着较严格的策略，每个node上只会运行一个实例。当节点数不够申请的worker时，这个app就会失败。如果都用作presot节点的话，那么worker的数量应该是集群中的nodemanager数量 -1 ，留一个作为coordinator。 yarn.memory。默认1500M，是site.global.jvm_args的-Xmx参数，被presto的jvm所使用的。slider推荐要比这个大一些。yarn.memory应该比任何jvm申请的堆都要大一丢丢，推荐最少大50%。 yarn.label.expression. coordinator或者worker。 参考：https://prestodb.io/presto-yarn/installation-yarn-configuration-options.html 使用ambari slider view 安装基于yarn的presto集群 安装ambari server 下载slider包 把presto包copy到ambari server的/var/lib/ambari-server/resources/apps/路径下 重启ambari-server 登陆ambari 安装基础组件：HDFS, YARN, Zk， Slider等 保证slider-env.sh里已经指定了JAVA_HOME和HADOOP_CONF_DIR。 对于zk，如果不是安装在/usr/lib/zookeeper: 在slider配置里添加zk.home配置变量 如果不是2181端口，添加slider.zookeeper.quorum配置 启动服务，到ambari创建slider view，然后创建app 提供presto服务的相关配置细节，UI会提供一些默认参数。 app名字应该是小写的：presto1 配置 准备slider的HDFS目录，这个根据global.app_user来的。 修改global.presto_server_port为8080之外的其他端口，以免跟ambari srever冲突 预创建所有节点上的数据目录var/lib/presto，可以自己修改 其他自定义配置 完成。这个动作相当于在bin/slider脚本执行package –install和create，然后就可以看到presto已经在yarn上运行起来了。 从slider view监控运行状态 Quick Links，观察yarn UI 如果job失败，可以到对应节点上找日志看 可以在View界面管理app的生命周期(start, stop, fliex, destory)。 手动使用slider安装presto集群]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tez笔记]]></title>
    <url>%2F2017%2F11%2F27%2Ftez%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[部署hadoop2.7以上版本 构建tezmvn clean package -DskipTests=true -Dmaven.javadoc.skip=true JDK8, MAVEN 3 protocol buffer 2.5.0 如果使用单元测试，把skipTests去掉就行 如果使用eclipse，可以使用import maven project引入。 把对应的tez包copy到HDFS，配置tez-site.xml tez包包含tez和hadoop的类库，tez-dist/tez-x.y.z-SNAPSHOT.tar.gz 假设tez放在了HDFS的/apps下，命令如下 12hadoop fs -mkdir /apps/tez-x.y.z-SNAPSHOThadoop fs -copyFromLocal tez-dist/target/tez-x.y.z-SNAPSHOT.tar.gz /apps/tez-x.y.z-SNAPSHOT tez-site.xml配置 设置tez.lib.uris指定HDFS上的tar.gz的位置。假设是上面的话，就设置成${fs.defaultFS}/apps/tez-x.y.z-SNAPSHOT/tez-x.y.z-SNAPSHOT.tar.gz 确认tez.cluster.hadoop-libs没有在tez-site.xml中设置，这个值应该是false 注意tar包版本应该与用来提交tez job的客户端版本一致。 可选的：如果在tez上运行已经存在的MR任务，修改mapred-site.xml，修改mapreduce.framework.name, 从yarn修改为yarn-tez. 配置client节点，把tez类库加入到hadoop类库中。 抽取tez最小的tar包到本地目录 设置TEZ_CONF_DIR为tez-site.xml的位置 添加$TEZ_CONF_DIR，${TEZ_JARS}/和${TEZ_JARS}/lib/到app的classpath。例如通过标准hadoop工具链设置的话： 1export HADOOP_CLASSPATH=$&#123;TEZ_CONF_DIR&#125;:$&#123;TEZ_JARS&#125;/*:$&#123;TEZ_JARS&#125;/lib/* 注意 *是必须的 下面是一个提交MR任务的例子：1$HADOOP_PREFIX/bin/hadoop jar tez-examples.jar orderedwordcount &lt;input&gt; &lt;output&gt; 这个会使用tez dag ApplicationMaster来运行wordcount job。这个wordcount相比简单的，多了个按照词频顺序输出。 Tez DAG可以分别运行在不同的app上，使用同一个TEZ session就可以了。tez-tests中有一个odrderedwordcount是支持session的使用的，同时处理多个input-output pairs。可以在不同的input/output上连续运行多个DAG。 1$HADOOP_PREFIX/bin/hadoop jar tez-tests.jar testorderedwordcount &lt;input1&gt; &lt;output1&gt; &lt;input2&gt; &lt;output2&gt; &lt;input3&gt; &lt;output3&gt; ... 上面的例子就会为每个input-output pair运行多个DAG了。 要使用tez session的话，设置 -DUSE_TEZ_SESSION=true1$HADOOP_PREFIX/bin/hadoop jar tez-tests.jar testorderedwordcount -DUSE_TEZ_SESSION=true &lt;input1&gt; &lt;output1&gt; &lt;input2&gt; &lt;output2&gt; 像平时一样提交一个MR任务1$HADOOP_PREFIX/bin/hadoop jar hadoop-mapreduce-client-jobclient-3.0.0-SNAPSHOT-tests.jar sleep -mt 1 -rt 1 -m 1 -r 1 这会使用TEZ DAG ApplicationMaster来运行MR job。可以通过查看YARN UI上的AM的log来验证一下。记住，mapred-site.xml里的mapreduce.framework.name需要设置成yarn-tez。 指定tez.lib.uris的几种方式tez.lib.uris属性支持逗号分隔的多个值，可以是单个文件，一个目录，压缩包(tar,zip等)。 对于文件和目录，tez会把第一层的文件放到tez运行时的工作目录中，然后放入classpath。对于压缩文件，会被解压到工作目录中。 hadoop依赖安装上面的使用tez的方式，也就是预装在hadoop类库中是我们推荐的方式。带有所有依赖的完整的tar包是一个更好的方式，可以保证已经存在的job在集群回滚或者升级的时候继续正常运行。 尽管tez.lib.uris配置项有很广泛的使用模型，但是还有两个主要的可选模式: A： 在hadoop类库可用的集群上使用tez tar包 B： 与hadoop tar包一起使用tez tar包 这两个模式需要一个没有hadoop依赖而编译地的tez，可以是tez-dist/target/tez-x.y.z-minimal.tar.gz。 对于模式A：通过yarn.application.classpath使用集群已有的hadoop类库对于使用rolling upgrade的集群不推荐这种方式。另外，用户需要负责保证tez版本与正在运行集群的hadoop的兼容性。对于上面的第三个步骤，也需要修改。后续的步骤应该使用tez-dist/target/tez-x.y.z-minimal.tar.gz而不是tez-dist/target/tez-x.y.z.tar.gz。 如果tez jar已经放在了HDFS的/apps里，那么minimal的tez就可以运行了 12 &quot;hadoop fs -mkdir /apps/tez-x.y.z&quot;&quot;hadoop fs -copyFromLocal tez-dist/target/tez-x.y.z-minimal.tar.gz /apps/tez-x.y.z&quot; tez-site.xml配置 设置tez.lib.uris为hdfs包含tez jar的位置。 设置tez.use.cluster.hadoop-libs为true 对于模式B：带有hadoo tar包的tez tar包这个模式是支持rolling upgrade的。但是用户需要确认自己选择的tez和hadoop版本兼容。也需要修改第三步： 假设tez的压缩包在HDFS的/apps下 12hadoop fs -mkdir /apps/tez-x.y.z hadoop fs -copyFromLocal tez-dist/target/tez-x.y.z-minimal.tar.gz /apps/tez-x.y.z 或者，可以把minimal目录直接放到HDFS，然后再把每个jar包放进去。 1hadoop fs -copyFromLocal tez-dist/target/tez-x.y.z-minimal/* /apps/tez-x.y.z 构建完hadoop之后，hadoop tar包在hadoop/hadoop-dist/target/hadoop-x.y.z-SNAPSHOT.tar.gz 假设hadoop jar包放在了HDFS上的/apps里 12hadoop fs -mkdir /apps/hadoop-x.y.zhadoop fs -copyFromLocal hadoop-dist/target/hadoop-x.y.z-SNAPSHOT.tar.gz /apps/hadoop-x.y.z tez-site.xml的配置 tez.lib.uris只想tez/hadoop需要的jar或者归档文件所在位置 例子：当时用tez和hadoop归档文件时，设置tez.lib.uris为${fs.defaultFS}/apps/tez-x.y.z/tez-x.y.z-minimal.tar.gz#tez,${fs.defaultFS}/apps/hadoop-x.y.z/hadoop-x.y.z-SNAPSHOT.tar.gz#hadoop-mapreduce 例子：当时用带有hadoop归档文件的tezjar的时候，设置tez.lib.uris为${fs.defaultFS}/apps/tez-x.y.z,${fs.defaultFS}/apps/tez-x.y.z/lib,${fs.defaultFS}/apps/hadoop-x.y.z/hadoop-x.y.z-SNAPSHOT.tar.gz#hadoop-mapreduce 在tez.lib.uris中，跟在#后面的文档会自动创建对应的fragment 链接。如果没有给出fragment，那么链接就被设置为归档的名字。fragment不应该是目录或者jar 如果在tez.lib.uris中指定了任何归档，就也要设置tez.lib.uris.classpath，定义好这些归档文件的classpath，因为归档文件结构是未知的。 例子：当使用tez和hadoop归档时，设置tez.lib.uris.classpath： 1./tez/*:./tez/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/common/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/common/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/hdfs/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/hdfs/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/yarn/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/yarn/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/mapreduce/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/mapreduce/lib/* 例子：当使用tez jar和hadoop归档文件时，设置tez.lib.uris.classpath为： 1./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/common/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/common/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/hdfs/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/hdfs/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/yarn/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/yarn/lib/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/mapreduce/*:./hadoop-mapreduce/hadoop-x.y.z-SNAPSHOT/share/hadoop/mapreduce/lib/*]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive2-LLAP]]></title>
    <url>%2F2017%2F11%2F27%2Fhive2-LLAP%2F</url>
    <content type="text"><![CDATA[Live Long And Process。 概览最近几年，有了tez和CBO等机制，hive的运行速度与特性已经有了很大提升。这次2.0又把hive带入了一个新的level 异步spindle-aware的IO 预抽取column chunk，缓存column chunk 多线程JIT友好的operator pipeline 上面的特性就是LLAP提供的了，LLAP提供了一个混合引擎模型。它包含了一个长期存在的daemon，替代直接与HDFS的Datanode交互。还有一个与DAG紧密结合的framework。对于缓存、字段pre-fetching、query的执行与ACL都被移动到了daemon中。small/short的query大部分都会被送到这个daemon直接处理，其他稍微重一些的操作都会被YARN的container运行。 跟DataNode类似，LLAP deamon也可以被其他的app使用，尤其是基于文件处理的关系型数据view。这个daemon也提供了可选的API(例如InputFormat)，供其它数据处理框架作为一个构建的block。 最后，细粒度的字段级别的acl在这个model中也得到了完美的呈现。 下图显示了LLAP的一个执行样例。Tez AM负责整体的编排工作。query的初始化stage被push给了LLAP。reduce stage，大的shuffle操作是在几个container中执行的。多个查询和app可以并发访问LLAP。 持久化daemon为了进一步优化缓存和JIT，还有能够更好的估算startup耗时等，集群中的worker节点上都会有一个daemon。这个daemon处理IO,缓存，query分片的执行。 所有的node都是无状态的。任何发送给LLAP节点的request都包含数据的位置和元信息。可以是本地的位置，也可以是远程的，数据本地化是调用者需要考虑的事情(YARN) recovery/resilency。失败和恢复都很简单，因为任何数据家电都可以用来处理输入数据的任何分片。Tez AM只需要简单的重新运行失败的分片即可。 节点间沟通。LLAP节点之间可以共享数据(例如抽取partition数据，广播数据分片等)。实现方式与Tez中一样的。 执行引擎LLAP和已经存在的，基于过程的hive引擎是可以兼容的，保留了hive的可扩展性和广泛性。它并没有取代已经存在的执行模型，而是对其进行增强。 daemon是可选的。在没有这些daemon的时候，hive仍然是可运行的。 外部编排和执行引擎。LLAP并不是一个像Tez或者MR的执行引擎。所有的执行都是被已经存在的hive执行引擎（比如tez）在LLAP节点上进行调度与监控的，跟普通的container一样。显然，LLAP级别的支持是基于每个执行引擎(目前是tez)的。MR的支持并没有计划，但是其他的引擎有可能会在后面也加入进来。其他的框架，例如pig，也可以选择使用LLAP daemon。 部分执行。LLAP daemon的执行结果可以说某个hive query的结果的一部分，也可以传递给外部的hive task，这个取决于具体的query 资源管理。YARN仍然负责资源的管理与分发。yarn container delegation被用来允许分配资源给LLAP。为了避免jvm内存设置的初始化，缓存的数据是放在off-heap的，大的buffer也是(比如groupby，join等操作)。通过这种方案，daemon可以只是用很小的内存，其他资源(CPU,内存等)会根据负载进行分发。 query fragment执行LLAP节点会执行一些query分片，比如filter，projection，数据转化，部分聚合，排序，分桶，hash join/semi-join等。在LLAP中只接受hive代码和udf。没有任何代码是可以在运行时生成和执行的。这是出于稳定性和安全性的考虑。 并发执行。一个LLAP节点允许不同query和session 的多个查询分片的并发执行。 interface。用户可以通过client API直接访问LLAP节点。他们可以指定关系转化，然后通过面向record的stream进行数据读取。 IOdaemon摆脱了压缩格式处理的IO，这些工作交给其他独立的线程。数据在就绪的时候会被传递给execution，就是说前一批数据正在被处理的同时，就可以准备下一批了。数据是以简单的RLE编码的列式格式传递给execution的，主要是为了后面方便进行vectorized processiong【向量化处理？】。这也是缓存的格式，可以减小IO，缓存，execution之间的copy流量。 多种文件格式。IO和缓存依赖于已经存在的文件格式(越高效越好)。因此，和vectorization工作类似，不同的文件格式都会通过插件形式被支持(目前是ORC)。另外，一个通用的，不那么搞笑的插件也可以加进来支持任意的hive输入格式。这些插件必须负责持有元信息，并且把原始数据转化到column chunk。 预测和bloom filter。SARGs和bloom filter是会被push down到存储层的。 缓存daemon会缓存输入文件和数据的元数据。即便数据还没有缓存，元数据和索引信息也是可以先缓存起来的。元数据是以java object的形式存在进程的，缓存的数据是根据IO部分的指定形式，保存在off-heap的。 驱逐策略【eviction policy】。取出策略是为了调优负载，应对频繁的表扫面的。初始阶段，是使用了LRFU的简单策略。这个策略是可插拔的。 缓存粒度。column-chunk是缓存数据的基本单位。这是在低开销的处理与高效存储之间的一个权衡。chunk的粒度取决于文件格式和执行引擎了。(Vectorized Row Batch Size, ORC stripe等) bloomfilter会自动创建，以提供动态运行时过滤。 workload管理YARN用来获取不同workload的资源。某个workload一旦获取到yarn分配的资源之后，这个执行引擎就可以选择把资源代理给LLAP，或者在单独的进程里启动hive executor。通过YARN的资源管理保证了节点不会过载运行，不管是因为LLAP或者其他container。daemon本身也是在YARN的控制之下的。 ACID支持LLAP是事务感知的。在数据放入缓存之前，将delta文件合并，可以生成一个table的特定的state。如果有多版本，可以在请求中指定使用哪个版本。这样的好处是可以异步merge，而且只用cache一次数据，从而避免了operator pipeline的攻击。 安全LLAP server是一个很自然的就可以进行acl的地方，比per-file的控制粒度还要细。因为LLAP的daemon知道当前要处理的column和record，所有针对这俩东西的策略都可以在这里实施。这并不会替代已有的安全机制，但是会有所增强，也可以提供给其他框架使用。 监控LLAP监控的配置是存储在resources.json， appConfig.json， metainfo.xml，都会存在于slider的templates.py中。 LLAP monitor daemon也运行在YARN container中，跟LLAP Daemon一样，而且默认监听同样的端口。 LLAP Metric Collection Server从所有的LLAP Daemon周期性地收集JMX metric LLAP daemon列表是在启动集群的zookeeper中抽取的。 web service json jmx数据 - /jmx jvm stack trace - /stacks LLAP daemon的xml配置 - /conf LLAP status - /status LLAP Peers - /peers 在slider上部署LLAP可以通过slider进行部署，绕过了节点安装与相关复杂处理 LLAP statusambari相关介绍了LLAP ap的状态，在hiveserver2中可用了。 例子1234567891011121314/current/hive-server2-hive2/bin/hive --service llapstatus --name &#123;llap_app_name&#125; [-f] [-w] [-i] [-t]-f,--findAppTimeout &lt;findAppTimeout&gt; Amount of time(s) that the tool will sleep to wait for the YARN application to start. negative values=wait forever, 0=Do not wait. default=20s-H,--help Print help information --hiveconf &lt;property=value&gt; Use value for given property. Overridden by explicit parameters-i,--refreshInterval &lt;refreshInterval&gt; Amount of time in seconds to wait until subsequent status checks in watch mode. Valid only for watch mode. (Default 1s)-n,--name &lt;name&gt; LLAP cluster name-o,--outputFile &lt;outputFile&gt; File to which output should be written (Default stdout)-r,--runningNodesThreshold &lt;runningNodesThreshold&gt; When watch mode is enabled (-w), wait until the specified threshold of nodes are running (Default 1.0 which means 100% nodes are running)-t,--watchTimeout &lt;watchTimeout&gt; Exit watch mode if the desired state is not attained until the specified timeout. (Default 300s)-w,--watch Watch mode waits until all LLAP daemons are running or subset of the nodes are running (threshold can be specified via -r option) (Default wait until all nodes are running)]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive2---轻松更新hive表2]]></title>
    <url>%2F2017%2F11%2F22%2Fhive2---%E8%BD%BB%E6%9D%BE%E6%9B%B4%E6%96%B0hive%E8%A1%A82%2F</url>
    <content type="text"><![CDATA[前面讲了使用MERGE,UPDATE,DELETE更新hive数据。现在我们进一步谈一下hive管理slowly-changing dimensions(SCDs，就是缓慢更新的维度表)的策略。在数据仓库中，SCDs更新数据是无规律的。对应于不同的业务需求，有不同的策略。假如你要一个用户维度表的所有历史，以跟踪某个用户随时间的变化情况。还有些情况，我们只关心最新的维度状态 。 下面是三种SCD更新策略： 使用新数据覆盖旧数据。很简单，如果只是要同步最新状态的话，就用这个，但是会丢失历史维度值。 添加带有version的新数据行。可以追踪到所有历史。但是随着时间的退役，可能会特别大，还有就是查询的时候需要只看最新版本的维度值。 添加新数据行，管理有限版本的历史。有一些历史数据，然是控制在一定范围内。 这个blog是讲一下怎样使用hive的MERGE来管理SCD。所有的例子都可以在这里找到。管理SCD是很麻烦的事情，所以最好能够用一些工作，比如数仓工具 SCD管理策略一览 基础所有的是例子都是从一个外部表，copy到hive的managed table，这个managed table就是merge target。第二个外部表，代表第二次从某个系统全量dump出来的数据。这两个外部表是一样的csv文件，包含字段:ID,Name, Email,State。初始化的数据有1000条，第二次数据有1100条，其中包含100个新纪录和93个需要更新项。 第一种策略有就直接替换，没有就添加。 12345678910merge into contacts_targetusing contacts_update_stage as stageon stage.id = contacts_target.idwhen matched then update set name = stage.name, email = stage.email, state = stage.statewhen not matched then insert values (stage.id, stage.name, stage.email, stage.state); 值得注意的是，上面的操作也是单独一个，是原子且独立的，如果出现错误会正确rollback。在SQL-on-Hasdoop方式里提供这些特性是很困难的，但是hive的MERGE操作就实现了。 第二种策略保留所有的历史版本，提供单独的版本相关字段:ValidFrom, ValidTo。 我们可以使用这个策略来满足并发用户对于正在更新的数据的数据读取。 1234567891011121314151617181920212223242526merge into contacts_targetusing ( — The base staging data. selectcontacts_update_stage.id as join_key,contacts_update_stage.* from contacts_update_stage union all— Generate an extra row for changed records. — The null join_key forces records down the insert path. select null, contacts_update_stage.* from contacts_update_stage join contacts_target on contacts_update_stage.id = contacts_target.id where ( contacts_update_stage.email &lt;&gt; contacts_target.email or contacts_update_stage.state &lt;&gt; contacts_target.state ) and contacts_target.valid_to is null) subon sub.join_key = contacts_target.idwhen matched and sub.email &lt;&gt; contacts_target.email or sub.state &lt;&gt; contacts_target.state then update set valid_to = current_date()when not matched then insert values (sub.id, sub.name, sub.email, sub.state, current_date(), null); 需要注意的是，using语句中对于每个更新的row会输出2个record。这些record会有一个null join key(就会成为一个insert了), 还会有一个valid jonk key(这是一个update)。如果都过去i安眠文章的话，其实有类似于在分区之间移动数据，只不过是使用update而不是delete。 看下93条记录的情况。 第三种类型第二种类型其实挺强大的了，不过比较复杂，而且维度表会无限增长下去。第三种策略中维度表基本跟数据源大小差不多，但是只提供部分历史。 下面我们就只保存上一个版本的纬度值 当update的时候，我们任务就是把当前的版本放到last的值里。 1234567891011121314merge into contacts_targetusing contacts_update_stage as stageon stage.id = contacts_target.idwhen matched and contacts_target.email &lt;&gt; stage.email or contacts_target.state &lt;&gt; stage.state — change detection then update set last_email = contacts_target.email, email = stage.email, — email history last_state = contacts_target.state, state = stage.state — state historywhen not matched then insert values (stage.id, stage.name, stage.email, stage.email, stage.state, stage.state); 我们看到相比第二种策略，这种就简单多了。 一个更简单的变化追踪方法如果有很多字段需要比较，那么对于变化的探测逻辑会比较笨重。幸运的是，hive引入了一个hash UDF让这个变得简单，可以接收任意数量的参数，然会一个checksum。 1234567891011121314merge into contacts_targetusing contacts_update_stage as stageon stage.id = contacts_target.idwhen matched and hash(contacts_target.email, contacts_target.state) &lt;&gt; hash(stage.email, stage.state) then update set last_email = contacts_target.email, email = stage.email, — email history last_state = contacts_target.state, state = stage.state — state historywhen not matched then insert values (stage.id, stage.name, stage.email, stage.email, stage.state, stage.state); 好处就是，不管有多少个字段要比较，我们对于代码的修改可以几乎没有。 结论SCD管理是数据仓库机器重要的概念，是一个有很多策略和方法实现的子模块。有了ACID MERGE，hive让我们在hadoop上管理SCD变的简单。 参考：https://zh.hortonworks.com/blog/update-hive-tables-easy-way-2/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive2---轻松更新hive表1]]></title>
    <url>%2F2017%2F11%2F22%2Fhive2---%E8%BD%BB%E6%9D%BE%E6%9B%B4%E6%96%B0hive%E8%A1%A81%2F</url>
    <content type="text"><![CDATA[首先是merge、insert、update、delete。 以前，让hive里的数据持续更新是需要很复杂的成本的，很难去维护与执行。HDP2.6借助hive里的merge语法彻底地简化了数据维护成本，完成了INSERT, UPDATE, DELETE能力。 这个blog会说明怎样解释以下三种问题： hive update，从RDBMS同步数据到hive 更新hive里数据的分区 选择性地mask或者purge数据 基础操作：SQL MERGE, UPDATE AND DELETEMERGE是SQL 2008标准里的，是一个强大的SQL语句，它可以在同一个statement中inert，update，delete数据。MERGE让两个系统一致性工作变得简单。咱们看一下MERGE的语法：12345678MERGE INTO &lt;target table&gt; USING &lt;table reference&gt;ON &lt;search condition&gt; &lt;merge when clause&gt;...WHEN MATCHED [ AND &lt;search condition&gt; ]THEN &lt;merge update or delete specification&gt;WHEN NOT MATCHED [ AND &lt;search condition&gt; ]THEN &lt;merge insert specification&gt; WHEN MATCHED/WHEN NOT MATCHED语句可以无限量的。 我们也会使用到比较熟悉的UPDATE，语法123UPDATE &lt;target table&gt;SET &lt;set clause list&gt;[ WHERE &lt;search condition&gt; ] 当没必要把insert 和update的数据在一个sql statement里进行合并的时候，就可以使用update了。 保持数据fresh在HDP2.6中，需要先做两个工作： 开启Hive transaction。 我们table必须是一个transactional table。就是说这个table必须是clustered的，必须是ORCFile存储格式，而且有一个table属性：transactional=true。下面是一个例子：12345create table customer_partitioned (id int, name string, email string, state string) partitioned by (signup date) clustered by (id) into 2 buckets stored as orc tblproperties(&quot;transactional&quot;=&quot;true&quot;); 例1：HIVE UPSERT假设我们有一个源数据库，想要load进hadoop来运行ing大批量的分析。这个RDBMS中的数据不断的被添加和修改，而且并没有log告诉你哪些数据有变化【就是说没有binlog】。最简单的处理就是每24小时完整的copy一下这个RDBMS的数据镜像。 下面我们创建table：12345create table customer_partitioned(id int, name string, email string, state string) partitioned by (signup date) clustered by (id) into 2 buckets stored as orc tblproperties(&quot;transactional&quot;=&quot;true&quot;); 假设我们的数据在Time = 1的时候是这样的： 在Time = 2的时候是这样的 Upsert操作是吧update和insert放在一个操作里，这样我们就不用关心这些数据是不是原来就已经存在于目标table中。MERGE就是用来做这个事情的：12345678merge into customer_partitioned using all_updates on customer_partitioned.id = all_updates.id when matched then update set email=all_updates.email, state=all_updates.state when not matched then insert values(all_updates.id, all_updates.name, all_updates.email, all_updates.state, all_updates.signup); 注意我们的两个when条件语句是用来管理update或者insert的。在merge过后，这个managed table就与Time=2的staged Table完全一样了，而且所有数据也都在其对应的分区中。 例2：更新hive partitionhive中很多会用日期作为partition策略。这样可以简化数据加载，提升性能。只是有时我们偶尔会出现数据进入错误的分区的情形。例如，假设用户数据是由一个第三方提供的，里面包含一个用户的singup日期。如果这个第三方数据提供者提供的数据开始有问题，后面又修正了，那么前面的在错误分区里的数据就应该被清除了。 注意到ID为2的数据第一次跟第二次的signup日期是不一样的，这就需要更新2017-01-08分区，从里面把它删除，然后把它加入到2017-01-10里面去。 在MERGE出现之前，基本不可能管理这些分区裱花的。Hive的MERGE statement不是原生支持更新partition key，但是有一个小的技巧。 We introduce a delete marker which we set any time the partition keys and UNION this with a second query that produces an extra row on-the-fly for each of these non-matching records.12345678910111213141516171819202122232425262728merge into customer_partitioned using (-- Updates with matching partitions or net new records.-- 更新符合分区的或者select case when all_updates.signup &lt;&gt; customer_partitioned.signup then 1 else 0 end as delete_flag, all_updates.id as match_key, all_updates.* from all_updates left join customer_partitioned on all_updates.id = customer_partitioned.id union all -- Produce new records when partitions don’t match. -- 分区不匹配的时候，生成新的record select 0, null, all_updates.* from all_updates, customer_partitioned where all_updates.id = customer_partitioned.id and all_updates.signup &lt;&gt; customer_partitioned.signup ) subon customer_partitioned.id = sub.match_key when matched and delete_flag=1 then delete when matched and delete_flag=0 then update set email=sub.email, state=sub.state when not matched then insert values(sub.id, sub.name, sub.email, sub.state, sub.signup); 在MERGE处理过这个managed table之后，它就跟源数据表完全一致了。虽然过程中有分区的修改，但是这是一个操作，是原子、且独立的操作。 例3：mask或者purge hive的数据假设有一天你们公司的安全部门过来，让我们把某个用户的所有数据进行mask或者purge操作。那么我们就需要花费很长的时间对很多收到影响的分区进行数据重写。 假设有一个contact table 我们对应的hive表是：1234create table contacts (id int, name string, customer string, phone string) clustered by (id) into 2 buckets stored as orc tblproperties(&quot;transactional&quot;=&quot;true&quot;); 安全部门提出以下要求： 把MaxLeads的所有的电话号码mask掉我们可以使用hive内置的mask方法1update contacts set phone = mask(phone) where customer = &apos;MaxLeads&apos;; 把所有LeadMax的记录都purge掉1delete from contacts where customer = &apos;LeadMax&apos;; 把给定id列表的所有记录都删掉1delete from contacts where id in ( select id from purge_list ); 结论hive的MERGE和ACID事务让hive里的数据管理工作变得简单、强大、而且兼容于现有的EDW平台。 参考：https://zh.hortonworks.com/blog/update-hive-tables-easy-way/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实时系统的特性]]></title>
    <url>%2F2017%2F11%2F22%2F%E5%AE%9E%E6%97%B6%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[关键特性首先，各个微服务之间应该是解耦的，最好通过stream进行结构。 持久化，把数据流暂存起来，供消费。具体持久化的时间，可以根据自己的业务处理 高性能，最快的处理每个环节，最大化单个处理单元的吞吐量 易扩展性，主要是易于应对数据量的海量增加 参考：https://www.youtube.com/watch?v=4lUxf5pzAHs Move from State to Flow把微服务中的state转移到flow中去。 如果多个微服务都更新自己的state到同一个本地存储【比如mysql】，那么这个mysql就会变成一个比较危险的地方。如果我们单个微服务要修改mysql的配置，其他几个服务也要受到影响。 但是我们期望，每个微服务自己有修改的时候，相互之间没有影响，每个微服务的变化都是独立的。假设有A,B,C三个微服务，如果给C单独的mysql，那么c对于A,B就是完全独立的。然后针对A,B,C的更新操作全部看作business event放入一个queue，然后再由consumer从queue里消费event，针对不同的event进行处理。 参考：https://www.youtube.com/watch?v=_xqK0Es9zP4 财政部门的flow数据架构 state flow 共享的DB 不共享任何，只共享flow 复杂的语义，需要小心的使用事务 好的沟通规范 人们较熟悉 生活中的真实流程 控制流 + state更新 控制流 + offset/message bank1和bank2都监听同一个message queue，处理过后持久化到自己的私有db。 参考：https://www.youtube.com/watch?v=vG2qxjkqOgA]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flink特性]]></title>
    <url>%2F2017%2F11%2F22%2FFlink%E7%89%B9%E6%80%A7%2F</url>
    <content type="text"><![CDATA[stateful operator可以追溯到前面的状态，例如count操作 stateful 聚合操作 complex event processing ML pattern stateless 数据抽取 数据清洗 stateless的转化 queryable state如果设定了一个小时的窗口，在到达一小时之前这个数据一般是不可查询的。但是有了 queryable 是state，我们可以在未到时间窗口的时候，就进行查询计算。 不想中断数据处理，但又想 修改worker的数量 迁移到另一个集群 修复代码bug 升级flink 测试不同的算法 …… 以上这些对于stateless任务其实是很简单的，直接操作就可以。停掉，重启。对于statefule的任务，就比较棘手。对于添加worker的需求，state需要reload或者redistribute。 Flink方案：savepoint。 创建savepoint 修改 从savepoint重启 对于session window的state也是很棘手的，Flink1.1会支持这个场景。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink的容错机制]]></title>
    <url>%2F2017%2F11%2F21%2Fflink%E7%9A%84%E5%AE%B9%E9%94%99%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[简介Flink容错机制保证了，即便出现失败，程序的state也最终reflect每条记录exactly once，也可以降级到at least once。 这个容错机制持续的描绘出分布式数据流的snapshot。对于少有state的流式应用来说，这些snapshot是非常轻量的，可以在不影响到性能的前提下快速完成。流式应用的state会被存储在配置好的地方(比如master节点、hdfs等)。 如果程序失败了(机器原因、网络原因、软件问题等)，Flink会踢掉分布式数据流。然后系统会重启所有的operator，然后把他们重置到最近的成功的checkpoint。输入流也被重置到这个state snapshot的点。作为重新启动的并行dataflow的一部分，已经处理过的所有的记录都保证不属于前一个checkpoint state里的一部分。 注意：默认checkpoint是未启用的。 注意：要完全保证这个机制，需要数据流的source(一般是消息队列或者broker)能够支持重置位移。例如kafka就可以重置位移。 注意：以你为Flink的checkpoints是通过分布式snapshot实现的，我们可以交换使用snapshot和checkpoint这两个词语。 checkpointFlinke容错机制的核心就是为分布式的数据流和操作状态进行snapshot。这些snapshot作为一致性的checkpoint供错误恢复使用。Flink执行snapshot的机制在 “Lightweight Asynchronous Snapshots for Distributed Dataflows”.有介绍。是由Chandy-Lamport algorithm实现的，这是一种分布式snapshot方法，而且兼容Flink的执行模型。 Barrier栅栏Flink分布式snapshot的核心元素是stream barrier。这些栅栏被注入到data stream和flow当中作为data stream的一部分存在。barrier永远不会超过记录，flow是严格线性的。barrier负责把data stream中的数据进行切分，切分为两部分，一部分到当前的snapshot中，然后另外一部分是下一个snapshot里的。Each barrier carries the ID of the snapshot whose records it pushed in front of it. barrier并不会中断数据流处理，十分轻量。不同snampshot的多个barrier可能同时存在于stream中，这就是说可能会多个snapshot并发执行。 Stream barrier在stream source被注入到并发的data flow中。snapshot n的barrier的点Sn被注入的位置是source stream知道覆盖到数据的snapshot【The point where the barriers for snapshot n are injected (let’s call it Sn) is the position in the source stream up to which the snapshot covers the data】。例如在kafka中，这个位置就是当前分区上一次访问的记录的offset。这个位置Sn就被汇报给checkpoint coordinator(Flink里就是JobManager)。 然后barrier继续向下执行。当一个中间oerator抽取到snapshot n的barrier时，它发射一个snapshot n的barrier给它所有的outgoing stream。一旦一个sink operator(DAG的最后)收到barrier n的时候，它就向checkpoint coordinator确认ack snapshot n。所有的sink都ack了这个snapshot的时候，它就被认为已经成功了。 snapshot n完成之后，job再也不会请求Sn之前的数据记录了，因为这些数据记录已经通过了整个数据流处理拓扑。 接收多个input stream的operator必须排列多个input stream的snapshot barriers。上图内容 operator接到单个input stream的snapshot barrier n的时候，不能立即处理这个stream的数据，需要等着其他input stream的barrier n也到达的时候才行。否则，它会把snapshot n的数据记录和snapshot n+1的数据记录弄混了。 已经汇报barrier n的input stream暂时被搁置。收到的数据记录暂时不处理，而是放进一个input buffer 最后一个stream也收到barrier n的时候，这个operator就发射所有pending的outing 记录，然后自己也发射snapshot n的barrier。 最后，它恢复处理所有input stream的数据记录，处理在此之前的input buffer里的数据记录，然后处理stream的记录。 state如果operator包含任意形式的state，这个state就必须成为snapshot的一部分。operator statue可能是下面的几种： user-defined state。通过转化方法(例如map、filter)直接创建与修改的state。 system state。例如operator计算的一部分，数据缓存等。典型的例子是window buffer，在window buffer中通常收集聚合数据记录，直到window的计算或者被清空。 operator在收到了它所有input stream的barrier n，并且没有发送barrier n给它的所有output stream之前，snapshot它的state。在这个时间点，barrier之前的所有的数据记录导致的state更新已经完成，并且没有根据已经执行的的barrier之后的数据记录进行任何更新。因为snapshot 的state可能会很大，它可以被存储在一个可配置的后端。默认情况下，是放在JobManager的内存里，对于生产环境，最好放在可靠的存储介质上，例如HDFS。在state被存储之后，operator ack这个checkpoint，发送这个snapshot barrier到output stream中。 现在snapshot包含了： 对于每个并行的stream data source，snapshot开始的时候stream中的位置信息或者offset 对于每个operator，一个指向snapshot state仓库的pointer Exactly Once还是At least once对于streaming program来说alignment操作可能会造成一些延迟。通差，这部分延迟是早毫秒级的，但是我们见过一些非常突出的延迟。对于那些坚持要求低延迟的应用，Flink有一个开关可以关闭checkpoint过程中的alignment操作。checkpoint snapshot仍然很快。 当alignment被跳过后，即便一些checkpoint barrier n已经到了，operator也要继续处理所有的输入。这样的话，在snapshot n完结之前，operator也会处理属于 snapshot n+1的数据。再回复的时候，这些记录就会被重复处理，因为他们都被记录在了snapshot n的state snapshot里，所以也会作为checkpoint n之后的数据被replay。 注意：alignment只会发生在具有多个前辈(join)，或者多个sender(在一个stream被repartition/shuffle)的operator。因此，对于只包含并行处理操作(map, flatmap, filter等)的dataflow来说，at least once模式下其实也是exactly once的。 异步state snapshot注意上面描述的机制意味着operator在奥村他们的snapshot state到state backend的时候，需要停止处理输入的记录。这个同步的state snapshot操作会造成一个小的时间延迟。 如果让这个存储操作异步执行的话，就能让operator继续执行下面的步骤了。要这样的话，operator必循能够生成一个state对象，这个state 对象能够被存储，而且后面operator state的更新不会影响到它。例如，copy-on-write数据结构，rocksdb里有这种操作。 接收到所有input stream的checkpoint barrier后，operator开始异步snapshot，copy他的state。它会马上发送barrier到它的output，然后继续下面的数据流处理。一旦后台的copy进程完成后，它就会向checkpoint coordinator(JobManager) ack这个checkpoint。 这个checkpoint现在只在所有的sink都收到barrier后，所有的statefule operator都ack他们完成了backup后(这个可能比barrier到达sink还要慢)才算完成。 recovery在这个机制下的recovery很直接：一旦发生失败，Flink选择最新的完成的checkpoint k。然后这个system重新部署整个的分布式数据流，然后给每个operator这个snapshot k对应的state，因为这也是checkpoint k的一部分。而且需要从数据源的Sk的位置开始读取数据流。如果是kafka的话，就是从offset为Sk的地方开始抽取数据。 如果state是增量snapshot的，那么operator要使用最新full snapshot的state，然后把后续的更新操作应用到这个state上。 更多 operator snapshot的实现当operator执行snapshot的时候，有两个部分：同步的和异步的。 operator和state backend是以java FutureTask的形式提供他们的snapshot的。这个task包含了同步部分已完成的state，但是异步部分可能还在pending状态。异步部分后面会被这个checkpoint的一个后台线程执行。 使用纯同步方式的operator checkpint会返回一个已经完成了的FutrueTask。如果异步操作需要被执行，就执行FutrueTask的run方法。 这些task是可以被cancel的，这样stream和其他的资源消费句柄就可以被release了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flink的window]]></title>
    <url>%2F2017%2F11%2F21%2Fflink%E7%9A%84window%2F</url>
    <content type="text"><![CDATA[滑动窗口,sliding windows，比如我们每30s统计一下上一分钟的和。 当只有一个窗口处理器的时候，我们就只能串行处理一个数据流。每个流里的书都要去向某个指定的window。在Flink中对于Windows on a full stream are【一个完整的流的窗口】 称为 AllWindows。对很多app来说，数据流都要分发进入多个逻辑流，然后会有window operator处理每个逻辑流。假设我们从多个交通传感器获取交通工具的流量，每个交通传感器都监控不同地址的流量。我们可以把这些信息流通过交通传感器的id进行分组，然后分别并发的计算每个交通传感器所在位置的流量信息。在Flink中，我们把这种partitioned window叫做simple window，因为这是对分布式数据流很常见的处理方式。下图展示了一个通过(sernsorID, count)的流进行数据收集的滚动窗口 通常，一个window在无穷的数据流中定义了一组有穷数据。这组数据可以是基于时间、计数、时间与技术结合、自定义的一些逻辑去分window。Flink的DataStream API提供了简洁的操作供常用的窗口操作，也留了接口让用户提供自定义的分窗口的逻辑。下面我们详细看一下基于时间与计数的窗口机制。 基于时间的窗口下面是一个每分钟滚动一次的窗口，对所有的数据执行某个函数操作。1234567891011121314151617// (sensorId, carCnt)形式的数据流val vehicleCnts: DataStream[(Int, Int)] = ...// 0，1是在数据中的位置val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts // 通过sensorId分区 .keyBy(0) // 1分钟的窗口时间 .timeWindow(Time.minutes(1)) // 计算carCnt的和 .sum(1) val slidingCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // 每30秒触发一次1分钟的数据窗口的执行 .timeWindow(Time.minutes(1), Time.seconds(30)) .sum(1) 还有一个时间的概念我们要说明 processing time。窗口是根据当前主机的1分钟去构建与计算窗口内数据的。 event time。event产生时的时间。相比processing time更好一些 Ingestion time。是processing time和event time的杂交品种。一旦数据到达，就把当前机器的时间戳赋给这个数据记录，然后基于这个时间戳，使用event time去持续处理。 基于计数的窗口一个100个event作为窗口的程序。123456789101112131415// (sensorId, carCnt)形式的数据流val vehicleCnts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = vehicleCnts // 用sensorId分区 .keyBy(0) // 100个数据为一个窗口 .countWindow(100) .sum(1)val slidingCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // 100个数据作为一个窗口，每10个触发一次窗口处理 .countWindow(100, 10) .sum(1) 深入了解Flink内置的基于时间、计数的窗口已经覆盖了大多数的应用，但是对于一些需要自定义窗口划分逻辑的，需要使用DataStream API暴露的接口，这些接口给了窗口构建与计算的很有条理的控制方式。 下图是Flink窗口机制的详细图 到达window operator的数据会先发给WindowAssigner. WindowAssigner把数据分配给一个或者多个window，也有可能要创建新的window。一个Window是一个一组数据、元数据(对于TimeWindow来说是起止时间)的唯一标识符。注意数据是可以被加入到多个window的，也就是说可能会同时存在多个window。 每个window拥有一个Trigger，它来决定当前window什么时候计算或者清空。这个trigger在每条数据到来的时候都会检验，如果前面注册的timer超时就会触发操作：执行计算、清空窗口、先计算再清空。如果只是触发计算，那么所有的数据就还留在window里，下次触发的时候还可以计算。一个window在被清空之前一直都可以被计算，这也代表着内存占用。 计算函数接收window的所有数据，输出一个或多个结果。DataStream API接受不同类型的计算函数，包括一些预定义的sum、min、max，ReduceFunction、FlodFunciton、WindowFunction等。最常见的是WindowFunction，它接收window对象的元数据，一组window对象，window key(如果是分区的window)作为参数。 这些组件构成了Flink的窗口机制。我们现在一步步看一下怎样实现自定义窗口逻辑。我们以DataStream[IN]类型的stream开始，用一个key选择器函数来抽取key，获得一个Keydtream[IN, KEY].123456789101112131415161718192021val input: DataStream[IN] = ...val keyed: KeyedStream[IN, KEY] = input .keyBy(myKeySel: (IN) =&gt; KEY)// 通过WindowAssigner创建一个分窗口的stream。WindowAssigner 有默认的Trigger实现var windowed: WindowedStream[IN, KEY, WINDOW] = keyed .window(myAssigner: WindowAssigner[IN, WINDOW]) // 不适用WindowAssigner默认的triggerwindowed = windowed .trigger(myTrigger: Trigger[IN, WINDOW])// 指定可选的evictorwindowed = windowed .evictor(myEvictor: Evictor[IN, WINDOW])// 最后，把window function传递给windowed streamval output: DataStream[OUT] = windowed .apply(myWinFunc: WindowFunction[IN, OUT, KEY, WINDOW])]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos上搭建ftp服务器]]></title>
    <url>%2F2017%2F11%2F20%2Fcentos%E4%B8%8A%E6%90%AD%E5%BB%BAftp%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[首先安装服务：1yum install -y vsftpd 创建只读的dataman用户：12useradd -s /sbin/nologin datamanpasswd dataman 编辑配置文件/etc/vsftpd/vsftpd.conf：123456789101112# 修改默认端口listen_port=6888# 禁止匿名登陆anonymous_enable=NO# 指定用户单独的配置信息所在目录user_config_dir=/var/ftp# 不许用户登陆后切换目录chroot_local_user=YES 然后到/var/ftp下新建dataman用户的ftp配置文件，这里只需要指定他的root目录就好了。1local_root=/server/files 参考 https://jasonhzy.github.io/2016/03/11/linux-ftp/ http://cn.linux.vbird.org/linux_server/0410vsftpd.php 坑 我们的网络原因导致，测试环境要连接线上的ftp服务器的话，需要通过一层运维提供的HAProxy代理。 这就引出了ftp服务器主动传输与被动传输的概念区别。 FTP协议有两种工作方式：PORT方式和PASV方式，中文意思为主动式和被动式。 PORT（主动）方式的连接过程是：客 户端向服务器的FTP端口（默认是21）发送连接请求，服务器接受连接，建立一条命令链路。当需要传送数据时，客户端在命令链路上用PORT命令告诉服务 器：“我打开了XXXX端口，你过来连接我”。于是服务器从20端口向客户端的XXXX端口发送连接请求，建立一条数据链路来传送数据。 PASV（被动）方式的连接过程是：客 户端向服务器的FTP端口（默认是21）发送连接请求，服务器接受连接，建立一条命令链路。当需要传送数据时，服务器在命令链路上用PASV命令告诉客户 端：“我打开了XXXX端口，你过来连接我”。于是客户端向服务器的XXXX端口发送连接请求，建立一条数据链路来传送数据。 概括： 主动模式：服务器向客户端敲门，然后客户端开门被动模式：客户端向服务器敲门，然后服务器开门 所以，如果你是如果通过代理上网的话，就不能用主动模式，因为服务器敲的是上网代理服务器的门，而不是敲客户端的门而且有时候，客户端也不是轻易就开门的，因为有防火墙阻挡，除非客户端开放大于1024的高端端口 vsftpd服务器端修改配置：12345678910# passivetcp_wrappers=YESpasv_promiscuous=NO# 这个比较关键，10.103.70.27是我们的haproxy所在的IPpasv_address=10.103.70.27port_enable=YESport_promiscuous=NOpasv_enable=YESpasv_min_port=10000pasv_max_port=10010 同时，HAProxy也需要配置到这些端口范围1234listen ftp bind *:6888,*:10000-10010 mode tcp server ftpserver 10.2.19.62 check inter 3000 port 6888 http://blog.sina.com.cn/s/blog_7f1d56650102v57p.html http://www.cnblogs.com/exclm/archive/2009/05/08/1452893.html http://blog.sina.com.cn/s/blog_5cdb72780100jwjt.html https://serverfault.com/questions/441721/ftp-through-haproxy]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ETL日常调度任务类型的思考]]></title>
    <url>%2F2017%2F11%2F16%2FETL%E6%97%A5%E5%B8%B8%E8%B0%83%E5%BA%A6%E4%BB%BB%E5%8A%A1%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[对于有些需要等待上游非同质类型、而且是外部任务的任务方案： 信号灯即所有的每个任务在完成之后，都设置一个信号灯，然后后面的任务需要此信号灯满足某个信号的时候才能触发执行。 信号灯的查看方式可以有多种： 数据库访问，上游直接修改数据库，下游直接轮询数据库 rest接口访问，上有直接通过中心接口修改数据库，下游轮询一个中心接口 发布订阅模式，中间件设置eventbus 执行：sql查询、接口与状态确认 提供shell具体咋依赖，全都自己去写，这里啥也不限制，而且啥也不伺候 安全系数低]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[druid设计]]></title>
    <url>%2F2017%2F11%2F15%2Fdruid%E8%AE%BE%E8%AE%A1%2F</url>
    <content type="text"><![CDATA[segmentDruid的索引是存在segment文件中的，segment文件是按照时间分区的。一个segment文件对应一个时间间隔，时间间隔根据配置项granularitySpec里的segmentGranularity进行配置。推荐segment文件大小设置在300~700M之间。如果你的segment文件太大，那么请考虑修改时间间隔，或者为数据做分区，修改partitioningSpec的targetPartitionSize参数(可以试试五百万行)。 文件数据结构segment是列式存储的：每一列都在不同的数据结构中。每个列都分开存储，druid就可以只访问查询应用到的列的数据。有三个基本的列类型：timestamp、维度、metric。 timestamp和metric列比较简单：都是int或者float数组，使用LZ4算法压缩。一个query只要知道他需要查询那些行，就解压相关列的这部分文件，抽取对应的行，然后执行聚合计算操作即可。 dimension列有些不同，它要支持filter和groupby操作，所以需要下面三个数据结构 一个字典，把值映射成int类型的id 使用上面字典编码的列值的list 列中每个不同的列指，都要弄一个bitmap对应所有的行，说明哪些行带有这个列指 把原本的值映射成int的id是为了节省空间。上面第三个里的bitmap也就是倒排索引(inverted indexes)可以提供快速的过滤工作。最后，上面第二个结构里的值，是用来处理group by和topN拆线呢的。也就是说，基于filter的单独聚合指标不要要接触到第二个数据结构中的维度值。 基于上图中的数据，看一下例子，下面是针对列Page的1234567891011121314151: 映射编码的字典 &#123; &quot;Justin Bieber&quot;: 0, &quot;Ke$ha&quot;: 1 &#125;2: 按照1中映射后，这一列的字段值 [0, 0, 1, 1]3: 每一个或者说每一种列值都有自己的Bitmaps value=&quot;Justin Bieber&quot;: [1,1,0,0] value=&quot;Ke$ha&quot;: [0,0,1,1] 注意bitmap数据结构与前面个不同，前两个是随数据量线性增长的，而bitmap是数据量 * 每列各种值的个数。 多值的列假设Page的第二行数据为Ke$ha,Justin Bieber，那么数据结构变化为下面：123456789101112131415161718191: Dictionary that encodes column values &#123; &quot;Justin Bieber&quot;: 0, &quot;Ke$ha&quot;: 1 &#125;2: Column data [0, [0,1], &lt;--Row value of multi-value column can have array of values 1, 1]3: Bitmaps - one for each unique value value=&quot;Justin Bieber&quot;: [1,1,0,0] value=&quot;Ke$ha&quot;: [0,1,1,1] ^ | | Multi-value column has multiple non-zero entries 命名约定segment文件的标识符一般是由数据源、开始时间、结束时间、版本构成的。如果数据在某个时间范围外还sharded了，那么也会带有一个分区数字。 例子：datasource_intervalStart_intervalEnd_version_partitionNum segment组件一个segment由几个文件组成 version.bin。 4个字节，代表segment的version。 meta.smoosh。带有其他smoosh文件内容元数据(文件名、offset)的文件。 xxxx.smoosh。多个这种文件，都是二进制数据。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive2版本亮点]]></title>
    <url>%2F2017%2F11%2F15%2Fhive2%E7%89%88%E6%9C%AC%E4%BA%AE%E7%82%B9%2F</url>
    <content type="text"><![CDATA[LLAP。 Live Long and Process智能将数据缓存到多台机器的内存中，并允许所有客户端共享这些缓存的数据，同时保留了弹性伸缩能力。开启LLAP后，性能相比1版本提升25倍。 LLAP提供了一个高级的执行模式，他启用一个长时间存活的守护程序去和HDFS DataNode直接交互，也是一个紧密集成的DAG框架。这个守护程序中加入了缓存、预抓取、查询过程和访问控制等功能。短小的查询由守护程序执行，大的重的操作由YARN执行。 支持使用HPL/SQL的存储过程 支持使用变量、表达式、控制流声明、迭代来实现业务逻辑，支持使用异常处理程序和条件处理器来实现高级错误处理。 使用sql on hadoop更加动态：支持使用高级表达式、各种内置函数，基于用户配置、先前查询的结果、来自文件或者非hadoop数据源的数据、即时动态的生成SQL条件。 利用已有的存储过程SQL，提供函数和声明。 方便集成和支持多种类型数据仓库，可以实现单个脚本处理hadoop、RDBMS、Nossql等多个系统的数据。 更智能的成本优化其CBO 提供全面的监控和诊断工具。hive server2的UI界面、LLAP 的UI、Tez的UI等。 升级只需要升级hive的元数据库信息即可，2版本提供了升级脚本1hive-metastore/bin/schematool -upgradeSchema -dbType]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TPC-DS基准测试]]></title>
    <url>%2F2017%2F11%2F15%2FTPC-DS%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[测试数据加载 系统准备 数据文件生成 测试数据库创建 基础表创建 数据加载 约束验证 辅助数据结构创建 表和辅助数据统计分析等 查询顺序执行。power测试是单个查询的处理能力。 查询并行执行。throughput是并发查询的处理能力，分为数据查询与数据维护两个子步骤。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scalatra入门]]></title>
    <url>%2F2017%2F11%2F09%2Fsbt%E7%AE%80%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[生成scalatra项目下载项目，并配置相关信息：123456789101112131415161718192021222324252627# 从github上下载项目root@will-vm:/usr/local/will/learning# sbt new scalatra/scalatra.g8[info] Set current project to learning (in build file:/usr/local/will/learning/)# 用来发布项目的，一般就是反序域名[联想一下java包]organization [com.example]: com.will# scalatra app名字name [My Scalatra Web App]: willup# 项目版本，自己随便定义，比如0.0.1version [0.1.0-SNAPSHOT]: # 我们的servlet类的名字servlet_name [MyScalatraServlet]: Will# 所有的scala文件都属于一个pacakgepackage [com.example.app]: com.will.app# 使用的scala版本scala_version [2.12.3]: 2.12.1# 使用的sbt版本sbt_version [1.0.2]: 1.0.3# 使用的scalatra版本scalatra_version [2.5.4]: # 初始项目生成在willup目录了，有些像djangoTemplate applied in ./willuproot@will-vm:/usr/local/will/learning# ll总用量 16drwxr-xr-x 4 root root 4096 11月 9 17:12 ./drwxr-xr-x 8 root root 4096 11月 9 16:06 ../drwxr-xr-x 3 root root 4096 11月 9 17:11 target/drwxr-xr-x 4 root root 4096 11月 9 17:12 willup/ 构建进入willup, 执行sbt命令，sbt就会自动帮我们自动下载Scalatra的开发环境了，这需要些时间。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546root@will-vm:/usr/local/will/learning/willup# sbt[info] Loading settings from plugins.sbt ...[info] Loading project definition from /usr/local/will/learning/willup/project[info] Updating &#123;file:/usr/local/will/learning/willup/project/&#125;willup-build...[info] downloading https://repo1.maven.org/maven2/com/amazonaws/jmespath-java/1.11.105/jmespath-java-1.11.105.jar ...[info] downloading https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.typesafe.sbt/sbt-twirl/scala_2.12/sbt_1.0/1.3.12/jars/sbt-twirl.jar ...[info] [SUCCESSFUL ] com.amazonaws#jmespath-java;1.11.105!jmespath-java.jar (824ms)[info] downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar ...[info] [SUCCESSFUL ] com.typesafe.sbt#sbt-twirl;1.3.12!sbt-twirl.jar (3335ms)[info] downloading https://repo1.maven.org/maven2/org/scalatra/sbt/sbt-scalatra_2.12_1.0/1.0.1/sbt-scalatra-1.0.1.jar ...[info] [SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.2!httpclient.jar (3126ms)[info] downloading https://repo1.maven.org/maven2/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar ...[info] [SUCCESSFUL ] org.scalatra.sbt#sbt-scalatra;1.0.1!sbt-scalatra.jar (1806ms)[info] downloading https://repo1.maven.org/maven2/com/typesafe/play/twirl-compiler_2.12/1.3.12/twirl-compiler_2.12-1.3.12.jar ...[info] [SUCCESSFUL ] com.typesafe.play#twirl-compiler_2.12;1.3.12!twirl-compiler_2.12.jar (883ms)[info] downloading https://repo1.maven.org/maven2/com/typesafe/play/twirl-api_2.12/1.3.12/twirl-api_2.12-1.3.12.jar ...[info] [SUCCESSFUL ] software.amazon.ion#ion-java;1.0.2!ion-java.jar(bundle) (1941ms)[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.6.6/jackson-databind-2.6.6.jar ...[info] [SUCCESSFUL ] com.typesafe.play#twirl-api_2.12;1.3.12!twirl-api_2.12.jar (791ms)[info] downloading https://repo1.maven.org/maven2/com/typesafe/play/twirl-parser_2.12/1.3.12/twirl-parser_2.12-1.3.12.jar ...[info] [SUCCESSFUL ] com.typesafe.play#twirl-parser_2.12;1.3.12!twirl-parser_2.12.jar (866ms)[info] downloading https://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/com.earldouglas/xsbt-web-plugin/scala_2.12/sbt_1.0/4.0.1/jars/xsbt-web-plugin.jar ...[info] [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.6.6!jackson-databind.jar(bundle) (2843ms)[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.6.6/jackson-dataformat-cbor-2.6.6.jar ...[info] [SUCCESSFUL ] com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.6.6!jackson-dataformat-cbor.jar(bundle) (789ms)[info] downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.8.1/joda-time-2.8.1.jar ...[info] [SUCCESSFUL ] com.earldouglas#xsbt-web-plugin;4.0.1!xsbt-web-plugin.jar (2691ms)[info] downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-elasticbeanstalk/1.11.105/aws-java-sdk-elasticbeanstalk-1.11.105.jar ...[info] [SUCCESSFUL ] joda-time#joda-time;2.8.1!joda-time.jar (1708ms)[info] downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar ...[info] [SUCCESSFUL ] com.amazonaws#aws-java-sdk-elasticbeanstalk;1.11.105!aws-java-sdk-elasticbeanstalk.jar (1578ms)[info] downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-s3/1.11.105/aws-java-sdk-s3-1.11.105.jar ...[info] [SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.4!httpcore.jar (1054ms)[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.6.0/jackson-annotations-2.6.0.jar ...[info] [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.6.0!jackson-annotations.jar(bundle) (831ms)[info] downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.6.6/jackson-core-2.6.6.jar ...[info] [SUCCESSFUL ] com.amazonaws#aws-java-sdk-s3;1.11.105!aws-java-sdk-s3.jar (1627ms)[info] downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-core/1.11.105/aws-java-sdk-core-1.11.105.jar ...[info] [SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.6.6!jackson-core.jar(bundle) (1238ms)[info] downloading https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-kms/1.11.105/aws-java-sdk-kms-1.11.105.jar ...[info] [SUCCESSFUL ] com.amazonaws#aws-java-sdk-core;1.11.105!aws-java-sdk-core.jar (1825ms)[info] [SUCCESSFUL ] com.amazonaws#aws-java-sdk-kms;1.11.105!aws-java-sdk-kms.jar (1140ms)[info] Done updating.[info] Loading settings from build.sbt ...[info] Set current project to willup (in build file:/usr/local/will/learning/willup/)[info] sbt server started at 127.0.0.1:4841 Hello world到此为止Scalatra已经安装完了，咱们着手弄个小app吧。]]></content>
      <tags>
        <tag>scala</tag>
        <tag>scalatra</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scala集合视图]]></title>
    <url>%2F2017%2F11%2F09%2Fscala%E9%9B%86%E5%90%88%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[场景： 性能 像处理数据库视图一样处理集合 视图就是推迟执行，该用多大内存还使用多大内存，该遍历多少元素还是遍历多少元素。说白了scala视图就跟数据库视图一样，不使用视图就跟数据库建立临时表一样。使用视图，当原始集合改变的时候，不需要重新跑transformers方法，使用视图则每次使用视图的时候都会跑一次transformers方法内容。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ubuntu磁盘相关]]></title>
    <url>%2F2017%2F11%2F08%2Fubuntu%E7%A3%81%E7%9B%98%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[主要是自己工作用的ubuntu虚拟机因为最开始分配的磁盘不够，需要从外面额外扩展一下磁盘。 过程中，小小记录一下一些点，下次处理快一些。 图形化磁盘分析工具baobab，这个工具是ubuntu16里已经默认有了的。其实就类似du –max-depth=1 -h /，然后不用自己一层层去找所有目录的大小。它是一下子帮我们分析出所有的目录空间，当然时间就会长一些。相比命令行，优点当然就是查看与比较起来更加方便，快速定位到大磁盘，审慎地处理一些没用的数据。 当然，因为要统计所有目录空间大小，再呈现出来，所以速度会慢一些。 图形化磁盘分区工具通过vmware给虚拟机扩展磁盘到80以后，执行fdisk -l，发现并没有出现新的40G空间。 gparted, 这个需要额外通过apt进行安装。可以方便地完成磁盘格式化等操作，自己敲命令进行分区与格式化，会稍微有些不自信。 方案确认之后，点击上面的对勾，开始执行所有的变更操作。之后fdisk -l就可以看到新的空间了。 挂载磁盘一次性挂载，就使用mount命令就可以了。 永久的话，就需要编辑/etc/fstab了。但是在此之前，我们要通过blkid命令先获取到要挂载磁盘的UUID,，然后再添加下面内容。 1UUID=e23a1c1e-8d91-4df8-8fba-f0656a1080ab /will_data ext4 defaults,errors=remount-ro 0 1 参考： https://gist.github.com/gaoyifan/019ad7766f030ab5be50 http://www.jianshu.com/p/ec5579ef15a6 后记妈蛋，后来发现是自己的Trash一直没有清空过….有5G的数据。清空后空间够了…..]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hue中添加spark-shell的支持]]></title>
    <url>%2F2017%2F11%2F07%2Fhue%E4%B8%AD%E6%B7%BB%E5%8A%A0spark-shell%E7%9A%84%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[计划： livy + hue + spark 通过ambari已经部署完成了livy，并且通过了curl的spark测试。 参考： http://gethue.com/how-to-use-the-livy-spark-rest-job-server-for-interactive-spark-2-2/ 配置1234567891011121314151617181920212223242526272829# 修改spark配置[spark] # Host address of the Livy Server. livy_server_host=servicenode02.will.com # Port of the Livy Server. livy_server_port=8998 # Configure livy to start in local &apos;process&apos; mode, or &apos;yarn&apos; workers. ivy_server_session_kind=yarn # If livy should use proxy users when submitting a job. ## livy_impersonation_enabled=true # Host of the Sql Server ## sql_server_host=localhost # Port of the Sql Server ## sql_server_port=10000[[[spark]]] name=Scala interface=livy[[[pyspark]]] name=PySpark interface=livy 但是部署在hue中的时候，总是出现session问题, 在hue的example中下载了spark的测试notebook，执行pyspark的1+1+1时候出现提示 123456789[07/Nov/2017 01:06:17 -0800] decorators ERROR error running &lt;function execute at 0x7f0ca465ae60&gt;Traceback (most recent call last): File &quot;/server/hue/desktop/libs/notebook/src/notebook/decorators.py&quot;, line 81, in decorator return func(*args, **kwargs) File &quot;/server/hue/desktop/libs/notebook/src/notebook/api.py&quot;, line 109, in execute response[&apos;handle&apos;] = get_api(request, snippet).execute(notebook, snippet) File &quot;/server/hue/desktop/libs/notebook/src/notebook/connectors/spark_shell.py&quot;, line 194, in execute raise eRestException: &quot;Session &apos;-1&apos; not found.&quot; (error 404) 然后可以在livy server的rest api中看到session123456789101112131415161718192021222324&#123; &quot;from&quot;: 0, &quot;total&quot;: 1, &quot;sessions&quot;: [ &#123; &quot;state&quot;: &quot;idle&quot;, &quot;proxyUser&quot;: null, &quot;id&quot;: 0, &quot;kind&quot;: &quot;pyspark&quot;, &quot;log&quot;: [ &quot;17/11/07 16:01:15 INFO SparkContext: Added file file:/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip at file:/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip with timestamp 1510041675612&quot;, &quot;17/11/07 16:01:15 INFO Executor: Starting executor ID driver on host localhost&quot;, &quot;17/11/07 16:01:15 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 42523.&quot;, &quot;17/11/07 16:01:15 INFO NettyBlockTransferService: Server created on 42523&quot;, &quot;17/11/07 16:01:15 INFO BlockManagerMaster: Trying to register BlockManager&quot;, &quot;17/11/07 16:01:15 INFO BlockManagerMasterEndpoint: Registering block manager localhost:42523 with 511.1 MB RAM, BlockManagerId(driver, localhost, 42523)&quot;, &quot;17/11/07 16:01:15 INFO BlockManagerMaster: Registered BlockManager&quot;, &quot;17/11/07 16:01:16 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.&quot;, &quot;17/11/07 16:01:16 INFO EventLoggingListener: Logging events to hdfs:///spark-history/local-1510041675669&quot;, &quot;17/11/07 16:01:18 INFO ScalatraBootstrap: Calling http://servicenode02.will.com:8998/sessions/0/callback...&quot; ] &#125; ]&#125; 点击recreate scala 的session后，发现scala代码是可以执行的。那么，我猜测，有可能pyspark也是一样的道理。 scala代码如下：123val data = Array(0,2,3,45,2);val disData = sc.parallelize(data);disData.map(s=&gt;s+1).collect(); 而且观察到此时livy的sessions接口里的session也有了变化1234567891011121314151617&#123; &quot;from&quot;: 0, &quot;total&quot;: 1, &quot;sessions&quot;: [ &#123; &quot;state&quot;: &quot;idle&quot;, &quot;proxyUser&quot;: null, &quot;id&quot;: 1, &quot;kind&quot;: &quot;spark&quot;, &quot;log&quot;: [ &quot;17/11/07 17:43:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2909 ms on localhost (5/8)&quot;, ............ &quot;17/11/07 17:43:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:36806 in memory (size: 1257.0 B, free: 511.1 MB)&quot; ] &#125; ]&#125; 到notebook的上面，点击pspark的recreate的地方，观察session变化：12345678910111213141516171819202122232425262728&#123; &quot;from&quot;: 0, &quot;total&quot;: 2, &quot;sessions&quot;: [ &#123; &quot;state&quot;: &quot;idle&quot;, &quot;proxyUser&quot;: null, &quot;id&quot;: 2, &quot;kind&quot;: &quot;pyspark&quot;, &quot;log&quot;: [ &quot;17/11/07 17:45:18 INFO SparkContext: Added file file:/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip at file:/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip with timestamp 1510047918034&quot;, ....... &quot;17/11/07 17:45:24 INFO ScalatraBootstrap: Calling http://servicenode02.will.com:8998/sessions/2/callback...&quot; ] &#125;, &#123; &quot;state&quot;: &quot;idle&quot;, &quot;proxyUser&quot;: null, &quot;id&quot;: 1, &quot;kind&quot;: &quot;spark&quot;, &quot;log&quot;: [ &quot;17/11/07 17:43:10 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2909 ms on localhost (5/8)&quot;, ...... &quot;17/11/07 17:43:21 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:36806 in memory (size: 1257.0 B, free: 511.1 MB)&quot; ] &#125; ]&#125; 多了一个。好，那么再次去执行我们的代码。 123456file = sc.textFile("/apps/hive/warehouse/sample_08/sample_08")file = file.flatMap(lambda line: line.split("\t")).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)for row in file.collect()[:5]: print row 然后就可以了。哈哈 查看一下livy server的log123456789101112131417/11/07 17:44:50 INFO SparkProcessBuilder: Running sh -c /usr/hdp/current/spark-client/bin/spark-submit--name Livy --jars /usr/hdp/current/livy-server/repl-jars/mime-util-2.1.3.jar,/usr/hdp/current/livy-server/repl-jars/jetty-io-9.2.10.v20150310.jar,/usr/hdp/current/livy-server/repl-jars/jetty-security-9.2.10.v20150310.jar,/usr/hdp/current/livy-server/repl-jars/jetty-util-9.2.10.v20150310.jar,/usr/hdp/current/livy-server/repl-jars/livy-api-0.2.0.2.4.2.0-258.jar,/usr/hdp/current/livy-server/repl-jars/livy-core-0.2.0.2.4.2.0-258.jar,/usr/hdp/current/livy-server/repl-jars/rl_2.10-0.4.10.jar,/usr/hdp/current/livy-server/repl-jars/async-http-client-1.9.33.jar,/usr/hdp/current/livy-server/repl-jars/jetty-server-9.2.10.v20150310.jar,/usr/hdp/current/livy-server/repl-jars/commons-codec-1.9.jar,/usr/hdp/current/livy-server/repl-jars/joda-convert-1.6.jar,/usr/hdp/current/livy-server/repl-jars/scalatra-json_2.10-2.3.0.jar,/usr/hdp/current/livy-server/repl-jars/kryo-2.22.jar,/usr/hdp/current/livy-server/repl-jars/joda-time-2.3.jar,/usr/hdp/current/livy-server/repl-jars/grizzled-slf4j_2.10-1.0.2.jar,/usr/hdp/current/livy-server/repl-jars/scalatra-common_2.10-2.3.0.jar,/usr/hdp/current/livy-server/repl-jars/livy-client-common-0.2.0.2.4.2.0-258.jar,/usr/hdp/current/livy-server/repl-jars/livy-repl-0.2.0.2.4.2.0-258.jar,/usr/hdp/current/livy-server/repl-jars/jetty-http-9.2.10.v20150310.jar,/usr/hdp/current/livy-server/repl-jars/jetty-servlet-9.2.10.v20150310.jar,/usr/hdp/current/livy-server/repl-jars/scalatra_2.10-2.3.0.jar,/usr/hdp/current/livy-server/repl-jars/juniversalchardet-1.0.3.jar,/usr/hdp/current/livy-server/repl-jars/javax.servlet-api-3.1.0.jar,/usr/hdp/current/livy-server/repl-jars/netty-3.10.5.Final.jar --files /usr/hdp/current/spark-client/python/lib/pyspark.zip,/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip --class com.cloudera.livy.repl.Main --conf spark.executor.memory=1G --conf spark.driver.memory=1G --conf spark.submit.pyFiles=/usr/hdp/current/spark-client/python/lib/pyspark.zip,/usr/hdp/current/spark-client/python/lib/py4j-0.9-src.zip --conf spark.driver.cores=1 --conf spark.livy.callbackUrl=http://servicenode02.will.com:8998/sessions/2/callback --conf spark.livy.port=0 --conf spark.yarn.isPython=true --conf spark.executor.cores=1 --queue default spark-internal pyspark17/11/07 17:44:50 INFO SessionManager: Registering new session 2 竟然没有提交到yarn上执行！！上面的参数我们明明配置了yarn啊。再查一下livy server的相关配置信息。 到ambari添加配置1livy.spark.master=yarn-cluster 重启。并没有生效。 看下配置，竟然是这样的12345livy.environment productionlivy.impersonation.enabled falselivy.server.port 8998livy.server.session.timeout 3600000livy.spark.master yarn-cluster 中间的等号都没有了。 手动修改，暂时不用ambari了。12345livy.environment =productionlivy.impersonation.enabled= falselivy.server.port= 8998livy.server.session.timeout =3600000livy.spark.master= yarn-cluster 而且通过ps -ef | grep livy发现，ambari 2.2.2.0的livy对于stop并不完整，会有泄露问题存在。虽然livy server关掉了，但是它打开的spark进程却仍然存在。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables端口转发]]></title>
    <url>%2F2017%2F11%2F06%2Fiptables%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91%2F</url>
    <content type="text"><![CDATA[先放一张iptables的处理流程图 12345678910# 在PREROUTING中添加转发规则:把从eth0网卡进来的1234端口的访问，定向到10.103.27.171:2224。iptables -t nat -A PREROUTING -p tcp -i eth0 --dport 1234 -j DNAT --to 10.103.27.171:2224# j后面是操作，masquerade的意思是伪装iptables -t nat -A POSTROUTING -j MASQUERADE# 保存并重启service iptables saveservice iptables restart 查看iptables的nat表相关配置1234567891011121314151617181920212223242526272829303132333435363738394041424344[root@node83 nginxserver_new]# iptables -t nat -L -vnChain PREROUTING (policy ACCEPT 12 packets, 650 bytes) pkts bytes target prot opt in out source destination 0 0 DNAT tcp -- * * 0.0.0.0/0 60.1.1.1 tcp dpt:80 to:10.103.70.27:2224 16 2578 DNAT tcp -- * * 0.0.0.0/0 10.103.27.171 tcp dpt:80 to:10.103.70.27:2224 10 520 DNAT tcp -- * * 0.0.0.0/0 10.103.27.171 tcp dpt:1234 to:10.103.70.27:2224 0 0 DNAT tcp -- eth0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:1234 to:10.103.27.171:2224 Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 69 5738 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 43 packets, 2640 bytes) pkts bytes target prot opt in out source destination# 删除nat表第4个规则[root@node83 nginxserver_new]# iptables -t nat -D PREROUTING 4# 再看一下规则[root@node83 nginxserver_new]# iptables -t nat -L -vnChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 0 0 DNAT tcp -- * * 0.0.0.0/0 60.1.1.1 tcp dpt:80 to:10.103.70.27:2224 130 8506 DNAT tcp -- * * 0.0.0.0/0 10.103.27.171 tcp dpt:80 to:10.103.70.27:2224 10 520 DNAT tcp -- * * 0.0.0.0/0 10.103.27.171 tcp dpt:1234 to:10.103.70.27:2224 Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 394 24326 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destination # 清空NAT表中PREROUTTING的规则[root@node83 nginxserver_new]# iptables -t nat -F PREROUTING # 再看一下规则[root@node83 nginxserver_new]# iptables -t nat -L -vnChain PREROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination Chain POSTROUTING (policy ACCEPT 0 packets, 0 bytes) pkts bytes target prot opt in out source destination 420 25806 MASQUERADE all -- * * 0.0.0.0/0 0.0.0.0/0 Chain OUTPUT (policy ACCEPT 2 packets, 120 bytes) pkts bytes target prot opt in out source destination 参考： http://xstarcd.github.io/wiki/Linux/iptables_forward_internetshare.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kudu简介]]></title>
    <url>%2F2017%2F11%2F02%2Fkudu%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[为apache hadoop平台开发的列式存储管理器。kudo拥有hadoop生态的很多特性：廉价硬件、水平扩展、高可用。 背景 HDFS可以快速追加和查询，尤其是顺序扫描，如果能和parquet结合就可以有更好的表现。 hbase擅长处理易变的数据，快速查询和写单条数据。 kudu介于两者之间。 kudu结合了parquet和hbase的特性，创建了一个本地列式存储的系统。 kudu特性: 动态管理分区，动态创建明天的分区，删除100天前的分区 删除指定range的数据 支持kerberos认证，内部的task也使用kerberos给的token进行内部验证 server之间、server和client之间TLS加密处理，不影响框架处理速度。如果都在本机，就不用走这个 三种访问角色：client、admin、service[给daemon使用的]。也有白名单之类 ksck工具链接到master查看整体状态 rack aware 严格结构化数据，不支持blob 多语言API 与spark和impala整合好的API 所有table都被分区，每个分区叫做tablet，每个tablet有多个备份，使用raft达成最终一致，都存在本地磁盘，并不基于HDFS。HDFS的机制不能支持分布式存储，不能快速处理failover，还有内存部分快速访问的设计，这里有些像kafka的思想。 metadata，把所有的metadata放在内存中，以便快速访问或处理 spark、impala、MR、flume、drill整合 给spark带来啥 像parquet的性能，而且insert和update都无延迟 pushdown predicate filter，更快更高效的扫描 相比parquet还有主键索引 在master中metadata的查找，精准找到目标partition 针对spark的优化 只读取有关的字段 自动将where转化为kudu predicate kudu predicate自动转化为主键扫描等 最佳实践可以在spark中注册临时表，然后针对临时表执行kudu查询的操作。 其他如果查询的数据在内存，那么kudu比parquet快，否则差不多会比parquet慢一倍。单条数据处理，虽然比较快了，但还是会比phenix慢很多。 场景 顺序或随机的读写 最小化数据延迟 时间序列数据 实时报表/实时数据仓库(ODS) 参考： https://www.youtube.com/watch?v=NEg1nn9UyBY https://www.youtube.com/watch?v=CLP6jHs2z14]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari源码解析-告警问题]]></title>
    <url>%2F2017%2F11%2F02%2Fambari%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%91%8A%E8%AD%A6%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[按照官网部署了一个简单的notice target脚本，就是把所有的脚本参数都输出到一个临时文件，然后发现虽然ambari server的web页面中有一大堆alert，但是实际触发alert_notice的很少，而且与web页面中看到的alert定义的check interval严重不符。到ambari-server的log里也没有找到heartbeat的连续信息之类。。。1echo $* &gt;&gt; /tmp/ambari_alerts 回顾一下上面的代码: 只有接收到AlertStateChangeEvent才会触发持久化到alert_notice表，也就是说只针对alert有状态变更才发送notice。尼玛，那如果报了一次之后，后面一直没有收到新的相关notice的话，就默认它是原状？？？ 细想好像这样也对，可以省掉最新。可是并不符合我们最初设想的持续告警。 另外还有一个问题，就是脚本参数里并不包含对应的host信息：123456datanode_heap_usage DataNode Heap Usage HDFS WARNING Used Heap:[81%, 1633.3276 MB], Max Heap: 2028.0 MBdatanode_heap_usage DataNode Heap Usage HDFS OK Used Heap:[78%, 1587.691 MB], Max Heap: 2028.0 MBdatanode_heap_usage DataNode Heap Usage HDFS WARNING Used Heap:[80%, 1623.7538 MB], Max Heap: 2028.0 MBdatanode_heap_usage DataNode Heap Usage HDFS OK Used Heap:[78%, 1584.6304 MB], Max Heap: 2028.0 MBdatanode_heap_usage DataNode Heap Usage HDFS OK Used Heap:[78%, 1577.2762 MB], Max Heap: 2028.0 MBdatanode_heap_usage DataNode Heap Usage HDFS WARNING Used Heap:[81%, 1650.835 MB], Max Heap: 2028.0 MB 看来需要一系列的代码改造才能完全满足我们的需求，就是每次收到heartbeat中的alert信息之后，根据优先级进行过滤，这个优先级最好是读取配置文件的。然后把所有的alert都持久化到alert_notice表中。如果要兼容官网原生的话，就在AlertReceivedListener的onAlertEvent的修改中加入一个option，比如1代表使用官网的notice，2代表使用自定义的notice，这个option可以是全局的，也可以是附属在alert_definition里的【后者需要修改alert_definition, 动作偏大，而且一般环境中只有一种告警方式就够了】。 写点儿伪代码，体会一下，先修改AlertReceivedListener.onAlertEvent。1234567891011121314151617181920212223242526....for (Map.Entry&lt;Alert, AlertCurrentEntity&gt; entry : toCreateHistoryAndMerge.entrySet()) &#123; Alert alert = entry.getKey(); AlertCurrentEntity entity = entry.getValue(); Long clusterId = getClusterIdByName(alert.getCluster()); boolean fire = false; if(m_configuration.getCustom_alert() &amp;&amp; alert.getState() == AlertState.CRITICAL) &#123; fire = true; &#125; else &#123; if (clusterId == null) &#123; //super rare case, cluster was removed after isValid() check LOG.error("Unable to process alert &#123;&#125; for an invalid cluster named &#123;&#125;", alert.getName(), alert.getCluster()); continue; &#125; fire = true; &#125; if (fire) &#123; AlertStateChangeEvent alertChangedEvent = new AlertStateChangeEvent(clusterId, alert, entity, oldStates.get(alert)); m_alertEventPublisher.publish(alertChangedEvent); &#125;&#125;... 其实按照编程命名规范，如果是所有的alert都要按照级别过滤后发送的话，最好自定义一种类型的event，然后再添加一个对应的eventListener。 鉴于上面提到的没有host信息，也要host加到dispatcher的参数中。我们整一下AlertScriptDispatcher, 发现他的接口参数AlertNotification中是包含hostname的 12345678910111213141516171819202122232425262728293031323334ProcessBuilder getProcessBuilder(String script, AlertNotification notification) &#123; final String shellCommand; final String shellCommandOption; if (SystemUtils.IS_OS_WINDOWS) &#123; shellCommand = &quot;cmd&quot;; shellCommandOption = &quot;/c&quot;; &#125; else &#123; shellCommand = &quot;sh&quot;; shellCommandOption = &quot;-c&quot;; &#125; AlertInfo alertInfo = notification.getAlertInfo(); AlertDefinitionEntity definition = alertInfo.getAlertDefinition(); String definitionName = definition.getDefinitionName(); AlertState alertState = alertInfo.getAlertState(); String serviceName = alertInfo.getServiceName(); // 这里 String hostname = alertInfo.getHostName(); // these could have spaces in them, so quote them so they don&apos;t mess up the // command line String alertLabel = &quot;\&quot;&quot; + SHELL_ESCAPE.escape(definition.getLabel()) + &quot;\&quot;&quot;; String alertText = &quot;\&quot;&quot; + SHELL_ESCAPE.escape(alertInfo.getAlertText()) + &quot;\&quot;&quot;; // 构建的参数里也加上 Object[] params = new Object[] &#123; script, definitionName, alertLabel, serviceName, hostname, alertState.name(), alertText &#125;; String foo = StringUtils.join(params, &quot; &quot;); // sh -c &apos;/foo/sys_logger.py ambari_server_agent_heartbeat &quot;Agent Heartbeat&quot; // AMBARI CRITICAL &quot;Something went wrong with the host&quot;&apos; return new ProcessBuilder(shellCommand, shellCommandOption, foo); &#125; 至此，改造完成。构建完成后，发布到ambari-server的对应位置，重启就可以了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari源码解析-告警流程]]></title>
    <url>%2F2017%2F10%2F31%2Fambari%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E5%91%8A%E8%AD%A6%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[从ambari的包找起，找到一个alert的package，下面有一个类StaleAlertRunnable，看一下注释：。那么谁调用它呢？AmbariServerAlertService，与它在同一个目录的还有一个名字很显眼的类AlertNoticeDispatchService。 它是用来扫描表alert_notice中状态为pending的记录，并调用各种dispatcher执行告警分发的。那么谁把notice放进这个表里的呢？没有追踪到。 不过我们发现它继承自guava的AbstractScheduledService类，那么就搜一下这个类的使用，发现在ControllerModule里出现，而ControllerModule是出现在AmbariServer.main之中的。 1Injector injector = Guice.createInjector(new ControllerModule()); 使用工具guice执行了ControllerModule中的依赖注入工作。我们知道Guice.createInjector接收AbstractModule接口的类，一般我们需要实现AbstractModule接口的configure方法。然后在configure方法中将依赖的接口与其具体实现进行绑定，也就是执行bind(xxx.class).to(xxx.class)等工作。下面我们就看一下ControllerModule()做了哪些依赖注入声明的工作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596protected void configure() &#123; // 各种工厂接口与其实现类的绑定 installFactories(); // 绑定session相关manager到具体实例 final SessionIdManager sessionIdManager = new HashSessionIdManager(); final SessionManager sessionManager = new HashSessionManager(); sessionManager.getSessionCookieConfig().setPath(&quot;/&quot;); sessionManager.setSessionIdManager(sessionIdManager); bind(SessionManager.class).toInstance(sessionManager); bind(SessionIdManager.class).toInstance(sessionIdManager); // kerberos相关 bind(KerberosOperationHandlerFactory.class); bind(KerberosDescriptorFactory.class); bind(KerberosServiceDescriptorFactory.class); bind(KerberosHelper.class).to(KerberosHelperImpl.class); bind(CredentialStoreService.class).to(CredentialStoreServiceImpl.class); bind(Configuration.class).toInstance(configuration); bind(OsFamily.class).toInstance(os_family); bind(HostsMap.class).toInstance(hostsMap); bind(PasswordEncoder.class).toInstance(new StandardPasswordEncoder()); bind(DelegatingFilterProxy.class).toInstance(new DelegatingFilterProxy() &#123; &#123; setTargetBeanName(&quot;springSecurityFilterChain&quot;); &#125; &#125;); bind(Gson.class).annotatedWith(Names.named(&quot;prettyGson&quot;)).toInstance(prettyGson); // jpa持久化相关 install(buildJpaPersistModule()); bind(Gson.class).in(Scopes.SINGLETON); bind(SecureRandom.class).in(Scopes.SINGLETON); bind(Clusters.class).to(ClustersImpl.class); bind(AmbariCustomCommandExecutionHelper.class); bind(ActionDBAccessor.class).to(ActionDBAccessorImpl.class); bindConstant().annotatedWith(Names.named(&quot;schedulerSleeptime&quot;)).to( configuration.getExecutionSchedulerWait()); // This time is added to summary timeout time of all tasks in stage // So it&apos;s an &quot;additional time&quot;, given to stage to finish execution before // it is considered as timed out bindConstant().annotatedWith(Names.named(&quot;actionTimeout&quot;)).to(600000L); bindConstant().annotatedWith(Names.named(&quot;dbInitNeeded&quot;)).to(dbInitNeeded); bindConstant().annotatedWith(Names.named(&quot;statusCheckInterval&quot;)).to(5000L); //ExecutionCommands cache size bindConstant().annotatedWith(Names.named(&quot;executionCommandCacheSize&quot;)). to(configuration.getExecutionCommandsCacheSize()); // Host role commands status summary max cache enable/disable bindConstant().annotatedWith(Names.named(HostRoleCommandDAO.HRC_STATUS_SUMMARY_CACHE_ENABLED)). to(configuration.getHostRoleCommandStatusSummaryCacheEnabled()); // Host role commands status summary max cache size bindConstant().annotatedWith(Names.named(HostRoleCommandDAO.HRC_STATUS_SUMMARY_CACHE_SIZE)). to(configuration.getHostRoleCommandStatusSummaryCacheSize()); // Host role command status summary cache expiry duration in minutes bindConstant().annotatedWith(Names.named(HostRoleCommandDAO.HRC_STATUS_SUMMARY_CACHE_EXPIRY_DURATION_MINUTES)). to(configuration.getHostRoleCommandStatusSummaryCacheExpiryDuration()); bind(AmbariManagementController.class).to( AmbariManagementControllerImpl.class); bind(AbstractRootServiceResponseFactory.class).to(RootServiceResponseFactory.class); bind(ExecutionScheduler.class).to(ExecutionSchedulerImpl.class); bind(DBAccessor.class).to(DBAccessorImpl.class); bind(ViewInstanceHandlerList.class).to(AmbariHandlerList.class); bind(TimelineMetricCacheProvider.class); bind(TimelineMetricCacheEntryFactory.class); bind(SecurityConfigurationFactory.class).in(Scopes.SINGLETON); bind(PersistedState.class).to(PersistedStateImpl.class); requestStaticInjection(ExecutionCommandWrapper.class); requestStaticInjection(DatabaseChecker.class); requestStaticInjection(KerberosChecker.class); // 这里是ControllerModule自定义的一个方法，根据注解执行绑定工作 bindByAnnotation(null); // 这里是我们关心的dispatcher相关绑定 bindNotificationDispatchers(); registerUpgradeChecks(); &#125; 我们发现serviceManager就在这个类中出现。它的说明是为需要注入的带有某种注解的接口提供初始化工作。典型应用一：是用来处理一些没有入口的单例，换种表达方式，就是那些没有被任何接口依赖注入，但是仍然被整体的guice框架需要的类。典型应用二：某些类需要注入静态static成员的时候。传入的参数beanDefinitions为空的时候，会默认扫描带有注解EagerSingleton,StaticallyInject,AmbariService的类。这三个注解都是ambari自定义的。我们看到上面调用这个方法的时候传入参数是null。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172protected Set&lt;BeanDefinition&gt; bindByAnnotation(Set&lt;BeanDefinition&gt; beanDefinitions) &#123; List&lt;Class&lt;? extends Annotation&gt;&gt; classes = Arrays.asList( EagerSingleton.class, StaticallyInject.class, AmbariService.class); if (null == beanDefinitions || beanDefinitions.size() == 0) &#123; ClassPathScanningCandidateComponentProvider scanner = new ClassPathScanningCandidateComponentProvider(false); // match only singletons that are eager listeners // 为scanner加入目标注解过滤器 for (Class&lt;? extends Annotation&gt; cls : classes) &#123; scanner.addIncludeFilter(new AnnotationTypeFilter(cls)); &#125; // 找出所有的带有注解的类 beanDefinitions = scanner.findCandidateComponents(AMBARI_PACKAGE); &#125; if (null == beanDefinitions || beanDefinitions.size() == 0) &#123; LOG.warn("No instances of &#123;&#125; found to register", classes); return beanDefinitions; &#125; Set&lt;com.google.common.util.concurrent.Service&gt; services = new HashSet&lt;com.google.common.util.concurrent.Service&gt;(); for (BeanDefinition beanDefinition : beanDefinitions) &#123; String className = beanDefinition.getBeanClassName(); Class&lt;?&gt; clazz = ClassUtils.resolveClassName(className, ClassUtils.getDefaultClassLoader()); if (null != clazz.getAnnotation(EagerSingleton.class)) &#123; bind(clazz).asEagerSingleton(); LOG.debug("Binding singleton &#123;&#125; eagerly", clazz); &#125; if (null != clazz.getAnnotation(StaticallyInject.class)) &#123; requestStaticInjection(clazz); LOG.debug("Statically injecting &#123;&#125; ", clazz); &#125; // Ambari services are registered with Guava // 使用Guava的AmbariService注解类 if (null != clazz.getAnnotation(AmbariService.class)) &#123; // 看看是不是一个Guava service if (!AbstractScheduledService.class.isAssignableFrom(clazz)) &#123; String message = MessageFormat.format( "Unable to register service &#123;0&#125; because it is not an AbstractScheduledService", clazz); LOG.warn(message); throw new RuntimeException(message); &#125; // 实例化，并作为单例注入到guice中，然后再添加到services中 AbstractScheduledService service = null; try &#123; service = (AbstractScheduledService) clazz.newInstance(); bind((Class&lt;AbstractScheduledService&gt;) clazz).toInstance(service); services.add(service); LOG.debug("Registering service &#123;&#125; ", clazz); &#125; catch (Exception exception) &#123; LOG.error("Unable to register &#123;&#125; as a service", clazz, exception); throw new RuntimeException(exception); &#125; &#125; &#125; // 使用上面扫描完获取到的所有services，实例化serviceManager，并单例化注入guice ServiceManager manager = new ServiceManager(services); bind(ServiceManager.class).toInstance(manager); return beanDefinitions; &#125; ServiceManager作为AmbariServer的依赖项被注入，然后在run方法中被调用其startAsync方法，我们看到其实是遍历调用了services中的service的startAsync方法：12345678910111213public ServiceManager startAsync() &#123; for (Service service : services) &#123; State state = service.state(); checkState(state == NEW, &quot;Service %s is %s, cannot start it.&quot;, service, state); &#125; for (Service service : services) &#123; try &#123; service.startAsync(); &#125; catch (IllegalStateException e) &#123; &#125; &#125; return this;&#125; 参考：https://github.com/google/guice/wiki/GettingStarted 看了这么多，但是实际alert来源呢？一般都是agent在本地执行检查，符合告警规则的才发送给server端。那么是怎样发送到server的呢？首先想到的是rest api里的AlertNoticeService，可是这个类只有get方法，没有POST接口。那么下一个嫌疑就是HeartBeat信息了。看下HeartBeat的定义12345678910private long responseId = -1;private long timestamp;private String hostname;List&lt;CommandReport&gt; reports = new ArrayList&lt;CommandReport&gt;();List&lt;ComponentStatus&gt; componentStatus = new ArrayList&lt;ComponentStatus&gt;();private List&lt;DiskInfo&gt; mounts = new ArrayList&lt;DiskInfo&gt;();HostStatus nodeStatus;private AgentEnv agentEnv = null;private List&lt;Alert&gt; alerts = null;private RecoveryReport recoveryReport; 果然，那么我们就也能找到HeartBeat相关的resource了：AgentResource。HeartBeatHandler处理过程中会把heartbeat放进HeartbeatProcessor的队列里，做异步处理，告警部分的持久化就在这个异步处理的代码里了。1234567891011121314151617181920212223@Path("heartbeat/&#123;hostName&#125;") @POST @Consumes(MediaType.APPLICATION_JSON) @Produces(&#123;MediaType.APPLICATION_JSON&#125;) public HeartBeatResponse heartbeat(HeartBeat message) throws WebApplicationException &#123; if (LOG.isDebugEnabled()) &#123; LOG.debug("Received Heartbeat message " + message); &#125; HeartBeatResponse heartBeatResponse; try &#123; // hh是一个单例的HeartBeatHandler heartBeatResponse = hh.handleHeartBeat(message); if (LOG.isDebugEnabled()) &#123; LOG.debug("Sending heartbeat response with response id " + heartBeatResponse.getResponseId()); LOG.debug("Response details " + heartBeatResponse); &#125; &#125; catch (Exception e) &#123; LOG.warn("Error in HeartBeat", e); throw new WebApplicationException(500); &#125; return heartBeatResponse; &#125; 中间经过guava的eventbus的多次事件中转后，在AlertStateChangedListener中被完全持久化进入数据库中。 123456789101112131415161718192021222324public void onAlertEvent(AlertStateChangeEvent event) &#123; ... ... for (AlertGroupEntity group : groups) &#123; Set&lt;AlertTargetEntity&gt; targets = group.getAlertTargets(); if (null == targets || targets.size() == 0) &#123; continue; &#125; for (AlertTargetEntity target : targets) &#123; if (!isAlertTargetInterested(target, history)) &#123; continue; &#125; AlertNoticeEntity notice = new AlertNoticeEntity(); notice.setUuid(UUID.randomUUID().toString()); notice.setAlertTarget(target); notice.setAlertHistory(event.getNewHistoricalEntry()); notice.setNotifyState(NotificationState.PENDING); notices.add(notice); &#125; &#125; m_alertsDispatchDao.createNotices(notices); &#125; 启动HeartBeatHandler的地方出现在AmbariServer.run的代码中123....AgentResource.statHeartBeatHandler();.... 至于agent那边具体实现，我们暂时不太关心，因为暂时不需要自定义alert。 综上，对于告警渠道的添加，我们需要自己实现guava的AbstractScheduledService抽象类接口，依照既有的SNNP或者EMAIL等写Dispatcher，然后把它们放置到ServiceManager.AMBARI_PACKAGE的位置。然后，最好是通过ambari提供的REST API添加alert_target，因为要处理跟alert_group相关的事情，自己直接操作数据库比较复杂一些，基本上要复制一遍代码里的所有操作。 大功告成！]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari源码解析-资源状态监听]]></title>
    <url>%2F2017%2F10%2F31%2Fambari%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E8%B5%84%E6%BA%90%E7%8A%B6%E6%80%81%E7%9B%91%E5%90%AC%2F</url>
    <content type="text"><![CDATA[AmbariServie.run()入口处找到1234...LOG.info(&quot;********* Reconciling Alert Definitions **********&quot;);ambariMetaInfo.reconcileAlertDefinitions(clusters);... 调用了AmbariMetaInfo.reconcileAlertDefinitions(Clusters clusters)方法，这个方法主要负责比较stack中定义的alert和数据库中的alert有没有什么变化。它会首先确认一下alert定义所在的service是不是已经被安装了，没有安装的就没有必要关心了。 这个方法还会监测agent定义的alert，这些应该在agent host上运行。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171public void reconcileAlertDefinitions(Clusters clusters) throws AmbariException &#123; Map&lt;String, Cluster&gt; clusterMap = clusters.getClusters(); if (null == clusterMap || clusterMap.size() == 0) &#123; return; &#125; // 遍历cluster for (Cluster cluster : clusterMap.values()) &#123; long clusterId = cluster.getClusterId(); StackId stackId = cluster.getDesiredStackVersion(); StackInfo stackInfo = getStack(stackId.getStackName(), stackId.getStackVersion()); // 创建service/conponent和名字的映射，方便查找 Collection&lt;ServiceInfo&gt; stackServices = stackInfo.getServices(); Map&lt;String, ServiceInfo&gt; stackServiceMap = new HashMap&lt;String, ServiceInfo&gt;(); Map&lt;String, ComponentInfo&gt; stackComponentMap = new HashMap&lt;String, ComponentInfo&gt;(); for (ServiceInfo stackService : stackServices) &#123; stackServiceMap.put(stackService.getName(), stackService); List&lt;ComponentInfo&gt; components = stackService.getComponents(); for (ComponentInfo component : components) &#123; stackComponentMap.put(component.getName(), component); &#125; &#125; Map&lt;String, Service&gt; clusterServiceMap = cluster.getServices(); Set&lt;String&gt; clusterServiceNames = clusterServiceMap.keySet(); // 对于当前cluster上已经安装的所有service，获取其metainfo和alert定义， 添加到stackDefinitions List&lt;AlertDefinition&gt; stackDefinitions = new ArrayList&lt;AlertDefinition&gt;(50); for (String clusterServiceName : clusterServiceNames) &#123; ServiceInfo stackService = stackServiceMap.get(clusterServiceName); if (null == stackService) &#123; continue; &#125; Set&lt;AlertDefinition&gt; serviceDefinitions = getAlertDefinitions(stackService); stackDefinitions.addAll(serviceDefinitions); &#125; // 抽取数据库中所有的alert定义，把list转化为以alert定义名字为key的map List&lt;AlertDefinitionEntity&gt; persist = new ArrayList&lt;AlertDefinitionEntity&gt;(); List&lt;AlertDefinitionEntity&gt; entities = alertDefinitionDao.findAll(clusterId); Map&lt;String, AlertDefinitionEntity&gt; mappedEntities = new HashMap&lt;String, AlertDefinitionEntity&gt;(100); for (AlertDefinitionEntity entity : entities) &#123; mappedEntities.put(entity.getDefinitionName(), entity); &#125; // 对每个stack definition，查看是否存在，如果存在，就详细比较一下变化 for( AlertDefinition stackDefinition : stackDefinitions )&#123; AlertDefinitionEntity entity = mappedEntities.get(stackDefinition.getName()); // 如果是null，就代表是新的 if (null == entity) &#123; entity = alertDefinitionFactory.coerce(clusterId, stackDefinition); persist.add(entity); continue; &#125; // 修改过的stack中的定义不会被覆盖，需要使用REST API来修改 AlertDefinition databaseDefinition = alertDefinitionFactory.coerce(entity); // 如果stackfinition与数据库中不相等....暂时没有处理任何东西，只是打了个log if (!stackDefinition.deeplyEquals(databaseDefinition)) &#123; // this is the code that would normally merge the stack definition // into the database; this is not the behavior we want today // entity = alertDefinitionFactory.merge(stackDefinition, entity); // persist.add(entity); LOG.debug( "The alert named &#123;&#125; has been modified from the stack definition and will not be merged", stackDefinition.getName()); &#125; &#125; // 把ambari agent主机上的alert定义添加到持久化列表里 List&lt;AlertDefinition&gt; agentDefinitions = ambariServiceAlertDefinitions.getAgentDefinitions(); for (AlertDefinition agentDefinition : agentDefinitions) &#123; AlertDefinitionEntity entity = mappedEntities.get(agentDefinition.getName()); // no entity means this is new; create a new entity if (null == entity) &#123; entity = alertDefinitionFactory.coerce(clusterId, agentDefinition); persist.add(entity); &#125; &#125; // 把ambari server 的alert定义添加到持久化列表里 List&lt;AlertDefinition&gt; serverDefinitions = ambariServiceAlertDefinitions.getServerDefinitions(); for (AlertDefinition serverDefinition : serverDefinitions) &#123; AlertDefinitionEntity entity = mappedEntities.get(serverDefinition.getName()); // no entity means this is new; create a new entity if (null == entity) &#123; entity = alertDefinitionFactory.coerce(clusterId, serverDefinition); persist.add(entity); &#125; &#125; // 持久化新的，或者需要更新的alert定义 for (AlertDefinitionEntity entity : persist) &#123; if (LOG.isDebugEnabled()) &#123; LOG.info("Merging Alert Definition &#123;&#125; into the database", entity.getDefinitionName()); &#125; alertDefinitionDao.createOrUpdate(entity); &#125; // all definition resolved; publish their registration // 所有的定义都处理完成后，发布它们的注册event也就是AlertDefinitionRegistrationEvent到eventPublisher for (AlertDefinitionEntity def : alertDefinitionDao.findAll(cluster.getClusterId())) &#123; AlertDefinition realDef = alertDefinitionFactory.coerce(def); AlertDefinitionRegistrationEvent event = new AlertDefinitionRegistrationEvent( cluster.getClusterId(), realDef); eventPublisher.publish(event); &#125; // for every definition, determine if the service and the component are // still valid; if they are not, disable them - this covers the case // with STORM/REST_API where that component was removed from the // stack but still exists in the database - we disable the alert to // preserve historical references List&lt;AlertDefinitionEntity&gt; definitions = alertDefinitionDao.findAllEnabled(clusterId); List&lt;AlertDefinitionEntity&gt; definitionsToDisable = new ArrayList&lt;AlertDefinitionEntity&gt;(); for (AlertDefinitionEntity definition : definitions) &#123; String serviceName = definition.getServiceName(); String componentName = definition.getComponentName(); // the AMBARI service is special, skip it here if (Services.AMBARI.name().equals(serviceName)) &#123; continue; &#125; if (!stackServiceMap.containsKey(serviceName)) &#123; LOG.info( "The &#123;&#125; service has been marked as deleted for stack &#123;&#125;, disabling alert &#123;&#125;", serviceName, stackId, definition.getDefinitionName()); definitionsToDisable.add(definition); &#125; else if (null != componentName &amp;&amp; !stackComponentMap.containsKey(componentName)) &#123; LOG.info( "The &#123;&#125; component &#123;&#125; has been marked as deleted for stack &#123;&#125;, disabling alert &#123;&#125;", serviceName, componentName, stackId, definition.getDefinitionName()); definitionsToDisable.add(definition); &#125; &#125; // disable definitions and fire the event // 将alert定义diable掉，并发送这个event到eventPublisher for (AlertDefinitionEntity definition : definitionsToDisable) &#123; definition.setEnabled(false); alertDefinitionDao.merge(definition); AlertDefinitionDisabledEvent event = new AlertDefinitionDisabledEvent( clusterId, definition.getDefinitionId()); eventPublisher.publish(event); &#125; &#125; &#125; 好了，上面看到所有的alert定义都处理完后，会发送一个AlertDefinitionRegistrationEvent事件到AmbariEventPublisher。那么我们看下有谁监听了AmbariEventPublisher的AlertDefinitionRegistrationEvent事件就可以了吧。然后就定位到了AlertLifecycleListener。 尼玛，这好像不是具体执行alert的地方。。。。而是管理ambari中各种资源的变化情况的地方。具体可以看下AmbariEvent的实现类列表~~~基本都是维护ambari中各种概念的状态变化的。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari源码解析-REST解析]]></title>
    <url>%2F2017%2F10%2F31%2Fambari%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-REST%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[先看下入口的地方, ambariServer.run()：12345678910111213ServletHolder sh = new ServletHolder(ServletContainer.class);sh.setInitParameter("com.sun.jersey.config.property.resourceConfigClass", "com.sun.jersey.api.core.PackagesResourceConfig");sh.setInitParameter("com.sun.jersey.config.property.packages","org.apache.ambari.server.api.rest;" + "org.apache.ambari.server.api.services;" + "org.apache.ambari.eventdb.webservice;" + "org.apache.ambari.server.api");sh.setInitParameter("com.sun.jersey.api.json.POJOMappingFeature", "true");root.addServlet(sh, "/api/v1/*");sh.setInitOrder(2); 就是ServletHolder.setInitParameter里com.sun.jersey.config.property.packages对应所有的包路径下的类都被jettyserver看作rest资源，然后所有的带有jersey的REST注解的方法，就都是对应一个path的了。 然后看下ambari自己定制的东西，拿我关注的alert_targets接口来看,对应类为AlertTargetService，看POST方法1234567891011121314151617@POST@Produces("text/plain")public Response createTarget(String body, @Context HttpHeaders headers, @Context UriInfo ui) &#123; return handleRequest(headers, body, ui, Request.Type.POST, createAlertTargetResource(null));&#125;private ResourceInstance createAlertTargetResource(Long targetId) &#123; Map&lt;Resource.Type, String&gt; mapIds = new HashMap&lt;Resource.Type, String&gt;(); mapIds.put(Resource.Type.AlertTarget, null == targetId ? null : targetId.toString()); return createResource(Resource.Type.AlertTarget, mapIds);&#125; ambari中所有的Service都继承自BaseService，上面的createAlertTargetResource调用的createResource以及handleRequest都是继承自BaseService的。 BaseService的createResource方法实现，下面的type应该是Resource.Type.AlertTarget，是一个枚举值。12345678910public ResourceInstance createResource(Resource.Type type, Map&lt;Resource.Type, String&gt; mapIds) &#123; /** * 找到type对应的ResourceDefinition， 这里返回的是AlertTargetResourceDefinition */ ResourceDefinition resourceDefinition = getResourceDefinition(type, mapIds); // 使用AlertTargetResourceDefinition构建QueryImpl并返回，mapIds为null return new QueryImpl(mapIds, resourceDefinition, ClusterControllerHelper.getClusterController()); &#125; 再看下BaseService的handleRequest方法, 通过这个方法可以让所有的请求处理都通过一个公用逻辑。它会创建一个request实例，然后触发它的process方法。123456789101112131415161718192021222324252627protected Response handleRequest(HttpHeaders headers, String body, UriInfo uriInfo, Request.Type requestType, MediaType mediaType, ResourceInstance resource) &#123; // 结果 Result result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.OK)); try &#123; // 解析请求内容，使用JsonRequestBodyParser执行解析，其实就是解析成一系列kv Set&lt;RequestBody&gt; requestBodySet = getBodyParser().parse(body); Iterator&lt;RequestBody&gt; iterator = requestBodySet.iterator(); while (iterator.hasNext() &amp;&amp; result.getStatus().getStatus().equals(ResultStatus.STATUS.OK)) &#123; RequestBody requestBody = iterator.next(); // 使用RequestFactory创建request，我们会调用createPostRequest方法 Request request = getRequestFactory().createRequest( headers, requestBody, uriInfo, requestType, resource); // 执行request result = request.process(); &#125; &#125; .... .... return builder.build(); &#125; 创建request的代码部分： 123456789private Request createPostRequest(HttpHeaders headers, RequestBody body, UriInfo uriInfo, ResourceInstance resource) &#123; boolean batchCreate = !applyDirectives(Request.Type.POST, body, uriInfo, resource);; // 如果是批处理，就返回QueryPostRequest, 否则返回PostRequest，我们这里返回的是PostRequest // PostRequest会与一个CreateHandler一一对应，作为它的handler存在 return (batchCreate) ? new QueryPostRequest(headers, body, uriInfo, resource) : new PostRequest(headers, body, uriInfo, resource); &#125; request的process部分，那么就看PostRequest的实现，发现它继承了BaseRequest，实际调用的是BaseRequest的process方法：12345678910111213141516Result result;try &#123; parseRenderer(); parseQueryPredicate(); result = getRequestHandler().handleRequest(this);&#125; catch (InvalidQueryException e) &#123; result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, "Unable to compile query predicate: " + e.getMessage()));&#125; catch (IllegalArgumentException e) &#123; result = new ResultImpl(new ResultStatus(ResultStatus.STATUS.BAD_REQUEST, "Invalid Request: " + e.getMessage()));&#125;if (! result.getStatus().isErrorState()) &#123; getResultPostProcessor().process(result);&#125; BaseManagementHandler中又调用了persist方法1234567891011@Overridepublic Result handleRequest(Request request) &#123; Query query = request.getResource().getQuery(); Predicate queryPredicate = request.getQueryPredicate(); query.setRenderer(request.getRenderer()); if (queryPredicate != null) &#123; query.setUserPredicate(queryPredicate); &#125; return persist(request.getResource(), request.getBody());&#125; persist(ResourceInstance resource, RequestBody body)方法在CreateHandler中有实现, 代码中的PersistenceManager只有一个实现PersistenceManagerImpl。12RequestStatus status = getPersistenceManager().create(resource, body);result = createResult(status); PersistenceManagerImpl的create(ResourceInstance resource, RequestBody requestBody)相关部分。1234567891011121314151617181920212223242526// 获取resource对应的主键、外键Map&lt;Resource.Type, String&gt; mapResourceIds = resource.getKeyValueMap();Resource.Type type = resource.getResourceDefinition().getType();Schema schema = m_controller.getSchema(type);// 获取body中解析的各种键值Set&lt;NamedPropertySet&gt; setProperties = requestBody.getNamedPropertySets();if (setProperties.isEmpty()) &#123;requestBody.addPropertySet(new NamedPropertySet(&quot;&quot;, new HashMap&lt;String, Object&gt;()));&#125;// 遍历body中的键值for (NamedPropertySet propertySet : setProperties) &#123;for (Map.Entry&lt;Resource.Type, String&gt; entry : mapResourceIds.entrySet()) &#123; Map&lt;String, Object&gt; mapProperties = propertySet.getProperties(); // schema中组合了ResourceProvider，这里就是AlertTargetResourceProvider，getKeyPropertyId方法是调用AlertTargetResourceProvider获取可以唯一确认一条记录的唯一键 // AlertTarget中定义的唯一键是&quot;AlertTarget/id&quot;和&quot;AlertTarget/name&quot; String property = schema.getKeyPropertyId(entry.getKey()); // 如果不包含唯一键，就添加进入mapProperties if (!mapProperties.containsKey(property)) &#123; mapProperties.put(property, entry.getValue()); &#125;&#125;&#125;// 创建resourcereturn m_controller.createResources(type, createControllerRequest(requestBody)); 最后一步就是调用了ClusterControllerImpl的createResources(Type type, Request request)方法1234567891011public RequestStatus createResources(Type type, Request request)&#123; // 获取对应的ResrouceProvider，如果找不到就用DefaultProviderModule通过type去createResourceProvider对应的ResrouceProvider，我们这里对应的是上面提到的AlertTargetResourceProvider ResourceProvider provider = ensureResourceProvider(type); if (provider != null) &#123; checkProperties(type, request, null); return provider.createResources(request); &#125; return null;&#125; 最后一步AlertTargetResourceProvider, 然后会调用到AlertDispatchDAO的create(AlertTargetEntity alertTarget)方法，就入库了。12345678910111213141516public RequestStatus createResources(final Request request) throws SystemException, UnsupportedPropertyException, ResourceAlreadyExistsException, NoSuchParentResourceException &#123; createResources(new Command&lt;Void&gt;() &#123; @Override public Void invoke() throws AmbariException &#123; createAlertTargets(request.getProperties(), request.getRequestInfoProperties()); return null; &#125; &#125;); notifyCreate(Resource.Type.AlertTarget, request); return getRequestStatus(null);&#125; 细看一下，是个事务接口1234567891011121314public void create(AlertTargetEntity alertTarget) &#123; // 持久化target到数据库 entityManagerProvider.get().persist(alertTarget); // 如果这个target是global的，就遍历所有的group，与这个target关联起来 // 就是更新数据库中的关联关系 if (alertTarget.isGlobal()) &#123; List&lt;AlertGroupEntity&gt; groups = findAllGroups(); for (AlertGroupEntity group : groups) &#123; group.addAlertTarget(alertTarget); merge(group); &#125; &#125;&#125; 感想没有看到什么特别的好处，整个处理过程特别绕…如果只是为了统一处理请求的话，使用spring的filter之类的不能搞定么？为什么要弄一个handleRequest的抽象方法呢？]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari源码解析-自定义告警]]></title>
    <url>%2F2017%2F10%2F31%2Fambari%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%91%8A%E8%AD%A6%2F</url>
    <content type="text"><![CDATA[ambari原生支持两种告警手段：邮件告警、SNMP告警。 个人主要是研发，对SNMP一直不太了解，试着搜了一些资料，稍显麻烦，而且目前我们的数据集群运维工作中并没有使用到SNMP相关的工具，所以暂时并不认为必要。 我们期望的告警方式是短信告警，具有最高的时效性，已经具有了短信告警接口。所以下一步就是在ambari这边找到调用此接口，并传送有效告警信息的方式。 翻阅文档，发现amabri也是支持脚本告警的。单独看文档稍微有些懵逼，还是看下对应源码吧。 关键字dispatcher，找到对应的类AlertScriptDispatcher。 1234567891011121314/** * 在amabri.properties中配置执行告警的脚本的配置项 */ public static final String SCRIPT_CONFIG_DEFAULT_KEY = &quot;notification.dispatch.alert.script&quot;; /** * 在amabri.properties中配置以毫秒为单位的执行告警超时的脚本的配置项，默认是5000，也就是5s */ public static final String SCRIPT_CONFIG_TIMEOUT_KEY = &quot;notification.dispatch.alert.script.timeout&quot;; /** * 用来定义指定执行告警脚本的配置项的关键字，如果没有配置的话，就默认是SCRIPT_CONFIG_DEFAULT_KEY[也就是notification.dispatch.alert.script] */ public static final String DISPATCH_PROPERTY_SCRIPT_CONFIG_KEY = &quot;ambari.dispatch-property.script&quot;; 调用逻辑123456789101112131415161718public void dispatch(Notification notification) &#123; String scriptKey = getScriptConfigurationKey(notification); String script = m_configuration.getProperty(scriptKey); ..... ..... // execute the script asynchronously long timeout = getScriptConfigurationTimeout(); TimeUnit timeUnit = TimeUnit.MILLISECONDS; AlertNotification alertNotification = (AlertNotification) notification; ProcessBuilder processBuilder = getProcessBuilder(script, alertNotification); AlertScriptRunnable runnable = new AlertScriptRunnable(alertNotification, script, processBuilder, timeout, timeUnit); m_executor.execute(runnable); &#125; 脚步组装123456789101112131415161718192021222324252627282930ProcessBuilder getProcessBuilder(String script, AlertNotification notification) &#123; final String shellCommand; final String shellCommandOption; if (SystemUtils.IS_OS_WINDOWS) &#123; shellCommand = "cmd"; shellCommandOption = "/c"; &#125; else &#123; shellCommand = "sh"; shellCommandOption = "-c"; &#125; AlertInfo alertInfo = notification.getAlertInfo(); AlertDefinitionEntity definition = alertInfo.getAlertDefinition(); String definitionName = definition.getDefinitionName(); AlertState alertState = alertInfo.getAlertState(); String serviceName = alertInfo.getServiceName(); .... .... Object[] params = new Object[] &#123; script, definitionName, alertLabel, serviceName, alertState.name(), alertText &#125;; String foo = StringUtils.join(params, " "); // 示例 // sh -c '/foo/sys_logger.py ambari_server_agent_heartbeat "Agent Heartbeat" // AMBARI CRITICAL "Something went wrong with the host"' return new ProcessBuilder(shellCommand, shellCommandOption, foo); &#125; 对应了文档中123456789101112# :param definitionName: alert的唯一名字# :param definitionLabel: 可读性强的alert标签# :param serviceName: 告警对应的service名称# :param alertState: 告警状态 (OK, WARNING, etc)# :param alertText: 告警内容 def main(): definitionName = sys.argv[1] definitionLabel = sys.argv[2] serviceName = sys.argv[3] alertState = sys.argv[4] alertText = sys.argv[5] 负责告警触发的类是AlertNoticeDispatchService。它会扫描数据表alert_notice中的pending状态的告警实例，通过告警分发系统处理这些告警实例。然后分发系统会回调AlertNoticeDispatchCallback处理告警实例的状态更新。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql从库crash]]></title>
    <url>%2F2017%2F10%2F27%2Fmysql%E4%BB%8E%E5%BA%93crash%2F</url>
    <content type="text"><![CDATA[mysql主从架构，从库crash掉。恢复以后，出现主从同步错误问题。 1234567891011121314 2017-10-27 16:23:38 2098 [ERROR] /server/mysql/bin/mysqld: Table &apos;./mysql/proc&apos; is marked as crashed and should be repaired2017-10-27 16:23:38 2098 [Warning] Checking table: &apos;./mysql/proc&apos;2017-10-27 16:23:47 2098 [Warning] Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: NO)2017-10-27 16:30:03 2098 [Warning] Access denied for user &apos;root&apos;@&apos;localhost&apos; (using password: NO)2017-10-27 16:31:06 2098 [Warning] Slave SQL: If a crash happens this configuration does not guarantee that the relay log info will be consistent, Error_code: 02017-10-27 16:31:06 2098 [Note] Slave SQL thread initialized, starting replication in log &apos;mysql-bin.004001&apos; at position 1000175864, relay log &apos;./prd-mysql03-relay-bin.000994&apos; position: 41452017-10-27 16:31:06 2098 [ERROR] Slave SQL: Could not execute Write_rows event on table YKX_DW.JLC_APP_MARKPOINT_REALTIME; Duplicate entry &apos;212630&apos; for key &apos;PRIMARY&apos;, Error_code: 1062; handler error HA_ERR_FOUND_DUPP_KEY; the event&apos;s master log mysql-bin.004001, end_log_pos 1000176163, Error_code: 10622017-10-27 16:31:06 2098 [Warning] Slave: Duplicate entry &apos;212630&apos; for key &apos;PRIMARY&apos; Error_code: 10622017-10-27 16:31:06 2098 [ERROR] Error running query, slave SQL thread aborted. Fix the problem, and restart the slave SQL thread with &quot;SLAVE START&quot;. We stopped at log &apos;mysql-bin.004001&apos; position 1000175864 之后show slave status12Slave_IO_Running: Yes Slave_SQL_Running: No 先stop slave，然后执行了一下提示的语句，就是把重复的主键记录删除，再SET GLOBAL SQL_SLAVE_SKIP_COUNTER=1; START SLAVE; 参考：http://www.linuxidc.com/Linux/2017-02/141056.htm http://blog.csdn.net/donghaixiaolongwang/article/details/74923971]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对于spark-streaming通过runningdata运行的简单规划]]></title>
    <url>%2F2017%2F10%2F26%2F%E5%AF%B9%E4%BA%8Espark-streaming%E9%80%9A%E8%BF%87runningdata%E8%BF%90%E8%A1%8C%E7%9A%84%E7%AE%80%E5%8D%95%E8%A7%84%E5%88%92%2F</url>
    <content type="text"><![CDATA[提交流程 添加spark jar 添加参数 配置调度 后台流程 扫描updatetime，检测新更新的spark streaming程序 检查jar包修改，将修改后的jar包重新上传到HDFS/mesos集群公共访问部分 创建/修改marathon app配置 重启/启动marathon app 日志检查这方面其实marathon已经做得很好了。可以直接拼接marathon的日志API搞定。 marathon api1http://10.2.19.125:5051/files/download?path=/server/mesos/agent/slaves/5fc969e7-c2da-4435-a0cb-278240c52f25-S2/frameworks/d2f4a97d-de35-4d1b-bf6d-07b8f2ad7666-0000/executors/willsparkstreamingtest01.cfcd8dde-ba34-11e7-9beb-3e15ac098cb2/runs/1128e325-ad92-4266-8348-04f43cfaa4c4/stderr mesos api1http://10.2.19.124:5050/#/agents/5fc969e7-c2da-4435-a0cb-278240c52f25-S2/browse?path=/server/mesos/agent/slaves/5fc969e7-c2da-4435-a0cb-278240c52f25-S2/frameworks/d2f4a97d-de35-4d1b-bf6d-07b8f2ad7666-0000/executors/willsparkstreamingtest01.cfcd8dde-ba34-11e7-9beb-3e15ac098cb2/runs/latest mesos 增量 1http://10.2.19.125:5051/files/read?path=/server/mesos/agent/slaves/5fc969e7-c2da-4435-a0cb-278240c52f25-S2/frameworks/d2f4a97d-de35-4d1b-bf6d-07b8f2ad7666-0000/executors/willsparkstreamingtest01.cfcd8dde-ba34-11e7-9beb-3e15ac098cb2/runs/latest/stdout&amp;offset=115771&amp;length=50000&amp;jsonp=jQuery17109253805122640539_1509014182353&amp;_=1509014197811 mesos 下载1http://10.2.19.125:5051/files/download?path=/server/mesos/agent/slaves/5fc969e7-c2da-4435-a0cb-278240c52f25-S2/frameworks/d2f4a97d-de35-4d1b-bf6d-07b8f2ad7666-0000/executors/willsparkstreamingtest01.cfcd8dde-ba34-11e7-9beb-3e15ac098cb2/runs/latest/stdout 因为有增量，所以我们会优先考虑过滤改造mesos的API。 对于上面所有的路径信息，主要就是framework的id，mesos salve的id，task的id。 slaveid, taskid可以通过marathon的task接口获取到。1curl http://10.2.19.124:8080/v2/apps/willsparkstreamingtest01 framework的ID可以去mesos的api里找到。其实后面这个就包含了上面的所有信息了。 另外，考虑到可能出现程序错误，导致app在marathon上不停部署，那么此时我们是需要获取前面失败的log的。失败log其实也是一次部署的log，所以路径上没有什么区别，但是可能taskid会有所变化，仔细观察上面的数据里其实是有lastTaskFailure的信息的，可以找到上个失败task的id，进而找到失败的log。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari添加异构OS]]></title>
    <url>%2F2017%2F10%2F26%2Fambari%E6%B7%BB%E5%8A%A0%E5%BC%82%E6%9E%84OS%2F</url>
    <content type="text"><![CDATA[首先声明，HDFS集群中肯定是最好不要有异构机器存在的，会造成很多兼容性问题。 背景 系统 作用 数量 centos6 yarn/HDFS等服务的选定集群，主要用来运行日常任务 50 centos7 marathon/docker所在的小集群，预想用来调度任务 5 其实我们已经基于centos6.8 final有了一个比较稳定的集群，上面通过ambari安装并稳健运行着HDFS、MR、Yarn、Spark、HBase等大数据服务。但是我们想要将spark streaming、hive等ETL工作的客户端放置在mesos上运行，因为当这些工作比较多的时候，也同样需要资源管理工作。鉴于mesos的资源管理相对yarn来说更加可定制化、而且已经有很多开源且稳定的framework可用，我们就选用mesos来做这个客户端集群的工作。对于long time的spark streaming，我们期望使用marathon来管理。一段时间的hive ETL任务，可以使用docker或者chronos来执行。 鉴于以上计划，我们需要在mesos的agent机器上安装spark、hive等客户端配置文件。考虑到易操作性和软件的版本统一兼容问题，决定继续使用ambari在mesos agent上安装这些软件的client端，然后就可以直接调用了。 问题在添加新host的过程中，ambari会比较智能的告诉我们，新的OS与既有的集群OS可能存在不兼容的问题，并阻止我们进一步执行。 处理考虑到我们只是使用客户端，而且我们的程序客户端是基于java的，具有可移植性。所以我就去找到ambari-server在新增机器的时候，对于OS的检测脚本，暂时注释掉，添加完新的host之后，再放开。 2.2.2.0的脚本位置:/usr/lib/python2.6/site-packages/ambari_server/bootstrap.py：123456789101112131415161718def run(self): &quot;&quot;&quot; Copy files and run commands on remote host &quot;&quot;&quot; self.status[&quot;start_time&quot;] = time.time() # Population of action queue action_queue = [self.createTargetDir, self.copyCommonFunctions, self.copyOsCheckScript, self.runOsCheckScript, self.checkSudoPackage ] if self.hasPassword(): action_queue.extend([self.copyPasswordFile, self.changePasswordFileModeOnHost]) action_queue.extend([ self.copyNeededFiles, self.runSetupAgent, ]) .... 把action_queue里的OSCheck相关的暂时注释掉就可以了。然后去添加host的界面上点击retry failed。只要注册成功就可以了。 install过程跳过去了，但是注册的时候出现了问题。 报错：123456INFO 2017-10-26 16:14:42,747 NetUtil.py:60 - Connecting to https://namenode01.will.com:8440/caERROR 2017-10-26 16:14:42,848 NetUtil.py:84 - [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:579)ERROR 2017-10-26 16:14:42,848 NetUtil.py:85 - SSLError: Failed to connect. Please check openssl library versions. Refer to: https://bugzilla.redhat.com/show_bug.cgi?id=1022468 for more details.WARNING 2017-10-26 16:14:42,850 NetUtil.py:112 - Server at https://namenode01.will.com:8440 is not reachable, sleeping for 10 seconds...&apos;, None) 在新的host上找到ambari-agent相关代码：1234567891011121314151617181920212223242526def checkURL(self, url): &quot;&quot;&quot;Try to connect to a given url. Result is True if url returns HTTP code 200, in any other case (like unreachable server or wrong HTTP code) result will be False. Additionally returns body of request, if available &quot;&quot;&quot; logger.info(&quot;Connecting to &quot; + url) responseBody = &quot;&quot; try: parsedurl = urlparse(url) if sys.version_info &gt;= (2,7,9): import ssl ca_connection = httplib.HTTPSConnection(parsedurl[1], context=ssl._create_unverified_context()) else: ca_connection = httplib.HTTPSConnection(parsedurl[1]) ... except SSLError as slerror: logger.error(str(slerror)) logger.error(ERROR_SSL_WRONG_VERSION) return False, responseBody except Exception, e: logger.warning(&quot;Failed to connect to &quot; + str(url) + &quot; due to &quot; + str(e) + &quot; &quot;) return False, responseBody 从报错来看，是出现了SSLError，好像是SSL的兼容问题。对比看一下ssl的版本： centos7 openssl-1.0.2k-8.el7.x86_64 centos6 openssl-1.0.1e-57.el6.x86_64 考虑到既有集群的稳定性，失败告终。配置同步问题转由rsync、nfs之类的方案解决吧。 参考： https://community.hortonworks.com/questions/4324/hdp-support-for-mix-of-os-releases-within-a-cluste.html https://community.hortonworks.com/questions/18479/how-to-register-host-with-different-os-to-ambari.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[主从备份问题]]></title>
    <url>%2F2017%2F10%2F25%2F%E4%B8%BB%E4%BB%8E%E5%A4%87%E4%BB%BD%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[昨天同事聊起mysql的主从备份问题。我们都知道mysql的主从同步是通过binlog实现的，抛开已经成行的主从不说，如果要新增从库的话，那么具体是精准的copy数据库镜像，然后记录binlog位置的呢？如果说copy镜像的话，那么一定要停库或者锁表了，如果是copy主库的话，那势必会影响线上业务了。 参考了一些资料，自己脑袋也跟着转了一下，总结出以下几种情况。 当前已经有了健壮的主从结构/或者是健壮的双热备，只需要新增一个从库。 将从库暂时停掉 进行这个mysql的所有数据的拷贝 在目标新从库修改server-id 启动新老从库 验证确认 当前只有主库，前面并没有配置从库，但是已经有了binlog。 最稳妥的：停掉，copy，配置，重启。 使用mysqldump，这种最不推荐，因为会锁表，那么对于线上应用来说一样是会收到影响。反正都会影响线上，还不如停掉更为稳妥 使用xtrabackup进行备份，不锁表，不影响线上业务。具体参考xtrabackup原理及实施。大概流程如下图：具体来说，首先在logfile中找到并记录最后一个checkpoint（“last checkpoint LSN”），然后开始从LSN的位置开始拷贝InnoDB的logfile到xtrabackup_logfile；然后开始拷贝全部的数据文件.ibd；在拷贝全部数据文件结束之后，才停止拷贝logfile。因为logfile里面记录全部的数据修改情况，所以即使在备份过程中数据文件被修改过了，恢复时仍然能够通过解析xtrabackup_logfile保持数据的一致。 参考： http://sofar.blog.51cto.com/353572/1313649 http://database.51cto.com/art/201507/483499.htm http://lizhenliang.blog.51cto.com/7876557/1612800]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一次查询hang事件]]></title>
    <url>%2F2017%2F10%2F16%2F%E4%B8%80%E6%AC%A1%E6%9F%A5%E8%AF%A2hang%E4%BA%8B%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[场景同事在navicat上为table A修改了了一个字段长度，然后navicati卡死了。重启后这个表就不能查询了。 排查1show processlist 查看慢查询发现有status为Waiting for table metadata lock的查询出现。 查看运行中的事务1select * from information_schema.innodb_trx 根据trx_started找到运行时间最长的kill掉慢查询对应的trx_mysql_thread_id，之后就恢复了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Chronos]]></title>
    <url>%2F2017%2F10%2F10%2FChronos%2F</url>
    <content type="text"><![CDATA[Chronos 内部很简单： 从zk的state store中读取所有job的state job都在scheduler里注册过了，会别加载进job graph，以追踪依赖关系 根据host时间，job被分成不同的list中，有些现在就要跑，有些不用 需要跑的job会被放进队列，只要有充足的资源就马上运行 等再有job需要run的时候，就重复1 一个job只有在它的所有parent都运行完成过才能运行。它跑完之后，这个轮次就结束了。 这段代码在mainLoop()方法中。https://github.com/mesos/chronos/blob/be96c4540b331b08d9742442e82c4516b4eaee85/src/main/scala/org/apache/mesos/chronos/scheduler/jobs/JobScheduler.scala#L469-L498 其他特性 向cassandra写入job metrics，用来分析、估计等 发送不同的通知：email、slack等 导出metric到graphite或其他地方 不能做的 解决所有分布式问题 保证精准调度 保证时间同步 保证job真正运行了 实例架构 UI 增删改job 运行job 展示job依赖关系 已经执行完的job的统计量 安装要是已经安装了DC/OS的话1dcos package install chronos 另外也可以使用docker启动，要保证两个端口资源：HTTP API， libprocess。 1docker run --net=host -e PORT0=8080 -e PORT1=8081 mesosphere/chronos:v3.0.0 --zk_hosts 192.168.65.90:2181 --master zk://192.168.65.90:2181/mesos 源码构建12345export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.sogit clone https://github.com/mesos/chronos.gitcd chronosmvn packagejava -cp target/chronos*.jar org.apache.mesos.chronos.scheduler.Main --master zk://localhost:2181/mesos --zk_hosts localhost:2181 运行1java -jar chronos.jar --master zk://127.0.0.1:2181/mesos --zk_hosts 127.0.0.1:2181]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mesos框架概览：-mesos-master,-mesosframework,-kibana,-minimesos]]></title>
    <url>%2F2017%2F10%2F10%2Fmesos%E6%A1%86%E6%9E%B6%E6%A6%82%E8%A7%88%EF%BC%9A-mesos-master%2C-mesosframework%2C-kibana%2C-minimesos%2F</url>
    <content type="text"><![CDATA[提纲 使用mesos-starter做的自定义framework 使用mesos framework做的一个通用framework 一个为kibana定制的mesos framework 使用minimesos进行测试 mesos-starter这是一个spring工具，[简单参考(http://container-solutions.com/mesos-starter/)。下面是一些特性 暴露一个bean来控制实例数量(提供水平扩展能力) 管理端口。用户可以请求静态的、或者mesos制定的端口，并且把它们用作环境变量 配置docker网络类型 mesos鉴权 用户可以指定任务的sandbox里下载二进制包以及其他附件资源的的URI 这些新特性可以支撑framework更高的灵活性。例如，我们若是在minimesos中使用docker containerizer，就必须使用dcoker bridge模式来保护host上的端口不会冲突(所有的agent都使用同一个docker daemon)。但是在实际情况中，我们需要docker 运行在host模式，这样它们就可以直接使用主机ip了。 mesosFrameworkMesosFramework是一个分布式的基于spring的二进制包，或者基于mesos-starter的docker容器。用户可以使用一个简单的配置文件快速创建一个可以运行在mesos上的framework，只需要告诉MesosFramework它应该运行什么任务就可以了。MesosFramework很通用，用户可以完全控制deployment。 我们讨论过为什么会想自己写一个mesos framwork，希望你考虑使用Mesos-starter.许多简单的app都可以通过一个类似marathon的orchestration tool来启动。但是有些用例是介于这两个极端之间的，例如 既想利用mesos-starter一些好的特性，又不想编写任何代码(authorisation， 一个orchestraion strategy等) 要打包一个特定的framework配置，这样用户就不用关心配置了（还是没有代码） 为某framework快速部署一个POC 对我们来说，有很多frameworkd都落入了这个灰色地带。主要的好处就是减少了代码编写，也就减少了维护成本。 MesosFramework还添加了一些Mesos-Starter之外的特性。比如：查看task状态的REST接口、水平扩容等。 Kibana framework新的Kibana framework就是基于MesosFramework和Mesos-starter构建的。它继承了这两个项目的所有特性。也就是说对于这两个项目的更新与特性发布会自动包含进像kibana这种下游项目中。这就是天堂，不用管理代码就能获得最新的特性。 但是最大的特点其实是：没有代码！！同样的重构也会应用到Logstash framework中。很多mesos framework都有对于代码的过度引用。每个framework都使用同样的方式添加mesos authorisation特性。全球的工程师都在同样的代码。我们都知道：代码越多，bug越多。 移除kibana的一些特定代码之后，我们同时搞定了潜在的bug，添加了新的特性，还降低了维护成本。 minimesos以前，我们使用一个真正的mesos cluster给我们的demo使用。现在只需要使用minimesos就可以了。例如，有一天我想要测试Elasticsearch framework是不是能够在mesos0.27版本正常工作(它本身是针对0.25版本编译的)。以前我需要写一个0.27版本集群的脚本，现在我只需要修改nimimesos配置文件中的一行就可以了。 参考：http://container-solutions.com/mesos-framework-overview/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[presto架构]]></title>
    <url>%2F2017%2F10%2F09%2Fpresto%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[server 类型CoordinatorWorker数据源Connector用来适配presto到类似hive的RDBMS。可以把connector当成database的driver。是Presto的SPI的一个实现，允许presto使用标准API与资源进行交互。 Presto包含几个内置的connectors：JMX, HIVE, System connector用来提供访问内部系统表的访问方式， TPCH connector用来提供TPC-H benchmark数据。还有一些第三方的connector。 每个catalog是与一个指定的connector进行协作的。如果你看过catalog的配置文件，你会发现每个都必须包含一个connector.name属性，catalog manager用这个来为指定catalog创建对应的connector。可以多个catalog共用同一个connector来连接两个相同数据库的不同实例。例如，假设我们有两个hive集群，我们就配置两个catalog实例，都用hive connector，这样甚至能在同一个SQL查询中查询到两个hive集群中的数据。 catalog一个catalog包含多个数据库schema和通过一个connector关联的数据源。在运行sql语句的时候，可以在一个或多个catalog中进行数据查询。 在presto中，数据表的全名是catalog的根部。例如hive.test.data.test就是hive catalog中的test_data数据库中的test表。 catalog是通过presto配置目录中的properties文件定义的。 schemaschema是用来组织table的。catalog和schema用来定义一组可供查询的数据表。在prestor查询hive或者mysql等关系型数据库的时候，schema就对应于数据库名。其他类型的connector也可以根据具体场景进行对应 Table未排序的数据行的组合，每个列都有自己的类型。与关系型数据库定义一样。 查询模型StatementPresto执行ANSI兼容的SQL语句。当一个statement被执行的时候，presto创建一个query和一个query plan，然后这个plan被分布到presto worker中执行。 QueryPresto解析statement的时候，把statement转化成一个query，创建一个分布式的query plan，然后形成一系列相互关联的stage，运行到不同的presto worker上。当我们抽取一个查询的信息的时候，我们会获取到对应于这个satement的每个组件的状态快照。 statement和query的区别很简单，statement可以看作传递给presto的SQL语句，一个query则对应于执行这个statement的配置与实例化的组件。一个query包含了stage、task、split、connector、数据源以及其他组件。 Stagepresto执行query时，会把query plan打成一系列有层级的stage。例如，如果presto需要聚合hive里的一亿行数据，他需要创建一个root stage来对其他stage的output执行聚合操作，这些其他stage负责query plan中的不同部分。 这些stage可以使用tree来表示。每个query都有一个root stage来执行其他stage的聚合操作。coordinator 是coordinator用来模型化分布式query plan的，但是stage本身并不运行在presto worker上。 Task前面提到，stage只是模型化分布式查询计划的一个特定部分，而不在presto worker上执行。一个stage其实又被一系列task实现，分布式运行在presto worker上。 presto task是有多个input和output的，可以与一系列driver并行执行。 Splitstage是通过connector从数据源以split为单位抽取数据的，然后中间的stage或高层stage又从其他stage中抽取数据。 prestor执行query时，coordinator会从一个connector查到指定table的所有的split。coordinator追踪哪些机器在运行哪些task，哪些split在被哪些task处理。 Drivertask包含一个或多个并行的driver，一个driver只有一个input和output。dirver整合operator来生成output，这个output会被task聚合，再传递给其他stage的一个task。一个driver是一个operator实例的序列，可以把他看成内存中的一系列operator集合。它是presto并发执行的最低层单元了。 Operatoroperator消费、转换、生产数据。例如，一个table scan从connector中抽取数据，然后生产可以被其他operator消费的数据。一个filter operator消费数据，生产已经过滤掉的一个数据子集。 ExchangeExchange负责在Presto节点间传递同一个query的不同stage的数据。task把数据生成进一个output buffer，并使用exchange client消费其他task生成的数据。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[impala架构]]></title>
    <url>%2F2017%2F10%2F09%2Fimpala%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[impala Daemon核心的impala组件是impala daemon，它需要在每个datanode上部署一个，进程名是impalad。它负责读写数据文件、接收来自impala-shell命令行、hue、JDBC或者ODBC的查询；并行执行查询、为集群分配分布式任务、再把中间查询结果发送回coordinator node。 可以通过任何一个datanode上的impala daemon提交查询，然后这个node上的impala daemon就作为这个查询的coordinator node。其他node把部分结果回传给coordinator node，coordinator node负责把这些结果进行组织，并产生最终结果。当通过impala-shell命令运行查询的时候，我们可能需要一直连接着同一个impala daemon。对于集群生产环境，我们需要考虑负载均衡，将查询放到不同的node上去提交。 impala daemon需要与statestore持续联系，以确认哪些node是健康的，可以接受任务。 impala daemon还要接收catalogd的广播信息，获取集群中哪些impala node新建、修改、drop一些类型的对象、或insert或者load data语句的执行。这个动作最小化了REFRESH和INVALIDATE METADATA语句的执行次数，这两个动作在1.2之前必须要协调所有node的元数据。 在2.9或更高版本中，我们可以控制哪些node可以用作query coordinator，那些是query executor，以此提升扩展性。 impala statestorestatestore负责检查所有 datanode上的impala daemon的健康状态，同时也通知每个impala daemon它的新发现。他是一个独立的进程，statestored，需要集群中的某个机器上有一个就可以。如果一个impala daemon掉线的话，statestore会通知所有其他的impala daemon，这要后面的查询任务就不会分配到这个掉线的node上。 因为statestore的职责是处理问题情况，所以它并不是impala 集群执行正常任务的核心所在。如果statestore挂掉，impala daemon会像往常一样继续运行、分配任务、执行任务。只是集群会在某些impala daemon挂掉的时候变得不那么强壮。当statestore恢复之后，它会重建与impala daemon的联系。 多数既有的HA和LB方案都可以用于impala daemon。而statestored和catalogd进程就不需要，因为它们挂掉并不会造成数据丢失。如果这些进程变为不可用的话，可以直接停掉impala服务，删掉基友的impala statestore和impala catalog server，然后把它们分配到其他的node上，重启impala service就可以了。 impala catalog服务catalog服务负责将impala SQL语句导致的源数据变化传递给集群中的所有的Datanode节点，进程为catalogd，整个集群只需要一个。因为查询请求会通过statestore daemon，最好将statestored和catalogd两个服务运行在同一个host上。 catalog服务避免了由元数据变化引起的REFRESH和INVALIDATE METADATA操作。当我们通过hive创建表、加载数据的时候，我们必须要执行REFRESH和INVALIDATE METADATA操作才能执行数据查询。 这个特性有关于impala的以下几个方面： 见安装impala、升级impala 通过impala执行CREATE TABLE, INSERT或者其他修改表、修改数据的操作并不需要REFRESH和INVALIDATE METADATA操作。但是如果是通过hive或者手动加载HDFS文件方式的话，就需要。 默认地，加载元数据和加载缓存是异步的，所以impala可以立刻接收请求。可以通过参数配置load_catalog_in_background=false.。 impala对于hadoop生态的兼容与适配hiveimpala维护了table定义，也就是metastore，其实就是hive 的metastore的数据库。impala还要追踪对应的数据文件：HDFS中数据的block的具体位置。 对于数据量特别大、分区数特别多的表来说，单抽取元数据就可能花掉数分钟的时间。因此，每个impala node都会把这些信息缓存起来以供后面重用。 如果某个表的定义改变或者数据被更新，那么其他所有的impala daemon都要在对这个表执行查询之前同步最新的metadata，替换掉老的那份缓存。1.2版本之后，这个过程是自动的通过catalogd进程完成。 对于未通过impala操作的修改，则需要执行REFRESH（已经存在的表加入新的数据文件或更新）或者INVALIDATE METADATA(创建新标、删除表、执行了HDFS rebalance、 删除了数据文件等)。INVALIDATE METATDATA会重新抽取所有的表的metadata。所有如果要是知道哪个表有变化的话，可以使用REFRESH table_name来执行只拉取这个表的相关元数据。 HDFSimpala使用HDFS作为主要的存储介质，利用HDFS的冗余来保证数据不丢失。impala表数据物理上就是HDFS的数据文件，可以是HDFS支持的任何格式或压缩格式。当数据文件是放在一个目录作为一个表存在是，impala会把这个目录下所有的文件都作为表的内容进行读取。通过impala新增的数据文件，则会由impala进行命名。 HBaseHDFS的替代方案。需要在impala中额外定义对应表。 并发控制通过准入控制来闲置资源的方式之一是限制一个最大并发查询数量。在我们没有内存使用情况的足够信息的时候这是一个比较初步的方式。这个参数可以分别给每个动态资源池进行设置。 这个配置也可以结合下面章节中内存控制的配置一起使用。不管是达到了内存最大使用量，或者超过了最大并发查询数，后续的查询都会被放到队列里，等待资源。 内存控制每个动态资源池都可以配置一个最大内存使用量。最好是在已经明确知道工作量与使用大小的时候使用这个配置。 每个host上都要指定一个默认的最大内存Default QueryMemory Limit，相当于在这个资源池中为每个query指定一个MEM_LIMIT参数。 另外，也可以根据每个node的最大内存计算出一个集群级别的最大内存限制，来控制最多并行查询数。 例如下面场景： 五个datanode上运行了impalad 设置一个动态资源池的最大内存为100G host级别的最大内存Default QueryMemory Limit设置为10G，那么每个查询最多使用 5 * 10 = 50G 在这个动态资源池中可以并行运行的查询数为2，100 / 50 = 2. 如果查询并没有用满host或者集群级别的内存上限额度，并不会有什么惩罚措施。这些上限值只是用来估算资源池里可以并行多少查询。 注意： 如果你为某个impala动态资源池指定了集群级别的最大内存，就必须指定host级别的最大内存Default QueryMemory Limit。 与其他资源管理工具的关系admission control是轻量级、去中心化的。他设置的是一个soft limit，并不是像yarn把资源进行all-or-nothing方式处理。 因为admission control并不与其他hadoop组件(MR)交互，我们可以使用yarn的部分静态资源池，与其他hadoop应用共享yarn。要使用impala多租户的时候推荐使用此种配置。admission control控制查询并发和内存上限，yarn来管理其它的组件。这种情况中，impala的资源并不是由yarn管理的。 impala的admission control可以使用yarn的user与pool的认证关系。 虽然impala admission control使用了fair-scheduelr.xml配置文件，但是这个文件并不关心YARN实际使用哪个scheduler，也就是说它仍可以使用capacity scheduler。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hadoop-vs-mpp]]></title>
    <url>%2F2017%2F10%2F09%2Fhadoop-vs-mpp%2F</url>
    <content type="text"><![CDATA[什么是MPPMPP全名是massively parallel processing，是网格计算的一种实现，网格中的所有节点都参与计算过程。MPP DBMS是基于MPP的DBMS。每个查询都需要先切分计算任务，这会比传统的SMP RDBMS更快一些。另外，MPP还具有扩展性，我们可以方便的向网格中添加节点来扩展计算与存储能力。为了处理大量数据，需要对数据进行在节点之间进行shard，这样每个节点都只处理自己本地的数据。这进一步提升了数据处理速度，因为使用共享存储的话会出现一个很大的问题—— 更复杂，更昂贵，不易扩展，更高的网络IO，更小的并行度。这就是大多数MPP DBMS方案share-nothing不共享任何东西，运行在DAS（direct-attached storage）或者小组server共享机架存储的原因。Teradata、Greenplum、Vertica、Netezza等都是使用的这种方式。这些工具都有一个复杂且成熟，专为MPP方案订制的SQL optimizer。而且他们都是可扩展的，围绕这些方案有很多的工具包来支持用户的需求：地理位置分析、全文检索等。他们都是闭源的复杂的企业级解决方案(Greenplum在2015年开源)。 hadoop那么hadoop呢？不是一个单一的技术，而是一个生态。基友好处也有坏处，最大的优点就是延展性好 - 催生了一些列的新组件（spark），而且这些组件的核心都与hadoop息息相关，这就给集群的扩展性很多可能性。一个缺点是弄一个整套的生态是比较费事儿的，现在没人手工弄，都是借助工具了。 hadoop的存储技术是一个完全不同的方式。并不是根据key去把数据进行shard，而是把数据弄成指定大小的block块进行切分，以block为单位存储在不同节点上。这些数据块一般是比较大且只读的，整个文件系统也是只读的。简单来说，对于加载100行数据表，MPP引擎会基于某个key把数据进行shard，对于较大的集群来说这种方式可能会出现每个node上只有一条记录的现象。而如果使用HDFS的话，这个小的表会被写在一个数据块中，对于datanode的文件系统来说只是一个普通的 文件而已。 资源管理呢？不同于MPP方案，hadoop的资源管理器具有更细粒度。MR任务不需要所有的计算任务都并行执行，如果集群资源满载的时候，我们甚至可以把某个任务的数据和所有计算任务都丢到一个节点上执行。还有一系列的其他延展性，支持长时间运行的container等..但是事实上，它比MPP的资源管理器会慢一些，而且对于并发管理表现也稍逊一筹。 可选对于Hadoop的SQL接口工具，有很多工具可以选择：基于MR/Tez/Spark的hive、sparksql、impala、HAQW或者IBM BigSQL、或者完全不同的Splice Machine。因为有太多工具可选，也很容易迷失。 hive把sql转化为MR/Tez/Spark任务，在集群上执行。所有的这些任务类型其实都是基于MR思想，可以很好的利用集群，也方便与其他的hadoop技术栈进行整合。缺点也很明显 —— 执行查询的延迟较大，对于表之间的join操作性能堪忧，没有查询优化器引擎只按照你说的方式进行执行。下图覆盖了废弃的MR1的设计图： MPP类似impala和HAWQ的解决方案则与hive相对，使用了基于HDFS的MPP执行引擎。他们对于查询有更小的延迟、更少的处理时间，但是牺牲了一些扩展性和稳定性。 SparkSQLsparkSQL是一个介于MR和MPP-over-hadoop之间的解决方案。与MR类似，它把job切分为一组task进行分别调度，这样来保证比较好的稳定性。又类似于MPP，试图把各个执行的stage之间流式串联起来来提高处理速度。他还使用类似于MPP的固定的executor来降低查询延迟。它继承了优点，同样缺点也跟了过来 —— 不及MPP快，不及MR稳定。 总结 指标 MPP Hadoop 平台开放性 闭塞且专利。对于一些技术甚至连文档都是闭源的 完全开源 硬件选择 很多都是必须依托服务方的，很少能部署在自己的集群上。所有的解决方案都需要企业级的硬件，比如快速硬盘，大ECC RAM的服务器，无线带宽等 任何硬件都可用。最推荐的就是使用廉价的带有DAS的硬件【就是磁盘单挂】 节点水平扩展 平均几十台，最多100-200台 平均100台，最多几千台 用户数据扩展性 几十TB，最多PB级别 几百TB, 最多几十PB 查询延迟 10-20 milliseconds 10-20 seconds 平均查询时间 5-7 seconds 10-15 minutes 最大查询时间 1-2 hours 1-2 weeks 查询优化 复杂的企业级查询优化引擎使用全局最优的方式 没有optimizer，或者有限制，有时甚至都不支持cost-based 查询debug与profile 有查询计划与查询静态量，提供错误信息 OON问题和java堆溢出分析，GC停止信息，分离的任务日志 技术代价 每个节点几千或者几万美元 每个节点最多几千美元 终端用户的访问 SQL接口和一些简单的数据库函数 SQL并不完全兼容。用户需要关注执行逻辑，数据的分布。函数一般需要用java进行编写，放到集群中 目标用户 商业分析师 java dev和经验丰富的DBA 单job冗余度 低，MPP节点失败，则job失败 高，job失败后有重试 目标系统 一般的DWH和分析系统 为特定目标构建的数据处理引擎 vendor lock-in typical case Rare case usually caused by technology misuse 推荐的最小的数据大小 任何 GB 最大并发 几十到几百个query 10-20个job 技术延展性 只能使用厂家提供的工具 可以随意组合开源工具 需要的DBA技术级别 一般水平 精通java和RDBMS背景 方案实现服务性 中 高 有了以上信息，我们知道为什么hadoop不能完全取代传统企业级数据仓库，但是他是可以用作处理海量数据的一种分布式引擎。facebook有一个300PB的hadoop集群，它还额外用了50TB的Vertica集群。Linkedin有一个巨大的hadoop集群，也同样有一个Aster数据中心（Teradata提供的MPP工具）。 参考：https://0x0fff.com/hadoop-vs-mpp/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7-微服务相关环境部署]]></title>
    <url>%2F2017%2F09%2F30%2Fcentos7-%E5%BE%AE%E6%9C%8D%E5%8A%A1%E7%9B%B8%E5%85%B3%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[系统环境1234567hostnamectl set-hostname xx.will.com --staticyum install -y wget git vim ntpdatetimedatectl set-timezone Asia/Shanghai/usr/sbin/ntpdate us.pool.ntp.org docker安装123456789101112131415161718192021tee /etc/yum.repos.d/docker.repo &lt;&lt;-&apos;EOF&apos;[dockerrepo]name=Docker Repositorybaseurl=https://yum.dockerproject.org/repo/main/centos/7/enabled=1gpgcheck=1gpgkey=https://yum.dockerproject.org/gpgEOFyum install docker-engine -y# 获取最新版本的docker# curl -fsSL https://get.docker.com/ | shsystemctl enable docker.servicesystemctl restart dockerdocker version ubuntu123456789101112131415161718# step 1: 安装必要的一些系统工具sudo apt-get updatesudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# step 2: 安装GPG证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# Step 3: 写入软件源信息sudo add-apt-repository &quot;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;# Step 4: 更新并安装 Docker-CEsudo apt-get -y updatesudo apt-get -y install docker-ce# 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:# apt-cache madison docker-ce# docker-ce | 17.03.1~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages# docker-ce | 17.03.0~ce-0~ubuntu-xenial | http://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages# Step 2: 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.1~ce-0~ubuntu-xenial)# sudo apt-get -y install docker-ce=[VERSION] centos71234567891011121314151617181920212223242526# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装 Docker-CEsudo yum makecache fastsudo yum -y install docker-ce# Step 4: 开启Docker服务sudo service docker start# 注意：# 官方软件源默认启用了最新的软件，您可以通过编辑软件源的方式获取各个版本的软件包。例如官方并没有将测试版本的软件源置为可用，你可以通过以下方式开启。同理可以开启各种测试版本等。# vim /etc/yum.repos.d/docker-ee.repo# 将 [docker-ce-test] 下方的 enabled=0 修改为 enabled=1## 安装指定版本的Docker-CE:# Step 1: 查找Docker-CE的版本:# yum list docker-ce.x86_64 --showduplicates | sort -r# Loading mirror speeds from cached hostfile# Loaded plugins: branch, fastestmirror, langpacks# docker-ce.x86_64 17.03.1.ce-1.el7.centos docker-ce-stable# docker-ce.x86_64 17.03.1.ce-1.el7.centos @docker-ce-stable# docker-ce.x86_64 17.03.0.ce-1.el7.centos docker-ce-stable# Available Packages# Step2 : 安装指定版本的Docker-CE: (VERSION 例如上面的 17.03.0.ce.1-1.el7.centos)# sudo yum -y install docker-ce-[VERSION] https://yq.aliyun.com/articles/110806 配置文件 /etc/docker/daemon.json123456&#123; &quot;graph&quot;: &quot;/server/dspace&quot;, &quot;insecure-registries&quot;: [&quot;10.1.5.129&quot;], &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;http://3a35aff6.m.daocloud.io&quot;,&quot;http://ethanzhu.m.tenxcloud.net&quot;], &quot;hosts&quot;: [&quot;tcp://0.0.0.0:4243&quot;,&quot;unix:///var/run/docker.sock&quot;]&#125; 1234567&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;http://3a35aff6.m.daocloud.io&quot;,&quot;http://ethanzhu.m.tenxcloud.net&quot;], &quot;metrics-addr&quot; : &quot;10.2.19.112:9323&quot;, &quot;experimental&quot; : true, &quot;dns&quot;: [&quot;10.2.19.112&quot;], &quot;dns-search&quot;: [&quot;will.com&quot;]&#125; mesos + marathon12rpm -Uvh http://repos.mesosphere.io/el/7/noarch/RPMS/mesosphere-el-repo-7-1.noarch.rpmyum -y install mesos marathon mesosphere-zookeeper master环境脚本 mesos_master_start.sh12345678910export MESOS_cluster=will#export MESOS_zk=zk://10.1.5.65:2181/wesosexport MESOS_zk=zk://datanode01.will.com:2181,datanode02.will.com:2181,datanode04.will.com:2181,servicenode05.will.com:2181,servicenode06.will.com:2181/wesosexport MESOS_work_dir=/var/lib/mesos/masterexport MESOS_quorum=1export MESOS_log_dir=/server/mesos_logexport MESOS_zk_session_timeout=20secsexport MESOS_logging_level=WARNING#export MESOS_log_dir=/var/log/mesos_masterexport MESOS_hostname=10.2.19.124 启动脚本：1. /server/mesos_master_start.sh &amp;&amp; nohup /usr/sbin/mesos-master &amp;&gt; mesos_master.out&amp; mesos_agent_env.sh123456export MESOS_executor_registration_timeout=5minsexport MESOS_containerizers=docker,mesosexport MESOS_isolation=cgroups/cpu,cgroups/memexport MESOS_work_dir=/server/mesos/agentexport MESOS_master=zk://datanode01.will.com:2181,datanode02.will.com:2181,datanode04.will.com:2181/wesos_onlineexport MESOS_resources=&apos;ports:[10000-60000]&apos; 启动脚本1. /server/mesos_agent_env.sh &amp;&amp; nohup mesos-agent &amp;&gt; mesos_agent.out&amp; marathon1nohup marathon --master zk://datanode01.will.com:2181,datanode02.will.com:2181,datanode04.will.com:2181/wesos_online --zk zk://datanode01.will.com:2181,datanode02.will.com:2181,datanode04.will.com:2181/mt_online --zk_timeout 15000 &amp;&gt; marathon.out&amp;]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基础设施代码化]]></title>
    <url>%2F2017%2F09%2F29%2F%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E4%BB%A3%E7%A0%81%E5%8C%96%2F</url>
    <content type="text"><![CDATA[实践使用定义文件。所有的配置都放在可执行的脚本里，shell, anisible playbook, chef recipe, puppet manifest等。不用出现任何人为参与，就可以实施实时调整。 自描述系统和进程相对于人们按照文档描述进行执行，代码更能精确地执行。甚至可以通过根据代码自动生成文档。 所有东西都版本化使用版本控制工具将所有的变化都储存起来，方便重现问题与排错。 持续的测试系统测试让我们能快速找到基础设置配置中存在的问题。可以设置一个发布流程，在continuousDelivery【持续交付】时进行测试。 敏感于小的变化变化越大，潜在问题可能性越大。 持续保持服务可用对于更新部署，服务不能宕机。使用蓝绿部署和parallelchange来解决这个问题 好处以上这些约束能过够让我们快速启动新的server，并且在更新配置等情况下安全的处理在线的server。创建新的server对我们来说只是运行一下脚本而已。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自制python的安装包]]></title>
    <url>%2F2017%2F09%2F28%2F%E8%87%AA%E5%88%B6python%E7%9A%84%E5%AE%89%E8%A3%85%E5%8C%85%2F</url>
    <content type="text"><![CDATA[起步名字 小写 pypi中没有重复的 目录结构1234funniest/ funniest/ __init__.py setup.py 在init.py中添加123def joke(): return (u'Wenn ist das Nunst\u00fcck git und Slotermeyer? Ja! ... ' u'Beiherhund das Oder die Flipperwaldt gersput.') setup.py里应该包含一个方法setuptools.setup():1234567891011from setuptools import setupsetup(name=&apos;funniest&apos;, version=&apos;0.1&apos;, description=&apos;The funniest joke in the world&apos;, url=&apos;http://github.com/storborg/funniest&apos;, author=&apos;Flying Circus&apos;, author_email=&apos;flyingcircus@example.com&apos;, license=&apos;MIT&apos;, packages=[&apos;funniest&apos;], zip_safe=False) 然后就可以本地安装一下了。1pip install . 用下面命令的话，可以让这个module的使用者实时更新变化12345root@will-vm:~/gitLearning/funiest# pip install -e .Obtaining file:///root/gitLearning/funiestInstalling collected packages: funiest Running setup.py develop for funiestSuccessfully installed funiest 引用：123&gt;&gt;&gt; import funiest&gt;&gt;&gt; print funiest.joke()Wenn ist das Nunstück git und Slotermeyer? Ja! ... Beiherhund das Oder die Flipperwaldt gersput. 发布到pypisetup.py同样也是我们在PYPI上注册包名，并上传源码的入口。 打包：12345678910111213141516171819202122232425262728293031root@will-vm:~/gitLearning/funiest# python setup.py sdistrunning sdistrunning egg_infowriting funiest.egg-info/PKG-INFOwriting top-level names to funiest.egg-info/top_level.txtwriting dependency_links to funiest.egg-info/dependency_links.txtreading manifest file &apos;funiest.egg-info/SOURCES.txt&apos;writing manifest file &apos;funiest.egg-info/SOURCES.txt&apos;warning: sdist: standard file not found: should have one of README, README.rst, README.txtrunning checkcreating funiest-0.1creating funiest-0.1/funiestcreating funiest-0.1/funiest.egg-infomaking hard links in funiest-0.1...hard linking setup.py -&gt; funiest-0.1hard linking funiest/__init__.py -&gt; funiest-0.1/funiesthard linking funiest.egg-info/PKG-INFO -&gt; funiest-0.1/funiest.egg-infohard linking funiest.egg-info/SOURCES.txt -&gt; funiest-0.1/funiest.egg-infohard linking funiest.egg-info/dependency_links.txt -&gt; funiest-0.1/funiest.egg-infohard linking funiest.egg-info/not-zip-safe -&gt; funiest-0.1/funiest.egg-infohard linking funiest.egg-info/top_level.txt -&gt; funiest-0.1/funiest.egg-infoWriting funiest-0.1/setup.cfgcreating distCreating tar archiveremoving &apos;funiest-0.1&apos; (and everything under it)root@will-vm:~/gitLearning/funiest# ll dist/总用量 12drwxr-xr-x 2 root root 4096 9月 28 14:35 ./drwxr-xr-x 5 root root 4096 9月 28 14:35 ../-rw-r--r-- 1 root root 912 9月 28 14:35 funiest-0.1.tar.gz 这个包并没有经过build，在使用pip安装的时候需要再执行个build的步骤，尽管是纯python的也要有这个步骤。 wheelswheel是一个已经build过的包，安装的时候不用走build步骤了，相对终端用户安装过程会快一些。 如果我们的项目是纯python(不包含编译的扩展)，本身就支持python2和python3，那么就叫做universal wheel 如果是纯python，但是不是原生支持python2和3，就叫做pure python wheel 如果包含便宜扩展，就叫 platform wheel 在把我们的project构建wheel之前，先安装wheel1pip install wheel universal wheel1python setup.py bdist_wheel --universal 可以在setup.cfg中指定参数12[bdist_wheel]universal=1 只有在你的project对于python2和python3都支持，而且不包含c扩展的时候才能用universal。 pure python wheel1python setup.py bdist_wheel bdist_wheel会检测是不是纯python，会生成一个与当前python版本兼容的结果。要是想支持多个版本，就需要在2、3的版本python环境下分别构建。 platform wheel用来指定运行平台：linux, windows等，通常都带着C扩展1python setup.py bdist_wheel 上传到PYPI因为我还没有过pypi的用户，所以要根据提示先注册账户。 可以把注册信息加入~/.pypirc中：123[pypi]username = &lt;username&gt;password = &lt;password&gt; 上传：1pip install twine 123456root@will-vm:~/gitLearning/funiest# twine upload dist/*Uploading distributions to https://upload.pypi.org/legacy/Enter your username: willcupEnter your password: Uploading funiest-0.1-py2.py3-none-any.whlUploading funiest-0.1.tar.gz 然后就可以看到了 https://pypi.python.org/pypi/funiest/0.1 至此，已经可以到别的机器上下载并使用了 参考： https://packaging.python.org/tutorials/distributing-packages/#setup-py https://github.com/pypa/sampleproject/blob/master/setup.py https://python-packaging.readthedocs.io/en/latest/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus：理解告警的几个延迟点]]></title>
    <url>%2F2017%2F09%2F22%2Fprometheus%EF%BC%9A%E7%90%86%E8%A7%A3%E5%91%8A%E8%AD%A6%E7%9A%84%E5%87%A0%E4%B8%AA%E5%BB%B6%E8%BF%9F%E7%82%B9%2F</url>
    <content type="text"><![CDATA[抓取、计算、与告警prometheus会在指定配置周期内进行metric的抓取，这个周期参数是scrape_interval，默认是1分钟。可以全局定义一个，然后每个job再自定义覆盖。每次爬取到数据之后，prometheus会计算告警规则里的表达式，并根据结果修改告警状态。 一个告警会有以下状态： inactive pending. 已经检验失败，但是还没有到告警规则中for指定的持续时间。 firing. 已经检验失败，而且已经超过了告警规则中for指定的持续时间。 告警状态之间的变化只会发生在计算阶段，主要是取决于表达式和FOR语法定义内容 for是一个可选的定义。 没有定义for的告警会立即触发 定义了for的会先到pending，时间超过后再转化到firing。所以会有至少两个计算阶段。 告警周期来个例子 告警规则：123ALERT NODE_LOAD_1M IF node_load1 &gt; 20 FOR 1m 相关配置123global: scrape_interval: 20s evaluation_interval: 1m 问题来了：发送这个告警到底会花费多少时间？ 答案：介于1m和20s+1m+1m之间。 下图是事件发生的时序图： alertmanageralertManager是整个流程的最后一个环节，它会接收inactive和firing的信号。它可以将相似的告警进行分组，以一个通告发出去。这种小组发送的支持可以避免大量重复信息频繁发送的问题。但是小组发送的缺点也比较明显，就是可能会有更大的延迟。 下面是相关设置：123group_by: [ &apos;a-label&apos;, &apos;another-label&apos; ]group_wait: 30sgroup_interval: 5m 在新的告警已经被fire后，它会等待group_wait时间到了才会发送。 但是如果第一批还没发送完的时候，第二批又来了咋整？第二批就不看group_wait了，而是看group_interval。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JMX-与docker的问题]]></title>
    <url>%2F2017%2F09%2F22%2FJMX-%E4%B8%8Edocker%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[docker容器依赖于外部某些服务，运行一段时间后网络不通 我的所有的采样jmx数据的容器全都是这样，开始的时候好好的，一段时间后就连不上目标主机了。然后重启又恢复正常 下面是使用prometheus/jmx_exporter自己制作docker镜像后产生的问题。报错异常如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152SEVERE: JMX scrape failed: java.io.IOException: Failed to retrieve RMIServer stub: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 10.2.19.64; nested exception is: java.net.ConnectException: Connection refused (Connection refused)] at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:369) at javax.management.remote.JMXConnectorFactory.connect(JMXConnectorFactory.java:270) at io.prometheus.jmx.JmxScraper.doScrape(JmxScraper.java:106) at io.prometheus.jmx.JmxCollector.collect(JmxCollector.java:436) at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.findNextElement(CollectorRegistry.java:180) at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:213) at io.prometheus.client.CollectorRegistry$MetricFamilySamplesEnumeration.nextElement(CollectorRegistry.java:134) at io.prometheus.client.exporter.common.TextFormat.write004(TextFormat.java:22) at io.prometheus.client.exporter.HTTPServer$HTTPMetricHandler.handle(HTTPServer.java:43) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79) at sun.net.httpserver.AuthFilter.doFilter(AuthFilter.java:83) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:82) at sun.net.httpserver.ServerImpl$Exchange$LinkHandler.handle(ServerImpl.java:675) at com.sun.net.httpserver.Filter$Chain.doFilter(Filter.java:79) at sun.net.httpserver.ServerImpl$Exchange.run(ServerImpl.java:647) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: javax.naming.ServiceUnavailableException [Root exception is java.rmi.ConnectException: Connection refused to host: 10.2.19.64; nested exception is: java.net.ConnectException: Connection refused (Connection refused)] at com.sun.jndi.rmi.registry.RegistryContext.lookup(RegistryContext.java:122) at com.sun.jndi.toolkit.url.GenericURLContext.lookup(GenericURLContext.java:205) at javax.naming.InitialContext.lookup(InitialContext.java:417) at javax.management.remote.rmi.RMIConnector.findRMIServerJNDI(RMIConnector.java:1955) at javax.management.remote.rmi.RMIConnector.findRMIServer(RMIConnector.java:1922) at javax.management.remote.rmi.RMIConnector.connect(RMIConnector.java:287) ... 17 moreCaused by: java.rmi.ConnectException: Connection refused to host: 10.2.19.64; nested exception is: java.net.ConnectException: Connection refused (Connection refused) at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:619) at sun.rmi.transport.tcp.TCPChannel.createConnection(TCPChannel.java:216) at sun.rmi.transport.tcp.TCPChannel.newConnection(TCPChannel.java:202) at sun.rmi.server.UnicastRef.newCall(UnicastRef.java:342) at sun.rmi.registry.RegistryImpl_Stub.lookup(Unknown Source) at com.sun.jndi.rmi.registry.RegistryContext.lookup(RegistryContext.java:118) ... 22 moreCaused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) at java.net.Socket.connect(Socket.java:589) at java.net.Socket.connect(Socket.java:538) at java.net.Socket.&lt;init&gt;(Socket.java:434) at java.net.Socket.&lt;init&gt;(Socket.java:211) at sun.rmi.transport.proxy.RMIDirectSocketFactory.createSocket(RMIDirectSocketFactory.java:40) at sun.rmi.transport.proxy.RMIMasterSocketFactory.createSocket(RMIMasterSocketFactory.java:148) at sun.rmi.transport.tcp.TCPEndpoint.newSocket(TCPEndpoint.java:613) ... 27 more]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django的单元测试]]></title>
    <url>%2F2017%2F09%2F18%2Fdjango%E7%9A%84%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[编写测试使用了unittest1234567891011121314from django.test import TestCasefrom myapp.models import Animalclass AnimalTestCase(TestCase): def setUp(self): Animal.objects.create(name=&quot;lion&quot;, sound=&quot;roar&quot;) Animal.objects.create(name=&quot;cat&quot;, sound=&quot;meow&quot;) def test_animals_can_speak(self): &quot;&quot;&quot;Animals that can speak are correctly identified&quot;&quot;&quot; lion = Animal.objects.get(name=&quot;lion&quot;) cat = Animal.objects.get(name=&quot;cat&quot;) self.assertEqual(lion.speak(), &apos;The lion says &quot;roar&quot;&apos;) self.assertEqual(cat.speak(), &apos;The cat says &quot;meow&quot;&apos;) 运行测试123456789101112131415# Run all the tests in the animals.tests module$ ./manage.py test animals.tests# Run all the tests found within the &apos;animals&apos; package$ ./manage.py test animals# Run just one test case$ ./manage.py test animals.tests.AnimalTestCase# Run just one test method$ ./manage.py test animals.tests.AnimalTestCase.test_animals_can_speak$ ./manage.py test animals/$ ./manage.py test --pattern=&quot;tests_*.py&quot; 测试数据库通过settings文件中的DATABASES里的TEST字段指定测试数据库的名称。要求对应的用户有创建测试数据库的权限。 执行顺序为确保所有的TestCase代码都运行在一个clean的database之上，django的测试运行顺序如下： 所有的django.test.TestCase子类 所有SimpleTestCase的子类，包括TransactionTestCase。他们的运行顺序不能保证 unittest.TestCase的子类，这些类可能会修改db，而且不能恢复db的原始状态 你可以通过test –reverse选项，反转执行顺序。这样可以确保所有的TestCase之间都是无依赖的。 回滚任何在migration里加载的初始数据都可以在TestCase中被使用，但是在TranactionTestCase里就不能使用，除非对于后台引擎支持事务才行，比如MyISAM引擎就不行。 django可以通过设置serialized_rollback 为true，为每个TestCase reload数据，但是注意这会让速度变慢。 第三方的app必须要设置这个，通常，我们都是用事务性数据库，就没有必要使用了。 初始序列化工作其实挺快的，但是如果有些APP并不需要序列化的话，可以通过TEST_NON_SERIALIZED_APPS进行配置。 加速测试 使用并行测试 test –parallel 3 密码hash。默认密码hasher比较慢，可以指定自己的。 test –keepdb，测试完成后不删除测试数据库。跳过创建于删除动作]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cloudnative]]></title>
    <url>%2F2017%2F09%2F15%2Fcloudnative%2F</url>
    <content type="text"><![CDATA[sdf 特点 说明 pet 1.物理机 dsf cattle pet 物理机 或 虚拟机 OS + app + data stateful : LB后的server对state有粘性sticky【1对1】 server对OS环境的依赖性极大，迁移扩展困难 cattle 虚拟技术(container) + manifest【记录环境部署的一切】，迁移或扩展方便 state存储于第三方，对于所有app共享【数据、server启动的配置信息、manifest信息】 puppet等devops工具栈 app与OS解耦？ 聚焦于app + data，不关心OS等环境 参考： https://www.youtube.com/watch?v=zWgq6sd1Ols]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mesos-+-marathon快速启动]]></title>
    <url>%2F2017%2F09%2F14%2Fmesos-%2B-marathon%E5%BF%AB%E9%80%9F%E5%90%AF%E5%8A%A8%2F</url>
    <content type="text"><![CDATA[mastermesos_master_start.sh12345export MESOS_cluster=willexport MESOS_zk=zk://10.1.5.65:2181/wesosexport MESOS_work_dir=/var/lib/mesos/masterexport MESOS_quorum=1export MESOS_log_dir=/server/mesos_log 1. mesos_master_start.sh &amp;&amp; mesos-master agent通过zk与mesos master建立关系 mesos_agent_env.sh12345export MESOS_executor_registration_timeout=5minsexport MESOS_containerizers=docker,mesosexport MESOS_isolation=cgroups/cpu,cgroups/memexport MESOS_work_dir=/var/lib/mesos/agentexport MESOS_master=zk://10.1.5.65:2181/wesos 1. mesos_agent_env.sh &amp;&amp; mesos-agent --resources=&apos;ports:[10000-60000]&apos; marathon通过zk与mesos master建立关系1marathon --master zk://10.1.5.65:2181/wesos --zk zk://10.1.5.65:2181/mt]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[marathon坑记---restart失败]]></title>
    <url>%2F2017%2F09%2F08%2Fmarathon%E5%9D%91%E8%AE%B0---restart%E5%A4%B1%E8%B4%A5%2F</url>
    <content type="text"><![CDATA[接上一篇的场景。 我们通过marathon的REST API修改某个app的json配置，然后会触发restart操作。 因为json里指定了host的port，所以会导致mesos agent的31111端口被占用，然后新的app就起不来，然后就一直处于waiting状态了。 这个被占用的信息可以从mesos master的/state接口里used_resources看到。 尝试把hostPort解放开，重新设置成0…..果然就成功了。12345678910111213141516&#123; &quot;id&quot;: &quot;basic-3&quot;, &quot;cmd&quot;: &quot;python3 -m http.server 8080&quot;, &quot;cpus&quot;: 0.5, &quot;mem&quot;: 32.0, &quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;python:3&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 8080, &quot;hostPort&quot;: 0 &#125; ] &#125; &#125;&#125; 这样的话，对于服务的使用有以下几个方案 HAProxy。需要在HAProxy注册每个service的端口就行了。hostPort不用开，都通过HAProxy去访问。 继续单个服务指定端口。需要每次都先将app缩容到0，再扩容回来。 使用smartstack或者consul等服务发现工具，对HAProxy进行动态修改。 我们的应用是监控平台，每个监控app只需要有一个实例，而且并不要求每个实例有严格高可用。所以暂时使用上面最简单的方案2.]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Yelp开源的pass平台工具]]></title>
    <url>%2F2017%2F09%2F08%2FYelp%E5%BC%80%E6%BA%90%E7%9A%84pass%E5%B9%B3%E5%8F%B0%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[git push到git仓库 jenkins pipeline - jenkins执行git pull jenkins pipeline - jenkins执行单元测试 jenkins pipeline - 通过测试后添加git tag: passta-cluseter1.servicename-20160505T090305-deploy、passta-cluseter1.servicename-20160505T090305-start jenkins pipeline - docker build &amp;&amp; 推送到仓库 jenkins pipeline - marathon拉取指定tag的commit id，开始运行service服务。 使用smartstack做服务发现工作 sensu进行监控工作 https://github.com/Yelp/paasta https://www.youtube.com/watch?v=vISUXKeoqXM&amp;feature=youtu.be]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[marathon---event-bus]]></title>
    <url>%2F2017%2F09%2F07%2Fmarathon---event-bus%2F</url>
    <content type="text"><![CDATA[marathon有一个内部的event bus，处理所有的API请求和扩容event。通过订阅这个eventbus，我们可以实时了解发生的所有事件。event bus对于整合基于状态的实体很有用处，比如load balancer，或者compile statistics。 Evet可通过插件化的subscriber进行订阅。 event bus有两个API： event stream。详情见/v2/events/ 回调endpoint，以json的方式把event发送到一个或多个endpoint中。这个方式已经过期了。 event stream相对好处 容易设置 传递较快。没有req/resp的处理 event是有序的 回调方式1./bin/start --master ... --event_subscriber http_callback --http_endpoints http://host1/foo,http://host2/bar event类型API请求marathon接收到修改某个app的时候收到的API请求。1234567891011121314151617181920212223242526272829303132&#123; &quot;eventType&quot;: &quot;api_post_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;clientIp&quot;: &quot;0:0:0:0:0:0:0:1&quot;, &quot;uri&quot;: &quot;/v2/apps/my-app&quot;, &quot;appDefinition&quot;: &#123; &quot;args&quot;: [], &quot;backoffFactor&quot;: 1.15, &quot;backoffSeconds&quot;: 1, &quot;cmd&quot;: &quot;sleep 30&quot;, &quot;constraints&quot;: [], &quot;container&quot;: null, &quot;cpus&quot;: 0.2, &quot;dependencies&quot;: [], &quot;disk&quot;: 0.0, &quot;env&quot;: &#123;&#125;, &quot;executor&quot;: &quot;&quot;, &quot;healthChecks&quot;: [], &quot;id&quot;: &quot;/my-app&quot;, &quot;instances&quot;: 2, &quot;mem&quot;: 32.0, &quot;ports&quot;: [10001], &quot;requirePorts&quot;: false, &quot;storeUrls&quot;: [], &quot;upgradeStrategy&quot;: &#123; &quot;minimumHealthCapacity&quot;: 1.0 &#125;, &quot;uris&quot;: [], &quot;user&quot;: null, &quot;version&quot;: &quot;2014-09-09T05:57:50.866Z&quot; &#125;&#125; 状态更新task的状态变更时发送的event:1234567891011&#123; &quot;eventType&quot;: &quot;status_update_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;slaveId&quot;: &quot;20140909-054127-177048842-5050-1494-0&quot;, &quot;taskId&quot;: &quot;my-app_0-1396592784349&quot;, &quot;taskStatus&quot;: &quot;TASK_RUNNING&quot;, &quot;appId&quot;: &quot;/my-app&quot;, &quot;host&quot;: &quot;slave-1234.acme.org&quot;, &quot;ports&quot;: [31372], &quot;version&quot;: &quot;2014-04-04T06:26:23.051Z&quot;&#125; 所有的状态 TASK_STAGING TASK_STARTING TASK_RUNNING TASK_FINISHED TASK_FAILED TASK_KILLING TASK_KILLED TASK_LOST后面四种是终止状态。 framework message1234567&#123; &quot;eventType&quot;: &quot;framework_message_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;slaveId&quot;: &quot;20140909-054127-177048842-5050-1494-0&quot;, &quot;executorId&quot;: &quot;my-app.3f80d17a-37e6-11e4-b05e-56847afe9799&quot;, &quot;message&quot;: &quot;aGVsbG8gd29ybGQh&quot;&#125; Event订阅订阅者消息有变化时触发。123456&#123; &quot;eventType&quot;: &quot;subscribe_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;clientIp&quot;: &quot;1.2.3.4&quot;, &quot;callbackUrl&quot;: &quot;http://subscriber.acme.org/callbacks&quot;&#125; 123456&#123; &quot;eventType&quot;: &quot;unsubscribe_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;clientIp&quot;: &quot;1.2.3.4&quot;, &quot;callbackUrl&quot;: &quot;http://subscriber.acme.org/callbacks&quot;&#125; Health Check1234567891011121314&#123; &quot;eventType&quot;: &quot;add_health_check_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;appId&quot;: &quot;/my-app&quot;, &quot;healthCheck&quot;: &#123; &quot;protocol&quot;: &quot;HTTP&quot;, &quot;path&quot;: &quot;/health&quot;, &quot;portIndex&quot;: 0, &quot;gracePeriodSeconds&quot;: 5, &quot;intervalSeconds&quot;: 10, &quot;timeoutSeconds&quot;: 10, &quot;maxConsecutiveFailures&quot;: 3 &#125;&#125; 1234567891011121314&#123; &quot;eventType&quot;: &quot;remove_health_check_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;appId&quot;: &quot;/my-app&quot;, &quot;healthCheck&quot;: &#123; &quot;protocol&quot;: &quot;HTTP&quot;, &quot;path&quot;: &quot;/health&quot;, &quot;portIndex&quot;: 0, &quot;gracePeriodSeconds&quot;: 5, &quot;intervalSeconds&quot;: 10, &quot;timeoutSeconds&quot;: 10, &quot;maxConsecutiveFailures&quot;: 3 &#125;&#125; 123456789101112131415&#123; &quot;eventType&quot;: &quot;failed_health_check_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;appId&quot;: &quot;/my-app&quot;, &quot;taskId&quot;: &quot;my-app_0-1396592784349&quot;, &quot;healthCheck&quot;: &#123; &quot;protocol&quot;: &quot;HTTP&quot;, &quot;path&quot;: &quot;/health&quot;, &quot;portIndex&quot;: 0, &quot;gracePeriodSeconds&quot;: 5, &quot;intervalSeconds&quot;: 10, &quot;timeoutSeconds&quot;: 10, &quot;maxConsecutiveFailures&quot;: 3 &#125;&#125; 12345678&#123; &quot;eventType&quot;: &quot;health_status_changed_event&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;appId&quot;: &quot;/my-app&quot;, &quot;instanceId&quot;: &quot;my-app.instance-c7c311a4-b669-11e6-a48f-0ea4f4b1778c&quot;, &quot;version&quot;: &quot;2014-04-04T06:26:23.051Z&quot;, &quot;alive&quot;: true&#125; 12345678910&#123; &quot;appId&quot;: &quot;/my-app&quot;, &quot;taskId&quot;: &quot;my-app_0-1396592784349&quot;, &quot;version&quot;: &quot;2016-03-16T13:05:00.590Z&quot;, &quot;reason&quot;: &quot;500 Internal Server Error&quot;, &quot;host&quot;: &quot;localhost&quot;, &quot;slaveId&quot;: &quot;4fb620fa-ba8d-4eb0-8ae3-f2912aaf015c-S0&quot;, &quot;eventType&quot;: &quot;unhealthy_task_kill_event&quot;, &quot;timestamp&quot;: &quot;2016-03-21T09:15:10.764Z&quot;&#125; deployment123456&#123; &quot;eventType&quot;: &quot;group_change_success&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;groupId&quot;: &quot;/product-a/backend&quot;, &quot;version&quot;: &quot;2014-04-04T06:26:23.051Z&quot;&#125; 1234567&#123; &quot;eventType&quot;: &quot;group_change_failed&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;groupId&quot;: &quot;/product-a/backend&quot;, &quot;version&quot;: &quot;2014-04-04T06:26:23.051Z&quot;, &quot;reason&quot;: &quot;&quot;&#125; 12345&#123; &quot;eventType&quot;: &quot;deployment_success&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;id&quot;: &quot;867ed450-f6a8-4d33-9b0e-e11c5513990b&quot;&#125; 12345&#123; &quot;eventType&quot;: &quot;deployment_failed&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;id&quot;: &quot;867ed450-f6a8-4d33-9b0e-e11c5513990b&quot;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&#123; &quot;eventType&quot;: &quot;deployment_info&quot;, &quot;timestamp&quot;: &quot;2014-03-01T23:29:30.158Z&quot;, &quot;plan&quot;: &#123; &quot;id&quot;: &quot;867ed450-f6a8-4d33-9b0e-e11c5513990b&quot;, &quot;original&quot;: &#123; &quot;apps&quot;: [], &quot;dependencies&quot;: [], &quot;groups&quot;: [], &quot;id&quot;: &quot;/&quot;, &quot;version&quot;: &quot;2014-09-09T06:30:49.667Z&quot; &#125;, &quot;target&quot;: &#123; &quot;apps&quot;: [ &#123; &quot;args&quot;: [], &quot;backoffFactor&quot;: 1.15, &quot;backoffSeconds&quot;: 1, &quot;cmd&quot;: &quot;sleep 30&quot;, &quot;constraints&quot;: [], &quot;container&quot;: null, &quot;cpus&quot;: 0.2, &quot;dependencies&quot;: [], &quot;disk&quot;: 0.0, &quot;env&quot;: &#123;&#125;, &quot;executor&quot;: &quot;&quot;, &quot;healthChecks&quot;: [], &quot;id&quot;: &quot;/my-app&quot;, &quot;instances&quot;: 2, &quot;mem&quot;: 32.0, &quot;ports&quot;: [10001], &quot;requirePorts&quot;: false, &quot;storeUrls&quot;: [], &quot;upgradeStrategy&quot;: &#123; &quot;minimumHealthCapacity&quot;: 1.0 &#125;, &quot;uris&quot;: [], &quot;user&quot;: null, &quot;version&quot;: &quot;2014-09-09T05:57:50.866Z&quot; &#125; ], &quot;dependencies&quot;: [], &quot;groups&quot;: [], &quot;id&quot;: &quot;/&quot;, &quot;version&quot;: &quot;2014-09-09T05:57:50.866Z&quot; &#125;, &quot;steps&quot;: [ &#123; &quot;action&quot;: &quot;ScaleApplication&quot;, &quot;app&quot;: &quot;/my-app&quot; &#125; ], &quot;version&quot;: &quot;2014-03-01T23:24:14.846Z&quot; &#125;, &quot;currentStep&quot;: &#123; &quot;actions&quot;: [ &#123; &quot;type&quot;: &quot;ScaleApplication&quot;, &quot;app&quot;: &quot;/my-app&quot; &#125; ] &#125;&#125;]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[marathon---constraints]]></title>
    <url>%2F2017%2F09%2F07%2Fmarathon---constraints%2F</url>
    <content type="text"><![CDATA[constraint控制app运行的位置，可以用来调优容错以及本地部署。constraint包含3个部分：fieldname， operator， 可选参数。field可以是agent node 的hostname或者agent node的其他属性。 FieldsHostname支持所有的marathon operator 。 Attribute如果filed name不是hostname，就会被看作一个mesos agent node 的attribute。一个mesos agent attribute可以用来给agent node打tag。可以看一下mesos-slave –help. 如果制定的attribute还没有在agent node上定义，那么多数的operator都会拒绝在agent node上运行任务。目前是有UNLIKE operator会接受这个offer，其他的都会拒绝。 attribute field支持marathon所有的operator。 Marathon 支持text，scalar，range以及set类型的attribute的值。对于scalar、range、set类型的值，marathon会基于格式化的值执行一个字符串对比操作。对于range和set类型，格式分别是[begin-end,….]和{item,….}。例如: [100-200]和{a,b,c] LIKE和UNLIKE的operator支持正则表达式。 OperatorUNIQUE operatorUNIQUE是告诉marathon对于一个app的所有task保证attribute的唯一性。例如下面的contraint保证了每个host上只运行一个app任务。 123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-unique&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;hostname&quot;, &quot;UNIQUE&quot;]] &#125;&apos; CLUSTER operatorCLUSTER可以让app 的task运行在共享指定attribute的agent node上。对于有硬件要求的任务比较适用，或者想要把任务集中运行在同一个机栈中。123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-cluster&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;rack_id&quot;, &quot;CLUSTER&quot;, &quot;rack-1&quot;]] &#125;&apos; 也可以指定一个hostname，把app绑定在指定的一个机器上。123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-cluster&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;hostname&quot;, &quot;CLUSTER&quot;, &quot;a.specific.node.com&quot;]] &#125;&apos; GROUP_BY operator风来在不同的rack或者数据中心中均匀地分配任务，实现HA。 123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-group-by&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;rack_id&quot;, &quot;GROUP_BY&quot;]] &#125;&apos; marathon会分析mesos进来的offer，获取不同的rack_id属性。如果任务没有覆盖到所有的值，那么在constraint中指定一个数。如果不指定，可能会出现任务只被部署到一个rack的情况。123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-group-by&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;rack_id&quot;, &quot;GROUP_BY&quot;, &quot;3&quot;]] &#125;&apos; LIKE operator接收一个正则表达式作为参数，允许filed值满足这个正则的所有agent node运行当前任务。123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-group-by&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;rack_id&quot;, &quot;LIKE&quot;, &quot;rack-[1-3]&quot;]] &#125;&apos; 注意，参数一定要有，不然会有警告。 UNLIKE operator和LIKE operator一样的道理。 123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-group-by&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;rack_id&quot;, &quot;UNLIKE&quot;, &quot;rack-[7-9]&quot;]] &#125;&apos; MAX_PER operator接受一个数字参数，制定每个group的最大大小。可以用来限制跨rack或者数据中心的任务数量。 123456curl -X POST -H &quot;Content-type: application/json&quot; localhost:8080/v2/apps -d &apos;&#123; &quot;id&quot;: &quot;sleep-group-by&quot;, &quot;cmd&quot;: &quot;sleep 60&quot;, &quot;instances&quot;: 3, &quot;constraints&quot;: [[&quot;rack_id&quot;, &quot;MAX_PER&quot;, &quot;2&quot;]] &#125;&apos;]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[compose版本]]></title>
    <url>%2F2017%2F09%2F05%2Fcompose%E7%89%88%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[compose文件是一个yaml文件，定义了docker app的service, network, volume等 有三个版本 version1， 在YAML文件中别指定version就是了。 version2.x。在YAML里指定version: ‘2’,或者version: ‘2.1’ version3.x。最新版本，也是最推荐使用的版本，可以兼容swarm模式。通过在YAML中指定version: ‘3’或者version: ‘3.1’声明。 如果使用了多个compose文件模式，或者用了extending service，那么每个文件的version声明应该是一样的。 Version1不额外指定version的compose文件被看作”version1”。所有的service都被定义在yaml文件的root上。 version1是compose 1.6.x开始支持的，未来会被放弃。 version1文件不能声明volume, network或者build arguments。 compose不能使用网络：每个container都是连接在default bridge网络上的，相互之间用IP沟通。可以使用links来启用container之间的相互发现。 例子：12345678910web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code links: - redisredis: image: redis version 2compose文件中，必须在root指定相应version，而且所有的service都要定义在services下。 version 2要求compose 1.6.0+， docker engine版本1.10。0+。 可以通过volumes指定volume，也可以通过networks指定容器的网络。 默认情况下，每个container都加入一个app级别的默认网络，而且可以通过自身的hostname被发现。也就是说links就没啥必要了，详情参考。 简单例子：12345678910version: &apos;2&apos;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code redis: image: redis 稍微复杂点儿的例子, 定义了volumes和network12345678910111213141516171819202122232425version: &apos;2&apos;services: web: build: . ports: - &quot;5000:5000&quot; volumes: - .:/code networks: - front-tier - back-tier redis: image: redis volumes: - redis-data:/var/lib/redis networks: - back-tiervolumes: redis-data: driver: localnetworks: front-tier: driver: bridge back-tier: driver: bridge 还有一些其他的选项来支持网络： aliases depends_on用来指定service之间的依赖关系 1234567891011version: &apos;2&apos;services: web: build: . depends_on: - db - redis redis: image: redis db: image: postgres ipv4_address, ipv6_address Version2.1是version2的升级版，需要docker engine是1.12.0+， compose版本是1.9.0+。 添加了以下参数 link_local_ips isolation volume和network的labels volumes的name userns_mode healthcheck sysctls pids_limit Version2.2version2.1的升级，，需要docker engine是1.14.0+， compose版本是1.13.0+。允许指定默认的scale数。添加了下面参数： init scale version2.3version2.2的升级，，需要docker engine是17.06.0+， compose版本是1.16.0+。 添加了下面参数: target for build configurations start_period for healthchecks version3主要是解决compose和docker swarm mode的兼容性问题。 移除选项：volume_driver, volumes_from, cpu_shares, cpu_quota, cpuset, mem_limit, memswap_limit, extends, group_add 添加选项：deploy version3.3version3的升级版。添加以下参数： build的labels credential_spec configs deploy endpoint_mode]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[label的生命周期]]></title>
    <url>%2F2017%2F09%2F04%2Flabel%E7%9A%84%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%2F</url>
    <content type="text"><![CDATA[Prometheus label让我们为我们的app部署与组织间建立模型。直接支持所有可能出现的配置是不太可能，但是prometheus给力一个relabel功能，给出了足够大的灵活性。 基本原则：你的服务发现给你提供了一些元数据，例如机器类型、tag、region，这些都可以在_meta*的label中找到。然后你可以在relbel_configs配置中把他们进行定制化修改。还可以制定drop和keep操作。 在我们抓取target的时候也一样，metric_relabel_config配置可以让我们纠正数据的时间戳。还可以过滤掉一些比较昂贵，比如 太大的metric。 为了更好的理解这些内容，看下图： 这个图包含了创建target、爬取、在时序数据插入DB之前的操作等。 从服务发现获取target 基于配置设置_param*的label 如果job,scheme或者metrics_path的label没有设置，就基于配置设置一下 是否有address的label 没有，丢掉这个target 有，执行relabel_config drop操作的话直接drop掉 address有么有端口 有端口， address是否包含 “/“ drop target 删除所有以_meta开头的label 没有端口，scheme是http就默认80，https就默认443 下面是实际爬取过程中发生的事情： _param*的label包含每个URL参数的第一个值，可以进行relabel。在爬取的时候，这些会与第二个以及后续的参数值连接在一起。 因为metric_relabel_configs需要每次爬取的时候都执行一次，最好还是通过improve instrumentation。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus配置]]></title>
    <url>%2F2017%2F09%2F01%2FPrometheus%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[主要定义了需要抓取的job和这些job所包含的instance，还有告警规则文件。 可以执行 prometheus -h 来查看具体支持的命令行参数。 Prometheus可以在运行时reload，如果新的配置是有效的，我们只需要发送一个SIGHUP给到Prometheus进程，或者发送一个HTTP POST请求到/-/reload终端REST API。 配置文件可以通过-config.file指定具体文件，需要是yaml格式。 下面是来源于https://github.com/prometheus/prometheus/blob/master/config/testdata/conf.good.yml的示例配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228# 全局配置global: scrape_interval: 15s evaluation_interval: 30s # scrape_timeout is set to the global default (10s). external_labels: monitor: codelab foo: bar# 告警规则文件rule_files:- &quot;first.rules&quot;- &quot;my/*.rules&quot;# 与实际远程写入相关的配置remote_write: - url: http://remote1/push write_relabel_configs: - source_labels: [__name__] regex: expensive.* action: drop - url: http://remote2/push# 抓取配置scrape_configs:- job_name: prometheus honor_labels: true # scrape_interval is defined by the configured global (15s). # scrape_timeout is defined by the global default (10s). # metrics_path defaults to &apos;/metrics&apos; # scheme defaults to &apos;http&apos;. #基于文件的服务发现 file_sd_configs: - files: - foo/*.slow.json - foo/*.slow.yml - single/file.yml refresh_interval: 10m - files: - bar/*.yaml static_configs: - targets: [&apos;localhost:9090&apos;, &apos;localhost:9191&apos;] labels: my: label your: label relabel_configs: - source_labels: [job, __meta_dns_name] regex: (.*)some-[regex] target_label: job replacement: foo-$&#123;1&#125; # action defaults to &apos;replace&apos; - source_labels: [abc] target_label: cde - replacement: static target_label: abc - regex: replacement: static target_label: abc bearer_token_file: valid_token_file- job_name: service-x basic_auth: username: admin_name password: &quot;multiline\nmysecret\ntest&quot; scrape_interval: 50s scrape_timeout: 5s sample_limit: 1000 metrics_path: /my_path scheme: https dns_sd_configs: - refresh_interval: 15s names: - first.dns.address.domain.com - second.dns.address.domain.com - names: - first.dns.address.domain.com # refresh_interval defaults to 30s. relabel_configs: - source_labels: [job] regex: (.*)some-[regex] action: drop - source_labels: [__address__] modulus: 8 target_label: __tmp_hash action: hashmod - source_labels: [__tmp_hash] regex: 1 action: keep - action: labelmap regex: 1 - action: labeldrop regex: d - action: labelkeep regex: k metric_relabel_configs: - source_labels: [__name__] regex: expensive_metric.* action: drop- job_name: service-y# 基于consul的服务发现 consul_sd_configs: - server: &apos;localhost:1234&apos; token: mysecret services: [&apos;nginx&apos;, &apos;cache&apos;, &apos;mysql&apos;] scheme: https tls_config: ca_file: valid_ca_file cert_file: valid_cert_file key_file: valid_key_file insecure_skip_verify: false relabel_configs: - source_labels: [__meta_sd_consul_tags] separator: &apos;,&apos; regex: label:([^=]+)=([^,]+) target_label: $&#123;1&#125; replacement: $&#123;2&#125;- job_name: service-z tls_config: cert_file: valid_cert_file key_file: valid_key_file bearer_token: mysecret- job_name: service-kubernetes kubernetes_sd_configs: - role: endpoints api_server: &apos;https://localhost:1234&apos; basic_auth: username: &apos;myusername&apos; password: &apos;mysecret&apos;# 基于kubernetes的服务发现- job_name: service-kubernetes-namespaces kubernetes_sd_configs: - role: endpoints api_server: &apos;https://localhost:1234&apos; namespaces: names: - default# 基于marathon的服务发现- job_name: service-marathon marathon_sd_configs: - servers: - &apos;https://marathon.example.com:443&apos; tls_config: cert_file: valid_cert_file key_file: valid_key_file- job_name: service-ec2 ec2_sd_configs: - region: us-east-1 access_key: access secret_key: mysecret profile: profile- job_name: service-azure azure_sd_configs: - subscription_id: 11AAAA11-A11A-111A-A111-1111A1111A11 tenant_id: BBBB222B-B2B2-2B22-B222-2BB2222BB2B2 client_id: 333333CC-3C33-3333-CCC3-33C3CCCCC33C client_secret: mysecret port: 9100- job_name: service-nerve nerve_sd_configs: - servers: - localhost paths: - /monitoring- job_name: 0123service-xxx metrics_path: /metrics static_configs: - targets: - localhost:9090- job_name: 測試 metrics_path: /metrics static_configs: - targets: - localhost:9090- job_name: service-triton triton_sd_configs: - account: &apos;testAccount&apos; dns_suffix: &apos;triton.example.com&apos; endpoint: &apos;triton.example.com&apos; port: 9163 refresh_interval: 1m version: 1 tls_config: cert_file: testdata/valid_cert_file key_file: testdata/valid_key_file# 关于alertmanager相关的配置alerting: alertmanagers: - scheme: https static_configs: - targets: - &quot;1.2.3.4:9093&quot; - &quot;1.2.3.5:9093&quot; - &quot;1.2.3.6:9093&quot; scrape_config部分描述一些列的target和参数，指定抓取策略。一般情况下，一个scrape配置就对应一个job。 target可以是通过static_config静态配置的，也可以是使用某种动态发现机制进行动态获取的。 另外，relabel_config允许在抓取之前更新任意target的label。 已经测试过file_sd_config是可以跟随文件的变化，动态调整target的。但是对于rule文件的reload，并不能自动识别并执行，官网建议是通过定时触发prometheus的reload来实现这个东西。个人认为：如果rule 的修改也并不是那么频繁，而且既有的app里已经包含队列的话，可以在每次修改rule之后，每隔几分钟消费一次reload事件。或者可以修改prometheus的rule相关代码，让其定时消费mysql或者某些目录的rule文件，如有变化则更新rule的缓存。 其他与动态加载rule相关 https://groups.google.com/forum/#!topic/prometheus-developers/UsiCxDbFjN0 https://www.robustperception.io/reloading-prometheus-configuration/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Prometheus关键概念]]></title>
    <url>%2F2017%2F09%2F01%2FPrometheus%E5%85%B3%E9%94%AE%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[job和instance在prometheus看来，每个单独的抓取目标是一个instance，通常对应一个单独的进程。同种类型的instance的集合组成一种job。 例如，一个API server的job会有下面的instance： job: api-server instance 1.2.3.4:5769 instance 1.2.3.4:5768 自动产生的label在prometheus抓取target的时候，会自动在后面追加一些label： job instance 如果这些label是否已经出现在抓取的数据中，就参考honor_labels的配置部分。 每个instance的抓取，prometheus都存储一个sample： up{job=”“, instance=”“}: 1 表示这个instance是否健康的存在 scrape_duration_seconds{job=”“, instance=”“}: 抓取时间 scrape_samples_post_metric_relabeling{job=”“, instance=”“}:metric被relabel的sample个数 scrape_samples_scraped{job=”“, instance=”“}: target中暴露的sample个数 up时间序列一般用来监控instance的可用性。 但是，如果这样的话，我的instance就没有办法动态添加进去，因为Prometheus并没有提供动态修改target的REST API. https://prometheus.io/docs/querying/api/ 一片不错的prometheus监控使用总结 相关链接： https://groups.google.com/forum/#!topic/prometheus-users/fTPKx5Eg8bo https://github.com/prometheus/jmx_exporter/pull/180#issuecomment-326229999]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[marathon-蓝绿部署]]></title>
    <url>%2F2017%2F08%2F31%2Fmarathon-%E8%93%9D%E7%BB%BF%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[蓝绿部署是一种安全部署app的方式，它会创建两个版本的app(BLUE和GREEN)。要部署一个新版本的app，应该先将所有旧版本的请求、挂起的操作等处理完，并且将流量定向到新版本app那边。蓝绿部署预估app的停掉的时间，允许我们在必要时快速从BLUE版本恢复。详细过程，参考蓝绿部署 不错的解释：http://www.tuicool.com/articles/2Iji2ue https://www.v2ex.com/t/344341 在生产环境中，最好将这个过程脚本化，将它集成进已经存在的部署系统中。下面，我们提供一个使用DC/OS CLI的例子。 过程我们需要把BLUE，也就是当前版本的app，替换成GREEN(新版本)。 在marathon上启动新版本的app。给app名字一个唯一的id，比如git的commit id。在这个例子中，我们把GREEN添加到新版本app的名字中：12# launch green dcos marathon app add green-myapp.json 注意：如果你使用的不是dcos的CLI，而是API，那命令会长很多。1curl -H &quot;Content-Type: application/json&quot; -X POST -d @green-myapp.json &lt;hosturl&gt;/marathon/v2/apps 扩容GREEN的app。初始的时候一般是0，设置一下需要的实例数量。记住，现在还并没有流量过来：我们并没有到LB那边注册。 12# scale greendcos marathon app update /green-myapp instances=1 等所有的GREEN app都健康部署。 12# wait until healthydcos marathon app show /green-myapp | jq &apos;.tasks[].healthCheckResults[] | select (.alive == false)&apos; 如果看到又不健康的，就中止部署，开始rollback 把GREEN app加入到LB中 选一个或多个BLUE app。 12# pick tasks from bluedcos marathon task list /blue-myapp 更新LB，将这些BLUE app移除资源池 等所有的BLUE app都被移除完，这个过程可以通过app的metrics终端API进行监控。 一旦BLUE app的操作都完成，就杀掉BLUE app的进程，并缩容，避免被杀掉的重启.12# kill and scale blue tasksecho &quot;&#123;\&quot;ids\&quot;:[\&quot;&lt;task_id&gt;\&quot;]&#125;&quot; | curl -H &quot;Content-Type: application/json&quot; -X POST -d @- &lt;hosturl&gt;/marathon/v2/tasks/delete?scale=true marathon会移除指定的所有的实例。上面的hosturl是master node所在的位置10.重复步骤2-9，直到不再有BLUE app 从marathon中移除BLUE app。12# remove bluedcos marathon app remove /blue-myapp]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[marathon-应用部署]]></title>
    <url>%2F2017%2F08%2F31%2Fmarathon-%E5%BA%94%E7%94%A8%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[每次app的配置文件修改生效就出发一次deployment。一次deployment是一系列动作的集合： 启动或者停止一个或多个app 升级一个或多个app 扩展一个或多个app Deloyment需要花费时间，并不能马上使app恢复可用。 只要每个app只被一次deployment修改，那么你就可以同时执行多个deployment。在我们尝试执行一个已经被别人修改过的deployment 的时候，marathon会驳回此次deployment。 依赖没有依赖的app可以以任意顺序部署。如果app间有依赖，那么deployment就应该严格按照app的依赖顺序执行。 如上图，app是以来db的。 启动： 先启动db，再启动app停止： 先停止app，再停止db升级： 参考 Rolling Restart 部分扩容： db先扩容，然后app Rolling Restart我们可以通过 Rolling Restart来部署新版本的app。通常，有两个阶段：启动一些新版本的进程，然后停掉旧版本的进程。 在marathon中，可以在app级别使用minimumHealthCapacity 定义一个upgrade strategy。 minimumHealthCapacity是指达升级过程中，处于正常服务状态的实例不能少于整体的百分之多少。如果是0， 在新版本部署前，就把所有的旧版本杀掉。如果是1，就先启动所有的新版本实例，然后再杀掉所有的旧版本实例。 这个动作在有依赖关系的时候会复杂一些。上面的例子来说： 先升级db到所有的实例都完成； 升级app到所有的实例都完成。上面两个app的具体过程，还是按照upgradeStrategy来处理。假设我们设置的db和app对应的参数分别是0.6和0.8，那么过程中就会出现db有12个实例（6个新的，6个旧的），app出现32个实例（16个新的，16个旧的） 强制deployment一个app只能同时被一个deployment修改。其他的修改必须等待当前的完成才行。如果运行deployement的时候加上force标记就可以打破这个限制。 但是注意： force标记只适用于失败的deployment 如果指定了force，那么所有其他的修改都会被取消。这个操作可能让系统进入一个不一致状态，尤其是在一个app执行rolling upgrade的时候，会导致一部分新版本app与一些旧版本app并行的秦广。如果新的deployment没有更新这些app，就会一直处于这种不一致状态了。 唯一可以进行force-update的app是那些只有一个app实例的。要强制部署多个app的唯一理由，是修正一个失败的deployment。 失败的deploymentdeployment是包含很多严谨的、有次序的步骤的。 下面是一些永远不能成功的情景： 新app没启动成功 新app不能进入healthy状态 新app的依赖没有声明、或者不可用 集群资源耗尽 app使用docker container，docker相关配置没有启用 这些情况会导致app一直循环重启，又不能成功，需要重新部署正确的app配置才行。 /v2/deployment终端API参考 marathon的REST API]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RunningAlert]]></title>
    <url>%2F2017%2F08%2F31%2FRunningAlert%2F</url>
    <content type="text"><![CDATA[ReadMe一个大数据集群监控项目，主要依赖于当前比较流行的Prometheus。 流程1. 采集注册指的是向JsonExporter123app_name : h2_streaming_...url: http://sdfsfdsd:0080/metricsservice_type : spark_streaming 提供数据示例参考，也就是马上爬取一下metrics，并显示出来，以方便表达式编写。 2. json_exporter 开始采集启动marathon上json_exporter的docker镜像，并指定当前对应的参数。开始metric的采集工作。 3. 指定alertManager的告警规则直接开放给用户直接编辑 1234567891011121314151617ALERT &#123;&#123; pro_alert.alert_name &#125;&#125; IF absent(json_val&#123;rowkey=~&quot;&#123;&#123; pro_alert.key_regex &#125;&#125;&quot;, app_name=&quot;&#123;&#123; app_name &#125;&#125;&quot;&#125;) FOR &#123;&#123; pro_alert.time_spent &#125;&#125;s LABELS &#123; severity = &quot;critical&quot;&#125; ANNOTATIONS &#123; summary= &quot;&#123;&#123; pro_alert.summary &#125;&#125;&quot;, description= &quot;&#123;&#123; pro_alert.description &#125;&#125;&quot; &#125;ALERT &#123;&#123; pro_alert.alert_name &#125;&#125; IF consul_health_service_status&#123;check=&quot;service:web&quot;,node=&quot;agent-two&quot;,service_id=&quot;web&quot;,service_name=&quot;web&quot;,status=&quot;critical&quot;&#125; == 1 FOR &#123;&#123; pro_alert.time_spent &#125;&#125;s LABELS &#123; severity = &quot;critical&quot;&#125; ANNOTATIONS &#123; summary= &quot;&#123;&#123; pro_alert.summary &#125;&#125;&quot;, description= &quot;&#123;&#123; pro_alert.description &#125;&#125;&quot; &#125; 之后，保存或者保存并立即生效。 立即生效其实就是调用一下prometheus的alertManager的/-/reload/接口。 之后Prometheus会自动检验对应metric规则，并发送给AlertManager的receiver，并产生告警动作。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[marathon坑记]]></title>
    <url>%2F2017%2F08%2F30%2Fmarathon%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[资源不足场景运行marathon官网提供的docker get started案例(https://mesosphere.github.io/marathon/docs/application-basics.html)可以成功。12345678910111213141516&#123; &quot;id&quot;: &quot;basic-3&quot;, &quot;cmd&quot;: &quot;python3 -m http.server 8080&quot;, &quot;cpus&quot;: 0.5, &quot;mem&quot;: 32.0, &quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;python:3&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 8080, &quot;hostPort&quot;: 0 &#125; ] &#125; &#125;&#125; 我自己提了一个,屡次卡住不能deploy成功。12345678910111213141516171819202122232425262728293031323334353637383940&#123; &quot;id&quot;: &quot;/docker-tt&quot;, &quot;cmd&quot;: null, &quot;cpus&quot;: 1, &quot;mem&quot;: 128, &quot;disk&quot;: 0, &quot;instances&quot;: 1, &quot;acceptedResourceRoles&quot;: [ &quot;*&quot; ], &quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;volumes&quot;: [], &quot;docker&quot;: &#123; &quot;image&quot;: &quot;nginx:stable-alpine&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 80, &quot;hostPort&quot;: 880, &quot;servicePort&quot;: 10000, &quot;protocol&quot;: &quot;tcp&quot;, &quot;name&quot;: &quot;nn&quot;, &quot;labels&quot;: &#123;&#125; &#125; ], &quot;privileged&quot;: false, &quot;parameters&quot;: [], &quot;forcePullImage&quot;: false &#125; &#125;, &quot;portDefinitions&quot;: [ &#123; &quot;port&quot;: 10000, &quot;protocol&quot;: &quot;tcp&quot;, &quot;name&quot;: &quot;default&quot;, &quot;labels&quot;: &#123;&#125; &#125; ]&#125; 参考了官网的这个链接 的第一个，提示说：如果app一直都处于waiting的状态，那么很可能是由于资源不足的造成的，例如CPU，内存，磁盘等。 最后还说，如果还找不到问题，就到github提一个issue，把/state的内容贴近issue，这样就可以让社区的人帮忙分析。 按照提示，我么有找到任何资源匮乏，当前的framework占用资源都是0.在查看slaves提供的物理资源的时候被亮瞎了狗眼：1234567&quot;resources&quot;: &#123; &quot;disk&quot;: 12758, &quot;mem&quot;: 2767, &quot;gpus&quot;: 0, &quot;cpus&quot;: 2, &quot;ports&quot;: &quot;[31000-32000]&quot;&#125;, 注意ports…..每个slave为什么要把自己的端口范围也限制啊……跪了。把上面的docker启动host端口改为31111后，测试通过，成功部署了nginx服务。 其他官网端口映射相关 portMapping： 对于所有需要给外部访问的docker容器，端口映射都很重要。一个端口映射是一个包含host port、container port、service port以及protocaol的tuple。有时也需要多个端口映射定义。不固定的hostPort默认是0，这样marathons会随机选一个端口。在Docker的USER模式下，hostPort是几乎不变的：hostport并不是必要的，如果没有制定，那么marathon不会自动随机选一个。这样就允许container可以在USER网络里发布，只包含containerPort和发现信息，但是不暴露这些端口给hsot网络。 portDefinitions： 一个数据定义了端口资源池。只在使用HOST网络，并且没有指定portMapping的时候是必须要有的。 servicePort：当在marathon创建新的app时，要为其指定一个或多个service port。你可以指定合理的port，也可以设为0让marathon自动分配可用端口。如果自己选择，就需要确保这个端口在所有app中是唯一的。 如果containerport和hostport同时被设置为0，那么他俩的端口虽然是会被随机分配，但是他俩是一样的，比如8888，那么就都是8888。 配置实例hosthost网络模式是默认的网络模式，也是非docker app的唯一网络模式。注意在你的dockerfile里expose端口并不是必须的。 示例1234567&quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;my-image:1.0&quot;, &quot;network&quot;: &quot;HOST&quot; &#125;&#125;, 指定端口的两种方式123&quot;ports&quot;: [ 2001, 2002, 3000 ], 123&quot;portDefinitions&quot;: [ &#123;&quot;port&quot;: 2001&#125;, &#123;&quot;port&quot;: 2002&#125;, &#123;&quot;port&quot;: 3000&#125; ], 这时环境变量$PORT0,$PORT1, $PORT2就分别是2001，2002，3000，如果都是0的话就都是随机分配的。 另外，保证一个服务发现程序例如haproxy从service port请求host port也是必须的。那么我们或许希望这个app的service ports与host ports保持一致，那么就可以设置requirePorts为true。这样marathons就只调度这个app到有这些端口的agent上了。1234&quot;ports&quot;: [ 2001, 2002, 3000 ], &quot;requirePorts&quot; : true 现在，service 和host的ports就都是2001， 2002， 3000了。 定义portDefinitions队列可以让我们制定protocol，每个port都有一个名字和自己的label。在启动新的task的时候，marathon会把metadata发送给mesos。mesos会在这个task的discovery字段中暴露这些信息。自定义网络发现服务可以读取这个字段，发现app服务。 12345678&quot;portDefinitions&quot;: [ &#123; &quot;port&quot;: 0, &quot;protocol&quot;: &quot;tcp&quot;, &quot;name&quot;: &quot;http&quot;, &quot;labels&quot;: &#123;&quot;VIP_0&quot;: &quot;10.0.0.1:80&quot;&#125; &#125; ], 这是一个动态的tcp端口，name为http，并包含一个label。 port字段是必须的，protocol、name、labels是可选的。如果portDefinitions中只定义port字段的时候，其实跟定义ports数组没有分别。 注意：如果ports和portDefinitions内容不一致，那么就不能同时定义。 bridgebridge网络模型允许映射hostport到内部的container端口，而且只能用于docker container。通常在使用一个拥有固定端口的docker image的时候只用这种方式。 1234567&quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;my-image:1.0&quot;, &quot;network&quot;: &quot;BRIDGE&quot; &#125; &#125;, 指定USER网络12345678910&quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;my-image:1.0&quot;, &quot;network&quot;: &quot;USER&quot; &#125; &#125;, &quot;ipAddress&quot;: &#123; &quot;networkName&quot;: &quot;someUserNetwork&quot; &#125; 指定端口 端口隐射与把-p参数传递给docker命令行差不多。123456789101112&quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;my-image:1.0&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 0, &quot;hostPort&quot;: 0 &#125;, &#123; &quot;containerPort&quot;: 0, &quot;hostPort&quot;: 0 &#125;, &#123; &quot;containerPort&quot;: 0, &quot;hostPort&quot;: 0 &#125; ] &#125;&#125;, 上面的例子中，containerPort会和hostPort一致，都是随机的。 123456789101112&quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;my-image:1.0&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 80, &quot;hostPort&quot;: 0 &#125;, &#123; &quot;containerPort&quot;: 443, &quot;hostPort&quot;: 0 &#125;, &#123; &quot;containerPort&quot;: 4000, &quot;hostPort&quot;: 0 &#125; ] &#125;&#125;, 这个例子，marathon会随机分配端口来与容器的80，443，4000进行对应。 marathon会为每port都穿件service port。service port用来给服务发现程序使用，通常用一些常见的port。123456789101112&quot;container&quot;: &#123; &quot;type&quot;: &quot;DOCKER&quot;, &quot;docker&quot;: &#123; &quot;image&quot;: &quot;my-image:1.0&quot;, &quot;network&quot;: &quot;BRIDGE&quot;, &quot;portMappings&quot;: [ &#123; &quot;containerPort&quot;: 80, &quot;hostPort&quot;: 0, &quot;protocol&quot;: &quot;tcp&quot;, &quot;servicePort&quot;: 2000 &#125;, &#123; &quot;containerPort&quot;: 443, &quot;hostPort&quot;: 0, &quot;protocol&quot;: &quot;tcp&quot;, &quot;servicePort&quot;: 2001 &#125;, &#123; &quot;containerPort&quot;: 4000, &quot;hostPort&quot;: 0, &quot;protocol&quot;: &quot;udp&quot;, &quot;servicePort&quot;: 3000&#125; ] &#125;&#125;, host port还是随机的，但是service port固定了。HAProxy就可以配置路由从这些service port 到host port 了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus的alertmanager部分]]></title>
    <url>%2F2017%2F08%2F23%2Fprometheus%E7%9A%84alertmanager%E9%83%A8%E5%88%86%2F</url>
    <content type="text"><![CDATA[概念分组把近似的告警放进单个通知中。 例如：一个服务的几百台实例分布在不同的机器中，如果出现局部的网络问题，那么可能会报出部分服务大约100个报警信息，我们是需要知道哪些实例有问题的，这样才能快速定位区域。但是没有必要发送几百条通知，只需要一条通知，包含所有的实例列表就可以了。 inhibitinhibition是用来避免关联报警的。 例如：如果一个集群不可用的告警已经firing了。那么Altermanager可以让其他的与集群相关的告警不发出。这样能避免此种类型的告警爆炸问题。 silence一般用来指定不告警的时段。通常基于matcher进行配置。 告警规则12345ALERT &lt;alert name&gt; IF &lt;expression&gt; [ FOR &lt;duration&gt; ] 发生持续时长 [ LABELS &lt;label set&gt; ] 为告警指定label，同样label的告警会被最新的告警覆盖 [ ANNOTATIONS &lt;label set&gt; ] 例子123456789101112131415161718# Alert for any instance that is unreachable for &gt;5 minutes.ALERT InstanceDown IF up == 0 FOR 5m LABELS &#123; severity = &quot;page&quot; &#125; ANNOTATIONS &#123; summary = &quot;Instance &#123;&#123; $labels.instance &#125;&#125; down&quot;, description = &quot;&#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125; has been down for more than 5 minutes.&quot;, &#125;# Alert for any instance that have a median request latency &gt;1s.ALERT APIHighRequestLatency IF api_http_request_latencies_second&#123;quantile=&quot;0.5&quot;&#125; &gt; 1 FOR 1m ANNOTATIONS &#123; summary = &quot;High request latency on &#123;&#123; $labels.instance &#125;&#125;&quot;, description = &quot;&#123;&#123; $labels.instance &#125;&#125; has a median request latency above 1s (current value: &#123;&#123; $value &#125;&#125;s)&quot;, &#125; label存的是实例的label信息，value是IF表达式的结果。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[apache-atlas数据治理]]></title>
    <url>%2F2017%2F08%2F22%2Fapache-atlas%E6%95%B0%E6%8D%AE%E6%B2%BB%E7%90%86%2F</url>
    <content type="text"><![CDATA[Atlas是一个可扩展的一系列的核心基础管理服务。 特性 数据分类 为数据定义面向业务的分类标签 定义、annotate、自动获取数据集之间的关系【source、target、中间处理的process】 导出元数据到第三方系统 中心化监听 捕获每个app、process等数据交互的安全信息 捕获每个步骤【execution、step、activities】的操作信息 搜索和lineage 预定义导航路径来检索数据分类和监听信息 基于文本的索引，定位对应数据和监听的事件 对数据集的血统可视化，可以看到操作、安全等信息 安全和策略引擎 基于数据分类、属性、角色的运行时Rationalize compliance policy Advanced definition of policies for preventing data derivation based on classification (i.e. re-identification) – Prohibitions Column and Row level masking based on cell values and attibutes.]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[prometheus自定义exporter]]></title>
    <url>%2F2017%2F08%2F18%2Fprometheus%E8%87%AA%E5%AE%9A%E4%B9%89exporter%2F</url>
    <content type="text"><![CDATA[可维护性和简洁度最核心的地方是考虑一下获取metrics的成本与途径。 如果已经有了一个不错的、不怎么修改的metrics，那么就很容易搞定。例如haproxy exporter 如果有上百个metrcis，而且随着版本变化metric也一直在变，那就会有得受了。例如mysql exporter node exporter介于两者之间，是个别的module总在变化。 配置exporter不应该关心app在哪里的配置，也能够使用filter过滤一些太稀碎的metric。 与监控系统协作的时候，framework和protocol相关的东西会很复杂。 最好是既有系统的数据结构能有prometheus一致，或者近似。这样就可以自动转化metric的形式，例如Cloudwatch, SNMP and Collectd。 其实更多情况是不那么标准的。例如jmx exporter， 用户必须说明怎样进行metric转化才行。 YAML是标准的prometheus配置文件格式。 metrics命名label避免使用type作为label名称，太通用，而且无意义。应该是用避免使用一些类似region, zone, cluster,az,datacenter,dc,woner,customer,stage,environment, env等字眼。 实例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990from prometheus_client import start_http_server, Metric, REGISTRYimport jsonimport requestsimport sysimport timeclass JsonCollector(): def init(self, endpoint, appname, srvtype, separator='__'): self._endpoint = endpoint self._appname = appname self._separator = separator self._srvtype = srvtype def __init__(self, target): self.init(target.url, target.appname, target.srvtype) def handle_type(self, prefix, d, metric ''' 本来是应该使用具体的metric类型的，调用其add_metric接口。 但是由于这是一个JSON到prometheus的通用方法，所以没有那样处理。 ''' prefix = prefix.upper() if isinstance(d, dict): self.extract_dict(prefix, d, metric) elif isinstance(d, list): self.extract_list(prefix, d, metric) else: if not isinstance(d, (str, unicode)): metric.add_sample('json_val', value=d, labels=&#123;'app_name': self._appname, 'rowkey': prefix&#125;) else: try: val = float(d) metric.add_sample('json_val', value=val, labels=&#123;'app_name': self._appname, 'rowkey': prefix&#125;) except ValueError: print('Error convert %s to float for %s' % (d, prefix)) def extract_dict(self, prefix, dd, metric): for k, v in dd.items(): if len(prefix) == 0: self.handle_type(k, v, metric) else: self.handle_type(prefix + self._separator + k, v, metric) def extract_list(self, prefix, dd, metric): for i in dd: if len(prefix) == 0: self.handle_type(dd.index(i), i, metric) else: self.handle_type(prefix + self._separator + dd.index(i), i, metric) def collect(self): ''' 此处需要注意，接口返回的是这里的结果，不要把metric弄成成员变量。 应该每次都new一个返回去，不然add_sample会把所有的变量都累加进去 ''' try: response = json.loads(requests.get(self._endpoint).content.decode('UTF-8')) metric = Metric(self._srvtype, 'spark program id', 'summary') self.handle_type('', response, metric) yield metric except Exception, e: print e.messageclass Target(): def __init__(self, srvtype, appname, url): self.srvtype = srvtype self.appname = appname self.url = urlif __name__ == '__main__': # Usage: json_exporter.py port print(sys.argv) start_http_server(int(sys.argv[1])) targets = list() targets.append(Target('flume', 'haijun_flume_test1', 'http://10.2.19.94:34545/metrics')) targets.append(Target('spark_streaming', 'h5_streaming_test1', 'http://datanode02.will.com:8088/proxy/application_1501827559666_32228/metrics/json')) for target in targets: REGISTRY.register(JsonCollector(target)) #REGISTRY.register(JsonCollector()) while True: time.sleep(10) 下面是爬取yarn中指定名称规范的运行中的app的简单示例代码。1234567891011import requestsimport jsonimport reurl = 'http://datanode02.will.com:8088/ws/v1/cluster/apps?states=RUNNING'html_doc = requests.get(url).contentresult = json.loads(html_doc)for app in result['apps']['app']: if re.match('.*_online', app['name']): print(app['name'], app['trackingUrl'] + 'metrics/json')]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Quota]]></title>
    <url>%2F2017%2F07%2F28%2FQuota%2F</url>
    <content type="text"><![CDATA[在mesos上运行多个framework的时候有个问题，就是如果当前没有其他framework接受剩余资源，默认的WDRF分配器可能会把剩余资源分配给已经满足公平共享的framework。并没有把剩余资源留下备用的机制。尽管dynamic reservations允许operator和framework可在指定的mesos agent上动态保留资源，但是并不能从根本上解决问题，因为每个agent都可能挂掉。 Mesos 0.27提出了quota，用来保证一个role会获取指定大小的最小资源保留。当然，这个role也可以获得比这个quota配置的更多的资源。 quota有些类似reserved resource，稍有不同。quota资源不能绑定到指定的agent上，就是说一个quota保证一个role会有指定大小的资源，但是可以是在集群的任意位置。然而 reservations是用来指定特定的资源给指定role的特定的agent使用。quota只能让operator通过HTTP终端进行配置，而dynamic reservations则可以通过framework的principal验证后进行控制。 注意：reserved reservations是用来满足一个role的quota配置的。例如，如果一个role指定定了一个4CPU和一个agent上的2个CPU的reserved reservations。那么这个role有一个4CPU的最小保证，而不是6个。 Terminology【术语】operator是一个管理mesos集群的东西，可以是人，工具，或者脚本。 在计算机领域，quota通常指下面的事情： 一个最小保证 一个最大范围 以上两者 在mesos中，quota是一个role即将游泳的资源的最小保证。 动力和限制通过下面场景，进一步了解quota。 场景1： 贪婪framework在一个集群中有两个framework，每个都订阅了自己特有的role，每个role都有相同的权重。frameworkA定于了role RA，frameworkB订阅了role RB。集群中只有一种单一资源：100CPU. fA只需要10个CPU，而FB需要全部的CPU。如果没有Quota的话，FA就会根据默认的公平调度，每个framework都50个CPU，那么就会有40个是浪费的。 场景2： 新framework的资源贪婪framework FB订阅role RB，是当前集群唯一的framework，那么它就都占了100CPU.如果一个fa新加入进来，需要等待FB的任务完成后才能获取属于自己的50CPU资源。 对于场景2来说，quota自己是不足以解决这个问题的，然而可以一直保持一个资源池作为保留或者或者进行资源抢占。 Operator HTTP终端master的/quota HTTP终端使operator可以操控quota。目前是提供一种类REST接口，支持下面的操作： POST设置一个新的quota DELETE 移除一个已经存在的quota GET 查询当前的quota 目前并不支持更新原有的quota，需要先删除，后添加。 工作原理首先mesos master会处理quota相关的请求，然后allocator来实施quota。 要注意quota具体怎样被实施是依赖于你所使用的allocator的。还有就是实施过程中，可能会对正在运行的framework造成一些影响，因为要把资源重新分配一下。还有资源是只可扩展的资源，端口号这种不行…. quota请求处理过程添加的过程 认证 解析与验证request 如果开启了授权，就授权认证 运行capacity heuristic 存储quota 解除outstanding offer 移除的过程 认证 确认request有效性 鉴权 移除quota capacity heuristic检查配置错误的quota可能会导致所有的framework都分配不到资源。例如假设一个opetaror给不存在的role 设置了100CPU的quota，而当前集群只用100CPU. 为了防止这种极端情形的出现，mesos master会用一个capacity heuristic来检查quota是不是对于当前集群资源合理。这个heuristic会测试总的quota是不是超过了集群非静态保留的总资源。 总资源 - 静态保留资源 》= 总资源 + quota request 注意即便当前是有足够的资源的，但是agent随时可能挂掉，导致集群不能满足配置的quota的需求。 使用force标志可以让check通过，通常是因为operator知道后期会有新的资源加进来。 接触outstanding offer设置新quota的过程中，master会接触outstanding offer。这避免了当前剩余的没有offer出去，但是很多分配出去给其他framework的资源还没有被使用，造成的资源不足情况。因此，按照下面规则解除outstanding offer： 解除至少跟quota请求中一样多的资源 如果一个agent有一个offer需要被解除，那么这个agent的所有offer都会被解除。This is done in order to make the potential offer bigger, which increases the chances that a quota’ed framework will be able to use the offer. 尽可能多的解除当前quota的role的framework的offer。这会让每个订阅这个role的framework收到一个offer。 通过wDRF Allocator实施wDRF Allocator首先会根据quota设置的分配资源给role。如果所有的quota都满足了，就为剩余的role执行标准的wDRF。 注意： 一个已经quota的role不会会获取到多一些的未被预定的不可撤销的资源。如果这个role中framework没有选择可撤销的资源，那么他们可能会在role满足之后停止获取offer。这时设置任何比这个role的fair share还低的quota值都很可能导致给这个role的总资源降低。 注意：当前auota保证也是一个quota限制。就是说，一旦一个role的quota满足了，这个role就不会获得更多的资源了。这是为了减轻缺乏quota限制。 如果有多个framework订阅了一个role，这个role有一个quota，标准的wDRF算法来决定这些framework的offer比例。 默认的wDRF allocator认为只有不可撤销的资源才是quota可用的。 容灾如果一个role里有quota，master容灾恢复就很重要。在恢复过程中会有一段时间并不是所有的agent都向master注册了自己。因此不可能所有的quota都被满足。 为了定位这个问题，如果恢复的时候任何前面的quota被探测到后，allocator就进入recovery模式，在这个过程中，allocator并不保证offer。只有满足下面任何一个条件的时候才退出这个模式 指定数量的agent重新注册自己(默认80%) 一个超时时间(默认10分钟) 当前不足 quota不允许指定详细资源粒度(例如每个节点上10个CPU) 不允许指定约束条件(比如，在互斥节点上分配2 * 5个CPU，形成HA) 不能为默认的role *设置quota 目前不支持更新 参考： http://mesos.apache.org/documentation/latest/quota/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阿里中间件笔记]]></title>
    <url>%2F2017%2F07%2F27%2F%E9%98%BF%E9%87%8C%E4%B8%AD%E9%97%B4%E4%BB%B6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[AliSQL：何登成 问题 主从复制，数据一致性 HA依赖外部组件 解决【XCluster】基于paxos标准构建alisql集群 高性能、可异地部署、网络抖动高容忍 batching和pipelining，有了pipeline可以同个时间点传输多批数据，问题是乱序收取、日志空洞。等全部到来的时候再去处理，两外的方案是乱序也通过多数派确认提交。 独立基础库，可单独部署【并不限于alisql使用】 raft是不会出现日志空洞的，但是不能支持pipeline。poxos可以使用pipelining，但是会有乱序。 sysbench是一个模块化的、跨平台、多线程基准测试工具，主要用于评估测试各种不同系统参数下的数据库负载情况。 HIStroe：阿里OLAP， 焦方飞MPP计算 知识网格的列式存储 存储： succinct data lsm index 查询优化： 让更苛刻的条件优先执行 Hi-index原理，不需要解压数据就可以检索 rocketMQ: 万亿级数据洪峰下的消息引擎 冯嘉]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Roles]]></title>
    <url>%2F2017%2F07%2F25%2FRoles%2F</url>
    <content type="text"><![CDATA[role好多host-level的OS(例如Linux，BSDs等)都支持多用户。同样的，mesos也是一个多用户集群管理系统，使用一个mesos集群管理一个组织的所有资源，服务于这个组织的用户。 mesos里与资源相关的东西如下： 用户之间对于资源的公平共享 保证用户资源(优先级、隔离、限额等) 精确地资源计算能力 分别有多少资源是 已分配/未分配等等 基于每个用户计算 在mesos中，我们为所有用户分配一个或多个role。其实，role就是mesos资源的消费者。这个消费者可以代表一个组织的一个用户，也可能是一个组、一个群、一个service，一个framework等等。 调度器会订阅一个或多个role，监听他们申请到的资源，然后执行调度任务。 下面是一些例子： 保证一个role可以分配到指定的资源（通过quota指定） 保证个别agent上的一些或者所有资源都分配给一个指定的role(通过reservations) 保证资源是在role之间公平共享(通过DRF) 声明某些role获取相对更多的资源(通过weight) role与acl有两种方式来控制一个framework应该订阅哪些role。首先，可以指定ACLs。其次，可以在mesos master启动的时候传递–roles参数，代表role的白名单，逗号隔开。注意在HA环境下，要保证多个master的启动白名单是一致的。 role与framework的关联framework通过制定FrameworkInfo里的roles订阅role。 作为用户，你可以在启动framework的时候指定要订阅哪些role。具体怎样弄需要看你的framework相关的接口是怎样的。例如，一个单用户的scheduler可能会通过–mesos_role参数指定，多用户的scheduerl就通过 –mesos_roles参数指定。或者也可以通过一个LDAP系统动态调整framwork和role之间的订阅关系。 订阅多个role上面有提到，一个framework可以同时订阅多个role，通过MULTI_ROLE实现。 一个framework对外提供资源的时候，这些资源只能属于一个role。framework可以通过Offer对象的allocation_info.role字段来确认某个offer是提供给哪个role的，或者通过每个提供的Resource对象的allocation_info.role字段。（当前实现中，在一个Offer中的所有资源都是提供给同一个role的） 同一个role的多个framework多个framework可以订阅同一个role。例如：一个framework可以创建一个持久化的数据卷，然后往里面写数据。一旦这个写数据的任务完成后，这个数据卷就可以提供给订阅同一个role的其他的framewo使用了。第二个framework就可以读取前一个framework写的数据。 然而，配置多个framework使用同一个role的时候需要慎重一些，因为所有的framework都能够访问到这个role申请到的资源。例如，如果一个framework把一些敏感信息存到了数据卷中，其他的framework就可以读取了。类似地，如果一个framework创建了一个持久化的数据卷，其他framework可以直接偷走自己用来启动自己的任务。通常来说，订阅同一个role的framework之间是具有良好的协作关系的。 资源与role的关系资源通过reservation分配给一个role。资源可以永久静态地或者动态地提供给role使用。framework和operator可以指定一定量的资源作为某个role的最小持有量。 默认的role名称为*的role是一个特殊的role。没有分配的资源是用这个*代表的。 对于没有提供FrameworkInfo.role的framework注册，会分配给他的就是*这个role。 role与资源分配默认，mesos master使用基于权重的资源分配方式(weighted Dominant Resource Fairness)。 In particular, this implementation of wDRF first identifies which role is furthest below its fair share of the role’s dominant resource. 然后每个订阅这个role的framework就获取到返回的资源。 资源分配过程可以通过指定role的权重进行自定义控制。 Role与quota为了保证一个role能够分配到指定大小的资源，我们可以通过/quota终端指定quota。 资源分配者在公平分配剩余资源之前，会首先满足quota的需求。 Role与PrincipalPrincipal代表一个与mesos互动的实体，更像是用户名。例如，framework在向mesos master注册的时候会提供principal，在使用HTTP终端的时候会提供principal。一个实体需要提供principal来认证自己的身份。 Role，仅仅用来控制资源分配。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mesos的containerizer]]></title>
    <url>%2F2017%2F07%2F24%2Fmesos%E7%9A%84containerizer%2F</url>
    <content type="text"><![CDATA[Docker ContainerizerDocker containerizer使用dockers engine管理container。 container启动过程 Docker containerizer 在ContainerInfo::type为DOCKER的时候尝试在docker里启动task Docker containerizer先pull image 调用pre-launch的hook executor以下面两种方式启动起来 mesos agent运行在一个docker container中 这种方式需要提供–docker_mesos_image的参数。使用指定的docker image启动mesos agent 如果task包含自定义的executor，那么这个executor就在docker container中被启动 如果task并不包含executor。就是说它定义了一个command，默认的mesos-docker-executor就在docker container中通过docker Cli执行这个command。 mesos agent不运行在docker container中 如果task中包含自定义的executor，那么在docker container中执行 如果task不博阿涵自定义的executor，也就是定义了一个command，就fork一个子进程来执行默认的mesos-docker-executor. 然后mesos-docker-executor产生一个shell通过docker cli执行这个command。 Mesos ContainerizerMesos Containerizer是原生的。Mesos Containerizer会处理所有没有制定ContainerInfo::DockerInfo的executor/task。 container启动过程 在每个isolator上调用prepare 使用Launcher fork出executor。在isolated之前，这个被fork出来的executor是不能执行的 Isolate这个executor。通过pid调用每个isolator的isolate方法 抽取executor 执行executor。就是上面的executor会获得执行的信号。先执行一些准备的命令，然后再执行executor的内容。 LauncherLauncher负责产生和销毁container。 在containerizerd context中fork新的进程。这个子进程会执行给定路径的可执行文件，接受参数、flag和环境变量等。 子进程的IO会根据指定的方式进行重定向 linux launcher 为container创建一个freezer 的cgroup 创建一个posix的pipe为host和container进程提供通信通道 使用系统的clone方法生成子进程(也就是container进程) 把新生成的container放进freezer的对应层级中 通过上面的pip给子进程一个signal，让它继续执行 Posix launcher (TBD)IsolatorsIsolator负责创建container中的运行环境【CPU, NETWORK, 存储，内存等】 Containerizer statesDocker FETCHING PULLING RUNNING DESTROYING Mesos PREPARING ISOLATING FETCHING RUNNING DESTROYING]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker内OS的相关事宜]]></title>
    <url>%2F2017%2F07%2F20%2Fdocker%E5%86%85OS%E7%9A%84%E7%9B%B8%E5%85%B3%E4%BA%8B%E5%AE%9C%2F</url>
    <content type="text"><![CDATA[问题假设宿主机是centos7，然后docker镜像使用ubuntu16.04。 那么dockers容器在启动的时候是怎样运行ubuntu的，这种情境咋总是感觉跟启动虚拟机一样呢？ 各种cgroup什么的还好理解，但是这些os内核它是怎样运作起来的呢？ 答案容器是依赖宿主机内核的，如果内核不能满足容器需求，就不能run。 这么看来，其实docker也不是那么“万能”的。 甚至如果容器里需要使用高版本内核的时候，宿主机不能支持，那么这个容器就run不起来………….理论上是这样，用的时候就要注意了。不知道这种docker是不是会自动下载这个内核，然后完全单独运行的话，其实本质上，真的和虚拟机差不多了 都独立内核了，ns和cg都不能用宿主机的了，那就是彻底的虚拟机了。 docker 和 传统的虚拟机的区别就是 ， 传统虚拟机假装有一套新的硬件，系统跑在硬件上就行了， docker 假装有一个系统，进程跑在系统里就行了，其实个人认为准确说，应该是docker假装有一个内核。 翻译docker最开始使用的是LXC，后来切换到了libcontainer。这样就可以跟宿主机共享操作系统资源。docker使用一种分层的文件系统AUFS，并且管理自己的网络。 AUFS是一个分层的文件系统，将只读的和可写的部分merge在一起。OS公用的地方是只读的，每个container自己挂载的是可写的。 假设我们有一个1g的image，需要1000个节点的扩容。如果使用一个完整的VM的话，需要1000G的空间。使用Docker的话，这1000个container节点可以共享1g的OS资源。 一个完整的VM拥有自己的一组资源，基本不共享什么东西。用重量换得了更多的独立性。Docker相反，独立性相对差一些，但是很轻量。所以可以在宿主机上跑上千个container。 一个完整的VM通常要花费几分钟才能启动，一个docker container则只需要几秒。 两个东西各有优缺点。 参考： https://stackoverflow.com/questions/33112137/run-different-linux-os-in-docker-container https://stackoverflow.com/questions/33112137/run-different-linux-os-in-docker-container https://stackoverflow.com/questions/16047306/how-is-docker-different-from-a-normal-virtual-machine https://docs.docker.com/engine/faq/#what-is-different-between-a-docker-container-and-a-vm]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用portainer监控docker容器]]></title>
    <url>%2F2017%2F07%2F14%2F%E4%BD%BF%E7%94%A8portainer%E7%9B%91%E6%8E%A7docker%E5%AE%B9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[我们先监控一个远程的docker daemon，那么这个docker daemon先要允许远程访问才行。 修改113的配置文件/etc/docker/daemon.json1234&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;http://3a35aff6.m.daocloud.io&quot;,&quot;http://ethanzhu.m.tenxcloud.net&quot;], &quot;hosts&quot;: [&quot;tcp://0.0.0.0:2375&quot;]&#125; 注意，启用hosts，也就是远程接口之后，本地的命令行就不能用了123456[root@etl03 ~]# docker imagesCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?[root@etl03 ~]# docker -H 0.0.0.0:2375 imagesREPOSITORY TAG IMAGE ID CREATED SIZEsso latest f8fdedf3edca 13 hours ago 246MBcentos 7.3.1611 67591570dd29 7 months ago 192MB 在112上启动portainer1docker run -d -p 9000:9000 --rm -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer 如果要持久化一些配置文件的话，可以参考以下命令：1docker run -d -p 9000:9000 -v /path/on/host/data:/data portainer/portainer 配置初始密码，然后 参考： https://docs.docker.com/engine/reference/commandline/dockerd/#extended-description]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker问题整理]]></title>
    <url>%2F2017%2F07%2F14%2Fdocker%E9%97%AE%E9%A2%98%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[容器内ping不同外部宿主机默认使用bridge网络，应该是通畅的。通常就是简单的防火墙导致，需要注意的是：调整防火墙之后，需要重启docker engine才能生效，仅仅重启docker容器是不会起作用的。。。。 loopback devices的告警12345WARNING: devicemapper: usage of loopback devices is strongly discouraged for production use. Use `--storage-opt dm.thinpooldev` to specify a custom block storage device.WARNING: IPv4 forwarding is disabledWARNING: bridge-nf-call-iptables is disabledWARNING: bridge-nf-call-ip6tables is disabled 参考 http://www.dockerinfo.net/2182.html 容器名称变化主要是对于代理层，比如有三个backend服务节点，只要一重启，这三个容器的host和ip有可能都会变。 方案： 代理 + 动态服务发现。nginx/haproxy + consul/etcd traefix，专为此种场景提供了定制接口配置。 参考： http://blog.hypriot.com/post/microservices-bliss-with-docker-and-traefik/ https://www.consul.io/intro/getting-started/install.html devicemapper: Can’t set cookie dm_task_set_cookie failed1docker: failed to register layer: devmapper: Error activating devmapper device for &apos;24ea1e709520627a2862d2b9130b6aff77dde03fef39cc0169568cde5883b013&apos;: devicemapper: Can&apos;t set cookie dm_task_set_cookie failed. 问题貌似是在我重新启动 1Error response from daemon: driver &quot;devicemapper&quot; failed to remove root filesystem for 1d7c4b287c27ef61fd0d9ac37c401c5da213b81b31dcc42448b1864b6b838f14: failed to remove device f45fd625ac4cdaccddfdd0ea02ac97f9e529a13594a4cfc92a883b8674d2658a: devicemapper: Can not set cookie: dm_task_set_cookie failed 临时方案：echo ‘y’ | sudo dmsetup udevcomplete_allhttps://github.com/kubevirt/kubevirt/issues/321 动态配置一共有三个方案可以支持 使用自定义的DNS服务器这个方案仅仅是应对容器内需要访问容器外的其他机器的host时起作用，如果还需要变化其他配置，那么可以忽略掉 使用env或者env文件 首先要把应用依赖的外部配置从代码里面抽出来，做成配置文件 容器启动的时候，执行一个自定义脚本，脚本内容就是从容器系统环境变量里面读取环境变量，然后用sed替换配置文件里面的配置 那么在docker run启动容器的阶段，通过-e选项传递你的配置值到容器的环境变量，就能修改容器内应用的配置 迁移到其他环境，修改docker run -e的环境变量配置就OK 直接使用-v挂载不同环境的配置文件这种与第二种本质上没有太大区别。但是关键在于env或者env文件是在client端的，而-v挂载目录是需要在server端的。也就是是说每个server都需要有那个目录或者文件才行，而env方式的话，就只需要client端有就可以了。 鉴于server端一般是多于client端的，所以选用env的会相对多一些。 网络docker attach容器后，退出的时候ctrl + q docker默认的docker0bridge是不支持服务发现的，也就在这个网络里的docker容器相互之间并不能通过名称通信。要支持服务发现的话，需要自定义一个bridge网络。 只有swarm service可以连到overylay网络，独立的容器是连不上的。 docker是会动态修改宿主机的iptables规则的。 参考： https://docs.docker.com/engine/userguide/networking/ 关闭防火墙引起的问题在dockerd运行的时候，关闭防火墙的话，会清空iptables里的docker相关规则，导致docker容器启动失败。 centos7, docker ce17 1iptables: No chain/target/match by that name 解决办法，在关闭防火墙之后再启动dockerd，或者重启dockerd，它会自己根据现有docker网络和容器重建iptables相关规则。 devicemapper: Can not set cookie: dm_task_set_cookie failed重命名container名称导致。 解决：1echo &apos;y&apos; | sudo dmsetup udevcomplete_all 最好能执行docker rm $(docker ps -a -q)把现有container都移除。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7-搭建DNS服务器]]></title>
    <url>%2F2017%2F07%2F12%2Fcentos7-%E6%90%AD%E5%BB%BADNS%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
    <content type="text"><![CDATA[主服务器安装bind91yum install bind bind-utils -y 编辑/etc/named.conf123456789101112131415161718192021222324options &#123; listen-on port 53 &#123; any; &#125;; listen-on-v6 port 53 &#123; ::1; &#125;; directory &quot;/var/named&quot;; dump-file &quot;/var/named/data/cache_dump.db&quot;; statistics-file &quot;/var/named/data/named_stats.txt&quot;; memstatistics-file &quot;/var/named/data/named_mem_stats.txt&quot;; allow-query &#123; any; &#125;; allow-transfer&#123; localhost; 10.2.19.113; &#125;;.........zone &quot;will.com&quot; IN &#123;type master;file &quot;forward.unixmen&quot;;allow-update &#123; none; &#125;;&#125;;zone &quot;19.2.10.in-addr.arpa&quot; IN &#123;type master;file &quot;reverse.unixmen&quot;;allow-update &#123; none; &#125;;&#125;; 创建上面我们配置的zone文件vi /var/named/forward.unixmen1234567891011121314151617$TTL 86400@ IN SOA etl02.will.com. root.will.com. ( 2017071001 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL)@ IN NS etl02.will.com.@ IN NS etl03.will.com.@ IN NS schedule.will.com.@ IN A 10.2.19.112@ IN A 10.2.19.113@ IN A 10.2.19.62etl02 IN A 10.2.19.112etl03 IN A 10.2.19.113schedule IN A 10.2.19.62 检查配置123[root@etl02 ~]# named-checkzone will.com /var/named/forward.unixmenzone will.com/IN: loaded serial 2017071001OK vi /var/named/reverse.unixmen123456789101112131415161718$TTL 86400@ IN SOA etl02.will.com. root.will.com. ( 2017071001 ;Serial 3600 ;Refresh 1800 ;Retry 604800 ;Expire 86400 ;Minimum TTL)@ IN NS etl02.will.com.@ IN NS etl03.will.com.@ IN NS schedule.will.com.@ IN PTR will.com.etl02 IN A 10.2.19.112etl03 IN A 10.2.19.113schedule IN A 10.2.19.6262 IN PTR schedule.will.com.112 IN PTR etl02.will.com.113 IN PTR etl03.will.com. 同样检查一下123[root@etl02 ~]# named-checkzone will.com /var/named/reverse.unixmen zone will.com/IN: loaded serial 2017071001OK 启动DNS服务12systemctl enable namedsystemctl start named 测试 编辑/etc/resolv.conf文件, 修改nameserver为etl02.will.com 编辑网卡配置文件/etc/sysconfig/network-scripts/ifcfg-eth0, 修改DNS为我们的DNS服务器地址10.2.19.112 重启网络服务：systemctl restart network 1234567891011121314151617181920212223242526272829[root@etl02 ~]# dig etl02.will.com; &lt;&lt;&gt;&gt; DiG 9.9.4-RedHat-9.9.4-50.el7_3.1 &lt;&lt;&gt;&gt; etl02.will.com;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 48492;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 3, ADDITIONAL: 3;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096;; QUESTION SECTION:;etl02.will.com. IN A;; ANSWER SECTION:etl02.will.com. 86400 IN A 10.2.19.112;; AUTHORITY SECTION:will.com. 86400 IN NS etl02.will.com.will.com. 86400 IN NS schedule.will.com.will.com. 86400 IN NS etl03.will.com.;; ADDITIONAL SECTION:etl03.will.com. 86400 IN A 10.2.19.113schedule.will.com. 86400 IN A 10.2.19.62;; Query time: 0 msec;; SERVER: 127.0.0.1#53(127.0.0.1);; WHEN: Fri Jul 14 11:52:31 EDT 2017;; MSG SIZE rcvd: 150 参考：https://www.unixmen.com/setting-dns-server-centos-7/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自如爬虫]]></title>
    <url>%2F2017%2F07%2F04%2F%E8%87%AA%E5%A6%82%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[筛选房源的URL 筛选的是北土城的房子：1234567https://phoenix.ziroom.com/v7/room/list.json?uid=0&amp;os=android%3A4.2.2&amp;suggestion_type=3&amp;model=HTC+One+X+-+4.2.2+-+API+17+-+720x1280&amp;sort=2&amp;suggestion_value=%E5%8C%97%E5%9C%9F%E5%9F%8E&amp;imei=000000000000000&amp;app_version=5.2.7&amp;network=WIFI&amp;ip=10.0.3.15&amp;size=10&amp;sign=fed60ace709254dbe7ca5b471ea8bb62&amp;timestamp=1499155383&amp;price=%2C&amp;page=1&amp;sign_open=1&amp;city_code=110000-- 下拉一页面https://phoenix.ziroom.com/v7/room/list.json?uid=0&amp;os=android%3A4.2.2&amp;suggestion_type=3&amp;model=HTC+One+X+-+4.2.2+-+API+17+-+720x1280&amp;sort=2&amp;suggestion_value=%E5%8C%97%E5%9C%9F%E5%9F%8E&amp;imei=000000000000000&amp;app_version=5.2.7&amp;network=WIFI&amp;ip=10.0.3.15&amp;size=10&amp;sign=924565090bc8a24dfd8f9dd07e04f4af&amp;timestamp=1499155456&amp;price=%2C&amp;page=2&amp;sign_open=1&amp;city_code=110000-- 下拉一页面https://phoenix.ziroom.com/v7/room/list.json?uid=0&amp;os=android%3A4.2.2&amp;suggestion_type=3&amp;model=HTC+One+X+-+4.2.2+-+API+17+-+720x1280&amp;sort=2&amp;suggestion_value=%E5%8C%97%E5%9C%9F%E5%9F%8E&amp;imei=000000000000000&amp;app_version=5.2.7&amp;network=WIFI&amp;ip=10.0.3.15&amp;size=10&amp;sign=2d87dcd80f06daf1b967b9b445fb302e&amp;timestamp=1499155491&amp;price=%2C&amp;page=3&amp;sign_open=1&amp;city_code=110000 1234567891011121314151617uid 0os android:4.2.2suggestion_type 3model HTC One X - 4.2.2 - API 17 - 720x1280sort 2suggestion_value 北土城imei 000000000000000app_version 5.2.7network WIFIip 10.0.3.15size 10sign fed60ace709254dbe7ca5b471ea8bb62timestamp 1499155383price ,page 1sign_open 1city_code 110000 房子详情页1https://phoenix.ziroom.com/v7/room/detail.json?sign=a97bdae17004d4c9bc535068316b2185&amp;uid=0&amp;timestamp=1499155557&amp;id=146610&amp;os=android%3A4.2.2&amp;model=HTC+One+X+-+4.2.2+-+API+17+-+720x1280&amp;imei=000000000000000&amp;app_version=5.2.7&amp;sign_open=1&amp;city_code=110000&amp;house_id=20624&amp;network=WIFI&amp;ip=10.0.3.15 可视化一下：12345678910111213sign a97bdae17004d4c9bc535068316b2185uid 0timestamp 1499155557id 146610os android:4.2.2model HTC One X - 4.2.2 - API 17 - 720x1280imei 000000000000000app_version 5.2.7sign_open 1city_code 110000house_id 20624network WIFIip 10.0.3.15]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pyenv多版本安装]]></title>
    <url>%2F2017%2F07%2F04%2Fpyenv%E5%A4%9A%E7%89%88%E6%9C%AC%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[1234567891011yum install -y git python-pipgit clone https://github.com/pyenv/pyenv.git ~/.pyenvecho &apos;export PYENV_ROOT=&quot;$HOME/.pyenv&quot;&apos; &gt;&gt; ~/.bash_profileecho &apos;export PATH=&quot;$PYENV_ROOT/bin:$PATH&quot;&apos; &gt;&gt; ~/.bash_profileecho &apos;eval &quot;$(pyenv init -)&quot;&apos; &gt;&gt; ~/.bash_profile yum install readline readline-devel readline-static openssl openssl-devel openssl-static sqlite-devel bzip2-devel bzip2-libs -y pyenv install 3.6.1 git clone https://github.com/pyenv/pyenv-virtualenv.git $(pyenv root)/plugins/pyenv-virtualenv pyenv shell 3.6.1 echo &apos;eval &quot;$(pyenv virtualenv-init -)&quot;&apos; &gt;&gt; ~/.bash_profile pyenv virtualenv 3.6.1 will_3.6.1 123pyenv versionspyenv activate will_3.6.1 安装加速: 提前下载好对应python版本的tar.zx包，放到~/.pyenv/cache/下，然后再运行 pyenv install 3.6.1]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[RunningData执行用户迁移计划]]></title>
    <url>%2F2017%2F07%2F03%2FRunningData%E6%89%A7%E8%A1%8C%E7%94%A8%E6%88%B7%E8%BF%81%E7%A7%BB%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[超级用户准备 jlc wplan xiaov 文件权限HDFS 所有数仓调整owner为所在事业部的超级用户1234hdfs dfs -chown jlc:jlc -R /log/statisticshdfs dfs -ls /apps/hive/warehouse/ | awk &apos;&#123;print(&quot;hdfs dfs -chown -R &quot;, $4, &quot;:&quot;,$4, $8)&#125;&apos; 本地 名称 位置 操作q sqoop目录权限 /server/app/sqoop rm -vfr /server/app/sqoop/vo /server/metamap/metamap_django /server/metamap/metamap_django/*.java azkaban执行用户 sqoop hive 需要修改M2H，H2H，H2M以及新版本的generate_job_file里的执行用户，包含两个地方 生成执行脚本 生成命令【测试执行】 在settings文件中添加参数USE_ROOT jenkins调度资产匹配问题: 62远程调用106上的脚步任务12Message from syslogd@datanode17 at Jul 3 15:50:52 ... kernel:BUG: soft lockup - CPU#7 stuck for 67s! [python:2708]]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于kylin分时段merge之后体积变小]]></title>
    <url>%2F2017%2F06%2F26%2F%E5%85%B3%E4%BA%8Ekylin%E5%88%86%E6%97%B6%E6%AE%B5merge%E4%B9%8B%E5%90%8E%E4%BD%93%E7%A7%AF%E5%8F%98%E5%B0%8F%2F</url>
    <content type="text"><![CDATA[我们设置了一个cube，指定的mandatory dimension为create_year，问了一下，说是因为有按照年去查询指标的需求。 配置mandatory dimension: create_year 自动merge : 7天、28天 现象merge之前单天的数据大小平均为3G，而单周的数据大小平均为11G。 就是说： merge完之后数据变小了！ 那么为什么呢？ 几个人想了好半天，终于有个同事想到了….就是因为对于不包含merge时间【一般是天】的所有其他维度的组合，都减少了6倍的大小。 那么，由此延伸，我们是可以将create_day设置为mandatory dimension的，这样每天的cube就会减少很多大小，merge以后也不会有对应的压缩。这样对于单天记录的体积会有很大压缩提升。。。 那么对于年指标的查询应该怎样处理呢？由于这种比较少，建议可以前端解决。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hbase频繁挂掉]]></title>
    <url>%2F2017%2F06%2F25%2Fhbase%E9%A2%91%E7%B9%81%E6%8C%82%E6%8E%89%2F</url>
    <content type="text"><![CDATA[1232017-06-25 09:42:34,262 ERROR [RS_OPEN_REGION-datanode20:16020-0] coprocessor.CoprocessorHost: The coprocessor org.apache.kylin.storage.hbase.cube.v2.coprocessor.endpoint.CubeVisitService threw org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152532017-06-25 09:42:34,336 FATAL [RS_OPEN_REGION-datanode20:16020-2] regionserver.HRegionServer: ABORTING region server datanode20.yinker.com,16020,1498354685968: The coprocessor org.apache.kylin.storage.hbase.cube.v1.coprocessor.observer.AggregateRegionObserver threw org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby2017-06-25 09:42:34,336 FATAL [RS_OPEN_REGION-datanode20:16020-2] regionserver.HRegionServer: RegionServer abort: loaded coprocessors are: [org.apache.hadoop.hbase.security.access.SecureBulkLoadEndpoint]2017-06-25 09:42:34,338 FATAL [RS_OPEN_REGION-datanode20:16020-0] regionserver.HRegionServer: ABORTING region server datanode20.will.com,16020,1498354685968: The coprocessor org.apache.kylin.storage.hbase.cube.v1.coprocessor.observer.AggregateRegionObserver threw org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standby2017-06-25 09:42:34,370 INFO [RS_OPEN_REGION-datanode20:16020-1] regionserver.RegionCoprocessorHost: Loaded coprocessor org.apache.kylin.storage.hbase.cube.v2.coprocessor.endpoint.CubeVisitService from HTD of KYLIN_4L40B34P11 successfully.2017-06-25 09:42:34,379 ERROR [RS_OPEN_REGION-datanode20:16020-2] handler.OpenRegionHandler: Failed open of region=KYLIN_HBWCQ5ZQR1,\x001,1494915979852.71f4a908f1e460ac95dfadcf5ab8ecfd., starting to roll back the global memstore size.2017-06-25 09:42:34,381 INFO [RS_OPEN_REGION-datanode20:16020-2] coordination.ZkOpenRegionCoordination: Opening of region &#123;ENCODED =&gt; 71f4a908f1e460ac95dfadcf5ab8ecfd, NAME =&gt; &apos;KYLIN_HBWCQ5ZQR1,\x001,1494915979852.71f4a908f1e460ac95dfadcf5ab8ecfd.&apos;, STARTKEY =&gt; &apos;\x001&apos;, ENDKEY =&gt; &apos;\x002&apos;&#125; failed, transitioning from OPENING to FAILED_OPEN in ZK, expecting version 32017-06-25 09:42:34,382 ERROR [RS_OPEN_REGION-datanode20:16020-0] handler.OpenRegionHandler: Failed open of region=KYLIN_HBWCQ5ZQR1,\x00\x15,1494915979852.c4940efcc8efa3f3a9b7c2a546a1d1f5., starting to roll back the global memstore size.org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category READ is not supported in state standbyjava.io.IOException: Cannot append; log is closed at org.apache.hadoop.hbase.regionserver.wal.FSHLog.append(FSHLog.java:1223) at org.apache.hadoop.hbase.regionserver.wal.WALUtil.writeRegionEventMarker(WALUtil.java:95) at org.apache.hadoop.hbase.regionserver.HRegion.writeRegionOpenMarker(HRegion.java:978) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:6335) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:6289) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:6260) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:6216) at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:6167) at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.openRegion(OpenRegionHandler.java:362) ...............2017-06-25 09:42:44,168 INFO [regionserver/datanode20.will.com/10.2.19.109:16020.logRoller] regionserver.LogRoller: LogRoller exiting.2017-06-25 09:42:44,168 INFO [regionserver/datanode20.will.com/10.2.19.109:16020] regionserver.CompactSplitThread: Waiting for Split Thread to finish...2017-06-25 09:42:44,168 INFO [regionserver/datanode20.will.com/10.2.19.109:16020] regionserver.CompactSplitThread: Waiting for Merge Thread to finish...2017-06-25 09:42:44,168 INFO [regionserver/datanode20.will.com/10.2.19.109:16020] regionserver.CompactSplitThread: Waiting for Large Compaction Thread to finish...2017-06-25 09:42:44,168 INFO [regionserver/datanode20.will.com/10.2.19.109:16020] regionserver.CompactSplitThread: Waiting for Small Compaction Thread to finish...2017-06-25 09:42:44,170 INFO [regionserver/datanode20.will.com/10.2.19.109:16020.leaseChecker] regionserver.Leases: regionserver/datanode20.will.com/10.2.19.109:16020.leaseChecker closing leases2017-06-25 09:42:44,171 INFO [regionserver/datanode20.will.com/10.2.19.109:16020.leaseChecker] regionserver.Leases: regionserver/datanode20.will.com/10.2.19.109:16020.leaseChecker closed leases2017-06-25 09:42:44,183 INFO [regionserver/datanode20.will.com/10.2.19.109:16020] ipc.RpcServer: Stopping server on 16020]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NN-HA后引发的问题]]></title>
    <url>%2F2017%2F06%2F22%2FNN-HA%E5%90%8E%E5%BC%95%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[最表面问题:几乎所有用户执行权限都出现问题。 问题原因： 所有HDFS之上的应用都是使用linux的posix文件权限控制 当Active NN是DN17的时候，DN17本机并没有这些用户的相关权限信息。解决方法： 尝试copyNN01的用户以及组信息到DN171234567891011121314/etc/passwd/etc/shadow/etc/group/etc/gshadow[root@namenode01 hdfs]# cat /etc/group | grep xiaovxiaov:x:1015:zhoujian,wangchenqi,zhangyuntao,wangxu,wanghao_fengkong,linzhixin,yangqiankun,tianye,zhaoyanchao,tinyv,wangyin,lihaoyue,zhuxiuwei,yangluan,qizhenpeng,zhanglining,xiaovtest,yanghuanyuzi,zhanghaigang,shichengjie,luanzhiwei,wangyasen,kangyimin,luiyahui,liuyahui,liulinyuan,caoyunqing,tianyang,chengyao,gezhao,hehaojun,hujunjie,liuwei,xiongpengxiaovtest:x:1040:metastoremanager:x:1041:xiaovtest,wangchenqi 将上面所有的用户保存到另一台的xiaov文件中，然后执行下面的逻辑：123456789for name in `cat xiaov | awk &apos;&#123;split($0, atr,&quot;,&quot;); for (i=0;i&lt;length(atr);i++) &#123; print(atr[i]) &#125; &#125;&apos;`;douseradd $name -G xiaovdone 使用fabric维护用户权限相关信息，每次都同时操作两台NN。 123456789101112131415161718192021222324252627282930-- fabric.pynns=(&apos;namenode01.will.com&apos;, &apos;datanode17.will.com&apos;)env.roledefs = &#123; &apos;services&apos;: services, &apos;datanodes&apos;: datanodes, &apos;tmp&apos;: tmpnodes, &apos;nns&apos;: nns&#125;------------------[root@datanode21 ~]# fab cmd:roles=nns,cmd=&quot;echo hello from `hostname`&quot;This text is green!This sentence is red, except for these words, which are green.[namenode01.will.com] Executing task &apos;cmd&apos;[namenode01.will.com] run: echo hello from datanode21.will.com[namenode01.will.com] out: hello from datanode21.will.com[namenode01.will.com] out: [datanode17.will.com] Executing task &apos;cmd&apos;[datanode17.will.com] run: echo hello from datanode21.will.com[datanode17.will.com] out: hello from datanode21.will.com[datanode17.will.com] out: Done.Disconnecting from datanode17.will.com... done.Disconnecting from namenode01.will.com... done NN切换的相关日志NN01的log。 出现这个错误后，NN自己直接shutdown123456789101112131415161718192021222324252627282930313233343536373839402017-06-15 22:35:35,329 WARN client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 18014 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.2.19.121:8485]2017-06-15 22:35:36,331 WARN client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 19015 ms (timeout=20000 ms) for a response for sendEdits. Succeeded so far: [10.2.19.121:8485]2017-06-15 22:35:37,317 FATAL namenode.FSEditLog (JournalSet.java:mapJournalsAndReportErrors(398)) - Error: flush failed for required journal (JournalAndStream(mgr=QJM to [10.2.19.123:8485, 10.2.19.127:8485, 10.2.19.121:8485], stream=QuorumOutputStream starting at txid 118442295))java.io.IOException: Timed out waiting 20000ms for a quorum of nodes to respond. at org.apache.hadoop.hdfs.qjournal.client.AsyncLoggerSet.waitForWriteQuorum(AsyncLoggerSet.java:137) at org.apache.hadoop.hdfs.qjournal.client.QuorumOutputStream.flushAndSync(QuorumOutputStream.java:107) at org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:113) at org.apache.hadoop.hdfs.server.namenode.EditLogOutputStream.flush(EditLogOutputStream.java:107) at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream$8.apply(JournalSet.java:533) at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:393) at org.apache.hadoop.hdfs.server.namenode.JournalSet.access$100(JournalSet.java:57) at org.apache.hadoop.hdfs.server.namenode.JournalSet$JournalSetOutputStream.flush(JournalSet.java:529) at org.apache.hadoop.hdfs.server.namenode.FSEditLog.logSync(FSEditLog.java:647) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt(FSNamesystem.java:2512) at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2377) at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:708) at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.create(ClientNamenodeProtocolServerSideTranslatorPB.java:405) at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)2017-06-15 22:35:37,318 WARN client.QuorumJournalManager (QuorumOutputStream.java:abort(72)) - Aborting QuorumOutputStream starting at txid 1184422952017-06-15 22:35:37,337 INFO BlockStateChange (BlockManager.java:computeReplicationWorkForBlocks(1531)) - BLOCK* neededReplications = 0, pendingReplications = 0.2017-06-15 22:35:37,342 INFO util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 12017-06-15 22:35:37,396 INFO namenode.NameNode (LogAdapter.java:info(47)) - SHUTDOWN_MSG: /************************************************************SHUTDOWN_MSG: Shutting down NameNode at namenode01.will.com/10.2.19.72 DN17的log123456789102017-06-19 16:34:39,682 WARN client.QuorumJournalManager (QuorumCall.java:waitFor(134)) - Waited 27324 ms (timeout=20000 ms) for a response for selectInputStreams. No responses yet..........2017-06-19 16:34:46,859 INFO util.JvmPauseMonitor (JvmPauseMonitor.java:run(196)) - Detected pause in JVM or host machine (eg GC): pause of approximately 6029msNo GCs detected2017-06-19 16:34:49,739 INFO util.JvmPauseMonitor (JvmPauseMonitor.java:run(196)) - Detected pause in JVM or host machine (eg GC): pause of approximately 2380msNo GCs detected 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051522017-06-26 13:39:04,029 INFO ipc.Server (Server.java:doRead(891)) - Socket Reader #1 for port 8020: readAndProcess from client 10.2.19.83 threw exception [java.io.IOException: Connection reset by peer]java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:197) at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:380) at org.apache.hadoop.ipc.Server.channelRead(Server.java:2773) at org.apache.hadoop.ipc.Server.access$2800(Server.java:136) at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1606) at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:880) at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:746) at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:717)2017-06-26 13:39:04,066 INFO ipc.Server (Server.java:doRead(891)) - Socket Reader #1 for port 8020: readAndProcess from client 10.2.19.110 threw exception [java.io.IOException: Connection reset by peer]java.io.IOException: Connection reset by peer at sun.nio.ch.FileDispatcherImpl.read0(Native Method) at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223) at sun.nio.ch.IOUtil.read(IOUtil.java:197).......2017-06-26 13:39:08,310 WARN client.QuorumJournalManager (IPCLoggerChannel.java:call(406)) - Took 4136ms to send a batch of 3 edits (262 bytes) to remote journal 10.2.19.121:84852017-06-26 13:39:08,310 WARN client.QuorumJournalManager (IPCLoggerChannel.java:call(406)) - Took 4130ms to send a batch of 3 edits (262 bytes) to remote journal 10.2.19.123:84852017-06-26 13:39:08,310 WARN client.QuorumJournalManager (IPCLoggerChannel.java:call(388)) - Remote journal 10.2.19.127:8485 failed to write txns 134233020-134233022. Will try to write to this JN again after the next log roll.org.apache.hadoop.ipc.RemoteException(java.io.IOException): IPC&apos;s epoch 15 is less than the last promised epoch 16 at org.apache.hadoop.hdfs.qjournal.server.Journal.checkRequest(Journal.java:418) at org.apache.hadoop.hdfs.qjournal.server.Journal.checkWriteRequest(Journal.java:446) at org.apache.hadoop.hdfs.qjournal.server.Journal.journal(Journal.java:341) at org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.journal(JournalNodeRpcServer.java:148) at org.apache.hadoop.hdfs.qjournal.protocolPB.QJournalProtocolServerSideTranslatorPB.journal(QJournalProtocolServerSideTranslatorPB.java:158) at org.apache.hadoop.hdfs.qjournal.protocol.QJournalProtocolProtos$QJournalProtocolService$2.callBlockingMethod(QJournalProtocolProtos.java:25421) at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616) at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:969) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2206) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2202) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1709) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2200)..........org.apache.hadoop.hdfs.qjournal.client.QuorumException: Got too many exceptions to achieve quorum size 2/3. 3 exceptions thrown:10.2.19.127:8485: IPC&apos;s epoch 15 is less than the last promised epoch 16 又一次123456789102017-06-28 10:05:21,397 INFO namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1297)) - Stopping services started for standby state2017-06-28 10:05:21,398 WARN ha.EditLogTailer (EditLogTailer.java:doWork(349)) - Edit log tailer interruptedjava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:347) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$200(EditLogTailer.java:284) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:301) at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:449) at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:297)2017-06-28 10:05:21,403 INFO namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1098)) - Starting services required for active state]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[runningdatav2版本迁移计划]]></title>
    <url>%2F2017%2F06%2F13%2FRunningDataV2%E7%89%88%E6%9C%AC%E8%BF%81%E7%A7%BB%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[注意点 编辑、更新 各类ETL自动创建与之对应的ExecObj对象【重载save方法实现】 个别自动分析依赖的ETL同时新建或更新相关ExecBlood【H2H, H2M】 个别需要输入产出的ETL类型需要添加outputs字段，并在save的时候自创创建对应的NullETL对象【JAR】 根据自身特性创建自身的rel_name 【save的时候分析并添加上】 调度相关 (日月周)日常调度【完成依赖解析与任务文件生成，测试通过】 定时调度【执行部分使用既有方案】 临时调度【使用既有方案】 新建调度时，针对的对象应该是修改为ExecObj 影响范围 整体过程中xstorm新建、编辑不可用 任务调度中间可能出现失败 实施步骤 向各事业部发送不可用通知 数据库备份 确保新的save方法都已经注释掉,否则会影响清洗过程。 将数据库对应model字段全部添加上 ./manage.py migrate metamap 0052 –settings=metamap.config 可能需要手动添加exec_obj_id字段…【历史原因】1234567891011121314151617181920212223242526272829303132333435---- Add field exec_obj to anaetl--ALTER TABLE `metamap_anaetl` ADD COLUMN `exec_obj_id` integer NULL;ALTER TABLE `metamap_anaetl` ALTER COLUMN `exec_obj_id` DROP DEFAULT;---- Add field exec_obj to etl--ALTER TABLE `metamap_etl` ADD COLUMN `exec_obj_id` integer NULL;ALTER TABLE `metamap_etl` ALTER COLUMN `exec_obj_id` DROP DEFAULT;---- Add field exec_obj to jarapp--ALTER TABLE `metamap_jarapp` ADD COLUMN `exec_obj_id` integer NULL;ALTER TABLE `metamap_jarapp` ALTER COLUMN `exec_obj_id` DROP DEFAULT;---- Add field exec_obj to sqoophive2mysql--ALTER TABLE `metamap_sqoophive2mysql` ADD COLUMN `exec_obj_id` integer NULL;ALTER TABLE `metamap_sqoophive2mysql` ALTER COLUMN `exec_obj_id` DROP DEFAULT;---- Add field exec_obj to sqoopmysql2hive--ALTER TABLE `metamap_sqoopmysql2hive` ADD COLUMN `exec_obj_id` integer NULL;ALTER TABLE `metamap_sqoopmysql2hive` ALTER COLUMN `exec_obj_id` DROP DEFAULT;CREATE INDEX `metamap_anaetl_30a76f4d` ON `metamap_anaetl` (`exec_obj_id`);ALTER TABLE `metamap_anaetl` ADD CONSTRAINT `metamap_anaetl_exec_obj_id_1bac03db_fk_metamap_execobj_id` FOREIGN KEY (`exec_obj_id`) REFERENCES `metamap_execobj` (`id`);CREATE INDEX `metamap_etl_30a76f4d` ON `metamap_etl` (`exec_obj_id`);ALTER TABLE `metamap_etl` ADD CONSTRAINT `metamap_etl_exec_obj_id_8c6a086a_fk_metamap_execobj_id` FOREIGN KEY (`exec_obj_id`) REFERENCES `metamap_execobj` (`id`);CREATE INDEX `metamap_jarapp_30a76f4d` ON `metamap_jarapp` (`exec_obj_id`);ALTER TABLE `metamap_jarapp` ADD CONSTRAINT `metamap_jarapp_exec_obj_id_6340f92e_fk_metamap_execobj_id` FOREIGN KEY (`exec_obj_id`) REFERENCES `metamap_execobj` (`id`);CREATE INDEX `metamap_sqoophive2mysql_30a76f4d` ON `metamap_sqoophive2mysql` (`exec_obj_id`);ALTER TABLE `metamap_sqoophive2mysql` ADD CONSTRAINT `metamap_sqoophive2mys_exec_obj_id_ee2cb5c2_fk_metamap_execobj_id` FOREIGN KEY (`exec_obj_id`) REFERENCES `metamap_execobj` (`id`);CREATE INDEX `metamap_sqoopmysql2hive_30a76f4d` ON `metamap_sqoopmysql2hive` (`exec_obj_id`);ALTER TABLE `metamap_sqoopmysql2hive` ADD CONSTRAINT `metamap_sqoopmysql2hi_exec_obj_id_9d61c4ff_fk_metamap_execobj_id` FOREIGN KEY (`exec_obj_id`) REFERENCES `metamap_execobj` (`id`); 按照以下顺序开始清洗 clean_etl 生成对应ETL的ExecObj clean_rel 清洗H2M,M2H中不规范的name，规范化hive表名到rel_name字段中 clean_m2h 清洗M2H任务，生成对应的ExecObj。另外找到所有TBLBlood中父表是此表rel_name的记录，新建与之对应的ExecBlood记录 before_clean_blood 过滤一下TblBlood中的parent，对于不存在于H2H和M2好的父表，临时创建为之创建NULLETL，并创建对应ExecObj，然后再创建ExecBlood。 ==注意==：由于NULLETL的特殊性，需要在此创建ExecObj的deptask的日月周调度【在save方法中已实现】 clean_blood 清洗TblBlood到对应ExecBlood，因为前面已经处理了所有情况，不该出现任务child或者parent不存在的问题。 自检sql 12345SELECT name from metamap_execobj GROUP BY name HAVING count(1) &gt; 1;SELECT * from metamap_execobj where name = 'w_data_platform@drop_d_regula_stock';SELECT * from metamap_execobj where name = 'app_jlc@jlc_life_accumulative'; clean_h2m 清洗H2M到ExecObj对象，获取依赖ETL的H2H ExecObj对象，添加对应ExecBlood。 如果是JAR产生的表，暂不支持，需要额外处理【JAR添加自身的output】。 ==注意==： H2M中的个别现象，多个周期的数据存放在同一个表中： SELECT * from metamap_sqoophive2mysql where id in (71,72,73,74,75,76) 需要额外手动维护execblood clean_jar 清洗JAR到ExecObj对象 clean_email 清洗ANAETL到ExecObj对象 clean_task 分别执行clean， type=4, type=1等(1,2,3,4,6) 12--- 这些都是ana_etl本身的valid是0SELECT * from metamap_willdependencytask where id in (864, 870, 759, 643, 164, 143, 85) clean_ptask 更新定时任务的task参数，由原来的will_deptask的id，更新为type为100的最新清洗完的task id. clean_null 为所有的NULLETL对象创建日月周的task调度 测试各种任务调度【日常调度、定时调度、即时调度】 clean_exec_id 清洗原有的ETL对象的exec_obj_id字段 放开ETL对象的save方法注释 恢复线上使用，开启beta测试，随时解决未发现的新生bug 修改日调度脚本中的url到新版本 添加调度的页面覆盖回来[sche/edit_new.html， /views/sche_etl_v2.py] 稳定运行一周，将dep task中旧的task全部删除 TODO H2M的依赖手动添加以下。SELECT * from metamap_sqoophive2mysql where id in (71,72,73,74,75,76) 测试各种ETL的save方法 新增逻辑：]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gobblin-architecture译文]]></title>
    <url>%2F2017%2F06%2F09%2Fgobblin-architecture%E8%AF%91%E6%96%87%2F</url>
    <content type="text"><![CDATA[概览gobblin主要考虑的是扩展性，永固可以添加新的adapter或者集成已经存在的adapter来加入新的source，从新的source抽取数据。架构图： 一个gobblin job是一些列组件协同的结果。所有的组件都是可插拔可扩展的。(http://gobblin.readthedocs.io/en/latest/Gobblin-Architecture#gobblin-constructs) 一个gobblin job有一些列的task组成，每个task对应着一个工作单元，负责抽取一部分数据。这些task会根据配置选择(红色部分)通过gobblin runtime执行(上面橙色的部分)。 http://gobblin.readthedocs.io/en/latest/Gobblin-Architecture/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nginx代理HDP集群代码段]]></title>
    <url>%2F2017%2F06%2F07%2Fnginx%E4%BB%A3%E7%90%86HDP%E9%9B%86%E7%BE%A4%E4%BB%A3%E7%A0%81%E6%AE%B5%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122#user nobody;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream;upstream jobhis &#123; server servicenode03.will.com:19888 fail_timeout=30s; &#125; upstream azk &#123; server schedule.will.com:8081 fail_timeout=30s; &#125; upstream yarn &#123; server datanode02.will.com:8088 fail_timeout=30s; &#125; sendfile on; keepalive_timeout 65; server &#123; listen 81; server_name localhost; subs_filter_types *;# subs_filter overwrite willhistory i; subs_filter datanode02.will.com:8088 $host:$server_port; subs_filter 10.2.19.(\d*) unknown_ip ir; subs_filter (data|name|service)node(\d*).will.com $1_$2.data.com ir;# subs_filter will.com will.data.com ir; #charset koi8-r; #access_log logs/host.access.log main; location /cluster/app &#123; auth_request /auth; try_files $uri @yarn; &#125; location /proxy &#123; auth_request /auth; try_files $uri @yarn; &#125; location /jobhistory &#123; auth_request /auth; try_files $uri @jobhis; rewrite ^/jobhistory/logs/(data|service|name)_(\d*).data.com:(\d*)/(.*)$ /jobhistory/logs/$1node$2.will.com:$3/$4 break; &#125; location / &#123; try_files $uri @azk; &#125; location @jobhis&#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; proxy_pass http://jobhis; &#125; location = /auth &#123; internal; proxy_pass http://10.2.19.62:8088/metamap/nginx_auth_test; proxy_set_header Content-Length &quot;&quot;; proxy_set_header X-Original-URI $request_uri; proxy_pass_request_body off; &#125; location /static &#123; try_files $uri @yarn; &#125; location @yarn&#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; proxy_pass http://yarn; &#125; location @azk&#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header Host $http_host; proxy_redirect off; proxy_pass http://azk; subs_filter_types *; subs_filter datanode(\d\d).will.com will.will.com ir; subs_filter 10.2.19.(\d*) unknowip ir; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125; &#125;]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[fabric与anisible对比]]></title>
    <url>%2F2017%2F05%2F26%2Ffabric%E4%B8%8Eanisible%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[主要是跟anisible做对比，搞数据集群的环境越来越大，机器越来越多，不弄点儿自动化维护的工具，真心折腾不过来，而且容易出现失误。 这两天陪媳妇去图书馆自习，我就看书，看了一本不错的小说《兄弟》：余华写的，相当不错，笑中带泪，奔放中带着坚韧。因为近期工作中想要用一下docker来部署生产环境，为算法的同学提供动态可自定义的各种语言程序运行环境。前面听过几场关于docker环境部署的，听过几个人使用anisible进行docker部署，所以就在图书馆拿了本《奔跑吧：anisible》。 吸引里面提到anisible的我看中的功能性卖点： host分组 任务有顺序依赖的执行 获取上一个task的输出，比如docker容器mysql部署后，把这个容器的id保存给下一个容器web服务的配置文件使用。 配置文件模板化，接受外来变量赋值后，copy进入指定主机的指定目录。 问题概念但是有一个比较恶心的问题，要记住一些概念 inventory 主机 task module 具体执行的一些功能模块 handler 等待被task触发 fact 主机相关的一些信息，比如CPU、内存、ip啥的 问题 module之类的都是别人封装好的，自己要完全按照人家的规则走，不舒服 有些需要额外处理的东西还是要自己写python代码处理，因为有些自定义的处理逻辑，是不会存在于既有的module中的 那么恶心的事情就出现了：既要去使用module中别人给自己制定的规则， 又他么要自己写代码完成自己的逻辑。还不如fabric，完全按照自己的代码去执行逻辑。 fabric今儿早上看了一下fabric 主机分组 【Y：有role进行分组】 任务 【Y】 任务依赖 【Y：通过定义额外的任务组合惹怒我】 主机信息【并没有感觉有什么用】 task间变量传递【上一个任务给环境中的变量赋值，下一个任务可以直接接收到，比如dict】 —— 失败 直接上自己的脚本文件123456789101112131415161718from fabric.api import *# 不指定用户的话，就是当前用户，跟ssh一样env.hosts = [ &apos;user@192.168.1.1&apos;, &apos;user@192.168.1.2&apos;,]# 必须全面指定 ： user@ip:portenv.passwords = &#123; &apos;user@192.168.1.1:22&apos;: &apos;password1&apos;, &apos;user@192.168.1.2:22&apos;: &apos;password2&apos;,&#125;# 这个标识有么有都可以 1.6.3版本@taskdef echo(): run(&apos;echo &quot;hello,world&quot;&apos;) 动态为自己的机器分组, 那么就在fabric.py里执行以下逻辑：12345678910111213141516171819202122services = []datanodes = []hosts = list()def get_host(): with open(&apos;/etc/hosts&apos;, &apos;r&apos;) as hf: for line in hf.readlines(): ss = line.split() if &apos;node&apos; in ss[1]: hosts.append(ss[0]) if &apos;servicenode&apos; in ss[1]: services.append(ss[0]) if &apos;datanode&apos; in ss[1]: datanodes.append(ss[0]) hosts.remove(&apos;10.2.19.104&apos;)env.hosts=hostsenv.roledefs = &#123; &apos;services&apos;: services, &apos;datanodes&apos;: datanodes&#125; 其实这里的/etc/hosts就相当于anisible的inventory文件了。 调用的时候指定host组：1fab cmd:roles=services,cmd=&apos;df -h&apos; 至此：放弃fabric……不能支持依赖部署，或者容器编排。离开……【尝试在任务重使用变量赋值失败，因为每个remote机器赋值都是对于自己的，并不能对全局变量赋值。但是其实通过获取任务的output，所以不放弃了】 稻草一枚： http://fabric-chs.readthedocs.io/zh_CN/chs/api/core/tasks.html https://stackoverflow.com/questions/42946197/return-value-from-fabric-task 并行任务fabric现在也支持并行执行task，并且提供一次并行多少的粒度12345from fabric.api import *@parallel(pool_size=5)def heavy_task(): # lots of heavy local lifting or lots of IO here 指定一次并行5个远程机器1fab -P -z 5 heavy_task http://docs.fabfile.org/en/1.13/usage/parallel.html 获取任务结果1234567891011from fabric.api import task, execute, run, runs_once@taskdef workhorse(): return run(&quot;get my infos&quot;)@task@runs_oncedef go(): results = execute(workhorse) print results 注意：execute是在任务在每个remote host上执行完之后获取结果，而run的stdout则是在单个任务内返回。 http://docs.fabfile.org/en/1.13/usage/execution.html#intelligently-executing-tasks-with-execute 文件传递使用put放到远程机器123456with cd(&apos;/tmp&apos;): put(&apos;/path/to/local/test.txt&apos;, &apos;files&apos;) put(&apos;bin/project.zip&apos;, &apos;/tmp/project.zip&apos;)put(&apos;*.py&apos;, &apos;cgi-bin/&apos;)put(&apos;index.html&apos;, &apos;index.html&apos;, mode=0755) 使用get获取远程机器文件1get(&apos;/path/to/remote_file.txt&apos;, &apos;local_directory&apos;) 上面的put对比anisible来说，只有一方面不足，就是对于类似nginx配置文件中的一些变量信息，我们需要额外自己引入模板，渲染之后再put出去。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux中的hang进程]]></title>
    <url>%2F2017%2F05%2F25%2Flinux%E4%B8%AD%E7%9A%84hang%E8%BF%9B%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[5月24号晚上发现62上有两个hang的进程，占满了内存与cpu 第一个是M2H任务1root 10082 1 97 May21 ? 3-20:08:33 /server/java/jdk1.8.0_60/bin/java -Xmx4096m -Dhdp.version=2.4.2.0-258 -Djava.net.preferIPv4Stack=true -Dhdp.version=2.4.2.0-258 -Dhadoop.log.dir=/var/log/hadoop/root -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.4.2.0-258/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/2.4.2.0-258/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.2.0-258/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx4096m -Dhadoop.security.logger=INFO,NullAppender org.apache.sqoop.Sqoop import -Dmapreduce.job.queuename=xstorm --connect jdbc:mysql://10.0.100.73:3306/xiaodai?useCursorFetch=true&amp;dontTrackOpenResources=true&amp;defaultFetchSize=2000 --driver com.mysql.jdbc.Driver --username weixddata_read --password G7iu1BMo9LWs2e --hive-database ods_tinyv --columns ID,USER_ID,WB_USERNAME,MOBILE,BIZ_ID,MSG_TYPE,SMS_TYPE,DATA_SOURCE,SMS_ID,TASK_ID,SEND_STATUS,RES_CODE,REPORT_STATUS,SEND_TIME,REPORT_TIME,SEND_EXCEPTION --where DATE_FORMAT(SEND_TIME, &apos;%Y-%m-%d&apos;)=&apos;2017-05-20&apos; --table SMS_INFO --hive-import --hive-overwrite --target-dir ods_tinyv_SMS_INFO --outdir /server/app/sqoop/vo --bindir /server/app/sqoop/vo --verbose -m 1 --delete-target-dir --hive-import --hive-table o_wb_xiaodai_sms_info_i --hive-partition-key dt --hive-partition-value 2017-05-20 --null-string \\N --null-non-string \\N 还有一个HIVE任务1root 3483 1 96 May23 ? 1-11:12:07 /server/java/jdk1.8.0_60/bin/java -Xmx4096m -Dhdp.version=2.4.2.0-258 -Djava.net.preferIPv4Stack=true -Dhdp.version=2.4.2.0-258 -Djava.net.preferIPv4Stack=true -XX:NewRatio=12 -XX:MaxHeapFreeRatio=40 -XX:MinHeapFreeRatio=15 -XX:+UseNUMA -XX:+UseParallelGC -XX:-UseGCOverheadLimit -Dhdp.version=2.4.2.0-258 -Dhadoop.log.dir=/var/log/hadoop/root -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.4.2.0-258/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/2.4.2.0-258/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.2.0-258/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx4096m -Xmx1024m -Dhadoop.security.logger=INFO,NullAppender -Dhdp.version=2.4.2.0-258 -Dhadoop.log.dir=/var/log/hadoop/root -Dhadoop.log.file=hadoop.log -Dhadoop.home.dir=/usr/hdp/2.4.2.0-258/hadoop -Dhadoop.id.str=root -Dhadoop.root.logger=INFO,console -Djava.library.path=:/usr/hdp/2.4.2.0-258/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.2.0-258/hadoop/lib/native:/usr/hdp/2.4.2.0-258/hadoop/lib/native/Linux-amd64-64:/usr/hdp/2.4.2.0-258/hadoop/lib/native -Dhadoop.policy.file=hadoop-policy.xml -Djava.net.preferIPv4Stack=true -Xmx4096m -Xmx4096m -Xmx1024m -Dhadoop.security.logger=INFO,NullAppender org.apache.hadoop.util.RunJar /usr/hdp/2.4.2.0-258/hive/lib/hive-exec-1.2.1000.2.4.2.0-258.jar org.apache.hadoop.hive.ql.exec.mr.ExecDriver -localtask -plan file:/tmp/root/228f95e6-26ee-4466-bd3d-a91195c86bf4/hive_2017-05-23_10-38-11_000_7465740175512782543-1/-local-10004/plan.xml -jobconffile file:/tmp/root/228f95e6-26ee-4466-bd3d-a91195c86bf4/hive_2017-05-23_10-38-11_000_7465740175512782543-1/-local-10005/jobconf.xml 手贱，先kill掉了，不然还能看到上面的plan文件，和job配置文件什么的。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[runningdata改版记录]]></title>
    <url>%2F2017%2F05%2F18%2Frunningdata%E6%94%B9%E7%89%88%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[抽象ETLObjRelated，其他对象继承之 ETL的字段名tbl_name需要在数据库里修改为name tasks里的exec_etl_sche任务会调用etl.name【老表的tbl_name会报错找不到】 执行metamap 0045的migrations，这个只是添加字段，不会造成额外影响 sdf 确保新的save方法都已经注释掉 ./manage.py migrate metamap 0050 –settings=metamap.config 开始清洗 clean_etl clean_rel clean_m2h before_clean_blood clean_blood clean_h2m clean_jar clean_email clean_task[分别执行clean， type=4, type=1等等] clean_period 注意 H2M中的个别现象，多个周期的数据存放在同一个表中：SELECT * from metamap_sqoophive2mysql where id in (71,72,73,74,75,76)需要额外手动维护execblood 放开那些注释]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django用户组的思考]]></title>
    <url>%2F2017%2F05%2F10%2Fdjango%E7%94%A8%E6%88%B7%E7%BB%84%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[我们有两个不同的事业部A, B， 目前是用django的group来做的。。。都在我的平台上编辑自己的内容OBJ，我给OBJ指定了唯一一个group_id外键来保证AB事业部内容的隔离。。。。现在遇到问题，因为要控制一些权限，当加group的时候…..自己就懵逼了 不该这样处理AB，对吧。。。应该额外建立一个oaganization来作为AB隔离的。django的group用来做操作权限控制更好，而不是对象隔离。 所以现在的代码结构是：1234567891011121314class OrgGroup(models.Model): name = models.CharField(max_length=200, verbose_name=u"组织名称") owners = models.CharField(max_length=100, verbose_name=u"负责人", blank=True, default='') hdfs_path = models.CharField(max_length=100, verbose_name=u"HDFS临时路径", blank=True, default='') def __str__(self): return self.nameclass UserProfile(models.Model): user = models.OneToOneField(User) phone = models.BigIntegerField(default=110) org_group = models.ForeignKey(OrgGroup, on_delete=models.DO_NOTHING, related_name='user_cgroup', null=True) def __str__(self): return self.user.username 重新定义了一个组织的model，然后每个人都有属于某个特定的组织。 1234class JarApp(models.Model): ..... cgroup = models.ForeignKey(OrgGroup, on_delete=models.DO_NOTHING, ..... 上面的cgroup原来是1cgroup = models.ForeignKey(auth.Group, on_delete=models.DO_NOTHING, related_name=&apos;jar_cgroup&apos;, null=True) 然后还要重构JarApp的新建与查询的一些代码。 这样，使用自定义的组织来划分人员与面向的内容。使用django的group来控制是否组织内特定人员组具有的权限【ETL开发人员、与报表开发人员】。 自定义的组织用来限定报表和ETL所属的组织。而django的auth.group可以管理一组权限【对ETL、报表的访问权限、操作权限等】。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari个别component扩张]]></title>
    <url>%2F2017%2F05%2F09%2Fambari%E4%B8%AA%E5%88%ABcomponent%E6%89%A9%E5%BC%A0%2F</url>
    <content type="text"><![CDATA[需要扩展20个NodeManager，ambari只能手动到每个datanode界面上去安装NodeManager，并启动。啰嗦，麻烦。 看了一下ajax请求，整理了一个脚本。12345678910111213141516171819202122#!/bin/bashfor i in `seq 26 33`;doecho handing $i# 安装servicecurl -u admin:will@bj15. -H &quot;X-Requested-By: ambari&quot; -X POST -d &apos;&#123;&quot;RequestInfo&quot;:&#123;&quot;context&quot;:&quot;Install NodeManager&quot;&#125;,&quot;Body&quot;:&#123;&quot;host_components&quot;:[&#123;&quot;HostRoles&quot;:&#123;&quot;component_name&quot;:&quot;NODEMANAGER&quot;&#125;&#125;]&#125;&#125;&apos; http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts?Hosts/host_name=datanode$&#123;i&#125;.will.com# 安装componentcurl -u admin:will@bj15. -H &quot;X-Requested-By: ambari&quot; -X PUT -d &apos;&#123;&quot;RequestInfo&quot;:&#123;&quot;context&quot;:&quot;Install NodeManager&quot;,&quot;operation_level&quot;:&#123;&quot;level&quot;:&quot;HOST_COMPONENT&quot;,&quot;cluster_name&quot;:&quot;datacenter&quot;,&quot;host_name&quot;:&quot;datanode$&#123;i&#125;.will.com&quot;,&quot;service_name&quot;:&quot;YARN&quot;&#125;&#125;,&quot;Body&quot;:&#123;&quot;HostRoles&quot;:&#123;&quot;state&quot;:&quot;INSTALLED&quot;&#125;&#125;&#125;&apos; http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts/datanode$&#123;i&#125;.will.com/host_components/NODEMANAGER?HostRoles/state=INIT# 启动curl -u admin:will@bj15. -H &quot;X-Requested-By: ambari&quot; -X PUT -d &apos;&#123;&quot;RequestInfo&quot;:&#123;&quot;context&quot;:&quot;Start NodeManager&quot;,&quot;operation_level&quot;:&#123;&quot;level&quot;:&quot;HOST_COMPONENT&quot;,&quot;cluster_name&quot;:&quot;datacenter&quot;,&quot;host_name&quot;:&quot;datanode$&#123;i&#125;.will.com&quot;,&quot;service_name&quot;:&quot;YARN&quot;&#125;&#125;,&quot;Body&quot;:&#123;&quot;HostRoles&quot;:&#123;&quot;state&quot;:&quot;STARTED&quot;&#125;&#125;&#125;&apos; http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts/datanode$&#123;i&#125;.will.com/host_components/NODEMANAGERdone]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自制RPM包并发布到自己的yum源]]></title>
    <url>%2F2017%2F04%2F27%2F%E8%87%AA%E5%88%B6RPM%E5%8C%85%E5%B9%B6%E5%8F%91%E5%B8%83%E5%88%B0%E8%87%AA%E5%B7%B1%E7%9A%84yum%E6%BA%90%2F</url>
    <content type="text"><![CDATA[使用一个普通用户执行，以免对系统造成重伤。 1yum install -y rpmdevtools rpm-build 自定义工作空间方案1： 1rpmdev-setuptree 方案2 创建文件~/.rpmmacros1%_topdir /home/will/rpmbuild 配置了空间之后，还得手动创建一下1mkdir /home/will/rpmbuild 创建需要的目录12cd ~/rpmbuild mkdir -pv &#123;BUILD,BUILDROOT,RPMS,SOURCES,SPECS,SRPMS&#125; BUILD #编译之前，如解压包后存放的路径 BUILDROOT #编译后存放的路径 RPMS #打包完成后rpm包存放的路径 SOURCES #源包所放置的路径 SPECS #spec文档放置的路径 SPRMS #源码rpm包放置的路径 注：一般我们都把源码打包成tar.gz格式然后存放于SOURCES路径下，而在SPECS路径下编写spec文档，通过命令打包后，默认会把打包后的rpm包放在RPMS下，而源码包会被放置在SRPMS下 源码放到SOURCES目录1cp /tmp/redis-3.2.8.tar.gz SOURCES SPECS下创建配置到SPECS下创建指定配置，编辑的时候会自动生成模板。编辑SPECS/redis.spec12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061Name: redisVersion: 3.2.8 Release: 3%&#123;dist&#125;Summary: redis from will Group: System Environment/DaemonsLicense: GPLv2URL: https://github.com/willcupSource0: redis-3.2.8.tar.gzPackager: WillCup &lt;willcup@163.com&gt; BuildRequires: gcc,makeRequires: openssl,chkconfigBuildRoot: %_topdir/BUILDROOT%define PortFindDir /usr/local/will_redis%descriptionthis is build from will, just for ambari service.%prep%setup -q%buildmake %&#123;?_smp_mflags&#125;%install[ ! -e $RPM_BUILD_ROOT%&#123;PortFindDir&#125; ] &amp;&amp; mkdir -p $RPM_BUILD_ROOT%&#123;PortFindDir&#125;cp -rfv * $RPM_BUILD_ROOT%&#123;PortFindDir&#125;[ ! -e $RPM_BUILD_ROOT/etc/init.d ] &amp;&amp; mkdir -p $RPM_BUILD_ROOT/etc/init.d[ ! -e $RPM_BUILD_ROOT/usr/local/bin ] &amp;&amp; mkdir -p $RPM_BUILD_ROOT/usr/local/bincp -vf $RPM_BUILD_ROOT%&#123;PortFindDir&#125;/utils/redis_init_script $RPM_BUILD_ROOT/etc/init.d/rediscp -vf $RPM_BUILD_ROOT%&#123;PortFindDir&#125;/src/redis-server $RPM_BUILD_ROOT/usr/local/bincp -vf $RPM_BUILD_ROOT%&#123;PortFindDir&#125;/src/redis-cli $RPM_BUILD_ROOT/usr/local/bin#mkdir -p &quot;$RPM_BUILD_ROOT&quot;#cp -rvf * &quot;$RPM_BUILD_ROOT&quot;#ls -l &quot;$RPM_BUILD_ROOT&quot;echo &quot;no reason to install redis, just move it&quot;%files%defattr (-,root,root,0755)%dir %&#123;PortFindDir&#125;%attr(0755, root, root) %&#123;PortFindDir&#125;/*%attr(0755, root, root) /etc/init.d/redis%attr(0755, root, root) /usr/local/bin%doc%postun rm -fr %(PortFindDir&#125;rm -vf /usr/local/bin/redis-*rm -vf /etc/init.d/redis%changelog* Fri Dec 29 2016 willcup &lt;willcup@163.com&gt; - 3.2.8-1 - Initial version just changelog 其实上面的buildroot就相当于是后面rpm安装时候的系统根目录。所以一般都需要指定一个子目录。不然是行不通的，会发现弄完以后RPM包里没有任何文件。 开始构建1rpmbuild -ba redis.spec 可以看到已经有了结果 这个是构建过程中的源码目录：12345678910111213141516171819202122[will@datanode08 rpmbuild]$ ll BUILD/redis-3.2.8/total 204-rw-r--r--. 1 will will 85775 Feb 12 23:14 00-RELEASENOTES-rw-r--r--. 1 will will 53 Feb 12 23:14 BUGS-rw-r--r--. 1 will will 1805 Feb 12 23:14 CONTRIBUTING-rw-r--r--. 1 will will 1487 Feb 12 23:14 COPYING-rw-r--r--. 1 will will 0 Apr 27 12:25 debugfiles.list-rw-r--r--. 1 will will 0 Apr 27 12:25 debuglinks.list-rw-r--r--. 1 will will 0 Apr 27 12:25 debugsources.listdrwxr-xr-x. 7 will will 4096 Apr 27 12:25 deps-rw-r--r--. 1 will will 11 Feb 12 23:14 INSTALL-rw-r--r--. 1 will will 151 Feb 12 23:14 Makefile-rw-r--r--. 1 will will 4223 Feb 12 23:14 MANIFESTO-rw-r--r--. 1 will will 6834 Feb 12 23:14 README.md-rw-r--r--. 1 will will 46695 Feb 12 23:14 redis.conf-rwxr-xr-x. 1 will will 271 Feb 12 23:14 runtest-rwxr-xr-x. 1 will will 280 Feb 12 23:14 runtest-cluster-rwxr-xr-x. 1 will will 281 Feb 12 23:14 runtest-sentinel-rw-r--r--. 1 will will 7606 Feb 12 23:14 sentinel.confdrwxr-xr-x. 2 will will 4096 Apr 27 12:25 srcdrwxr-xr-x. 10 will will 4096 Feb 12 23:14 testsdrwxr-xr-x. 7 will will 4096 Feb 12 23:14 utils 打包完成后rpm包存放的路径：1234[will@datanode08 rpmbuild]$ ll RPMS/x86_64/total 8-rw-rw-r--. 1 will will 1752 Apr 27 12:25 redis-3.2.8-1.el6.x86_64.rpm-rw-rw-r--. 1 will will 1880 Apr 27 12:25 redis-debuginfo-3.2.8-1.el6.x86_64.rpm 源码rpm包放置的路径123[will@datanode08 rpmbuild]$ ll SRPMS/total 1516-rw-rw-r--. 1 will will 1549338 Apr 27 12:25 redis-3.2.8-1.el6.src.rpm 然而查看rpm包的文件，竟然什么都没有。12[will@datanode08 rpmbuild]$ rpm -qlp RPMS/x86_64/redis-3.2.8-1.el6.x86_64.rpm (contains no files) 修改之后，能够正常安装与使用了。 使用 rpm -ivh xx.rpm成功测试。 下一步，将RPM发布至我们自有的yum源上。 按照其他的类似hadoop之类，创建一个redis目录，然后把我们的rpm包放上去 目录样子123456789101112131415[root@datanode21 2.4.2.0]# lltotal 2146284.....drwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 hadoopdrwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 hadooplzodrwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 hbase.......drwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 ooziedrwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 phoenixdrwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 pigdrwxr-xr-x 2 ambari-qa users 4096 Apr 25 2016 rangerdrwxr-xr-x 2 root root 4096 Apr 27 16:45 redisYou have new mail in /var/spool/mail/root[root@datanode21 2.4.2.0]# pwd/server/www/html/hdp/HDP/centos6/2.x/updates/2.4.2.0 先看下我们的HDP.repo1234567[HDP-2.4]name=HDP-2.4baseurl=http://10.2.19.110/hdp/HDP/centos6/2.x/updates/2.4.2.0path=/enabled=1gpgcheck=0 可以看到是同一个目录，我们需要在这个目录下重新执行createrepo12345678910111213141516171819[root@datanode21 2.4.2.0]# createrepo .Spawning worker 0 with 182 pkgsWorkers FinishedGathering worker resultsSaving Primary metadataSaving file lists metadataSaving other metadataGenerating sqlite DBsSqlite DBs complete[root@datanode21 2.4.2.0]# ll repodatatotal 856-rw-r--r-- 1 root root 373992 Apr 27 17:59 458b198ae4b61c02b0f7bb03ae9421365dcb2076ffb6087e6808e7915ec31778-filelists.sqlite.bz2-rw-r--r-- 1 root root 9092 Apr 27 17:59 9a59871d45121b0be43f188ae7ffe788e2eceb07d03648717323dedd26faf567-other.xml.gz-rw-r--r-- 1 root root 12162 Apr 27 17:59 bb3c2dc4c1f7ae17b172f104e3df5171d0d09db797a5f5a9227ab4ae424a45b7-other.sqlite.bz2-rw-r--r-- 1 root root 37245 Apr 27 17:59 c23cec21ae4b3e9b89a029b593362ca1e2483316e1d07843a9035ad8e3c93053-primary.xml.gz-rw-r--r-- 1 root root 363468 Apr 27 17:59 c521ec682137ff19e61f7ec91e86c725d6d6fa7553be01c783a67ffb68b30afe-filelists.xml.gz-rw-r--r-- 1 root root 65483 Apr 27 17:59 eb98320b3ad040203135b91f26caee107900f2e9bdc86a4be6c5218fcc505036-primary.sqlite.bz2-rw-r--r-- 1 root root 2996 Apr 27 17:59 repomd.xml 然后更新客户机上，也就是要安装redis 的机器上的repo信息。123yum clean allyum updateyum search redis 能够搜到，至此结束。 问题我们要构建redis，主要是为了适应既有ambari的redis service，它执行安装的命令是yum install redis.3.2.8。当然我们是可以修改一下他的install代码的，但是考虑到以后其他组件很有可能还是通过yum安装，那还是现在就把自己动手构建yum的rpm包这个问题解决了吧，以绝后患。 问题来了，上面我们搜到的只是redis.x86_64,实际安装的时候请求的是3.2.8，需要版本号加进去才行。看到spec里有version关键字。。。折腾了好半天，都不是，构建出来search的时候都还是只有redis。 后来参考既有的hadoop、flume等yum源，成功搞定。123flume_2_4_2_0_258-1.5.2.2.4.2.0-258.el6.noarch.rpm 25-Apr-2016 19:54 40M flume_2_4_2_0_258-agent-1.5.2.2.4.2.0-258.el6.noarch.rpm 25-Apr-2016 19:54 6.4K 发现这个版本号貌似是跟着文件夹的。 所以，修改如下: 修改redis.spec为redis-3.2.8.spec 编辑spec文件12345Name: redis-3.2.8Version: 1_0_0URL: https://github.com/willcupSource0: redis-3.2.8.tar.gz 关键在于Name，后面都是喽啰 这样解压后会去redis-3.2.8-1_0_0，也就是根据name和version生成的规则，所以可能要调整一下tar.gz里的文件夹名称 重新构建预发布，即可12345redis-3.2.8-1_0_0-4.el6.x86_64.rpmyum search redisredis.x86_64 : redis from willredis-3.2.8.x86_64 : redis 3 2 8 from will 参考： https://docs.fedoraproject.org/en-US/Fedora_Draft_Documentation/0.1/html/Packagers_Guide/chap-Packagers_Guide-Spec_File_Reference-Preamble.html https://fedoraproject.org/wiki/How_to_create_an_RPM_package/zh-cn#.E5.AE.9E.E4.BE.8B]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TEZ-hadoop上数据处理的新篇章]]></title>
    <url>%2F2017%2F04%2F26%2FTEZ-hadoop%E4%B8%8A%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86%E7%9A%84%E6%96%B0%E7%AF%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[TEZ生成一个复杂的DAG task图 目前基本所有的hadoop任务都是MapReduce程序，面向批数据的特性使得它并不能够满足一些特定查询。TEZ是传统MR程序的另一个选择，可以更快地返回结果，而且减少过程中的吞吐。 动机分布式处理是hadoop的核心。存储与分析各种不同的大量的数据使得hadoop之上产生了很多工具。随着hadoop进入yarn时代，它把MR解耦了出来，这样可以使用其他的数据处理方式来迎接新的不同的挑战。 设计hive,pig这些高层次的数据处理工具需要一个引擎来解析他们的语句，然后执行。TEZ就是这样一个引擎。 解析，建模，执行TEZ把数据处理看作一个数据流图，节点代表数据处理逻辑，边代表着数据的流向。TEZ有一个很好的数据流相关API,能够让用户清晰表达出自己的复杂查询逻辑，也能够支持hive、pig等高级应用生成的查询计划。 如下图，使用range partitioning进行分布式sort的模型。Preprocessor stage中发送samples给Sampler，计算每个数据分区的range，以保证平均分配。然后这些range再被发送给Partition stage。然后partition stage和aggregate stage就读取分配的stage，执行数据的扫描与后续聚合工作。 灵活的input-processor-output任务模型TEZ中对于数据流中每个处理逻辑都是由三部分组成：input, processor, output。input和output决定数据接口。processor则是数据处理逻辑。TEZ只要求在同一个vertex task中这三部分之间是相互兼容的即可。 动态图配置的性能分布式数据处理天生就是动态的，所以很难提前知道最优并发和数据走向。更多的信息只能在运行时才获知，比如数据的内容和大小，这其实是可以帮助我们优化执行计划的。我们还注意到TEZ自身并不能总是自己执行这些动态优化。 https://hortonworks.com/blog/apache-tez-a-new-chapter-in-hadoop-data-processing/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hiveserver2负载均衡]]></title>
    <url>%2F2017%2F04%2F26%2Fhiveserver2%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[hiveserver2是通过在zookeeper注册一个namespace，然后管理所有的hiveserver实例，实现动态服务发现的。 znode样式：1/&lt;hiveserver2_namespace&gt;/serverUri=&lt;host:port&gt;;version=&lt;versionInfo&gt;; sequence=&lt;sequence_number&gt;, hiveserver实例会在znode上设置一个watch。当znode被修改的时候，watch会发送给hiveserver相关信息。这个通知能够让所有的hiveserver实例知道它是不是对于client端可用。 当有hiveserver实例退出时，会从zk的对应node里移除，但是只对新的客户端连接生效。(已经连接的session不能生效了)。只有已经连接到这个hiveserver的最后一个client的session结束后，才会自动把这个hiveserver完全关闭，下面这个命令就是做这个工作的。 使用下面命令移除一个hiveserver1hive --service hiveserver2 --deregister &lt;package ID&gt; 没有zk时的查询下面是一个传统的查询流程： 通过JDBC/ODBC driver连接到HS2实例，建立一个session 然后每次查询的时候client都发送语句给HS2，转化成Hadoop上的执行任务 每个查询的结果都写道一个临时文件中 客户端driver从HS2抽取临时文件中的数据记录 带有zk的查询因为可以使用动态服务发现，所以客户端driver必须知道怎样使用这个特性。对于HDP2.2或者JDBC driver2.0.0版本之后才能支持。 动态服务发现实现如下： 多个HS2实例使用zk注册自己 客户端driver连接zk jdbc:hive2://;serviceDiscoveryMode=zooKeeper; zooKeeperNamespace=&lt;hiveserver2_namespace zk随机返回一个host:port给客户端 客户端执行单个服务传统查询过程 配置hive.zookeeper.quorum zk列表hive.zookeeper.session.timeout 超时就关闭sessionhive.server2.support.dynamic.service.discovery 设置为truehive.server2.zookeeper.namespace 指定一个就行了，默认是hiveserver2 https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_hadoop-ha/content/ha-hs2-requests.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hue对于多个hiveserver2的支持]]></title>
    <url>%2F2017%2F04%2F26%2Fhue%E5%AF%B9%E4%BA%8E%E5%A4%9A%E4%B8%AAhiveserver2%E7%9A%84%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[相关连接代码：123456789def hiveserver2_jdbc_url(): urlbase = 'jdbc:hive2://%s:%s/default' % (beeswax.conf.HIVE_SERVER_HOST.get(), beeswax.conf.HIVE_SERVER_PORT.get()) if get_conf().get(_CNF_HIVESERVER2_USE_SSL, 'FALSE').upper() == 'TRUE': return '%s;ssl=true;sslTrustStore=%s;trustStorePassword=%s' % (urlbase, get_conf().get(_CNF_HIVESERVER2_TRUSTSTORE_PATH), get_conf().get(_CNF_HIVESERVER2_TRUSTSTORE_PASSWORD)) else: return urlbase 可以看到，除非启用SSL，否则咋样都拼不进去下面这种串：1jdbc:hive2://zkNode1:2181,zkNode2:2181,zkNode3:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2_zk Tip: 可以先用tengine配置TCP代理，然后在这里实现负载均衡，hue端只需要配置代理的地址就可以了。 —— 希望hive的driver可以支持，有待测试。 参考： http://lxw1234.com/archives/2016/05/675.htm]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive中使用向量化查询引擎Vectorized-Query-Execution]]></title>
    <url>%2F2017%2F04%2F25%2Fhive%E4%B8%AD%E4%BD%BF%E7%94%A8%E5%90%91%E9%87%8F%E5%8C%96%E6%9F%A5%E8%AF%A2%E5%BC%95%E6%93%8EVectorized-Query-Execution%2F</url>
    <content type="text"><![CDATA[简介向量化查询引擎可以在执行scan, filter, aggregate, join等操作的时候很大程度上降低CPU损耗。标准sql执行系统每次只处理一行数据，内部执行的时候会包括很长的代码路径和重要的metadata解析。向量化查询引擎简化了这个过程，一次读取1024行数据，在这个数据块中，每个字段都被存储成向量化的结构(一个原是数据类型的数组)。对于一些简单的算法可以通过快速迭代这个vector完成，只需要在循环过程中调用甚至不用调用几个函数。这些loop以一种流式方式编译，在固定时间内结束，为了提高效率，会使用processor pipline和内存缓存。详细设计 个人理解：就是每行都执行很小的计算，但是要处理N多次。改成一次处理多行，同时针对多行做计算处理，节省CPU时间。 使用启用要使用向量化查询引擎，必须先把hive表存储成ORC格式的。1set hive.vectorized.execution.enabled = true; 默认是没有开启向量化查询引擎的，需要手动开启。 支持的数据类型与操作 tinyint smallint int bigint boolean float double decimal date timestamp (see Limitations below) string其他的数据类型还是会按照传统方式一行行处理。 支持的表达式： 简单算法: +, -, *, /, % AND, OR, NOT 比较 &lt;, &gt;, &lt;=, &gt;=, =, !=, BETWEEN, IN ( list-of-constants ) as filters 布尔表达式 (non-filters) using AND, OR, NOT, &lt;, &gt;, &lt;=, &gt;=, =, != IS [NOT] NULL 所有的匹配函数 (SIN, LOG, etc.) string相关函数 SUBSTR, CONCAT, TRIM, LTRIM, RTRIM, LOWER, UPPER, LENGTH 类型转换 UDF 日期函数 (YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, UNIX_TIMESTAMP) IF 条件表达式 UDF通过向后兼容的方式支持，不过执行的时候就是执行向量化查询引擎，但是没有内置的快。向量化的filter操作是从左到右执行，所以最好把UDF放在带有and的where语句的最后：1where column1 = 10 and myUDF(column2) = "x" 限制Timestamps 只能是 1677-09-20 到2262-04-11.]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[集群service配置相关]]></title>
    <url>%2F2017%2F04%2F21%2F%E9%9B%86%E7%BE%A4service%E9%85%8D%E7%BD%AE%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[YARN 服务名称 现有配置 TO resourcemanager 1G 4G nodemanager 1G 2G timeline server 1G 4G resource + timline server = 2G HDFS 服务名称 现有配置 TO datanode 1G 2G namenode 3G namenode * 2 = 6G MAPRED 服务名称 现有配置 TO history server 1G 2G HIVE 服务名称 配置 hiveserver2 769M metastore 1G hcatserver 1G 3G HBASE 服务名称 配置 regionserver 2G master 1G regionserver 8 + master 2 = 18G ZK 服务名称 配置 zk server 1G zk * 3 = 3G KAFKA 服务名称 配置 broker 1G broker * 3 =3G SPAKR 服务名称 配置 spark history server 1G 需要copy的机器以下机器copy完成后，全部将内存提升至32G、CPU提升到8核 迁出机器 datanode15.will.com -&gt; NM，DN 1.103-prd-datanode14.will.com -&gt;kafka, DN,NM 1.108-prd-datanode19.will.com -&gt; regionserver, DN,NM 1.110-prd-datanode21.will.com -&gt; NM, DN 1.99-prd-datanode10.will.com -&gt; kafka, DB, NM 1.113-fengkong.data.com 风控 -&gt; NONE 1.87-prd-datanode05.data.com -&gt; DN, NM, regionserver 1.97-prd-datanode08.data.com -&gt; DN, NM, regionserver 1.95-prd-datanode06.data.com -&gt; kafka, DN, NM, SB HbaseMaster 1.96-prd-datanode07.data.com -&gt; DN, NM, regionserver 原有机器迁出会占新资源的内存32G 10 + CPU80 原有机器资源升级在原有资源基础上，添加16G内存，CPU核数翻倍【把迁移出去的机器空闲出来的资源充分利用】： 1.107-prd-datanode18.will.com 1.109-prd-datanode20.will.com 1.102-prd-datanode13.will.com 1.106-prd-datanode17.will.com 1.100-prd-datanode11.will.com 1.98-prd-datanode09.will.com 1.110-prd-datanode21.will.com 1.86-prd-datanode04.data.com 1.82-prd-datanode01.data.com 1.83-prd-datanode02.data.com 1.84-prd-datanode03.data.com 1.105-prd-datanode16.will.com 1.101-prd-datanode12.will.com 理论上，对原有机器不会有额外资源要求，对新机器资源也不会有影响。 需要新建的机器新服务节点可按照service01.will.com开始弄主机名。 单机配置： 32G内存 + 8核 + 50G硬【整机空间即可】 + CentOS release 6.8 (Final) 机器数量： 8台共占：内存32G 8 + CPU8 8 = 内存256G + CPU 64 新数据节点目前最新数据节点的主机名编号到了21，新机器可以datanode22.will.com开始。 单机配置：32G内存 + 8核CPU + 640G硬盘【/server/目录挂载空间】 + CentOS release 6.8 (Final) 机器数量： 12【如果物理机资源不够，可以暂时弄10台，以后酌情扩容】 共占：内存32G 12 + CPU8 12 = 内存384G + CPU 96]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari中启用hive-ACID事务]]></title>
    <url>%2F2017%2F04%2F20%2Fambari%E4%B8%AD%E5%90%AF%E7%94%A8hive-ACID%E4%BA%8B%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[场景 数据重新执行 数据流重新执行 逐渐改变的维度 维度历史变更 标准sql通过insert、update、delete、事务还有最近出现的merge方式来提供acid操作。这些已经被证明足够使用的了。 概念 事务表。hive支持单表事务，但是目标表必须需要声明是支持事务的。 分区表。hive支持表分区，把数据分开以提供快速查询。分区与ACID是相互独立的概念。通常大表都是有分区的。 ACID操作（insert/update/delete）。 主键 流式数据。数据可以通过storm、flume等流式进入hive的事务表中 优化并发。 压缩。数据必须时段性的被压缩一下，节省空间，优化数据访问。最好让系统自动处理这件事情。，不过也可以设置外部的调度器。 启用ACID事务hello创建一个事务表，并插入一些数据12345drop table if exists hello_acid;create table hello_acid (key int, value int)partitioned by (load_date date)clustered by(key) into 3 bucketsstored as orc tblproperties ('transactional'='true'); 1234insert into hello_acid partition (load_date='2016-03-01') values (1, 1);insert into hello_acid partition (load_date='2016-03-02') values (2, 2);insert into hello_acid partition (load_date='2016-03-03') values (3, 3);select * from hello_acid; 删除数据：12delete from hello_acid where key = 2;select * from hello_acid; 更新数据：12update hello_acid set value = 10 where key = 3;select * from hello_acid; 这些DML语句意在在大数据中执行少量修改，应该是记录级别的数据管理。如果你有小的多个批量修改，应该使用streaming data ingestion。 Streaming Data Ingestion许多时候我们需要处理连续的实时数据流，想要更方便的操作这些数据。hive可以自动把流式数据加入hive表中，也支持实时的数据抽取与查询。 现在hive提供两种： 已经存在是storm hive bolt方案，flume hive sink方案。这些工具对数据有较少的操作，重在转移数据 直接使用低级的Streaming Ingest API. 在使用streaming api之前，需要先创建好分区事务表。从查询角度来看，一切应该都是确定好的。 4. 实践插入几条数据对我们测试起来很简单，但是实际环境中，我们需要一次性处理几千或者几百万的数据。下面我们通过几个通用场景讨论一下怎样处理数据批次。 这些模型需要你建立一个主键。hive并不强制主键唯一，所以你必须在你app中控制一下。虽然hive2.1介绍了non-validating 外键，但是这东西目前还并没有被完整全面的验证过。 4.1 searched updateshive ACID支持searched updates，这是一种常见的更新方式。注意，基于hive ACID的架构，更新必须通过批处理的方式执行。一次更新一行这种事情在实际使用中是行不通的。如果想通过某种方式一次性更新大量数据，那么searched updates就可以帮你了。 假设有一个维度表，包含的一个flag，指示当前记录是不是最新的值。这样我们就可以沿时间追踪维度变更情况。当维度表发生更新的时候，我们就把已经存在记录的设置成old。1234drop table if exists mydim;create table mydim (key int, name string, zip string, is_current boolean)clustered by(key) into 3 bucketsstored as orc tblproperties ('transactional'='true'); 1234insert into mydim values (1, 'bob', '95136', true), (2, 'joe', '70068', true), (3, 'steve', '22150', true); 123drop table if exists updates_staging_table;create table updates_staging_table (key int, newzip string);insert into updates_staging_table values (1, 87102), (3, 45220); 执行更新,执行前后可以看一下维度表的数据变化，其实就是更新了flag而已。12update mydim set is_current=false where mydim.key in (select key from updates_staging_table); 4.2 searched deletes批量删除也可以使用staging table轻松搞定。但是需要你在表之间放置一个公共key，有些类似RDBMS中的主键。123delete from mydimwhere mydim.key in (select key from updates_staging_table);select * from mydim; 5. 批量覆盖更新有的时候我们需要批量更新一些数据。例如第一种SCD更新或者数据重导。hive目前还不支持merge操作，在支持之前，我们只能考虑使用先删除后插入的方式，但这样可能会造成查询客户端脏读。或者也可以在重新清洗的时候临时停掉查询。 6. ACID工具ACID事务执行过程中会创建一系列的锁。事务和他们的事务锁可以通过hive的一些工具来查看。 6.1 查看事务1show transactions 6.1 查看锁有read, update，X lock。update锁与update操作互斥，但是与read兼容。X锁，与所有锁都互斥，属于独占。1show locks 6.1 终止事务注意这并不会马上kill掉所有相关查询。ACID查询是周期性执行的，默认是2.5分钟，如果他们检测到自己的事务被kill，就会执行自动退出。1abort transactions T1 T2 T3 7. 性能考虑 创建分区 insert快，update和delete都会比较慢一些，因为要扫描整个分区。 如果你们的工作里需要大量更新数据，那么请周期行执行compaction操作。不然数据大小会越来越大，查询也会越来越慢。 8. 深入探究在使用之前一定要深入了解这套系统的工作原理，并且在你的可以容忍丢失的数据上执行测试。过程中也注意备份数据。 ACID表有一个隐藏字段row__id。这个系统内置的字段名称有可能会变。你应该构建一个基于这个字段的长远的方案。 这个字段记录的内容有： 数据被insert或者update的active的事务id 数据所在bucket的bucketid 此次事务或者bucket中的rowid 看一下1234hive&gt; select row__id from hello_acid;OK&#123;"transactionid":12,"bucketid":0,"rowid":0&#125;&#123;"transactionid":10,"bucketid":1,"rowid":0&#125; 常用场景就是确认所有的数据都load进来了。假设上游数据provider认为hive里的持久化数据丢失了，那么你的provider(storm Bolt比如)就会告诉你插入这些数据的事务ID。然后我们可以使用这个事务id计算一下实际的记录数。查询的时候使用X替换掉你的事务ID12set hive.optimize.ppd=false;select count(*) from hello_acid where row__id.transactionid = X; 记牢，事务中插入的数据有可能会被后续的update或者delete语句影响到，所以如果count数并不符合，那就可能是这些因素造成的。 参考： https://hortonworks.com/hadoop-tutorial/using-hive-acid-transactions-insert-update-delete-data/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HIVE创建bucketed表]]></title>
    <url>%2F2017%2F04%2F20%2FHIVE%E5%88%9B%E5%BB%BAbucketed%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[建表语句1234CREATE TABLE user_info_bucketed(user_id BIGINT, firstname STRING, lastname STRING)COMMENT &apos;A bucketed copy of user_info&apos;PARTITIONED BY(ds STRING)CLUSTERED BY(user_id) INTO 256 BUCKETS; 基于字段user_id分桶的。 使用12345set hive.enforce.bucketing = true; -- (Note: Not needed in Hive 2.x onward)FROM user_idINSERT OVERWRITE TABLE user_info_bucketedPARTITION (ds='2009-02-25')SELECT userid, firstname, lastname WHERE ds='2009-02-25'; hive怎样把row打散到不同的bucket中去的呢？一般是决定于hash函数，使用什么hash函数又取决于分桶字段的类型。比如整型字段就使用hash_int函数。 注意，如果分桶字段类型不同于insert进来的数据类型会出错的，或者手动使用不同类型的数据执行分桶操作也会出错。只要设置了set hive.enforce.bucketing=true，就能够正确的把数据发布到对应的地方。 再来一个例子Bucketed Sorted Table1234567891011CREATE TABLE page_view(viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING COMMENT 'IP Address of the User') COMMENT 'This is the page view table' PARTITIONED BY(dt STRING, country STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 这个表根据userid分桶，每个bucket中按照viewTime升序排列。这种组织结果允许用户更高效的抽取clustered的字段，这里就是userid字段了。排序属性让内部操作能够快速处理估算查询的任务。MAP KEYS和COLLECTION ITEMS关键词可以用来处理字段是list或者map的情况。 CLUSTERED BY 和 SORTED BY并不影响insert数据。用户需要注意reducer数必须要跟bucket的数量一致，在query语句中还要使用CLUSTER BY 和SORT BY语句。 还有一种倾斜表。适用于某些表中确认包含倾斜数据的情况。通过指定倾斜key，hive会把他们打散到不同的文件中。 12CREATE TABLE list_bucket_single (key STRING, value STRING) SKEWED BY (key) ON (1,5,6) [STORED AS DIRECTORIES]; 下面这个是有两个倾斜key12CREATE TABLE list_bucket_multiple (col1 STRING, col2 int, col3 STRING) SKEWED BY (col1, col2) ON ((&apos;s1&apos;,1), (&apos;s3&apos;,3), (&apos;s13&apos;,13), (&apos;s78&apos;,78)) [STORED AS DIRECTORIES]; 参考： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-BucketedSortedTables]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于hive-update]]></title>
    <url>%2F2017%2F04%2F19%2F%E5%85%B3%E4%BA%8Ehive-update%2F</url>
    <content type="text"><![CDATA[ACID A: atomicity。原子性，一个操作要么成功要么失败，不能存在中间状态。 C：consistency。一致性。一旦某个操作完成之后，后续所有的操作都能访问到上个操作完成之后的状态 I：isolation。隔离性。一个未完成的操作只对当前操作用户有影响，不能影响到其他用户 D：durability。持久性。一个操作完成之后，就会把成功的状态持久化，即便系统挂掉也要持久化。 0.13版本之前，只在分区层次上支持原子性、一致性、持久性。隔离性通过调整可用的锁机制来达成(通过zk或者内存)。0.13之后完全支持了row级别的ACID，这样就可以在别人读取数据的时候添加行，也不会有影响。 用例一般具有ACID特性的事务适用于以下场景 流式数据接受。许多用户使用flume、storm、kafka等将数据放到hadoop集群中。这些工具可能会以每秒几百条甚至更多的速度写入。hive只能每十五分钟或者1个小时创建一个分区来接受他们。添加分区的策略会导致表里的分区暴增。也可以把流式数据放到既有的分区中，但是这会导致用户脏读（他们可能会看到一些他们开始查询时还没有的数据），还有就是会有很多的小文件产生，这对namenode也是一个很大的压力。有了ACID之后，就可以解决这个问题了 慢慢改变的维度表。在一个典型的星型数据仓库架构中，维度表可能会随着时间推移慢慢修改。例如，零售商可能会慢慢增开新店，那么新店就应该被添加到store表里，或者已经存在的店面可能会修改店面大小或者其他的因素。这些改变都会产生数据的insert或者update。从0.14开始，支持了 数据重导。有时手机来的数据发现不对，需要纠正。或者，开始有90%的数据，后面又有全量数据了。或者业务规则改变需要修改对应规则的一些数据。0.14之后用户可以insert、update、delete。 使用SQL MERGE语句批量更新。 限制 BEGIN,COMMIT,ROLLBACK目前还不支持。所有的语言都是auto-commit。后面版本会支持 只支持ORC的文件格式。 默认这种事务是关闭的。 数据表必须是bucketed的。外部表不支持ACID，因为它不受compactor的控制HIVE-13175 在一个非ACID的session里执行ACID操作是不可以的。hive的事务manager必须设置成 org.apache.hadoop.hive.ql.lockmgr.DbTxnManager才能使用ACID表。 目前只支持快照级别的隔离性。当一个查询开始后，会给它提供一个一致的数据快照。不支持dirty read, read committed, repeatable read, seializable。 已经存在的zk或者内存锁manager与此类事务并不兼容。这里不详说了，参考这里 使用oracle作为元数据库可能会有些问题，设置datanucleus.connectionPoolingType=DBCP 语法改变 添加了 SHOW TRANSACTIONS、SHOW COMPACTIONS、ABORT TRANSACTIONS SHOW LOCKS命令改成了提供事务相关的锁的信息。如果在使用ak或者内存的lock manager的话，你会发现其实输出的东西没什么区别。 alter table的时候增加了一个是否执行表或者分区的压缩。通常用户不用管，系统会自动做这些事情。然而，如果compaction被关闭了，那么系统就不会自动处理了。alter table用来初始化压缩。具体参考这里。这会把压缩请求放进一个队列里。要看压缩进度的话，就使用SHOW COMPACTIONS语句。 基本设计HDFS是不支持直接修改文件的。当有人写数据到文件的时候也不提供读的一致性。为了提供这些特性，我们使用了其他的数据仓库工具。表和分区的数据存在一系列的base文件里。新的insert、update、delete存在delta文件中，读取的时候把这些更新都执行上。 base和delta文件夹以前一个分区的所有文件都放在一个单独的目录下。ACID的writer会有一个base文件的目录，还有一堆delta文件的目录。下面是一个没有分区的数据表“t”1234567hive&gt; dfs -ls -R /user/hive/warehouse/t;drwxr-xr-x - ekoifman staff 0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022-rw-r--r-- 1 ekoifman staff 602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000drwxr-xr-x - ekoifman staff 0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000-rw-r--r-- 1 ekoifman staff 611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000drwxr-xr-x - ekoifman staff 0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000-rw-r--r-- 1 ekoifman staff 610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000 compactorcompactor是在metastore中运行的一系列的后台进程，用来支持ACID系统。包含Initiator, Worker, Cleaner,AcidHouseKeeperService等。 delta文件压缩随着修改表数据的操作越来越多，delta文件也越来越多，需要压缩提高后面的处理效率。类似hbase，也是有minor compaction和major compaction Minor compaction把已经存在的同一个bucket的多个delta文件合并成一个 Majro compaction把bucket中的base文件和delta文件合并到一个新的base文件。这个会耗时多一些，不过也更高效。 压缩工作是后台自动运行的，不会阻止并发的读写操作。执行压缩之后，系统会等用户读完所有的旧文件之后移除这些旧文件。 Initiator这个模块是负责发现哪些table或者分区需要压缩。使用hive.compactor.initiator.on参数启用。每个compaction task负责处理一个分区(没分区的就是一个表)。如果连续压缩失败，超过hive.compactor.initiator.failed.compacts.threshold设置的阈值,就停止compaction了。 Worker一个Woker处理一个compaction task。通常是MR任务，名字格式是：主机名-compactor-数据库.表名.分区。使用hive.compactor.worker.threads设置每个metastore启动的compactor线程数。 Cleaner用来删除压缩之后，之前的一些旧文件 AcidHouseKeeperService找到心跳超时hive.txn.timeout的事务，终止它。释放资源。 SHOW COMPACTIONS这个命令用来显示目前正在运行的compaction信息，还有最近的压缩历史。 事务/锁 manager添加了新的逻辑实体事务管理器，包含原有的database/table/partition lock manager的概念在里面。原来默认的hive.lock.manager是org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager。新的事务管理器现在也负责管理事务锁了。默认的DummyTxnManager跟原有版本的hive一样：并没有事务，使用hive.lock.manager来为表、分区、数据库创建lock manager。新的DbTxnManager使用hive metastore中的DbLockManager管理所有的锁与事务（系统挂掉也不会碍事儿）。这就是说启用事务特性之后，并没有必要弄什么zookeeper来搞这些了。为了避免客户端挂掉、离开当前事务、或者锁悬停的情况，我们需要锁的拥有者和事务发起者以一定频率发送心跳给metastore。如果心跳超时几次，那么这个锁或者食物就会被终止。 在1.3.0版本中，DbLockManager会持续重试去获取锁。每次获取不到，都会double下一次获取锁的时间间隔，再重试，以支持短时间的查询，也降低metastore压力。假设初始等待时间是100ms，如果重试10次的话，总时间会是91m:42s:300ms。 注意DbTxnManager的lock manager会获取所有表的锁。默认情况下，对于非事务表的insert操作会获取一个排它锁，不允许其他的读写操作。虽然从技术角度来看是正确的，但是违背hive的传统工作方式的。为了向后兼容，提供了hive.txn.strict.locking.mode参数，可以让lock manager在对于非事务表insert操作的时候获取共享锁。这样就保证在读取的时候，不会出现数据被drop的情况。注意，对于事务表，insert操作是总会获取共享锁的，因为这些表在存储层支持MVCC架构，即便有并发写的时候也能够提供对于读的强一致性(快照隔离). 配置客户端 hive.support.concurrency - true hive.enforce.bucketing - true(2.0版本后不需要) hive.exec.dynamic.partition.mode - nonstrict hive.txn.manager - org.apache.hadoop.hive.ql.lockmgr.DbTxnManager 服务端(metastore) hive.compactor.initiator.on - true hive.compactor.worker.threads - 正数就行，看情况 事务相关的新配置参数表属性如果一个表要使用ACID写操作，那么必须在表上设置transactional=true。从0.14.0开始，一旦一个表通过·TBLPROPERTIES (“transactional”=”true”)·定义为一个ACID表之后，就不能在改成非ACID表了，就是说不能执行TBLPROPERTIES (“transactional”=”false”) 这种修改。还有，hive.txn.manager必须设置为org.apache.hadoop.hive.ql.lockmgr.DbTxnManager, 可以再hive-site.xml里设置，也可以在session开始的时候设置。不然insert就会按照老的方式运行，delete操作会被禁止。 如果一个表的owner不想让系统自动决定什么时候compact，那就在表上设置NO_AUTO_COMPACTION。 在表创建之后表属性通过TBLPROPERTIES语句设置。 更多关于压缩的选项可以通过TBLPROPERTIES设置。可以在创建表时设置，也可以执行修改表的时候设置。例如，要覆盖一个压缩任务的MR参数，就可在创建表或者修改表的时候加上compactor.&lt;mr property&gt;=xx。 表级别设置123456789101112CREATE TABLE table_name ( id int, name string)CLUSTERED BY (id) INTO 2 BUCKETS STORED AS ORCTBLPROPERTIES ("transactional"="true", "compactor.mapreduce.map.memory.mb"="2048", -- specify compaction map job properties "compactorthreshold.hive.compactor.delta.num.threshold"="4", -- trigger minor compaction if there are more than 4 delta directories "compactorthreshold.hive.compactor.delta.pct.threshold"="0.5" -- trigger major compaction if the ratio of size of delta files to -- size of base files is greater than 50%); 单次请求时候设置1234ALTER TABLE table_name COMPACT 'minor' WITH OVERWRITE TBLPROPERTIES ("compactor.mapreduce.map.memory.mb"="3072"); -- specify compaction map job propertiesALTER TABLE table_name COMPACT 'major' WITH OVERWRITE TBLPROPERTIES ("tblprops.orc.compress.size"="8192"); -- change any other Hive table properties 参考：https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions https://hortonworks.com/hadoop-tutorial/using-hive-acid-transactions-insert-update-delete-data/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[gradle一次性配置所有的maven仓库]]></title>
    <url>%2F2017%2F04%2F17%2Fgradle%E4%B8%80%E6%AC%A1%E6%80%A7%E9%85%8D%E7%BD%AE%E6%89%80%E6%9C%89%E7%9A%84maven%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[在~/.gradle/init.gradle文件中添加1234567891011121314151617allprojects&#123; repositories &#123; def REPOSITORY_URL = &apos;http://maven.oschina.net/content/groups/public&apos; all &#123; ArtifactRepository repo -&gt; if(repo instanceof MavenArtifactRepository)&#123; def url = repo.url.toString() if (url.startsWith(&apos;https://repo1.maven.org/maven2&apos;) || url.startsWith(&apos;https://jcenter.bintray.com/&apos;)) &#123; project.logger.lifecycle &quot;Repository $&#123;repo.url&#125; replaced by $REPOSITORY_URL.&quot; remove repo &#125; &#125; &#125; maven &#123; url REPOSITORY_URL &#125; &#125;&#125; 参考： https://docs.gradle.org/current/userguide/init_scripts.html https://yrom.net/blog/2015/02/07/change-gradle-maven-repo-url/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linkedin--gobblin]]></title>
    <url>%2F2017%2F04%2F16%2Flinkedin--gobblin%2F</url>
    <content type="text"><![CDATA[这些年来，linkedin的数据设施团队为集成多种数据源到hadoop生态系统中提供了很多解决方案。目前，有15中整合pipeline正在运行，包含重要的数据质量、元数据管理、元数据开发、运维相关的调整等等。 这些挑战与经验促使我们构建了gobblin。gobblin是为从各种数据源【数据库、rest api、 ftp server， 文件等】抽取、转化、加载数据到hadoop上。gobblin包含了日常通用的数据ETL操作任务，如任务调度、任务分区、错误处理、状态管理、数据质量检测、数据发布等等。Gobblin把多种数据源集成进同样的执行框架中，然后在同一个地方管理不同的数据源的元数据。 还有自动扩容、容错、数据质量保证、扩展性高、处理数据模型演化等特性，gobblin很易用、而且可以自我服务，是一个高效的数据集成框架。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari给hbase添加component]]></title>
    <url>%2F2017%2F04%2F05%2Fambari%E7%BB%99hbase%E6%B7%BB%E5%8A%A0component%2F</url>
    <content type="text"><![CDATA[因为原生的ambari并没有原生支持habse的thriftserver在UI端操作。而且业务部门需要使用这个，我们运维的过程中，总是需要去到指定机器启动才行，不太方便，总会遗漏。所以添加到ambari的hbase service里。 ps:按照参考的位置/var/lib/ambari-server/resources/stacks/HDP/2.4/services/HBASE/metainfo.xml里面空空如也。 添加component编辑/var/lib/ambari-server/resources/common-services/HBASE/0.96.0.2.0/metainfo.xml文件，添加对应component123456789101112 &lt;component&gt; &lt;name&gt;HBASE_THRIFT_SERVER&lt;/name&gt; &lt;displayName&gt;ThriftServer&lt;/displayName&gt; &lt;category&gt;MASTER&lt;/category&gt; &lt;cardinality&gt;1+&lt;/cardinality&gt; &lt;versionAdvertised&gt;true&lt;/versionAdvertised&gt; &lt;timelineAppid&gt;HBASE&lt;/timelineAppid&gt; &lt;commandScript&gt; &lt;script&gt;scripts/hbase_thrift_server.py&lt;/script&gt; &lt;scriptType&gt;PYTHON&lt;/scriptType&gt; &lt;/commandScript&gt;&lt;/component&gt; 添加对应脚本/var/lib/ambari-server/resources/common-services/HBASE/0.96.0.2.0/package/scripts/hbase_thrift_server.py:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118import sysfrom resource_management import *from resource_management.libraries.functions.security_commons import build_expectations, \ cached_kinit_executor, get_params_from_filesystem, validate_security_config_properties, \ FILE_TYPE_XMLfrom hbase import hbasefrom hbase_service import hbase_serviceimport upgradefrom ambari_commons import OSCheck, OSConstfrom ambari_commons.os_family_impl import OsFamilyImplclass HbaseThriftServer(Script): def install(self, env): # import params # self.install_packages(env, params.exclude_packages) print "Decommission not yet implemented!" def configure(self, env): # import params # env.set_params(params) # hbase(name='thrift') print "Decommission not yet implemented!" def decommission(self, env): print "Decommission not yet implemented!"@OsFamilyImpl(os_family=OsFamilyImpl.DEFAULT)class HbaseThriftServerDefault(HbaseThriftServer): def get_stack_to_component(self): return &#123;"HDP": "hbase-thrift"&#125; def pre_upgrade_restart(self, env, upgrade_type=None): import params env.set_params(params) upgrade.prestart(env, "hbase-thrift") def post_upgrade_restart(self, env, upgrade_type=None): import params env.set_params(params) upgrade.post_thrift(env) def start(self, env, upgrade_type=None): import params env.set_params(params) self.configure(env) # for security hbase_service( 'thrift', action = 'start' ) def stop(self, env, upgrade_type=None): import params env.set_params(params) hbase_service( 'thrift', action = 'stop' ) def status(self, env): import status_params env.set_params(status_params) pid_file = format("&#123;pid_dir&#125;/hbase-&#123;hbase_user&#125;-thrift.pid") check_process_status(pid_file) def security_status(self, env): import status_params env.set_params(status_params) if status_params.security_enabled: props_value_check = &#123;"hbase.security.authentication" : "kerberos", "hbase.security.authorization": "true"&#125; props_empty_check = ['hbase.thrift.keytab.file', 'hbase.thrift.kerberos.principal'] props_read_check = ['hbase.thrift.keytab.file'] hbase_site_expectations = build_expectations('hbase-site', props_value_check, props_empty_check, props_read_check) hbase_expectations = &#123;&#125; hbase_expectations.update(hbase_site_expectations) security_params = get_params_from_filesystem(status_params.hbase_conf_dir, &#123;'hbase-site.xml': FILE_TYPE_XML&#125;) result_issues = validate_security_config_properties(security_params, hbase_expectations) if not result_issues: # If all validations passed successfully try: # Double check the dict before calling execute if ( 'hbase-site' not in security_params or 'hbase.thrift.keytab.file' not in security_params['hbase-site'] or 'hbase.thrift.kerberos.principal' not in security_params['hbase-site']): self.put_structured_out(&#123;"securityState": "UNSECURED"&#125;) self.put_structured_out( &#123;"securityIssuesFound": "Keytab file or principal are not set property."&#125;) return cached_kinit_executor(status_params.kinit_path_local, status_params.hbase_user, security_params['hbase-site']['hbase.thrift.keytab.file'], security_params['hbase-site']['hbase.thrift.kerberos.principal'], status_params.hostname, status_params.tmp_dir) self.put_structured_out(&#123;"securityState": "SECURED_KERBEROS"&#125;) except Exception as e: self.put_structured_out(&#123;"securityState": "ERROR"&#125;) self.put_structured_out(&#123;"securityStateErrorInfo": str(e)&#125;) else: issues = [] for cf in result_issues: issues.append("Configuration file %s did not pass the validation. Reason: %s" % (cf, result_issues[cf])) self.put_structured_out(&#123;"securityIssuesFound": ". ".join(issues)&#125;) self.put_structured_out(&#123;"securityState": "UNSECURED"&#125;) else: self.put_structured_out(&#123;"securityState": "UNSECURED"&#125;)if __name__ == "__main__": HbaseThriftServer().execute() 其实是copy的regionserver的脚本，因为install什么的都不用，所以就全设置为print了。 重启ambari-server添加hbase服务插曲install的时候出现错误，启动的时候也会这样。。。看来是应该先注册一下自己的名字：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263642017-04-05 09:32:30,375 - Could not determine HDP version for component hbase-thrift by calling &apos;/usr/bin/hdp-select status hbase-thrift &gt; /tmp/tmpa6nhtU&apos;. Return Code: 1, Output: ERROR: Invalid package - hbase-thriftPackages: accumulo-client accumulo-gc accumulo-master accumulo-monitor accumulo-tablet accumulo-tracer atlas-server falcon-client falcon-server flume-server hadoop-client hadoop-hdfs-datanode hadoop-hdfs-journalnode hadoop-hdfs-namenode hadoop-hdfs-nfs3 hadoop-hdfs-portmap hadoop-hdfs-secondarynamenode hadoop-httpfs hadoop-mapreduce-historyserver hadoop-yarn-nodemanager hadoop-yarn-resourcemanager hadoop-yarn-timelineserver hbase-client hbase-master hbase-regionserver hive-metastore hive-server2 hive-webhcat kafka-broker knox-server livy-server mahout-client oozie-client oozie-server phoenix-client phoenix-server ranger-admin ranger-kms ranger-usersync slider-client spark-client spark-historyserver spark-thriftserver sqoop-client sqoop-server storm-client storm-nimbus storm-slider-client storm-supervisor zeppelin-server zookeeper-client zookeeper-serverAliases: accumulo-server all client hadoop-hdfs-server hadoop-mapreduce-server hadoop-yarn-server hive-server. 找了老半天，问题竟然在shell里，服了。在/usr/bin/hdp-select里的leaves字典里添加1&quot;hbase-thrift&quot;: &quot;hbase&quot;, 然后还要创建一个目录才行1ln -s /usr/hdp/2.4.2.0-258/hbase-regionserver /usr/hdp/current/hbase-thrift 参考： https://cwiki.apache.org/confluence/display/AMBARI/Defining+a+Custom+Stack+and+Services]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs-ACL开启后出现问题]]></title>
    <url>%2F2017%2F04%2F01%2Fhdfs-ACL%E5%BC%80%E5%90%AF%E5%90%8E%E5%87%BA%E7%8E%B0%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[已经用测试环境重现了此问题。 启用namenode的acl，在hdfs-site.xml中添加 1234&lt;property&gt; &lt;name&gt;dfs.namenode.acls.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 使用root用户创建初始文件夹, 并给以同生产环境的ACL设置： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950root@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -mkdir -p /tmp/ods_tinyv_outer.db/channel=wbroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -getfacl /tmp/ods_tinyv_outer.db/channel=wb# file: /tmp/ods_tinyv_outer.db/channel=wb# owner: root# group: supergroupuser::rwxgroup::r-xother::r-xroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -setfacl -R -m default:user::rwx /tmp/ods_tinyv_outer.dbroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -setfacl -R -m default:group:wil:rwx /tmp/ods_tinyv_outer.dbroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -getfacl /tmp/ods_tinyv_outer.db/channel=wb# file: /tmp/ods_tinyv_outer.db/channel=wb# owner: root# group: supergroupuser::rwxgroup::r-xother::r-xdefault:user::rwxdefault:group::r-xdefault:group:wil:rwxdefault:mask::rwxdefault:other::r-xroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -chown -R wil:wil /tmp/ods_tinyv_outer.db/channel=wbroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -getfacl /tmp/ods_tinyv_outer.db/channel=wb# file: /tmp/ods_tinyv_outer.db/channel=wb# owner: wil# group: wiluser::rwxgroup::r-xother::r-xdefault:user::rwxdefault:group::r-xdefault:group:wil:rwxdefault:mask::rwxdefault:other::r-xroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -chmod 770 /tmp/ods_tinyv_outer.db/channel=wbroot@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -getfacl /tmp/ods_tinyv_outer.db/channel=wb# file: /tmp/ods_tinyv_outer.db/channel=wb# owner: wil# group: wiluser::rwxgroup::rwxother::---default:user::rwxdefault:group::r-xdefault:group:wil:rwxdefault:mask::rwxdefault:other::r-x 重现问题 1234wil@will-vm:/opt/hadoop-2.7.1$ ./bin/hdfs dfs -rmr /tmp/ods_tinyv_outer.db/channel=wbrmr: DEPRECATED: Please use &apos;rm -r&apos; instead.17/04/02 23:01:41 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.rmr: Permission denied: user=wil, access=WRITE, inode=&quot;/tmp/ods_tinyv_outer.db/channel=wb&quot;:root:supergroup:drwxr-xr-x 这个文件明明是属于wil用户wil用户组的，并且从acl来看是有权限的。然而实际执行的时候，却提示没权，而且列出来的权限竟然是:root:supergroup:drwxr-xr-x，这个应该是刚刚创建时的权限。 解决1root@will-vm:/opt/hadoop-2.7.1# ./bin/hdfs dfs -chown wil:wil /tmp/ods_tinyv_outer.db]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hue metastore引出的HDFS权限问题]]></title>
    <url>%2F2017%2F03%2F31%2Fhue-metastore%E5%BC%95%E5%87%BA%E7%9A%84HDFS%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[需求支持的某个事业部提出一个需求：在hue里开放修改表或者字段的权限。 在hue里就给她一个人开了相应的权限，账户名是wcq。因为不知道wcq的密码，所以使用的mm的测试账户登陆的，在hue里给了两个人相同的权限组：lv和metastoremanger。 问题然而奇怪的现象就发生了：wcq在hue的metastore管理界面修改表的字段之后，看网络请求是成功了的。然而，刷新页面以后就没有了。mm这个账户确都可以使用。 追查经查验，mm这个用户在HDFS中除了是lv组的用户，而且还是hdfs组的用户，而hdfs组是超级用户组，拥有任何权限。所以这两个用户所在的组并不是对等的，只是在hue里对等而已。 理论上如果hue直接操作hive元数据库的话是不会有此问题的。 查一下网络请求有什么异常？发送的request包摘要：12345Request URL:http://10.1.5.83/metastore/table/dim_tinyv/data_dict/alter_columnRequest Method:POSTcolumn:idcomment:ss 返回的response内容：123456789&#123; "status": 0, "message": "", "data": &#123; "comment": "", "type": "int", "name": "id" &#125;&#125; hue源码确认逻辑以url为线索查看hue相关源码apps/metastore/src/metastore/urls.py中找到alter_table，在apps/metastore/src/metastore/views.py里： 123456789101112131415161718192021222324252627282930313233check_has_write_access_permission@require_http_methods(["POST"])def alter_column(request, database, table): db = dbms.get(request.user) response = &#123;'status': -1, 'message': ''&#125; try: column = request.POST.get('column', None) if column is None: raise PopupException(_('alter_column requires a column parameter')) column_obj = db.get_column(database, table, column) if column_obj: new_column_name = request.POST.get('new_column_name', column_obj.name) new_column_type = request.POST.get('new_column_type', column_obj.type) comment = request.POST.get('comment', None) partition_spec = request.POST.get('partition_spec', None) column_obj = db.alter_column(database, table, column, new_column_name, new_column_type, comment=comment, partition_spec=partition_spec) response['status'] = 0 response['data'] = &#123; 'name': column_obj.name, 'type': column_obj.type, 'comment': column_obj.comment &#125; else: raise PopupException(_('Column `%s`.`%s` `%s` not found') % (database, table, column)) except Exception, ex: response['status'] = 1 response['message'] = _("Failed to alter column `%s`.`%s` `%s`: %s") % (database, table, column, str(ex)) return JsonResponse(response) 看到是使用了dbms里返回的数据库执行的语句，找到apps/beeswax/src/beeswax/server/dbms.py，对应代码：1234567891011121314151617181920212223242526def get(user, query_server=None): global DBMS_CACHE global DBMS_CACHE_LOCK if query_server is None: query_server = get_query_server_config() DBMS_CACHE_LOCK.acquire() try: DBMS_CACHE.setdefault(user.username, &#123;&#125;) if query_server['server_name'] not in DBMS_CACHE[user.username]: # Avoid circular dependency from beeswax.server.hive_server2_lib import HiveServerClientCompatible if query_server['server_name'] == 'impala': from impala.dbms import ImpalaDbms from impala.server import ImpalaServerClient DBMS_CACHE[user.username][query_server['server_name']] = ImpalaDbms(HiveServerClientCompatible(ImpalaServerClient(query_server, user)), QueryHistory.SERVER_TYPE[1][0]) else: from beeswax.server.hive_server2_lib import HiveServerClient DBMS_CACHE[user.username][query_server['server_name']] = HiveServer2Dbms(HiveServerClientCompatible(HiveServerClient(query_server, user)), QueryHistory.SERVER_TYPE[1][0]) return DBMS_CACHE[user.username][query_server['server_name']] finally: DBMS_CACHE_LOCK.release() 可以看到是使用hiveserver2建立连接，搞定的。并不是最开始猜测的直接操作hive的元数据库。 修改数据字段的方法也在这里：1234567891011121314151617181920212223242526def alter_column(self, database, table_name, column_name, new_column_name, column_type, comment=None, partition_spec=None, cascade=False): hql = 'ALTER TABLE `%s`.`%s`' % (database, table_name) if partition_spec: hql += ' PARTITION (%s)' % partition_spec hql += ' CHANGE COLUMN `%s` `%s` %s' % (column_name, new_column_name, column_type.upper()) if comment: hql += " COMMENT '%s'" % comment if cascade: hql += ' CASCADE' timeout = SERVER_CONN_TIMEOUT.get() query = hql_query(hql) handle = self.execute_and_wait(query, timeout_sec=timeout) if handle: self.close(handle) else: msg = _("Failed to execute alter column statement: %s") % hql raise QueryServerException(msg) return self.get_column(database, table_name, new_column_name) 执行相应逻辑这样的话，我们直接在命令行里，切换到wcq用户执行hive命令行，应该是一样的。12hive&gt; alter table default.batting CHANGE COLUMN dtdontquery dtdontquery string COMMENT &apos;ss&apos;;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. java.security.AccessControlException: Permission denied: user=wcq, access=WRITE, inode=&quot;/apps/hive/warehouse/batting&quot;:root:jlc:drwxr-xr-x 此阶段结论：hue通过hiveserver执行hql语句，而且hive本身修改表元数据的时候也会验证是否有对HDFS目录有写权限。 解决那么应该怎样解决呢？ 当前的目录权限都是给的：drwxr-x---，也就是只有owner有写权限, 指定组的用户有读权限，匿名用户没有任何权限。 这样授权的理由在于，以事业部为组，所有同一个事业部的人都可读，但是只能通过我们的数据平台用户来进行写操作【当前是root】。 既不想违背这个初衷，又要满足新的需求，再不进行任何自主研发的前提下，只能启用HDFS的ACL，才能添加额外的权限。 hue中也支持*ACL修改，而且执行遍历目录修改. 步骤如下： 遗留对官网说的mask不太理解。字面意思好像是用来过滤所有用户、组、匿名组的权限的。 以官网例子来看：123456user::rw- user:bruce:rwx #effective:r-- group::r-x #effective:r-- group:sales:rwx #effective:r-- mask::r-- other::r-- 因为上面指定的mask是只读，所以即便其他指定了额外的权限也是白费的，生效的只有只读。所以这样理解，mask就是这个文件可以有的所有权限的总和。如果不单独设置mask的话，其默认值也是这样算出来的。 参考 https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#The_Super-User https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html#setfacl]]></content>
      <tags>
        <tag>hdfs</tag>
        <tag>hue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDFS权限问题]]></title>
    <url>%2F2017%2F03%2F31%2FHDFS%E6%9D%83%E9%99%90%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[支持的某个事业部提出一个需求：在hue里开放修改表或者字段的权限。 在hue里就给她一个人开了相应的权限，账户名是wcq。因为不知道wcq的密码，所以使用的mm的测试账户登陆的，在hue里给了两个人相同的权限组：lv和metastoremanger。 然而奇怪的现象就发生了：wcq在hue的metastore管理界面修改表的字段之后，看网络请求是成功了的。然而，刷新页面以后就没有了。mm这个账户确都可以使用。 经查验，mm这个用户在HDFS中除了是lv组的用户，而且还是hdfs组的用户，而hdfs组是超级用户组，拥有任何权限。所以这两个用户所在的组并不是对等的，只是在hue里对等而已。 理论上如果hue直接操作hive元数据库的话是不会有此问题的。 查一下网络请求有什么异常？发送的request包摘要：12345Request URL:http://10.1.5.83/metastore/table/dim_tinyv/data_dict/alter_columnRequest Method:POSTcolumn:idcomment:ss 返回的response内容：123456789&#123; "status": 0, "message": "", "data": &#123; "comment": "", "type": "int", "name": "id" &#125;&#125; 以url为线索查看hue相关源码apps/metastore/src/metastore/urls.py中找到alter_table，在apps/metastore/src/metastore/views.py里： 123456789101112131415161718192021222324252627282930313233check_has_write_access_permission@require_http_methods(["POST"])def alter_column(request, database, table): db = dbms.get(request.user) response = &#123;'status': -1, 'message': ''&#125; try: column = request.POST.get('column', None) if column is None: raise PopupException(_('alter_column requires a column parameter')) column_obj = db.get_column(database, table, column) if column_obj: new_column_name = request.POST.get('new_column_name', column_obj.name) new_column_type = request.POST.get('new_column_type', column_obj.type) comment = request.POST.get('comment', None) partition_spec = request.POST.get('partition_spec', None) column_obj = db.alter_column(database, table, column, new_column_name, new_column_type, comment=comment, partition_spec=partition_spec) response['status'] = 0 response['data'] = &#123; 'name': column_obj.name, 'type': column_obj.type, 'comment': column_obj.comment &#125; else: raise PopupException(_('Column `%s`.`%s` `%s` not found') % (database, table, column)) except Exception, ex: response['status'] = 1 response['message'] = _("Failed to alter column `%s`.`%s` `%s`: %s") % (database, table, column, str(ex)) return JsonResponse(response) 看到是使用了dbms里返回的数据库执行的语句，找到apps/beeswax/src/beeswax/server/dbms.py，对应代码：1234567891011121314151617181920212223242526def get(user, query_server=None): global DBMS_CACHE global DBMS_CACHE_LOCK if query_server is None: query_server = get_query_server_config() DBMS_CACHE_LOCK.acquire() try: DBMS_CACHE.setdefault(user.username, &#123;&#125;) if query_server['server_name'] not in DBMS_CACHE[user.username]: # Avoid circular dependency from beeswax.server.hive_server2_lib import HiveServerClientCompatible if query_server['server_name'] == 'impala': from impala.dbms import ImpalaDbms from impala.server import ImpalaServerClient DBMS_CACHE[user.username][query_server['server_name']] = ImpalaDbms(HiveServerClientCompatible(ImpalaServerClient(query_server, user)), QueryHistory.SERVER_TYPE[1][0]) else: from beeswax.server.hive_server2_lib import HiveServerClient DBMS_CACHE[user.username][query_server['server_name']] = HiveServer2Dbms(HiveServerClientCompatible(HiveServerClient(query_server, user)), QueryHistory.SERVER_TYPE[1][0]) return DBMS_CACHE[user.username][query_server['server_name']] finally: DBMS_CACHE_LOCK.release() 可以看到是使用hiveserver2建立连接，搞定的。并不是最开始猜测的直接操作hive的元数据库。 修改数据字段的方法也在这里：1234567891011121314151617181920212223242526def alter_column(self, database, table_name, column_name, new_column_name, column_type, comment=None, partition_spec=None, cascade=False): hql = 'ALTER TABLE `%s`.`%s`' % (database, table_name) if partition_spec: hql += ' PARTITION (%s)' % partition_spec hql += ' CHANGE COLUMN `%s` `%s` %s' % (column_name, new_column_name, column_type.upper()) if comment: hql += " COMMENT '%s'" % comment if cascade: hql += ' CASCADE' timeout = SERVER_CONN_TIMEOUT.get() query = hql_query(hql) handle = self.execute_and_wait(query, timeout_sec=timeout) if handle: self.close(handle) else: msg = _("Failed to execute alter column statement: %s") % hql raise QueryServerException(msg) return self.get_column(database, table_name, new_column_name) 这样的话，我们直接在命令行里，切换到wcq用户执行hive命令行，应该是一样的。12hive&gt; alter table default.batting CHANGE COLUMN dtdontquery dtdontquery string COMMENT &apos;ss&apos;;FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Unable to alter table. java.security.AccessControlException: Permission denied: user=wcq, access=WRITE, inode=&quot;/apps/hive/warehouse/batting&quot;:root:jlc:drwxr-xr-x 此阶段结论：hue通过hiveserver执行hql语句，而且hive本身修改表元数据的时候也会验证是否有对HDFS目录有写权限。 那么应该怎样解决呢？ 当前的目录权限都是给的：drwxr-x---，也就是只有owner有写权限, 指定组的用户有读权限，匿名用户没有任何权限。 这样授权的理由在于，以事业部为组，所有同一个事业部的人都可读，但是只能通过我们的数据平台用户来进行写操作【当前是root】。 既不想违背这个初衷，又要满足新的需求，再不进行任何自主研发的前提下，只能启用HDFS的ACL，才能添加额外的权限。 hue中也支持*ACL修改，而且执行遍历目录修改. 问题=对官网说的mask不太理解。字面意思好像是用来过滤所有用户、组、匿名组的权限的。 以官网例子来看：123456user::rw- user:bruce:rwx #effective:r-- group::r-x #effective:r-- group:sales:rwx #effective:r-- mask::r-- other::r-- 因为上面指定的mask是只读，所以即便其他指定了额外的权限也是白费的，生效的只有只读。所以这样理解，mask就是这个文件可以有的所有权限的总和。如果不单独设置mask的话，其默认值也是这样算出来的。 参考= https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsPermissionsGuide.html#The_Super-User https://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-common/FileSystemShell.html#setfacl]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari脚本更新配置]]></title>
    <url>%2F2017%2F03%2F31%2Fambari%E8%84%9A%E6%9C%AC%E6%9B%B4%E6%96%B0%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[类似于hdfs_log_dir_prefix这种配置项一旦在服务初始化完成之后，就不能在UI界面上进行修改了。具体ambari为什么要这样限制，个人表示无从知晓。 通过文件/var/lib/ambari-agent/cache/cluster_configuration/configurations.json，我们可以直接看到所有的环境相关配置。暂时猜测可能是为了统一一次性生成此文件，而所有UI的操作都是修改的各自的对应的xml配置文件。 官方提供了两种方式修改对于此json文件相关的配置。一个是API，另一个就是通过/var/lib/ambari-server/resources/scripts/config.sh脚本。 个人比较倾向于脚本，更不易出错。查看config.sh的使用方法：1234567891011121314151617181920212223242526272829303132333435/var/lib/ambari-server/resources/scripts/configs.py --helpUsage: configs.py [options]Options: -h, --help show this help message and exit -t PORT, --port=PORT Optional port number for Ambari server. Default is &apos;8080&apos;. Provide empty string to not use port. -s PROTOCOL, --protocol=PROTOCOL Optional support of SSL. Default protocol is &apos;http&apos; -a ACTION, --action=ACTION Script action: &lt;get&gt;, &lt;set&gt;, &lt;delete&gt; -l HOST, --host=HOST Server external host name -n CLUSTER, --cluster=CLUSTER Name given to cluster. Ex: &apos;c1&apos; -c CONFIG_TYPE, --config-type=CONFIG_TYPE One of the various configuration types in Ambari. Ex: core-site, hdfs-site, mapred-queue-acls, etc. To specify credentials please use &quot;-e&quot; OR &quot;-u&quot; and &quot;-p&apos;&quot;: -u USER, --user=USER Optional user ID to use for authentication. Default is &apos;admin&apos; -p PASSWORD, --password=PASSWORD Optional password to use for authentication. Default is &apos;admin&apos; -e CREDENTIALS_FILE, --credentials-file=CREDENTIALS_FILE Optional file with user credentials separated by new line. To specify property(s) please use &quot;-f&quot; OR &quot;-k&quot; and &quot;-v&apos;&quot;: -f FILE, --file=FILE File where entire configurations are saved to, or read from. Supported extensions (.xml, .json&gt;) -k KEY, --key=KEY Key that has to be set or deleted. Not necessary for &apos;get&apos; action. -v VALUE, --value=VALUE Optional value to be set. Not necessary for &apos;get&apos; or &apos;delete&apos; actions. 首先，我们可以先用api查看一下配置项的分布：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145[hdfs@data-test01 scripts]$ curl -k -s -u admin:admin &quot;http://10.1.5.79:8080/api/v1/clusters/willcluster?fields=Clusters/desired_configs&quot;&#123; &quot;href&quot; : &quot;http://10.1.5.79:8080/api/v1/clusters/willcluster?fields=Clusters/desired_configs&quot;, &quot;Clusters&quot; : &#123; &quot;cluster_name&quot; : &quot;willcluster&quot;, &quot;version&quot; : &quot;HDP-2.4&quot;, &quot;desired_configs&quot; : &#123; &quot;capacity-scheduler&quot; : &#123; &quot;tag&quot; : &quot;version1490254923891&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;cluster-env&quot; : &#123; &quot;tag&quot; : &quot;version1490254925199&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;core-site&quot; : &#123; &quot;tag&quot; : &quot;version1490345767824&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 5 &#125;, &quot;hadoop-env&quot; : &#123; &quot;tag&quot; : &quot;version1490254923436&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;hadoop-policy&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;hdfs-log4j&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;hdfs-site&quot; : &#123; &quot;tag&quot; : &quot;version1490344980013&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 4 &#125;, &quot;kerberos-env&quot; : &#123; &quot;tag&quot; : &quot;version1490254524258&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;krb5-conf&quot; : &#123; &quot;tag&quot; : &quot;version1490254524258&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;mapred-env&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;mapred-site&quot; : &#123; &quot;tag&quot; : &quot;version1490254925210&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;ranger-hdfs-audit&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-hdfs-plugin-properties&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-hdfs-policymgr-ssl&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-hdfs-security&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-yarn-audit&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-yarn-plugin-properties&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-yarn-policymgr-ssl&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ranger-yarn-security&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ssl-client&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;ssl-server&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;yarn-env&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;yarn-log4j&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;yarn-site&quot; : &#123; &quot;tag&quot; : &quot;version1490254924330&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 6 &#125;, &quot;zoo.cfg&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125;, &quot;zookeeper-env&quot; : &#123; &quot;tag&quot; : &quot;version1490254924765&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 2 &#125;, &quot;zookeeper-log4j&quot; : &#123; &quot;tag&quot; : &quot;version1&quot;, &quot;user&quot; : &quot;admin&quot;, &quot;version&quot; : 1 &#125; &#125; &#125;&#125; 上面的key就对应于config.sh set的一些可选项,也就是CONFIG_TYPE。 下面列出一些常用的样例。按照amabri的惯例，每个log目录都应该是由对应的user创建并修改的。这里我们的所有机器都只有/server目录，下面并没有添加或配置log目录，试一下能不能像初始化环境的时候自动配置对应权限。一试成功了。真好，哈哈1234./configs.sh set 10.1.5.79 willcluster hadoop-env hdfs_log_dir_prefix &quot;/server/log/hadoop&quot;./configs.sh set 10.1.5.79 willcluster mapred-env mapred_log_dir_prefix &quot;/server/log/hadoop-mapred&quot;./configs.sh set 10.1.5.79 willcluster yarn-env yarn_log_dir_prefix &quot;/server/log/yarn&quot;./configs.sh set 10.1.5.79 willcluster zookeeper-env zk_log_dir &quot;/server/log/zookeeper&quot; 刷新amabri的UI的配置页就可以看到，对应配置已经修改成功，等待重启生效了。 然后tail一下各个服务对应的日志，都没有问题。 参考： https://cwiki.apache.org/confluence/display/AMBARI/Modify+configurations]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[centos7-安装k8s]]></title>
    <url>%2F2017%2F03%2F30%2Fcentos7-%E5%AE%89%E8%A3%85k8s%2F</url>
    <content type="text"><![CDATA[代理设置=先弄一下代理，弄一下hosts文件，参考： https://github.com/racaljk/hosts 安装=1234567891011121314cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOFsetenforce 0yum install -y docker kubelet kubeadm kubectl kubernetes-cnisystemctl enable docker &amp;&amp; systemctl start dockersystemctl enable kubelet &amp;&amp; systemctl start kubelet 要是安装过docker了，就不用再弄了，最好保持一致。 初始化=controller组件运行在master上，它包含etcd和API server。 12345678910111213141516171819root@controller kubernetes]# kubeadm init[kubeadm] WARNING: kubeadm is in beta, please do not use it for production clusters.[init] Using Kubernetes version: v1.6.0[init] Using Authorization mode: RBAC[preflight] Running pre-flight checks[preflight] WARNING: docker version is greater than the most recently validated version. Docker version: 17.03.1-ce. Max validated version: 1.12[certificates] Generated CA certificate and key.[certificates] Generated API server certificate and key.[certificates] API Server serving cert is signed for DNS names [controller kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.1.5.129][certificates] Generated API server kubelet client certificate and key.[certificates] Generated service account token signing key and public key.[certificates] Generated front-proxy CA certificate and key.[certificates] Generated front-proxy client certificate and key.[certificates] Valid certificates and keys now exist in &quot;/etc/kubernetes/pki&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/kubelet.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/controller-manager.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/scheduler.conf&quot;[kubeconfig] Wrote KubeConfig file to disk: &quot;/etc/kubernetes/admin.conf&quot;[apiclient] Created API client, waiting for the control plane to become ready 这个命令会自动探查网络接口，告诉默认的网关master所在的地址。要是想指定额外的master地址，使用参数–api-advertise-addresses 。 上面的命令也会自动下载所有的必须的镜像：1234567gcr.io/google_containers/kube-proxy-amd64 v1.5.3 gcr.io/google_containers/kube-controller-manager-amd64 v1.5.3 gcr.io/google_containers/kube-scheduler-amd64 v1.5.3 gcr.io/google_containers/kube-apiserver-amd64 v1.5.3 gcr.io/google_containers/etcd-amd64 3.0.14-kubeadm gcr.io/google_containers/kube-discovery-amd64 1.0 gcr.io/google_containers/pause-amd64 3.0 中间提示我/proc/sys/net/bridge/bridge-nf-call-iptables的内容不是1，看了一下是0，修改了之后可以了。 注意 kubeadm init 只能在你确定的master上执行一次。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari---迁移某master-component]]></title>
    <url>%2F2017%2F03%2F27%2Fambari---%E8%BF%81%E7%A7%BB%E6%9F%90master-component%2F</url>
    <content type="text"><![CDATA[以AMBARI_METRICS服务的component：METRICS_COLLECTOR为例。 停止对应service 12curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X PUT -d &apos;&#123;&quot;RequestInfo&quot;:&#123;&quot;context&quot;:&quot;Stop Service&quot;&#125;,&quot;Body&quot;:&#123;&quot;ServiceInfo&quot;:&#123;&quot;state&quot;:&quot;INSTALLED&quot;&#125;&#125;&#125;&apos; http://namenode01.will.com:8080/api/v1/clusters/datacenter/services/AMBARI_METRICS/` 确认目前node上的component已经停止 1curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X PUT -d &apos;&#123;&quot;RequestInfo&quot;:&#123;&quot;context&quot;:&quot;Stop Component&quot;&#125;,&quot;Body&quot;:&#123;&quot;HostRoles&quot;:&#123;&quot;state&quot;:&quot;INSTALLED&quot;&#125;&#125;&#125;&apos; http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts/datanode04.will.com/host_components/METRICS_COLLECTOR 删除目前servcie里的component 1curl -i -H &quot;X-Requested-By:ambari&quot; -u admin:admin -X DELETE http://namenode01.will.com:8080/api/v1/clusters/datacenter/services/AMBARI_METRICS/components/METRICS_COLLECTOR 在service的元数据里再加上这个component 1curl -i -H &quot;X-Requested-By:ambari&quot; -u admin:admin -X POST http://namenode01.will.com:8080/api/v1/clusters/datacenter/services/AMBARI_METRICS/components/METRICS_COLLECTOR 安装component到指定node上 1curl -i -u admin:admin -H &quot;X-Requested-By:ambari&quot; -i -X POST -d &apos;&#123;&quot;host_components&quot; : [&#123;&quot;HostRoles&quot;:&#123;&quot;component_name&quot;:&quot;METRICS_COLLECTOR&quot;&#125;&#125;] &#125;&apos; http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts?Hosts/host_name=datanode17.will.com web端，到新的node上的component点击re-install，之后重启整个service。完成 ps:如果没有删除干净的话，就到指定node上删除component1curl -u admin:admin -H &quot;X-Requested-By: ambari&quot; -X DELETE http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts/datanode04.will.com/host_components/METRICS_COLLECTOR 参考： https://community.hortonworks.com/questions/82689/how-to-add-component-spark-jobhistoryserver-to-spa.html#answer-82695 https://cwiki.apache.org/confluence/display/AMBARI/Using+APIs+to+delete+a+service+or+all+host+components+on+a+host]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables手记]]></title>
    <url>%2F2017%2F03%2F24%2Fiptables%E6%89%8B%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[先看一下网络流程图，以及所有的控制点 查看filter表里的数据123456789[root@schedule ~]#iptables -t filter -LChain INPUT (policy ACCEPT)target prot opt source destinationChain FORWARD (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destination 添加一条规则， 在OUTPUT上记录所有tcp的日志，默认是添加在syslog里的。1234567891011[root@data-test03 hadoop-client]# iptables -t filter -A OUTPUT -p tcp -j LOG[root@data-test03 hadoop-client]# iptables -t filter -LChain INPUT (policy ACCEPT)target prot opt source destinationChain FORWARD (policy ACCEPT)target prot opt source destinationChain OUTPUT (policy ACCEPT)target prot opt source destinationLOG tcp -- anywhere anywhere LOG level warning 找一下syslog里是否生效：1234567891011[root@data-test03 hadoop-client]# tail -n 10 /var/log/messagesMar 23 18:30:35 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=32153 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=32154 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.80 LEN=72 TOS=0x00 PREC=0x00 TTL=64 ID=39647 DF PROTO=TCP SPT=56998 DPT=2888 WINDOW=501 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=228 TOS=0x00 PREC=0x00 TTL=64 ID=42802 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=14446 DF PROTO=TCP SPT=8025 DPT=36920 WINDOW=384 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=42803 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=32155 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.80 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=13946 DF PROTO=TCP SPT=8025 DPT=38421 WINDOW=499 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.79 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=15862 DF PROTO=TCP SPT=8025 DPT=37458 WINDOW=499 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=104 TOS=0x10 PREC=0x00 TTL=64 ID=32156 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 为日志添加前缀1iptables -t filter -A OUTPUT -p tcp -j LOG --log-prefix will 再看log123456[root@data-test03 hadoop-client]# tail -n 5 /var/log/messagesMar 23 18:33:58 data-test03 kernel: willIN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.80 LEN=72 TOS=0x00 PREC=0x00 TTL=64 ID=39849 DF PROTO=TCP SPT=56998 DPT=2888 WINDOW=501 RES=0x00 ACK PSH URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=228 TOS=0x00 PREC=0x00 TTL=64 ID=43206 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK PSH URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=14648 DF PROTO=TCP SPT=8025 DPT=36920 WINDOW=384 RES=0x00 ACK PSH URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43207 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=34907 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 命令规范：1iptables -t 表 操作 5个控制点之一 匹配规则 -j 动作 动作的选项 拒绝79对于8020端口的请求1iptables -t filter -A INPUT -p tcp --dport 8020 -s 10.1.5.79 -j REJECT 匹配包含：基本匹配、隐世匹配、等。 拒绝79所有请求, 第二条是给一个拒绝原因，顺序执行，如果一起都有的话，只执行第一条动作12iptables -t filter -A INPUT -s 10.1.5.79 -j REJECTiptables -t filter -A INPUT -s 10.1.5.79 -j REJECT --reject-with tcp-reset 这样就连ping都ping不通了。 网段知识补充 参考： http://www.cnblogs.com/yi-meng/p/3213925.html http://www.dabu.info/iptables-based-tutorial-grammar-rules.html http://v.youku.com/v_show/id_XNzIxOTAxODky.html?from=s1.8-1-1.2&amp;spm=a2h0k.8191407.0.0]]></content>
      <tags>
        <tag>iptables</tag>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iptables]]></title>
    <url>%2F2017%2F03%2F23%2Fiptables%2F</url>
    <content type="text"><![CDATA[查看filter表里的数据123456789[root@schedule ~]#iptables -t filter -LChain INPUT (policy ACCEPT)target prot opt source destination Chain FORWARD (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination 添加一条规则， 在OUTPUT上记录所有tcp的日志，默认是添加在syslog里的。1234567891011[root@data-test03 hadoop-client]# iptables -t filter -A OUTPUT -p tcp -j LOG[root@data-test03 hadoop-client]# iptables -t filter -LChain INPUT (policy ACCEPT)target prot opt source destination Chain FORWARD (policy ACCEPT)target prot opt source destination Chain OUTPUT (policy ACCEPT)target prot opt source destination LOG tcp -- anywhere anywhere LOG level warning 找一下syslog里是否生效：1234567891011[root@data-test03 hadoop-client]# tail -n 10 /var/log/messagesMar 23 18:30:35 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=32153 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=32154 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.80 LEN=72 TOS=0x00 PREC=0x00 TTL=64 ID=39647 DF PROTO=TCP SPT=56998 DPT=2888 WINDOW=501 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=228 TOS=0x00 PREC=0x00 TTL=64 ID=42802 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=14446 DF PROTO=TCP SPT=8025 DPT=36920 WINDOW=384 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=42803 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=32155 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.80 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=13946 DF PROTO=TCP SPT=8025 DPT=38421 WINDOW=499 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.79 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=15862 DF PROTO=TCP SPT=8025 DPT=37458 WINDOW=499 RES=0x00 ACK PSH URGP=0 Mar 23 18:30:36 data-test03 kernel: IN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=104 TOS=0x10 PREC=0x00 TTL=64 ID=32156 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 为日志添加前缀1iptables -t filter -A OUTPUT -p tcp -j LOG --log-prefix will 再看log123456[root@data-test03 hadoop-client]# tail -n 5 /var/log/messagesMar 23 18:33:58 data-test03 kernel: willIN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.80 LEN=72 TOS=0x00 PREC=0x00 TTL=64 ID=39849 DF PROTO=TCP SPT=56998 DPT=2888 WINDOW=501 RES=0x00 ACK PSH URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=228 TOS=0x00 PREC=0x00 TTL=64 ID=43206 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK PSH URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=93 TOS=0x00 PREC=0x00 TTL=64 ID=14648 DF PROTO=TCP SPT=8025 DPT=36920 WINDOW=384 RES=0x00 ACK PSH URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=lo SRC=10.1.5.81 DST=10.1.5.81 LEN=52 TOS=0x00 PREC=0x00 TTL=64 ID=43207 DF PROTO=TCP SPT=36920 DPT=8025 WINDOW=265 RES=0x00 ACK URGP=0 Mar 23 18:33:58 data-test03 kernel: willIN= OUT=eth0 SRC=10.1.5.81 DST=10.1.5.237 LEN=136 TOS=0x10 PREC=0x00 TTL=64 ID=34907 DF PROTO=TCP SPT=22 DPT=60206 WINDOW=2233 RES=0x00 ACK PSH URGP=0 命令规范：1iptables -t 表 操作 5个控制点之一 匹配规则 -j 动作 动作的选项 拒绝79对于8020端口的请求1iptables -t filter -A INPUT -p tcp --dport 8020 -s 10.1.5.79 -j REJECT 匹配包含：基本匹配、隐世匹配、等。 拒绝79所有请求, 第二条是给一个拒绝原因，顺序执行，如果一起都有的话，优先执行第一条动作12iptables -t filter -A INPUT -s 10.1.5.79 -j REJECTiptables -t filter -A INPUT -s 10.1.5.79 -j REJECT --reject-with tcp-reset 这样就连ping都ping不通了。 插入到第三行：1iptables -t filter -I INPUT 3 -p tcp -s 10.1.5.237 -j ACCEPT 删除第3行的规则：1iptables -t filter -D INPUT 3 网段知识补充 参考： http://www.cnblogs.com/yi-meng/p/3213925.html http://www.dabu.info/iptables-based-tutorial-grammar-rules.html http://v.youku.com/v_show/id_XNzIxOTAxODky.html?from=s1.8-1-1.2&amp;spm=a2h0k.8191407.0.0]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari集群添加kerberos认证]]></title>
    <url>%2F2017%2F03%2F22%2Fambari%E9%9B%86%E7%BE%A4%E6%B7%BB%E5%8A%A0kerberos%E8%AE%A4%E8%AF%81%2F</url>
    <content type="text"><![CDATA[host准备ip | 名字 | 备注—|—|—10.1.5.79 | data-test01 | ambari-server/KDC10.1.5.80 | data-test02 |10.1.5.81 | data-test03 | 1.KDC配置= a. 安装在test01上安装KDC:1yum install -y krb5-server krb5-libs krb5-auth-dialog krb5-workstation 其他机器安装客户端1yum install krb5-devel krb5-workstation -y b.编辑配置文件/etc/krb5.conf1234567891011121314151617181920212223[logging] default = FILE:/var/log/krb5libs.log kdc = FILE:/var/log/krb5kdc.log admin_server = FILE:/var/log/kadmind.log[libdefaults] default_realm = WILLCLUSTER.COM dns_lookup_realm = false dns_lookup_kdc = false ticket_lifetime = 24h renew_lifetime = 7d forwardable = true udp_preference_limit = 1[realms] WILLCLUSTER.COM = &#123; kdc = kerberos.willcluster.com admin_server = kerberos.willcluster.com &#125;[domain_realm] .willcluster.com = WILLCLUSTER.COM willcluster.com = WILLCLUSTER.COM /var/kerberos/krb5kdc/kdc.conf123456789101112[kdcdefaults] kdc_ports = 88 kdc_tcp_ports = 88[realms] WILLCLUSTER.COM = &#123; #master_key_type = aes256-cts acl_file = /var/kerberos/krb5kdc/kadm5.acl dict_file = /usr/share/dict/words admin_keytab = /var/kerberos/krb5kdc/kadm5.keytab supported_enctypes = aes256-cts:normal aes128-cts:normal des3-hmac-sha1:normal arcfour-hmac:normal des-hmac-sha1:normal des-cbc-md5:normal des-cbc-crc:normal &#125; c.分发/etc/krb5.conf到各个节点12scp /etc/krb5.conf data-test02:/etc/scp /etc/krb5.conf data-test03:/etc/ d. test01上启动服务1234chkconfig --level 35 krb5kdc onchkconfig --level 35 kadmin onservice krb5kdc startservice kadmin start 2.数据库= 创建kerberos数据库1kdb5_util create -r WILLCLUSTER.COM -s 在test01上创建超级用户1kadmin.local -q &quot;addprinc root/admin&quot; 创建ambari各个服务用户与相应权限HDP里的每个服务都必须有自己的principal。为避免每次都输入密码，我们使用keytab文件作为认证，keytab是从kerberos数据库中抽取出来的。 下面使用超级用户来创建对应service的principal12345678910111213141516kadmin.local -q &quot;addprinc -randkey yarn/data-test01@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey yarn/data-test02@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey yarn/data-test03@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey zookeeper/data-test01@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey zookeeper/data-test02@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey zookeeper/data-test03@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey hdfs/data-test01@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey hdfs/data-test02@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey hdfs/data-test03@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey nn/data-test01@WILLCLUSTER.COM&quot;kadmin.local -q &quot;addprinc -randkey nn/data-test02@WILLCLUSTER.COM&quot;... Tips：为什么要每个node都要弄一份呢？ 两个原因： 如果一个node的kerberos认证通过，并不能自动扩散到所有node。kerberos认证是以node为单位的 如果多个node共用同一个认证的话，万一这些node的kerberos认证同时发出去，也就是带有同样的时间戳，那么这些请求会被认为是重复的请求，后面来的会被拒绝 然后抽取keytab文件到本地目录123456789kadmin.local -q &quot;xst -k yarn.keytab yarn/data-test01@WILLCLUSTER.COM&quot;kadmin.local -q &quot;xst -k yarn.keytab yarn/data-test02@WILLCLUSTER.COM&quot;kadmin.local -q &quot;xst -k yarn.keytab yarn/data-test03@WILLCLUSTER.COM&quot;kadmin.local -q &quot;xst -k zookeeper.keytab zookeeper/data-test01@WILLCLUSTER.COM&quot;kadmin.local -q &quot;xst -k zookeeper.keytab zookeeper/data-test02@WILLCLUSTER.COM&quot;kadmin.local -q &quot;xst -k zookeeper.keytab zookeeper/data-test03@WILLCLUSTER.COM&quot;... 再把keytab文件分发到每个服务对应的每个节点12scp yarn.keytab zookeeper.keytab data-test02:/usr/hdp/current/hadoop-client/confscp yarn.keytab zookeeper.keytab data-test03:/usr/hdp/current/hadoop-client/conf 或者也可以先将所有的keytab合并到一个里面，方便copy，但是这也带来了各个node权限控制的问题。进入ktutil，读取原有的keytab，写入到统一的一个keytab文件。1234ktutil: rkt yarn.keytabktutil: rkt zookeeper.keytab....ktutil: wkt all_in_one.keytab 写完之后查看一下1klist -ket all_in_one.keytab 3.HDP相关= 给HDP配置kerberos包含两个部分： 创建unix系统里对应用户名的service principal。因为hadoop默认是使用的ShellBasedUnixGroupsMapping，它继承自GroupMappingServiceProvider接口，使用namenode的linux文件与用户权限系统。 在对应service的配置文件中添加kerberos相关配置。 3.1 创建unix系统里对应用户名的service principalHDP使用一个基于规则的系统来创建service principal和对应unix用户的映射。这些rule配置在core-site.xml的hadoop.security.auth_to_local属性。默认是把所有的principal对应于这个principal的第一个组件。 创建一个分层的rule来提供多种复杂的情形。每个rule由三个部分组成： base以对应的service principal名字的的组件个数开头，跟一个冒号，然后是用户名的pattern。 有点儿类似awk的处理，$0代表realm，$1代表第一个组件，以此类推例如： 123[1:$1@$0] 把 myusername@APACHE.ORG 对应到 myusername@APACHE.ORG [2:$1] 把 myusername/admin@APACHE.ORG 对应到 myusername [2:$1%$2] 把 myusername/admin@APACHE.ORG 对应到 “myusername%admin filter一个正则表达式, 过滤rule。 substitution【置换】把一个正则转化为一个固定字符串,类似sed，例如：123s/@ACME\.COM// 删除第一个 @ACME.DOMAINs/@[A-Z]*\.COM// 删除第一个后面跟着COM的@. s/X/Y/g 替换所有的 X成 Y 3.1.1 例子 如果你的默认realm是APACHE.COM,但是你想获取所有的ACME.COM的principal，下面的rule可以做到 12RULE:[1:$1@$0](.@ACME.COM)s/@.//DEFAULT 要把名字对应于第二个组件，使用下面规则： 12RULE:[1:$1@$0](.@ACME.COM)s/@.//RULE:[2:$1@$0](.@ACME.COM)s/@.// DEFAULT 把所有带有admin的APACHE.ORG对应于admin 12RULE[2:$1%$2@$0](.%admin@APACHE.ORG)s/./admin/DEFAULT 把所有用户名转为小写 1234RULE:[1:$1]/LRULE:[2:$1]/LRULE:[2:$1;$2](^.*;admin$)s/;admin$///LRULE:[2:$1;$2](^.*;guest$)s/;guest$//g/L 基于上面的rule，输入与对应输出如下：1234&quot;JOE@FOO.COM&quot; to &quot;joe&quot;&quot;Joe/root@FOO.COM&quot; to &quot;joe&quot;&quot;Joe/admin@FOO.COM&quot; to &quot;joe&quot;&quot;Joe/guestguest@FOO.COM&quot; to &quot;joe&quot; 3.2 修改配置文件首先，在hadoop-env.sh里配置JSVC_HOME1export JSVC_HOME=/usr/libexec/bigtop-utils core-site.xml Property Name Property Value Description hadoop.security.authentication kerberos hadoop.rpc.protection authentication; integrity; privacy 可选 hadoop.security.authorization true hadoop.security.auth_to_local 上面提到的rule 12345678910111213141516171819202122232425&lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;description&gt; Set the authentication for the cluster. Valid values are: simple or kerberos.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Enable authorization for different protocols.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt; &lt;value&gt; RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/mapred/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/hdfs/ RULE:[2:$1@$0](hm@.*EXAMPLE.COM)s/.*/hbase/ RULE:[2:$1@$0](rs@.*EXAMPLE.COM)s/.*/hbase/ DEFAULT &lt;/value&gt; &lt;description&gt;The mapping from kerberos principal names to local OS user names.&lt;/description&gt;&lt;/property&gt; 下面是有关hue和knox的配置123456789101112131415161718192021222324252627282930313233343536&lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;description&gt;Set the authentication for the cluster. Valid values are: simple or kerberos.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt;Enable authorization for different protocols. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt; &lt;value&gt; RULE:[2:$1@$0]([jt]t@.*EXAMPLE.COM)s/.*/mapred/ RULE:[2:$1@$0]([nd]n@.*EXAMPLE.COM)s/.*/hdfs/ RULE:[2:$1@$0](hm@.*EXAMPLE.COM)s/.*/hbase/ RULE:[2:$1@$0](rs@.*EXAMPLE.COM)s/.*/hbase/ DEFAULT &lt;/value&gt; &lt;description&gt;The mapping from kerberos principal names to local OS user names.&lt;/description&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.knox.groups&lt;/name&gt; &lt;value&gt;users&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.knox.hosts&lt;/name&gt; &lt;value&gt;Knox.EXAMPLE.COM&lt;/value&gt;&lt;/property&gt; HTTP Cookie 持久化默认HTTP 认证过程中，cookie是不会保留的。1234&lt;property&gt; &lt;name&gt;hadoop.http.authentication.cookie.persistent&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; hdfs-site.xml123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109&lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; If &quot;true&quot;, enable permission checking in HDFS. If &quot;false&quot;, permission checking is turned off, but all other behavior is unchanged. Switching from one parameter value to the other does not change the mode, owner or group of files or directories. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions.supergroup&lt;/name&gt; &lt;value&gt;hdfs&lt;/value&gt; &lt;description&gt;The name of the group of super-users.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;description&gt; If &quot;true&quot;, access tokens are used as capabilities for accessing datanodes. If &quot;false&quot;, no access tokens are checked on accessing datanodes. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt; &lt;value&gt;nn/_HOST@EXAMPLE.COM&lt;/value&gt; &lt;description&gt; Kerberos principal name for the NameNode &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt; &lt;value&gt;nn/_HOST@EXAMPLE.COM&lt;/value&gt; &lt;description&gt;Kerberos principal name for the secondary NameNode. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@EXAMPLE.COM&lt;/value&gt; &lt;description&gt; The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. The HTTP Kerberos principal MUST start with &apos;HTTP/&apos; per Kerberos HTTP SPNEGO specification. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/etc/security/keytabs/spnego.service.keytab&lt;/value&gt; &lt;description&gt;The Kerberos keytab file with the credentials for the HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt; &lt;value&gt;dn/_HOST@EXAMPLE.COM&lt;/value&gt; &lt;description&gt; The Kerberos principal that the DataNode runs as. &quot;_HOST&quot; is replaced by the real host name. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt; &lt;value&gt;/etc/security/keytabs/nn.service.keytab&lt;/value&gt; &lt;description&gt; Combined keytab file containing the namenode service and host principals. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt; &lt;value&gt;/etc/security/keytabs/nn.service.keytab&lt;/value&gt; &lt;description&gt; Combined keytab file containing the namenode service and host principals. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt; &lt;value&gt;/etc/security/keytabs/dn.service.keytab&lt;/value&gt; &lt;description&gt; The filename of the keytab file for the DataNode. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.access.time.precision&lt;/name&gt; &lt;value&gt;0&lt;/value&gt; &lt;description&gt;The access time for HDFS file is precise upto this value.The default value is 1 hour. Setting a value of 0 disables access times for HDFS. &lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.kerberos.internal.spnego.principal&lt;/name&gt; &lt;value&gt;$&#123;dfs.web.authentication.kerberos.principal&#125;&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.namenode.kerberos.internal.spnego.principal&lt;/name&gt; &lt;value&gt;$&#123;dfs.web.authentication.kerberos.principal&#125;&lt;/value&gt; &lt;/property&gt; 另外，还需加上如下配置： 12export HADOOP_SECURE_DN_USER=hdfsexport HADOOP_SECURE_DN_PID_DIR=/grid/0/var/run/hadoop/$HADOOP_SECURE_DN_USER yarn-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364&lt;property&gt; &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt; &lt;value&gt;yarn/localhost@EXAMPLE.COM&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt; &lt;value&gt;/etc/krb5.keytab&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt; &lt;value&gt;yarn/localhost@EXAMPLE.COM&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt; &lt;value&gt;/etc/krb5.keytab&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.container-executor.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.linux-container-executor.path&lt;/name&gt; &lt;value&gt;hadoop-3.0.0-SNAPSHOT/bin/container-executor&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.linux-container-executor.group&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.principal&lt;/name&gt; &lt;value&gt;yarn/localhost@EXAMPLE.COM&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.keytab&lt;/name&gt; &lt;value&gt;/etc/krb5.keytab&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.delegation-token-auth-filter.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.http-authentication.type&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.http-authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;HTTP/localhost@EXAMPLE.COM&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.timeline-service.http-authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/etc/krb5.keytab&lt;/value&gt;&lt;/property&gt; mapred-site.xml1234567891011121314151617181920212223242526272829&lt;property&gt; &lt;name&gt;mapreduce.jobhistory.keytab&lt;/name&gt; &lt;value&gt;/etc/security/keytabs/jhs.service.keytab&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.principal&lt;/name&gt; &lt;value&gt;jhs/_HOST@TODO-KERBEROS-DOMAIN&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;TODO-JOBHISTORYNODE-HOSTNAME:19888&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.https.address&lt;/name&gt; &lt;value&gt;TODO-JOBHISTORYNODE-HOSTNAME:19889&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.spnego-keytab-file&lt;/name&gt; &lt;value&gt;/etc/security/keytabs/spnego.service.keytab&lt;/value&gt;&lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.spnego-principal&lt;/name&gt; &lt;value&gt;HTTP/_HOST@TODO-KERBEROS-DOMAIN&lt;/value&gt;&lt;/property&gt; 参考： http://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_Security_Guide/content/install-kdc.html https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.2/bk_installing_manually_book/content/ch_security_for_manual_installs_chapter.html http://blog.csdn.net/wulantian/article/details/42173023]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《集体智慧编程》--5---优化]]></title>
    <url>%2F2017%2F03%2F17%2F%E3%80%8A%E9%9B%86%E4%BD%93%E6%99%BA%E6%85%A7%E7%BC%96%E7%A8%8B%E3%80%8B--5---%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[针对问题：受多种变量的影响，存在许多可能解，结果因这些变量组合的变化产生巨大反应。 优化算法是通过尝试许多不同题解并给这些题解打分以确认其质量的方式找到一个问题的最优解。 成本函数成本函数需要一个返回值代表方案的好坏。旅游团的例子中： 价格 旅行时间 等待时间 汽车租用时间 我们需要明确怎样把上面所有的成本综合在一起，表示最终的成本。 获取方案的方法 方法 原理 缺点 优点 随机搜索 随机生成N个方案，选取其中成本最低的方案 低效 爬山法 以一个随机解开始，在其临近解集中寻找更好的解，类似走成本的下坡路 陷入局部最优 模拟退火算法 退火是将合金加热后再慢慢冷却的过程。大量的原子因为受到激发而向四周跳跃，然后又逐渐稳定到一个低能的状态，所以这些原子能够找到一个低能阶的配置。以一个随机解开始，用一个变量表示温度，开始时高，逐渐变低。第一次迭代，算法会随机选中某个数据，然后朝某个方向变化。它总是会接受一个更优的解，但是在算法开始阶段也会接受表现稍差的解。随着算法进行，越来越不能接受较差的解，最后只接受更优解。 算法接受较差解的概率 P = exp[-(highcost-lowcost)/temperature] 前提：最优解接近其他优解 遗传算法 随机生成一组解，成为种群。计算整个种群的成本函数，得到一个有序列表，选出表现比较好的几个。然后对一个既有解做微小的、简单的、随机的改变，这称为变异。另一种方法是选择最优解中的两个，按照某种方式进行结合，称为交叉或者配对。这样就会衍生很多不同的解。 前提：最优解接近其他优解 注意：利用优化算法解决问题的基本要求是：问题本身有一个定义好的成本函数，并且相似的解会产生相似的结果。 学生分宿舍如何将有限的资源分配给表达了偏好的人，并尽可能使他们都满意。(或根据他们的意愿，尽可能让他们都满意)。 将违反意愿的程度作为成本函数借用上述算法求解。 网络可视化运用优化算法构建条例明晰的网络图。成本函数就是计算相互交叉线的条数。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[索引相关]]></title>
    <url>%2F2017%2F03%2F15%2F%E7%B4%A2%E5%BC%95%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[哈希存储引擎 哈希表的持久化实现，支持增、删、改以及随机读取操作，但不支持顺序扫描，对应的存储系统为key-value存储系统。对于key-value的插入以及查询，哈希表的复杂度都是O(1)，明显比树的操作O(n)快,如果不需要有序的遍历数据，哈希表就是your Mr.Right B树存储引擎 不仅支持单条记录的增、删、读、改操作，还支持顺序扫描（B+树的叶子节点之间的指针），对应的存储系统就是关系数据库（Mysql等）。 LSM树【Log-Structured Merge-Tree】 存储引擎和B树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且通过批量存储技术规避磁盘随机写入问题。当然凡事有利有弊，LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。 ==LSM树的设计思想==非常朴素：将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘，不过读取的时候稍微麻烦，需要合并磁盘中历史数据和内存中最近修改操作，所以写入性能大大提升，读取时可能需要先看是否命中内存，否则需要访问较多的磁盘文件。极端的说，基于LSM树实现的HBase的写性能比Mysql高了一个数量级，读性能低了一个数量级。 LSM树原理把一棵大树拆分成N棵小树，它首先写入内存中，随着小树越来越大，内存中的小树会flush到磁盘中，磁盘中的树定期可以做merge操作，合并成一棵大树，以优化读性能. 因为小树先写到内存中，为了防止内存数据丢失，写内存的同时需要暂时持久化到磁盘，对应了HBase的MemStore和HLog. MemStore上的树达到一定大小之后，需要flush到HRegion磁盘中（一般是Hadoop DataNode），这样MemStore就变成了DataNode上的磁盘文件StoreFile，定期HRegionServer对DataNode的数据做merge操作，彻底删除无效空间，多棵小树在这个时机合并成大树，来增强读性能。 按照大小进行compact有三个缺点： 因为不能确定单行数据的版本都在哪里，所以无法保证一致性。最坏的情况下，每个sstable里都有一个某row的版本，分别包含不同的字段； 如果有大量删除操作的话，在merge以前很多数据的空间都是没有必要占用的； 在每个sstable在merge完成之前还是会占用很大的数据空间。最坏的情况就是，一个sstable并没有任何东西删除，那么Cassandra就使用同样大小的空间来进行compact操作。 Leveled CompactionLeveled Compaction 创建相对小的固定大小的sstable(默认5M)，按照level分组。每层的sstable都不重复，每一层都是前一层的10倍大小。 解决了以上三个问题： level compaction保证90%的读请求只需要一个sstable里。最坏也就是每层有一个版本，加入10T数据，也就只有7个版本【10的7次方】 最多只有10%的空间浪费给要被删除的数据 每次只有10倍于sstable固定大小的空间被用于compaction的过程 由于level compaction上面的原理，相对于hbase 的基于大小的compaction，会有两倍的io。如果是以插入为主的系统，额外的io可能相对于上面的益处更严重一些，因为插入为主的话，还是很少出现row的相同版本的。 参考： http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.44.2782&amp;rep=rep1&amp;type=pdf http://www.cnblogs.com/yanghuahui/p/3483754.html http://www.open-open.com/lib/view/open1424916275249.html http://www.datastax.com/dev/blog/leveled-compaction-in-apache-cassandra http://hbasefly.com/2016/03/25/hbase-hfile/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[对于runningdata失败任务流重启的思考]]></title>
    <url>%2F2017%2F03%2F09%2F%E5%AF%B9%E4%BA%8Erunningdata%E5%A4%B1%E8%B4%A5%E4%BB%BB%E5%8A%A1%E6%B5%81%E9%87%8D%E5%90%AF%E7%9A%84%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[对于单个ETL的重导。可以在ETL里自己写IF语句，然后配置调度的时候. 重导的方案A：123456files=$1other_files=$2sh $other_filesif [ $? -eq 0 ]; then echo &quot;echo $other_files done&quot; &gt; $filefi 然后在azkaban执行这个就可以了。other_files里是我们实际要执行的命令，$file是我们的azkaban执行的command.一个更简便的方案，就是把azkaban的命令写成这样，省去自己判断上个命令推出状态：ipconfig &amp;&amp; echo xxxxxxxxxxxxxx &gt; test.sh如果第一个命令出问题，那么肯定就不会执行第二个了。这样就可以执行日常任务的时候进行任务恢复了，只要是某个任务出现功能性失败，那么就会报错，而且不影响要执行的命令脚本；而成功的任务则会在执行完成后，修改执行的命令脚本，只是echo一下这个任务已经执行过了就好了。然后每次要执行失败任务恢复的时候，就直接重新执行整个flow，前面成功执行过的就会自动pass，输出已经执行过了。 还有一个问题，就是如果是代码问题，例如sql问题，必须通过修改ETL里的sql或者配置、或者presql等来解决的话，应该怎样处理？ 最简单的支持方式是：直接提供一个页面给管理员，编辑azkaban执行的任务文件，再进行重调。 现在我们azkaban的任务调度形式：1hive -f /var/azkaban-metamap/h2h-20170308050120/fact_jlc@redeem_record.hql 对应修改成的就是：1hive -f /var/azkaban-metamap/h2h-20170308050120/fact_jlc@redeem_record.hql &amp;&amp; echo &quot;select &apos;/var/azkaban-metamap/h2h-20170308050120/fact_jlc@redeem_record.hql&apos; done&quot; 如果出现问题，需要从这个地方开始恢复数据的话，走以下流程： a. 编辑修改ETL b. 测试新ETL是否无误 c. review hql，如需要修改个别地方手动修改 d. 打开编辑/var/azkaban-metamap/h2h-20170308050120/fact_jlc@redeem_record.hql的web页面，将c步骤中获取的内容放进来 e. 到azkaban重新执行今天的任务 期望出现的现象： a. 前面已经执行过的任务，自动输出已执行并跳过 b. 失败任务继续执行，读取的/var/azkaban-metamap/h2h-20170308050120/fact_jlc@redeem_record.hql是已经修改了的没有问题的文件 额外提示： a. 如果使用此种失败重启的方式，那么所有依赖关系都会被认为是强依赖。假设一个极端情况，就是一个大宽表依赖10个小表，并服务于8个应用，如果只有一个小表失败，就得使得整个宽表都不能跑。有可能这个小表只服务于一个小业务模块，但是其他几个就很重要。这样影响面比较大。 b. 如果使用此种重启方式，使用弱依赖，也就是当前表的失败并不影响下游任务继续执行的话，那么重新调度的时下游就不能执行了，因为它可能已经echo自己是成功的了。变态支持此方案的话，需要在d步骤的时候，添加一个hook，遍历所有没有成功的ETL的所有子节点，然后把这些子节点恢复为可执行状态。 方案B指定ETL，生成这个ETL及其所有相关字节点的任务，上传到azkaban进行调度。 问题：如果多个ETL失败，交叉的子节点会重复执行。 解决方案：一次接收多个ETL为参数，放进set里，再去生成相应任务]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《集体智慧编程》--2---分级聚类]]></title>
    <url>%2F2017%2F03%2F07%2F%E3%80%8A%E9%9B%86%E4%BD%93%E6%99%BA%E6%85%A7%E7%BC%96%E7%A8%8B%E3%80%8B--2---%E5%88%86%E7%BA%A7%E8%81%9A%E7%B1%BB%2F</url>
    <content type="text"><![CDATA[分级聚类：每个群组都从单一元素开始，迭代过程中，分级聚类算法计算每两个群组间的距离，并将距离最近的两个群组合并成一个新的群组。这个过程一直重复下去，知道只剩下一个群组为止。 也可以用树表示，树状图还能通过节点距离快速获取两个类的紧密程度。 参考：http://jingyan.baidu.com/article/29697b9109d147ab21de3c44.html 【个人从分级聚类的概念出发，其实这个2是个可变的，也就是说可以5个为一群，对应的树就是每个节点有5个分叉】 这一章还是使用皮尔逊相关度系数来作为衡量标准。皮尔逊相关系数是一种度量两个变量间相关程度的方法。它是一个介于 1 和 -1 之间的值，其中，1 表示变量完全正相关， 0 表示无关，-1 表示完全负相关。 分层角力算法以一组对应于原始数据向的聚类开始。函数的主循环部分会尝试每一组可能的配对并计算他们的相关度，找出最佳配对。新生成的聚类中包含的数据是两个旧聚类的数据均值。 参考： 皮尔逊相关度系数 相似度度量]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fabric自动化运维脚本]]></title>
    <url>%2F2017%2F02%2F27%2FFabric%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E8%84%9A%E6%9C%AC%2F</url>
    <content type="text"><![CDATA[综合考察了salt和Fabric两个东西，最后使用灵活的Fabric来自动化运维HDFS集群的日志文件维护。 使用前提： node之间的ssh互信 本机安装fabric 下面是fabric.py的脚本：1234567891011121314151617181920212223242526272829303132333435from fabric.api import *def get_host(): hosts = list() with open('/etc/hosts', 'r') as hf: for line in hf.readlines(): ss = line.split() if 'node' in ss[1]: hosts.append(ss[0]) hosts.remove('10.0.1.111') hosts.remove('10.0.1.112') return hostsenv.hosts = get_host()def hello(): print("Hello world!")def will(): run('ifconfig')def clean_log():# run("ls /var/log/hadoop/hdfs/ | awk '&#123;if (index($0, '2017') &gt; 0) print $0&#125;' | exec rm -rf &#123;&#125; \ ; ") with settings(warn_only=True): local('echo `date` &gt;&gt; ~/clean_log.log') if run('find /var/log/hadoop/hdfs/ -mtime +3 -name "*log-2016*" -exec rm -rvf &#123;&#125; \;').failed: print('failed for %s ' % env.host) if run('find /var/log/hadoop/hdfs/ -mtime +3 -name "*will.com.out.[3,4,5,6,7,8]" -exec rm -rvf &#123;&#125; \;').failed: print('failed for %s ' % env.host) if run('find /var/log/hadoop/hdfs/ -mtime +3 -name "*will.com.log.[3,4,5,6,7,8]" -exec rm -rvf &#123;&#125; \;').failed: print('failed for %s ' % env.host) 上面例子中，是读取了当前/etc/hosts文件中的所有host的IP，之后逐一执行远程命令，删除过期的日志。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安装dockercenter的ucp]]></title>
    <url>%2F2017%2F02%2F17%2F%E5%AE%89%E8%A3%85dockercenter%E7%9A%84ucp%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930[root@controller wil]# docker pull docker/ucp:2.1.02.1.0: Pulling from docker/ucpb7f33cc0b48e: Pulling fs layer 8257d57cc15c: Pulling fs layer b7f33cc0b48e: Pull complete 8257d57cc15c: Extracting [==================================================&gt;] 348.8 kB/348.8 kB8257d57cc15c: Pull complete f691f14109b2: Pull complete Digest: sha256:fd43a6560b6f5731d0e383a7b5fe6da91cea11bc6b3fd65afe08f5e4a97dbc90Status: Downloaded newer image for docker/ucp:2.1.0[root@controller wil]# docker run --rm -it --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:2.1.0 install --host-address 10.1.5.129 --interactiveINFO[0000] Verifying your system is compatible with UCP INFO[0000] Your engine version 1.13.1, build 092cba3 (3.10.0-514.2.2.el7.x86_64) is compatible WARN[0003] Your system uses devicemapper. We can not accurately detect available storage space. Please make sure you have at least 3.00 GB available in /server/dspace Admin Username: unable to scan value: unexpected newlineFATA[0005] unable to get admin username, giving up [root@controller wil]# docker loginLogin with your Docker ID to push and pull images from Docker Hub. If you don&apos;t have a Docker ID, head over to https://hub.docker.com to create one.Username (willcup): Password: Login Succeeded[root@controller wil]# docker run --rm -it --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:2.1.0 install --host-address 10.1.5.129 --interactiveINFO[0000] Verifying your system is compatible with UCP INFO[0000] Your engine version 1.13.1, build 092cba3 (3.10.0-514.2.2.el7.x86_64) is compatible WARN[0001] Your system uses devicemapper. We can not accurately detect available storage space. Please make sure you have at least 3.00 GB available in /server/dspace Admin Username: willcupAdmin Password: Confirm Admin Password: INFO[0021] Pulling required images... (this may take a while) 可以看到如果不先docker login的话，启动时候就报错找不到admin name。然而启动的时候又让我们自己定义admin name和passwd。醉了。 打开debug：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118[root@controller wil]# docker run --rm -it --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:2.1.0 install --host-address 10.1.5.129 -D --interactiveDEBU[0000] New UCP Instance ID will be &quot;XZNK:CP6T:DHHE:XJZA:OXOM:CLS6:RIRW:SAVZ:4JIL:G3Q4:DJ7F:L7P3&quot; INFO[0000] Verifying your system is compatible with UCP DEBU[0000] Checking for compatible kernel version DEBU[0000] Kernel version 3.10.0-514.2.2.el7.x86_64 is compatible DEBU[0000] Checking for compatible engine version INFO[0000] Your engine version 1.13.1, build 092cba3 (3.10.0-514.2.2.el7.x86_64) is compatible DEBU[0000] Container ucp-phase2 not present: Error: No such container: ucp-phase2 DEBU[0000] Detected container ucp DEBU[0000] Container ucp-controller not present: Error: No such container: ucp-controller DEBU[0000] Container ucp-swarm-manager not present: Error: No such container: ucp-swarm-manager DEBU[0000] Container ucp-swarm-join not present: Error: No such container: ucp-swarm-join DEBU[0000] Container ucp-kv not present: Error: No such container: ucp-kv DEBU[0000] Container ucp-proxy not present: Error: No such container: ucp-proxy DEBU[0000] Container ucp-client-root-ca not present: Error: No such container: ucp-client-root-ca DEBU[0000] Container ucp-cluster-root-ca not present: Error: No such container: ucp-cluster-root-ca DEBU[0000] Container ucp-auth-store not present: Error: No such container: ucp-auth-store DEBU[0000] Container ucp-auth-api not present: Error: No such container: ucp-auth-api DEBU[0000] Container ucp-auth-worker not present: Error: No such container: ucp-auth-worker DEBU[0000] Container ucp-kv-backup not present: Error: No such container: ucp-kv-backup DEBU[0000] Container ucp-kv-restore not present: Error: No such container: ucp-kv-restore DEBU[0000] Container ucp-metrics not present: Error: No such container: ucp-metrics DEBU[0000] Validating base system meets minimum requirements DEBU[0000] Your system meets minimum memory requirements: 3.88 GB &gt;= 2.00 GB WARN[0000] Your system uses devicemapper. We can not accurately detect available storage space. Please make sure you have at least 3.00 GB available in /server/dspace Admin Username: willAdmin Password: Confirm Admin Password: DEBU[0207] Checking for images INFO[0207] All required images are present DEBU[0208] Local Name: controller WARN[0208] None of the hostnames we&apos;ll be using in the UCP certificates [controller 127.0.0.1 172.17.0.1] contain a domain component. Your generated certs may fail TLS validation unless you only use one of these shortnames or IPs to connect. You can use the --san flag to add more aliases You may enter additional aliases (SANs) now or press enter to proceed with the above list.Additional aliases: willaliasDEBU[0222] User entered: willalias DEBU[0222] Hostnames: [controller 127.0.0.1 172.17.0.1 willalias] DEBU[0245] Launching phase 2 with: [install --host-address 10.1.5.129 -D --interactive] (6f2f352ea35e614de1005fe6d93c9843c3f972dc1e7cf5191ea9ed7a0e46fb25) DEBU[0000] Beginning phase 2 install for instance XZNK:CP6T:DHHE:XJZA:OXOM:CLS6:RIRW:SAVZ:4JIL:G3Q4:DJ7F:L7P3 DEBU[0000] Checking for compatible kernel version DEBU[0000] Kernel version 3.10.0-514.2.2.el7.x86_64 is compatible DEBU[0000] Checking for compatible engine version DEBU[0000] Detected container ucp-phase2 DEBU[0001] Container ucp-controller not present: Error: No such container: ucp-controller DEBU[0001] Container ucp-swarm-manager not present: Error: No such container: ucp-swarm-manager DEBU[0001] Container ucp-swarm-join not present: Error: No such container: ucp-swarm-join DEBU[0001] Container ucp-kv not present: Error: No such container: ucp-kv DEBU[0001] Container ucp-proxy not present: Error: No such container: ucp-proxy DEBU[0001] Container ucp-client-root-ca not present: Error: No such container: ucp-client-root-ca DEBU[0001] Container ucp-cluster-root-ca not present: Error: No such container: ucp-cluster-root-ca DEBU[0001] Container ucp-auth-store not present: Error: No such container: ucp-auth-store DEBU[0001] Container ucp-auth-api not present: Error: No such container: ucp-auth-api DEBU[0001] Container ucp-auth-worker not present: Error: No such container: ucp-auth-worker DEBU[0001] Container ucp-kv-backup not present: Error: No such container: ucp-kv-backup DEBU[0001] Container ucp-kv-restore not present: Error: No such container: ucp-kv-restore DEBU[0001] Container ucp-metrics not present: Error: No such container: ucp-metrics DEBU[0001] Local Name: controller DEBU[0001] Hostnames: [controller 127.0.0.1 172.17.0.1 willalias] DEBU[0001] This node is known as efpood09fyouiyib1f78y5oa6 DEBU[0001] Got node IP 10.1.5.129 from swarm DEBU[0001] EnginePortCheck: port 2375, prpl http, err Get http://10.1.5.129:2375/info: dial tcp 10.1.5.129:2375: getsockopt: connection refused DEBU[0001] EnginePortCheck: port 2375, prpl https, err Get https://10.1.5.129:2375/info: dial tcp 10.1.5.129:2375: getsockopt: connection refused DEBU[0001] EnginePortCheck: port 2376, prpl http, err Get http://10.1.5.129:2376/info: dial tcp 10.1.5.129:2376: getsockopt: connection refused DEBU[0001] EnginePortCheck: port 2376, prpl https, err Get https://10.1.5.129:2376/info: dial tcp 10.1.5.129:2376: getsockopt: connection refused DEBU[0001] Checking for available and accessible port 12387 DEBU[0001] Checking for available and accessible port 12380 DEBU[0001] Checking for available and accessible port 12381 DEBU[0001] Checking for available and accessible port 443 DEBU[0001] Checking for available and accessible port 12382 DEBU[0001] Checking for available and accessible port 2376 DEBU[0001] Checking for available and accessible port 12383 DEBU[0001] Checking for available and accessible port 12376 DEBU[0001] Checking for available and accessible port 12384 DEBU[0001] Checking for available and accessible port 4789 DEBU[0001] Checking for available and accessible port 12385 DEBU[0001] Checking for available and accessible port 12379 DEBU[0001] Checking for available and accessible port 12386 DEBU[0038] Checking for liveness of http://10.1.5.129:4789/ DEBU[0039] Connected to http://10.1.5.129:4789/ DEBU[0063] Checking for liveness of http://10.1.5.129:12387/ DEBU[0063] Connected to http://10.1.5.129:12387/ DEBU[0089] Checking for liveness of http://10.1.5.129:12382/ DEBU[0089] Connected to http://10.1.5.129:12382/ DEBU[0114] Checking for liveness of http://10.1.5.129:12384/ DEBU[0114] Connected to http://10.1.5.129:12384/ DEBU[0126] Checking for liveness of http://10.1.5.129:12376/ DEBU[0126] Connected to http://10.1.5.129:12376/ DEBU[0158] Checking for liveness of http://10.1.5.129:12383/ DEBU[0161] Connected to http://10.1.5.129:12383/ DEBU[0183] Checking for liveness of http://10.1.5.129:443/ DEBU[0185] Connected to http://10.1.5.129:443/ DEBU[0199] Checking for liveness of http://10.1.5.129:2376/ DEBU[0201] Connected to http://10.1.5.129:2376/ DEBU[0235] Checking for liveness of http://10.1.5.129:12386/ DEBU[0240] Connected to http://10.1.5.129:12386/ DEBU[0260] Checking for liveness of http://10.1.5.129:12381/ DEBU[0260] Connected to http://10.1.5.129:12381/ DEBU[0271] Checking for liveness of http://10.1.5.129:12385/ DEBU[0271] Connected to http://10.1.5.129:12385/ DEBU[0295] Checking for liveness of http://10.1.5.129:12379/ DEBU[0295] Connected to http://10.1.5.129:12379/ DEBU[0321] Checking for liveness of http://10.1.5.129:12380/ DEBU[0321] Connected to http://10.1.5.129:12380/ DEBU[0429] All ports are open and available DEBU[0429] Purging old state from UCP volumes mounted in /var/lib/docker/ucp INFO[0429] Establishing mutual Cluster Root CA with Swarm INFO[0429] Installing UCP with host address 10.1.5.129 - If this is incorrect, please specify an alternative address with the &apos;--host-address&apos; flag INFO[0429] Generating UCP Client Root CA DEBU[0429] Writing /var/lib/docker/ucp/ucp-client-root-ca/key.pem INFO[0430] Deploying UCP Service DEBU[0433] Local task container not found, trying again. DEBU[0459] Local task container not found, trying again. ................................DEBU[0461] Local task container not found, trying again. ERRO[0461] Unable to get local task container for ucp-agent service FATA[0461] unable to find local task container 重新仔细阅读一下官方文档，发现有些前置条件没有满足：123Docker Engine 1.13.0Docker Remote API 1.25Compose 1.9 刚才docker remote没有开启，compose的版本也不够。 先升级一下所有docker节点的compose：1234567[root@dnode2 willyarnbase]# curl -L &quot;https://github.com/docker/compose/releases/download/1.11.1/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed100 600 0 600 0 0 534 0 --:--:-- 0:00:01 --:--:-- 534100 8053k 100 8053k 0 0 403k 0 0:00:19 0:00:19 --:--:-- 806k[root@dnode2 willyarnbase]# docker-compose -vdocker-compose version 1.11.1, build 7c5d5e4 再编辑/etc/docker/daemon.json配置文件，添加下面配置，启动docker remote API:1&quot;hosts&quot;: [&quot;tcp://0.0.0.0:4243&quot;,&quot;unix:///var/run/docker.sock&quot;] 然后访问此docker节点的4243接口，例如：http://10.1.5.129:4243/images/json，测试可以获取到当前节点的image信息，代表已经成功启用了docker remote api。 访问http://10.1.5.129:4243/version，查看相关版本信息：1234567891011&#123;&quot;Version&quot;: &quot;1.13.1&quot;,&quot;ApiVersion&quot;: &quot;1.26&quot;,&quot;MinAPIVersion&quot;: &quot;1.12&quot;,&quot;GitCommit&quot;: &quot;092cba3&quot;,&quot;GoVersion&quot;: &quot;go1.7.5&quot;,&quot;Os&quot;: &quot;linux&quot;,&quot;Arch&quot;: &quot;amd64&quot;,&quot;KernelVersion&quot;: &quot;3.10.0-514.2.2.el7.x86_64&quot;,&quot;BuildTime&quot;: &quot;2017-02-08T06:38:28.018621521+00:00&quot;&#125; 版本是1.26，已经满足前面的各个docker组件的版本需求，把这个配置scp给其他的docker节点即可。 参考： https://docs.docker.com/engine/api/v1.26/ 再去controller启动ucp：1234[root@controller wil]# docker run --rm -it --name ucp -v /var/run/docker.sock:/var/run/docker.sock docker/ucp:2.1.0 install --host-address 10.1.5.129 --interactiveINFO[0000] Verifying your system is compatible with UCP INFO[0000] Your engine version 1.13.1, build 092cba3 (3.10.0-514.2.2.el7.x86_64) is compatible FATA[0019] This swarm is already managed by UCP. Please either uninstall UCP first using the &apos;uninstall-ucp&apos; operation or upgrade your cluster to a newer version]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习与R语言——模型Tips]]></title>
    <url>%2F2017%2F02%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8ER%E8%AF%AD%E8%A8%80%E2%80%94%E2%80%94%E6%A8%A1%E5%9E%8BTips%2F</url>
    <content type="text"><![CDATA[通用=对于NA缺失值的处理，连续型的可以给以均值、枚举型应该给一个unknown的计数。 距离类的算法，需要将各个特征进行标准化【z-score、max-min】 kmeans聚类=选择类的数量：根据业务判断 聚类结果的各类中的数量不应相差过于悬殊【也跟业务有关】 可以根据各个cluster中的特征z-score看出各个cluster自己的特性，并为之冠以业务名称。 模型性能评价=我们通常只用了分类结果，但是隐藏的概率没有看到。 例如，一个分类器可以同99%的把握确信包含“免费”和“铃声”等词语的短信是垃圾短信，但是只有51%的把握确信包含“今晚”的短信是垃圾短信。这两种情况都会被预测为垃圾短信，但是对两者做判断的确信程度就很大。一个是99，另一个才只有51，后者就极容易出现错误判断。1&gt; head(subset(sms_results, actual_type != predict_type)) 混淆矩阵通常使用二元的混淆矩阵： 准确率、错误率kappa统计量灵敏度（真阳性lv）、特异性（真阴性率）精确度（阳性预测值）、召回率F度量（综合精确度和召回率） 使用ROC曲线来可视化模型的可用性，ROC曲线是Y为真阳性比例、X为假阳性比例的线图。AUC是ROC曲线下的面积，一般作为评价分类器的统计量。(Area Under the ROC) 抽样分层抽样：在从全量数据中抽取训练集或测试集的时候有个问题，就是每个类别中的数量可能过大或过小，因此我们应该从每个类别中按同样比例随机抽取数据。 K折交叉验证的方法默认就是使用分层抽样进行每次数据集的划分的。 较小数据集的时候，自助法抽样比K折的效果更好一些。 提高模型的性能可以使用caret进行自动参数调整，考虑三个问题： 需要使用数据来训练哪种机器学习模型 哪些模型参数是可以调整的，能调整的空间有多大 使用何种评价标准来评估模型从而找到最优的候选者 朴素贝叶斯=假设数据集的所有特征都具有相同的重要性和独立性。 在贝叶斯算法中，概率值是相乘的，所以概率为0的值将导致该消息是垃圾邮件的后验概率为0.这种问题使用一种叫做拉普拉斯估计(Laplace estimator)的方法解决，本质上就是给频率表中的每个技术都加上一个较小的数，保证每一类中每个特征发生的概率是非零的。 决策树=boosting算法思想：通过将很多能力弱的学习算法组合在一起，就可以创建一个团队，这比任何一个单独学习算法都强的多。每个模型都有一组特定的优点和缺点，对于特定的问题，可能更好，也可能更差，而使用优缺互补的多种学习方法的结合，就可以显著提高分类器的准确性。 C5.0中是添加一个trials参数，表示在模型增强团队中使用的独立决策树的数量。trials设置了一个上限，如果该算法识别出额外的试验似乎没有提高模型准确性，那么它将停止添加决策树。 为啥默认不用boosting呢？一是时间因素，二是如果训练数据集很杂乱，那么boosting可能根本不会改善模型性能。 针对具体业务，设置风险矩阵cost matrix，添加到模型训练过程中，虽然整体预测准确率会降低，但是能够降低业务风险。 规则学习=规则学习经常以一种类似决策树学习的方式被使用。分类规则代表的是if-else逻辑语句形式的知识，可以用来对未标记的案例指定一个分类。 与决策树不同的是，决策树必须从上倒下地应用，而规则是单独存在的事实。根据相同数据建立的模型，规则学习的结果通常比决策树的结果更简洁、更直接、更容易理解。 规则学习擅长识别偶发事件。 决策树是分而治之，规则学习是独立而治之，区别是考虑决策节点的时候是否受过去决策历史的影响。 分类规则也可以直接从决策树获得。从一个叶子节点开始沿着树枝回到树根，就获得一系列的决策，这些决策可以组合成一个单一的规则。 当训练模型事，如果制定rules=TRUE,C5.0函数就利用分类规则生成模型了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R数据结构]]></title>
    <url>%2F2017%2F02%2F08%2FR%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[类型= NULL表示没有任何值 NA 表示缺失值 因子=用类别值来代表特征的属性成为名义属性。尽管可以用字符型向量存储名义属性，但是R提供了因子的专用数据结构。 一个是省空间、另外一些机器学习算法是用特别的程序来处理分类变量的，把分类变量编码为因子可以确保模型能够合理地处理这类数据。 12345678&gt; gender &lt;- factor(c(&quot;MALE&quot;, &quot;FAMALE&quot;,&quot;FAMALE&quot;))&gt; gender[1] MALE FAMALE FAMALELevels: FAMALE MALE&gt; blood &lt;- factor(c(&quot;A&quot;, &quot;B&quot;), levels = c(&quot;A&quot;, &quot;B&quot;, &quot;O&quot;))&gt; blood[1] A BLevels: A B O levels是说这个字段可能会有多少值 集合=向量必须是同样类型的元素，而列表可以包含不同类型的元素，还可以指定元素的key。1234567891011121314151617181920212223&gt; obj &lt;- list(name=&quot;will&quot;, age=21, job=&quot;coder&quot;)&gt; obj$name[1] &quot;will&quot;$age[1] 21$job[1] &quot;coder&quot;&gt; obj$age[1] 21&gt; obj[2]$age[1] 21&gt; obj[c(&quot;name&quot;, &quot;age&quot;)]$name[1] &quot;will&quot;$age[1] 21 DataFrame可以看作是带有每列含义的matrix矩阵。 文件操作=123456&gt; p_data &lt;- read.csv(&quot;D:/qdsj-20170204094000&quot;)&gt; typeof(p_data)[1] &quot;list&quot;&gt; p_data$用户id[1][1] 0f1d4b839353400998b7b289f895e36967788 Levels: 000188cb8d044291bd9984b8e04a224a ... ffff69f594cf48eb8867e8edcdb21e87 可以看到有67788个level，也就是说这是已经把用户id这个字段自动因子化了。 12345&gt; p_data &lt;- read.csv(&quot;D:/qdsj-20170204094000&quot;, stringsAsFactors = FALSE)&gt; p_data$用户id[1][1] &quot;0f1d4b839353400998b7b289f895e369&quot;&gt; typeof(p_data)[1] &quot;list&quot; 探索数据结构= 进行可读化输出：123456789101112131415&gt; str(p_data)&apos;data.frame&apos;: 67788 obs. of 13 variables: $ 用户id : chr &quot;0f1d4b839353400998b7b289f895e369&quot; &quot;d57eb5a615004c00af1476471577bcf8&quot; &quot;8ffd3d4e84db4cad841d0ed8c31e0fb5&quot; &quot;49cdd310f2854af09ccfde82e3ea4e06&quot; ... $ 用户渠道 : chr &quot;laiqiang_cpa_01&quot; &quot;laiqiang_cpa_01&quot; &quot;laiqiang_cpa_01&quot; &quot;laiqiang_cpa_01&quot; ... $ 用户手机号 : chr &quot;186****5084&quot; &quot;134****3636&quot; &quot;188****2253&quot; &quot;134****3632&quot; ... $ 用户姓名 : chr &quot;马先生&quot; &quot;张先生&quot; &quot;&quot; &quot;&quot; ... $ 注册时间 : chr &quot;2016-10-19 17:46:28&quot; &quot;2016-10-20 16:07:05&quot; &quot;2016-10-20 16:08:37&quot; &quot;2016-10-20 19:21:22&quot; ... $ 开户时间 : chr &quot;2016-10-19 17:51:24&quot; &quot;2016-10-20 16:09:13&quot; &quot;null&quot; &quot;null&quot; ... $ 首投时间 : chr &quot;2016-10-19 17:51:54&quot; &quot;2016-10-20 16:29:34&quot; &quot;null&quot; &quot;null&quot; ... $ 首投金额 : num 1000 1000 0 0 1000 1000 1000 0 1000 1000 ... $ 二投时间 : chr &quot;null&quot; &quot;null&quot; &quot;null&quot; &quot;null&quot; ... $ 二投金额 : num 0 0 0 0 1208 ... $ 首投定期时间: chr &quot;2016-10-19 17:52:38&quot; &quot;2016-10-20 16:30:27&quot; &quot;null&quot; &quot;null&quot; ... $ 首投定期金额: num 1000 1000 0 0 0 1000 1000 0 1000 1000 ... $ 凌晨前存量 : chr &quot;\\N&quot; &quot;\\N&quot; &quot;\\N&quot; &quot;\\N&quot; ... 探索数值型变量123&gt; summary(p_data$首投金额) Min. 1st Qu. Median Mean 3rd Qu. Max. 0.0 0.0 0.0 128.1 0.0 50000.0 可以找到DataFrame中这列的数值属性：最小值、第一四分位数、中位数、平均数、第三四分位数、最大值。 也可以同时看两列的：12345678&gt; summary(p_data[c(&quot;首投金额&quot;, &quot;二投金额&quot;)]) 首投金额 二投金额 Min. : 0.0 Min. : 0.0 1st Qu.: 0.0 1st Qu.: 0.0 Median : 0.0 Median : 0.0 Mean : 128.1 Mean : 228.7 3rd Qu.: 0.0 3rd Qu.: 0.0 Max. :50000.0 Max. :200000.0 示例 解释 range(p_data$首投金额) 返回最小值和最大值 diff(a, b) a和b的差值 IQR(p_data$首投金额) 四分位数中，中间的50%的差值（第三四分位数减去第一四分位数） quantile(p_data$首投金额) 返回四分位的五个位置上的数。 还可以通过probs指定获取百分位数中任意位置的数 boxplot(p_data$首投金额, main=”sss”, ylab=”Money($)”) 将首投金额以箱图的形式绘图展示，名字是sss hist(p_data$首投定期金额, main=”首投定期金额”, xlab= “Money($”) 直方图 var(p_data$首投金额) 方差 sd(p_data$首投金额) 标准差 table(p_data$首投金额) 以表的形式输出数据分布情况，每种值的个数 prop.table(table(p_data$首投金额)) 以表的形式输出数据分布情况，每种值占总数的比例 plot(x=p_data$首投金额, y =p_data$二投金额, main=”变量关系”, xlab=”手头”, ylab=”二头”) 使用散点图展示两种特征值之间的关系， y是因变量，x是自变量 CrossTable(x=userdcars$model, y= userdcars$conservative) 使用双向交叉表检验变量之间的关系 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677&gt; max(p_data$首投金额)[1] 50000&gt; min(p_data$首投金额)[1] 0&gt; IQR(p_data$首投金额)[1] 0&gt; range(p_data$首投金额)[1] 0 50000&gt; diff(range(p_data$首投金额))[1] 50000&gt; quantile(p_data$首投金额) 0% 25% 50% 75% 100% 0 0 0 0 50000 &gt; quantile(p_data$首投金额, probs = c (0.25, 0.5, 0.75, 0.97, 0.99)) 25% 50% 75% 97% 99% 0 0 0 1000 3000&gt; quantile(p_data$首投金额, probs = c (0.25, 0.5, 0.75, 0.97, 0.9999))25% 50% 75% 97% 99.99% 0.0 0.0 0.0 1000.0 26106.5 &gt; quantile(p_data$首投金额, seq(from=0, to=1, by=0.1)) 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% 0 0 0 0 0 0 0 0 0 100 50000 &gt; table(p_data$首投金额) 0 100 101 105.58 110 112 120 150 168 170 60435 1282 3 1 2 1 1 1 1 1 &gt; prop.table(table(p_data$首投金额)) 0 100 101 105.58 110 112 8.915295e-01 1.891190e-02 4.425562e-05 1.475187e-05 2.950375e-05 1.475187e-05&gt; usedcars$conservative &lt;-+ usedcars$color %in% c(&quot;Black&quot;, &quot;Gray&quot;, &quot;Silver&quot;, &quot;White&quot;)&gt; table(usedcars$conservative)FALSE TRUE 51 99 &gt; CrossTable(x = usedcars$model, y = usedcars$conservative) Cell Contents|-------------------------|| N || Chi-square contribution || N / Row Total || N / Col Total || N / Table Total ||-------------------------| Total Observations in Table: 150 | usedcars$conservative usedcars$model | FALSE | TRUE | Row Total | ---------------|-----------|-----------|-----------| SE | 27 | 51 | 78 | | 0.009 | 0.004 | | | 0.346 | 0.654 | 0.520 | | 0.529 | 0.515 | | | 0.180 | 0.340 | | ---------------|-----------|-----------|-----------| SEL | 7 | 16 | 23 | | 0.086 | 0.044 | | | 0.304 | 0.696 | 0.153 | | 0.137 | 0.162 | | | 0.047 | 0.107 | | ---------------|-----------|-----------|-----------| SES | 17 | 32 | 49 | | 0.007 | 0.004 | | | 0.347 | 0.653 | 0.327 | | 0.333 | 0.323 | | | 0.113 | 0.213 | | ---------------|-----------|-----------|-----------| Column Total | 51 | 99 | 150 | | 0.340 | 0.660 | | ---------------|-----------|-----------|-----------| 根据每个人所属年级，添加对应的平均年龄]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R安装工具]]></title>
    <url>%2F2017%2F02%2F08%2FR%E5%AE%89%E8%A3%85%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[安装R的packages的时候官方提供了两种方式： 交互式REPL里执行install.packages() 命令行执行R CMD INSTALL XXX.tar.gz 由于是要初始化docker的R运行环境，如果要安装多个package，这两种方法都不太方便。 找到一个不错的工具R-littler, 安装完成后，可以通过执行自己定义的脚本进行R环境甚至程序的执行。 下面是实际使用的例子： 12345678910111213141516171819#!/usr/bin/env rrepos &lt;- &quot;http://mirrors.tuna.tsinghua.edu.cn/CRAN/&quot;lib.loc &lt;- &quot;/usr/lib64/R/library&quot;install.packages(&quot;data.table&quot;, lib.loc, repos)install.packages(&quot;rJava&quot;, lib.loc, repos)install.packages(&quot;dplyr&quot;, lib.loc, repos)install.packages(&quot;bit64&quot;, lib.loc, repos)install.packages(&quot;stringr&quot;, lib.loc, repos)install.packages(&quot;ggplot2&quot;, lib.loc, repos)install.packages(&quot;RODBC&quot;, lib.loc, repos)install.packages(&quot;RJDBC&quot;, lib.loc, repos)install.packages(&quot;timeSeries&quot;, lib.loc, repos)install.packages(&quot;fUnitRoots&quot;, lib.loc, repos)install.packages(&quot;forecast&quot;, lib.loc, repos)install.packages(&quot;sqldf&quot;, lib.loc, repos)install.packages(&quot;lubridate&quot;, lib.loc, repos) 把以上文件命名为install.r，赋予可执行权限即可。 这只是给准备环境而已，其实也可以尝试直接写自己的业务逻辑的。 参考： http://dirk.eddelbuettel.com/code/littler.examples.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git-同时push到两个远程仓库]]></title>
    <url>%2F2017%2F02%2F08%2Fgit-%E5%90%8C%E6%97%B6push%E5%88%B0%E4%B8%A4%E4%B8%AA%E8%BF%9C%E7%A8%8B%E4%BB%93%E5%BA%93%2F</url>
    <content type="text"><![CDATA[123root@will-vm:~/gitLearning/dobuleremote# git remote -vorigin git@github.com:WillCup/dobuleremote.git (fetch)origin git@github.com:WillCup/dobuleremote.git (push) 1git remote add ink git@10.1.5.83:/server/gitrepo/dpush.git 之后就可以看到12345root@will-vm:~/gitLearning/dobuleremote# git remote -vink git@10.1.5.83:/server/gitrepo/dpush.git (fetch)ink git@10.1.5.83:/server/gitrepo/dpush.git (push)origin git@github.com:WillCup/dobuleremote.git (fetch)origin git@github.com:WillCup/dobuleremote.git (push) 可以通过push ink推送到新的远程仓库。 但是执行git status的时候发现，他是以origin为准的123456789root@will-vm:~/gitLearning/dobuleremote# git st位于分支 master您的分支与上游分支 &apos;origin/master&apos; 一致。未跟踪的文件: （使用 &quot;git add &lt;文件&gt;...&quot; 以包含要提交的内容） both1提交为空，但是存在尚未跟踪的文件（使用 &quot;git add&quot; 建立跟踪） 新建一个both文件，然后不加参数提交：12345678910111213141516171819202122232425262728293031323334353637root@will-vm:~/gitLearning/dobuleremote# ll总用量 12drwxr-xr-x 3 root root 4096 2月 8 14:17 ./drwxr-xr-x 29 root root 4096 2月 8 14:21 ../-rw-r--r-- 1 root root 0 2月 8 14:17 both1drwxr-xr-x 8 root root 4096 2月 8 14:20 .git/-rw-r--r-- 1 root root 0 2月 8 14:05 githubside-rw-r--r-- 1 root root 0 2月 8 14:06 githubside2root@will-vm:~/gitLearning/dobuleremote# git pushwarning: push.default 尚未设置，它的默认值在 Git 2.0 已从 &apos;matching&apos;变更为 &apos;simple&apos;。若要不再显示本信息并保持传统习惯，进行如下设置： git config --global push.default matching若要不再显示本信息并从现在开始采用新的使用习惯，设置： git config --global push.default simple当 push.default 设置为 &apos;matching&apos; 后，git 将推送和远程同名的所有本地分支。从 Git 2.0 开始，Git 默认采用更为保守的 &apos;simple&apos; 模式，只推送当前分支到远程关联的同名分支，即 &apos;git push&apos; 推送当前分支。参见 &apos;git help config&apos; 并查找 &apos;push.default&apos; 以获取更多信息。（&apos;simple&apos; 模式由 Git 1.7.11 版本引入。如果您有时要使用老版本的 Git，为保持兼容，请用 &apos;current&apos; 代替 &apos;simple&apos;）对象计数中: 2, 完成.Delta compression using up to 2 threads.压缩对象中: 100% (2/2), 完成.写入对象中: 100% (2/2), 231 bytes | 0 bytes/s, 完成.Total 2 (delta 1), reused 0 (delta 0)remote: Resolving deltas: 100% (1/1), completed with 1 local objects.To git@github.com:WillCup/dobuleremote.git 2ba4199..8697e51 master -&gt; master 可以看到是推送到了github端。到dpush那边也查看了一下，最新版本并没有出现这个文件1234567root@will-vm:~/gitLearning/dpush# ll总用量 12drwxr-xr-x 3 root root 4096 2月 8 14:21 ./drwxr-xr-x 29 root root 4096 2月 8 14:21 ../drwxr-xr-x 8 root root 4096 2月 8 14:21 .git/-rw-r--r-- 1 root root 0 2月 8 14:21 githubside-rw-r--r-- 1 root root 0 2月 8 14:21 githubside2 其实也可以只为某个remote添加push，而并不添加pull：123456789101112131415root@will-vm:~/gitLearning/dobuleremote# git remote set-url --add --push ink git@github.com:WillCup/dobuleremote.gitroot@will-vm:~/gitLearning/dobuleremote# git remote -vink git@10.1.5.83:/server/gitrepo/dpush.git (fetch)ink git@github.com:WillCup/dobuleremote.git (push)origin git@github.com:WillCup/dobuleremote.git (fetch)origin git@github.com:WillCup/dobuleremote.git (push)root@will-vm:~/gitLearning/dobuleremote# git --versiongit version 2.7.4root@will-vm:~/gitLearning/dobuleremote# git remote set-url --add --push ink git@10.1.5.83:/server/gitrepo/dpush.gitroot@will-vm:~/gitLearning/dobuleremote# git remote -vink git@10.1.5.83:/server/gitrepo/dpush.git (fetch)ink git@github.com:WillCup/dobuleremote.git (push)ink git@10.1.5.83:/server/gitrepo/dpush.git (push)origin git@github.com:WillCup/dobuleremote.git (fetch)origin git@github.com:WillCup/dobuleremote.git (push) 再添加一个文件both2，然后再push测试一下12345678910111213141516171819202122232425262728293031root@will-vm:~/gitLearning/dobuleremote# touch both2root@will-vm:~/gitLearning/dobuleremote# git st位于分支 master您的分支与上游分支 &apos;origin/master&apos; 一致。未跟踪的文件: （使用 &quot;git add &lt;文件&gt;...&quot; 以包含要提交的内容） both2提交为空，但是存在尚未跟踪的文件（使用 &quot;git add&quot; 建立跟踪）root@will-vm:~/gitLearning/dobuleremote# git add .root@will-vm:~/gitLearning/dobuleremote# git ci -am &quot;both test 2&quot;[master 40cdebb] both test 2 1 file changed, 0 insertions(+), 0 deletions(-) create mode 100644 both2root@will-vm:~/gitLearning/dobuleremote# git push ink master对象计数中: 2, 完成.Delta compression using up to 2 threads.压缩对象中: 100% (2/2), 完成.写入对象中: 100% (2/2), 223 bytes | 0 bytes/s, 完成.Total 2 (delta 1), reused 0 (delta 0)remote: Resolving deltas: 100% (1/1), completed with 1 local objects.To git@github.com:WillCup/dobuleremote.git 8697e51..40cdebb master -&gt; master对象计数中: 4, 完成.Delta compression using up to 2 threads.压缩对象中: 100% (4/4), 完成.写入对象中: 100% (4/4), 414 bytes | 0 bytes/s, 完成.Total 4 (delta 1), reused 0 (delta 0)To git@10.1.5.83:/server/gitrepo/dpush.git 2ba4199..40cdebb master -&gt; master 从上面的输出就可以看到，是push到了两个remote端，刚才落下的提交也一并被push过去了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ETL平台生产计划]]></title>
    <url>%2F2017%2F01%2F20%2FETL%E5%B9%B3%E5%8F%B0%E7%94%9F%E4%BA%A7%E8%AE%A1%E5%88%92%2F</url>
    <content type="text"><![CDATA[模块 功能简介 功效 备注 进度 数据导出 分批定时 易于操作，一个调度时间，多个任务。减少人员修改数据调度的时间【当需要批量修改的时候】 调度的时候不再提供任务参数配置，只能在任务里这一个地方配置 任务恢复 任务执行中失败，修复后，原地爬起 避免重复执行前面已经成功的调度，提高资源使用价值，减少数据恢复需要的时间 方案：抽取azkaban的今日调度最新一次executions，找到失败的job，恢复执行。【看了一下azkaban的mysql数据结构，很难支持】 最后方案：通过生成指定几个etl，重新生成其子节点的调度任务 完成 调度审核 所有需要上线的调度，都需专人review 保证线上集群的稳定运行，将风险降到最低 数据导出依赖感知 数据导出对平日ETL调度的依赖感知 对每日调度产生强依赖，不空跑 细粒度感知： 血统父表完成 粗粒度感知：调度完成 指定重新执行的脉路 生成指定ETL的所有子节点的调度 由于单个节点数据问题，导致所有子节点都要重跑的时候，一键完成 可以与上面的任务回复同效 完成 生成二代ETL模型 抽象出一个有依赖、有自己引擎的ETL模型，m2h/h2h/h2m/spark等都与这个东西关联，生成DAG的时候按照这个东西去生成 解决多重任务之间的依赖调度问题 可以作为2.0计划，改动稍大 HDFS概览 记录指定文件夹每天的数据量变化 各个业务线可以方便的看到自己数据库甚至表的大小变化情况 优先级别并不高，主要是为了提前预知一些突飞猛涨的数据目录，采取限制或者扩展的措施 非正常操作报警 出现数据调度变更，或者ETL对象变更的时候， 如果发起人并不是前一个发起人的话，就发送邮件或者短信通知，告诉前一个发起人，有人修改了你负责的东西 跨部门或者同部门人员出现误操作后，可追溯到责任人 调度可以通过在WillDependencyTask里添加完成，ETL对象可以在通用被继承的对象里完成。最简单可以通过log实现，但是不具备及时性，而且log是会定时清理的 YARN界面开放 开发给业务端开发人员查看，但是需要登陆权限验证，还有就是需要替换掉一些后端的url为代理的url，尽量让业务端少知晓后面的任何事情 辅助业务端人员调试 这个跟上一个功能一样，是个辅助功能, 可以共同放在一个辅助模块中 docker化的nginx 1docker run -d -p 80:80 -p 7070:7070 -p 8088:8088 -p 7000:7000 -p 9000:9000 -p 9090:9090 --name willnginx -v /server/wil/willnginx/conf.d:/etc/nginx/conf.d nginx:1.10]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R语言简记]]></title>
    <url>%2F2017%2F01%2F18%2FR%E8%AF%AD%E8%A8%80%E7%AE%80%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[为新生成的data.frame指定列名：1colnames(df) &lt;- c(&quot;a&quot;,&quot;b&quot;) 12345## 聚合one ~ one, one ~ many, many ~ one, and many ~ many:aggregate(weight ~ feed, data = chickwts, mean)aggregate(breaks ~ wool + tension, data = warpbreaks, mean)aggregate(cbind(Ozone, Temp) ~ Month, data = airquality, mean)aggregate(cbind(ncases, ncontrols) ~ alcgp + tobgp, data = esoph, sum) set.seed=set.seed()是为了设置一个随机数的种子，当想产生同样的一组随机数字的时候，就设置一下。这在统计分析中是经常用到的。 dim=dim()返回一个data.table的行、列数量。12345678910&gt; x &lt;- 1:12 ; dim(x) &lt;- c(3,4)&gt; x [,1] [,2] [,3] [,4][1,] 1 4 7 10[2,] 2 5 8 11[3,] 3 6 9 12&gt; dim(x)[1][1] 3&gt; dim(x)[2][1] 4]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[使用harbor安装docker-registry]]></title>
    <url>%2F2017%2F01%2F17%2F%E4%BD%BF%E7%94%A8harbor%E5%AE%89%E8%A3%85docker-registry%2F</url>
    <content type="text"><![CDATA[1. 安装docker和docker compose123456sudo curl -sSL https://get.docker.com/ | sh# 设置Docker以非Root用户运行，确保安全。sudo usermod -aG docker your-user# 安装Compose：curl -L https://github.com/docker/compose/releases/download/1.7.0-rc1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-composechmod +x /usr/local/bin/docker-compose 获取SSL证书[可选] 如果使用域名绑定私有仓库，就必须开启SSL 123git clone https://github.com/letsencrypt/letsencryptcd letsencrypt./letsencrypt-auto certonly -d docker.will.me 安装harbor 12wget https://github.com/vmware/harbor/releases/download/0.5.0/harbor-offline-installer-0.5.0.tgztar zxvf harbor-offline-installer-0.5.0.tgz 修改一下harbor.cfg里的hostname1hostname = 10.1.5.129 其他的都没敢改，因为安装文档也没有特别深入的说明，暂时跑个demo而已，不要太拼。 但是目前有疑问的有两点： mysql的连接没有指定，只有一个db_password, 那元数据存在哪？ 没有指定docker image的数据文件存储位置，理论上这会是一个比较大的数据量 带着疑问，先开始安装：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061[root@controller harbor]# ./install.sh [Step 0]: checking installation environment ...Note: docker version: 1.12.6Note: docker-compose version: 1.7.0[Step 1]: loading Harbor images ...dd60b611baaa: Loading layer [==================================================&gt;] 133.2 MB/133.2 MBe541edf73dfb: Loading layer [==================================================&gt;] 3.072 kB/3.072 kBLoaded image: vmware/harbor-log:0.5.0&gt; ] 512 B/3.072 kBf96222d75c55: Loading layer [==================================================&gt;] 128.9 MB/128.9 MB5a3deeae1518: Loading layer [==================================================&gt;] 4.608 kB/4.608 kBLoaded image: vmware/harbor-db:0.5.0 ] 512 B/4.608 kB7b612c5c8104: Loading layer [==================================================&gt;] 1.536 kB/1.536 kB5c07a2fe7c73: Loading layer [==================================================&gt;] 20.94 MB/20.94 MBLoaded image: vmware/harbor-jobservice:0.5.0 ] 229.4 kB/20.94 MBfe4c16cbf7a4: Loading layer [==================================================&gt;] 128.9 MB/128.9 MBc4a8b7411af4: Loading layer [==================================================&gt;] 60.57 MB/60.57 MB3f117c44afbb: Loading layer [==================================================&gt;] 3.584 kB/3.584 kBLoaded image: nginx:1.11.5r [=======&gt; ] 512 B/3.584 kB4fe15f8d0ae6: Loading layer [==================================================&gt;] 5.046 MB/5.046 MB3bb5bc5ad373: Loading layer [==================================================&gt;] 2.048 kB/2.048 kBLoaded image: registry:2.5.0[============&gt; ] 512 B/2.048 kBLoaded image: photon:1.041be4a382617: Loading layer [==================================================&gt;] 58.31 MB/58.31 MB32ec73a38f19: Loading layer [==================================================&gt;] 23.62 MB/23.62 MBLoaded image: vmware/harbor-ui:0.5.0 ] 262.1 kB/23.62 MB[Step 2]: preparing environment ...generated and saved secret keyGenerated configuration file: ./common/config/nginx/nginx.confGenerated configuration file: ./common/config/ui/envGenerated configuration file: ./common/config/ui/app.confGenerated configuration file: ./common/config/registry/config.ymlGenerated configuration file: ./common/config/db/envGenerated configuration file: ./common/config/jobservice/envGenerated configuration file: ./common/config/jobservice/app.confGenerated configuration file: ./common/config/ui/private_key.pemGenerated configuration file: ./common/config/registry/root.crtThe configuration files are ready, please use docker-compose to start the service.[Step 3]: checking existing instance of Harbor ...[Step 4]: starting Harbor ...Creating network &quot;harbor_default&quot; with the default driverCreating harbor-logCreating registryCreating harbor-dbCreating harbor-uiCreating harbor-jobserviceCreating nginx✔ ----Harbor has been installed and started successfully.----Now you should be able to visit the admin portal at http://10.1.5.129. For more details, please visit https://github.com/vmware/harbor . 可以看到这是安装了一系列的docker iamge啊，难怪有离线和在线之分了。 闲着没事儿，看一下install.sh都干了啥1234567891011121314151617181920212223242526272829303132# 检查docker和docker-compose是不是符合版本要求check_dockercheck_dockercompose# load harbor的压缩包到当前docker的images库里if [ -f harbor*.tgz ]then h2 &quot;[Step $item]: loading Harbor images ...&quot;; let item+=1 docker load -i ./harbor*.tgzfiecho &quot;&quot;# prepare就是读取harbor.cfg配置文件，准备环境变量h2 &quot;[Step $item]: preparing environment ...&quot;; let item+=1if [ -n &quot;$host&quot; ]then sed &quot;s/^hostname = .*/hostname = $host/g&quot; -i ./harbor.cfgfi./prepare# 查看有没有正在运行的harbor实例，有的话就停掉h2 &quot;[Step $item]: checking existing instance of Harbor ...&quot;; let item+=1if [ -n &quot;$(docker-compose -f docker-compose*.yml ps -q)&quot; ]then note &quot;stopping existing Harbor instance ...&quot; docker-compose -f docker-compose*.yml downfiecho &quot;&quot;# 启动新的harbor实例h2 &quot;[Step $item]: starting Harbor ...&quot;docker-compose -f docker-compose*.yml up -d 以上是主要步骤，其他的全都是不重要的。 查看一下当前的docker进程：12345678[root@controller harbor]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESb8ed2fce6816 nginx:1.11.5 &quot;nginx -g &apos;daemon off&quot; 5 minutes ago Up 4 minutes 0.0.0.0:80-&gt;80/tcp, 0.0.0.0:443-&gt;443/tcp nginx8408e2b03a4b vmware/harbor-jobservice:0.5.0 &quot;/harbor/harbor_jobse&quot; 5 minutes ago Up 4 minutes harbor-jobservice48b280c1caba vmware/harbor-ui:0.5.0 &quot;/harbor/harbor_ui&quot; 6 minutes ago Up 5 minutes harbor-ui0235e55493d0 vmware/harbor-db:0.5.0 &quot;docker-entrypoint.sh&quot; 6 minutes ago Up 5 minutes 3306/tcp harbor-db5bc55096ba78 library/registry:2.5.0 &quot;/entrypoint.sh serve&quot; 6 minutes ago Up 5 minutes 5000/tcp registry805f744b40ae vmware/harbor-log:0.5.0 &quot;/bin/sh -c &apos;crond &amp;&amp;&quot; 6 minutes ago Up 6 minutes 0.0.0.0:1514-&gt;514/tcp harbor-log 该起来的全起来了，下面看一下harbor-db的参数，把数据弄到哪里去了？ docker inspect 看了一下123456789&quot;Mounts&quot;: [ &#123; &quot;Source&quot;: &quot;/data/database&quot;, &quot;Destination&quot;: &quot;/var/lib/mysql&quot;, &quot;Mode&quot;: &quot;rw&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125; ], mysql数据放到/data/database了。 再看一下registry的挂载情况：12345678910111213141516171819202122232425&quot;Mounts&quot;: [ &#123; &quot;Source&quot;: &quot;/data/registry&quot;, &quot;Destination&quot;: &quot;/storage&quot;, &quot;Mode&quot;: &quot;rw&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Source&quot;: &quot;/server/harbor/common/config/registry&quot;, &quot;Destination&quot;: &quot;/etc/registry&quot;, &quot;Mode&quot;: &quot;rw&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;rprivate&quot; &#125;, &#123; &quot;Name&quot;: &quot;e8aab221ad99604d0f660695895eec3ff425cc0847ac5f0a3a803ab83e70275a&quot;, &quot;Source&quot;: &quot;/var/lib/docker/volumes/e8aab221ad99604d0f660695895eec3ff425cc0847ac5f0a3a803ab83e70275a/_data&quot;, &quot;Destination&quot;: &quot;/var/lib/registry&quot;, &quot;Driver&quot;: &quot;local&quot;, &quot;Mode&quot;: &quot;&quot;, &quot;RW&quot;: true, &quot;Propagation&quot;: &quot;&quot; &#125; ], 此时以上两个目录还都是空的12345678910111213[root@controller harbor]# ll /server/harbor/common/config/registrytotal 8-rw-r--r-- 1 root root 694 Jan 18 03:05 config.yml-rw-r--r-- 1 root root 2151 Jan 18 03:05 root.crt[root@controller harbor]# ll /data/databasetotal 110608-rw-rw---- 1 systemd-bus-proxy ssh_keys 56 Jan 18 03:07 auto.cnf-rw-rw---- 1 systemd-bus-proxy ssh_keys 12582912 Jan 18 03:25 ibdata1-rw-rw---- 1 systemd-bus-proxy ssh_keys 50331648 Jan 18 03:25 ib_logfile0-rw-rw---- 1 systemd-bus-proxy ssh_keys 50331648 Jan 18 03:06 ib_logfile1drwx------ 2 systemd-bus-proxy ssh_keys 4096 Jan 18 03:07 mysqldrwx------ 2 systemd-bus-proxy ssh_keys 4096 Jan 18 03:07 performance_schemadrwx------ 2 systemd-bus-proxy ssh_keys 4096 Jan 18 03:07 registry 参考：https://github.com/vmware/harbor/blob/master/docs/installation_guide.md 4. 使用netstat -ntlp 发现80端口被启动了，应该就是它。下面就登陆一下harbor看一下 docker-compose.yml配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495version: '2'services: log: image: vmware/harbor-log:0.5.0 container_name: harbor-log restart: always volumes: - /var/log/harbor/:/var/log/docker/ ports: - 1514:514 registry: image: library/registry:2.5.0 container_name: registry restart: always volumes: - /data/registry:/storage - ./common/config/registry/:/etc/registry/ environment: - GODEBUG=netdns=cgo command: ["serve", "/etc/registry/config.yml"] depends_on: - log logging: driver: "syslog" options: syslog-address: "tcp://127.0.0.1:1514" tag: "registry" mysql: image: vmware/harbor-db:0.5.0 container_name: harbor-db restart: always volumes: - /data/database:/var/lib/mysql env_file: - ./common/config/db/env depends_on: - log logging: driver: "syslog" options: syslog-address: "tcp://127.0.0.1:1514" tag: "mysql" ui: image: vmware/harbor-ui:0.5.0 container_name: harbor-ui env_file: - ./common/config/ui/env restart: always volumes: - ./common/config/ui/app.conf:/etc/ui/app.conf - ./common/config/ui/private_key.pem:/etc/ui/private_key.pem - /data:/harbor_storage depends_on: - log logging: driver: "syslog" options: syslog-address: "tcp://127.0.0.1:1514" tag: "ui" jobservice: image: vmware/harbor-jobservice:0.5.0 container_name: harbor-jobservice env_file: - ./common/config/jobservice/env restart: always volumes: - /data/job_logs:/var/log/jobs - ./common/config/jobservice/app.conf:/etc/jobservice/app.conf depends_on: - ui logging: driver: "syslog" options: syslog-address: "tcp://127.0.0.1:1514" tag: "jobservice" proxy: image: nginx:1.11.5 container_name: nginx restart: always volumes: - ./common/config/nginx:/etc/nginx ports: - 80:80 - 443:443 depends_on: - mysql - registry - ui - log logging: driver: "syslog" options: syslog-address: "tcp://127.0.0.1:1514" tag: "proxy" 上传镜像找一个安装了docker的机器，在/etc/docker/daemon.json文件中指定insecure-registries【因为我们上面并没有启用HTTPS协议】12345&#123; "insecure-registries": ["10.1.5.129"], "registry-mirrors": ["https://docker.mirrors.ustc.edu.cn","http://3a35aff6.m.daocloud.io","http://ethanzhu.m.tenxcloud.net"]&#125; 把本地的镜像上传到1234567891011121314151617181920212223242526[root@dnode2 ~]# docker tag willr 10.1.5.129/library/willr:0.1[root@dnode2 ~]# docker login 10.1.5.129Username: adminPassword: Login Succeeded[root@dnode2 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE10.1.5.129/library/willr 0.1 2a33c792d4aa 5 hours ago 718.7 MBwillr latest 2a33c792d4aa 5 hours ago 718.7 MB&lt;none&gt; &lt;none&gt; 3dabf60e7d97 17 hours ago 714.7 MBuhopper/hadoop-nodemanager 2.7.2 046a3f819f63 41 hours ago 715.4 MBr-base 3.3.2 dbee2cbef542 4 weeks ago 682.8 MBr-base latest dbee2cbef542 4 weeks ago 682.8 MB[root@dnode2 ~]# docker push 10.1.5.129/library/willr:0.1The push refers to a repository [10.1.5.129/library/willr]0748650e84ed: Pushed 72547fe7a086: Pushing [==================================================&gt;] 509.8 MB2cd05b2f5db4: Pushed 72547fe7a086: Pushed c610e92140d8: Pushed 3b04b5a81444: Pushed 29d255cec883: Pushed 29d255cec883: Preparing 0.1: digest: sha256:9eb5d67aa235a1d7724b659310e8947c7e6cf967bef7ae02b955225fc7eb193a size: 1791[root@dnode2 ~]# 页面中的显示 参考： http://www.jianshu.com/p/141855241f2d https://vmware.github.io/harbor/index_cn.html https://my.oschina.net/u/1540325/blog/702260 http://www.zimug.com/317.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker整理]]></title>
    <url>%2F2017%2F01%2F17%2Fdocker%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[centos7的yum源 清华的docker源https://mirrors.tuna.tsinghua.edu.cn/help/docker/ 阿里的源http://mirrors.aliyun.com/ registry mirror编辑或添加文件 /etc/docker/daemon.json123&#123; &quot;registry-mirrors&quot;: [&quot;https://docker.mirrors.ustc.edu.cn&quot;,&quot;http://3a35aff6.m.daocloud.io&quot;,&quot;http://ethanzhu.m.tenxcloud.net&quot;]&#125; 之后重启docker-daemon就可以了。 参考： http://ethanzhu.leanote.com/post/docker-%E6%9B%B4%E6%94%B9 修改docker daemon配置查了好多资料，都说直接修改/etc/default/docker里的DOCKER_OPTS就可以制定registry mirror、镜像存储路径等各种docker daemon的启动参数，但是试了N次都没有成功， 其实细想一下，从启动脚本入手/etc/systemd/system/docker.service，官方说要在/etc/systemd/system/docker.service.d目录下创建自己的conf文件，修改启动命令的参数。我创建了一个will.conf123[Service]ExecStart=ExecStart=/usr/bin/dockerd --graph=&quot;/server/dspace&quot; 之后reload一下daemon的配置。供参考的参数信息 要修改的主要参数在于启动service里的ExecStart 1systemctl daemon-reload 再去看指定的目录和docker info信息，发现已经修改过来了。123456789101112[root@dnode2 ~]# ll /server/dspace/total 32drwx------ 2 root root 4096 Jan 18 23:48 containersdrwx------ 4 root root 4096 Jan 18 23:48 devicemapperdrwx------ 3 root root 4096 Jan 18 23:48 imagedrwxr-x--- 3 root root 4096 Jan 18 23:48 networkdrwx------ 2 root root 4096 Jan 18 23:48 swarmdrwx------ 2 root root 4096 Jan 18 23:48 tmpdrwx------ 2 root root 4096 Jan 18 23:48 trustdrwx------ 2 root root 4096 Jan 18 23:48 volumes[root@dnode2 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZE 刚才的images也都查不到了，停掉docker daemon，手动把原来的镜像全都copy过来，启动docker daemon就可以了。 综上，其实已经有两种方式配置docker daemon的启动参数了，一个是在service.docker.d目录里添加自己id配置文件will.conf，直接修改启动命令并指定参数。另一个就是编辑/etc/docker/daemon.json文件。 应该是两种方式都可以行得通，但是命令行和json的参数并不完全对应，需要找一下json的mapping key，现在觉得json 的方式会更加友好一些。 参考 官网 https://docs.docker.com/engine/admin/systemd/ http://blog.csdn.net/l6807718/article/details/51325431 杂记= docker容器的cpu和memory是可以调整集群资源参数的，有limit设置； Dockerfile里声明volume的目录是对于任何的volumes命令参数不可变的，会自动生成一个挂载点，目的是不让host直接修改，作为一个数据容器为其他镜像提供volume挂载服务。【–volumes-from】https://docs.docker.com/engine/reference/builder/#/volumehttp://www.cnblogs.com/51kata/p/5266626.html docker swarm= 1. 启动manager，初始化集群manager机器上执行：123456789101112131415161718192021222324252627282930313233[root@controller wil]# docker swarm init --advertise-addr 10.1.5.129Swarm initialized: current node (efpood09fyouiyib1f78y5oa6) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-50wivhmqel8u7pnorhfpu5a8qrfvged69b6r8v06jzscsysds2-3yllw8v8deku690gd8a8pfio1 \ 10.1.5.129:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions.[root@controller wil]# docker info..........Swarm: active NodeID: efpood09fyouiyib1f78y5oa6 Is Manager: true ClusterID: 4wnicl4f2vvtlpmpa2pyj28ji Managers: 1 Nodes: 1 Orchestration: Task History Retention Limit: 5 Raft: Snapshot Interval: 10000 Heartbeat Tick: 1 Election Tick: 3 Dispatcher: Heartbeat Period: 5 seconds CA Configuration: Expiry Duration: 3 months Node Address: 10.1.5.129Runtimes: runc.......... 可以看到初始化后，docker info里显示了swarm相关的信息。 查看当前swarm集群中的机器：123[root@controller wil]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUSefpood09fyouiyib1f78y5oa6 * controller Ready Active Leader * 代表的是当前机器 2.添加node可以在manager节点上确认一下加入集群需要的token：123456[root@controller wil]# docker swarm join-token workerTo add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-50wivhmqel8u7pnorhfpu5a8qrfvged69b6r8v06jzscsysds2-3yllw8v8deku690gd8a8pfio1 \ 10.1.5.129:2377 然后分别在dnode1、dnode2上运行此命令：12345[root@dnode2 willdjango]# docker swarm join \&gt; --token SWMTKN-1-50wivhmqel8u7pnorhfpu5a8qrfvged69b6r8v06jzscsysds2-3yllw8v8deku690gd8a8pfio1 \&gt; 10.1.5.129:2377This node joined a swarm as a worker. 12345[root@dnode1 server]# docker swarm join \&gt; --token SWMTKN-1-50wivhmqel8u7pnorhfpu5a8qrfvged69b6r8v06jzscsysds2-3yllw8v8deku690gd8a8pfio1 \&gt; 10.1.5.129:2377This node joined a swarm as a worker. 再查看当前集群node状态, 此命令只能在manager所在的节点上执行，worker节点没有权限：12345[root@controller wil]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS0c67km60csfft6paxdsodp8ss dnode2 Ready Active 759vbg6zxxzffj4i3pw21249y dnode1 Ready Active efpood09fyouiyib1f78y5oa6 * controller Ready Active Leader 3. 部署服务创建一个service：12345[root@controller wil]# docker service create --replicas 1 --name resourcemanager_swarm willcup/yarn:0.1 /bin/bash0nnc9cpf1ebhbw6ixbro9q8wn[root@controller wil]# docker service lsID NAME REPLICAS IMAGE COMMAND0nnc9cpf1ebh resourcemanager_swarm 0/1 willcup/yarn:0.1 /bin/bash 查看这个service的元信息：12345678910111213[root@controller wil]# docker service inspect --pretty resourcemanager_swarmID: 0nnc9cpf1ebhbw6ixbro9q8wnName: resourcemanager_swarmMode: Replicated Replicas: 1Placement:UpdateConfig: Parallelism: 1 On failure: pauseContainerSpec: Image: willcup/yarn:0.1 Args: /bin/bashResources: 查看这个service的进程所在：1234[root@controller wil]# docker service ps resourcemanager_swarmID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdss32jnwz7aaxtqemvxpke4gg resourcemanager_swarm.1 willcup/yarn:0.1 dnode2 Running Preparing 5 minutes ago 1az4a1ctb9y3io2oswew0zab9 \_ resourcemanager_swarm.1 willcup/yarn:0.1 controller Shutdown Complete 2 minutes ago 像这个我们的例子中就是，在controller中启动未成功，然后转到dnode1上运行了。但是两次执行的DESIRED STATE和CURRENT STATE并不吻合。看了一下是我们的manager机器【也就是上面的controller】并不认识dnode2，配置一下hosts。再查看一下：123456789101112[root@controller wil]# docker service ps resourcemanager_swarmID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORbvrtqkl8iybt5rr5kqlj6mio2 resourcemanager_swarm.1 willcup/yarn:0.1 controller Running Running 10 seconds ago dss32jnwz7aaxtqemvxpke4gg \_ resourcemanager_swarm.1 willcup/yarn:0.1 dnode2 Shutdown Complete 4 minutes ago 1az4a1ctb9y3io2oswew0zab9 \_ resourcemanager_swarm.1 willcup/yarn:0.1 controller Shutdown Complete 10 minutes ago [root@controller wil]# docker service ps resourcemanager_swarmID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORc2btlmupl7r9ss3dents28928 resourcemanager_swarm.1 willcup/yarn:0.1 controller Running Preparing 22 seconds ago 99hilqo2an723j9k7n2grkf0o \_ resourcemanager_swarm.1 willcup/yarn:0.1 dnode2 Shutdown Complete 3 minutes ago a16ktljr241g9zn6qluunx84l \_ resourcemanager_swarm.1 willcup/yarn:0.1 controller Shutdown Complete about a minute ago c52kek07d8uve9kqjsf3vcgh3 \_ resourcemanager_swarm.1 willcup/yarn:0.1 dnode2 Shutdown Complete 6 minutes ago bvrtqkl8iybt5rr5kqlj6mio2 \_ resourcemanager_swarm.1 willcup/yarn:0.1 controller Shutdown Complete 4 minutes ago 总是出问题。。。 还是用官网的吧：1234567891011121314[root@controller wil]# docker service create --replicas 1 --name helloworld alpine ping docker.com2q5p0ahn92aa8be9el23j93yx[root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdvjcrjztbqkj3x5e8fggraw0f helloworld.1 alpine controller Running Preparing 6 seconds ago [root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdvjcrjztbqkj3x5e8fggraw0f helloworld.1 alpine controller Running Starting 4 seconds ago [root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdvjcrjztbqkj3x5e8fggraw0f helloworld.1 alpine controller Running Starting 8 seconds ago [root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdvjcrjztbqkj3x5e8fggraw0f helloworld.1 alpine controller Running Running about a minute ago 说是在controller上运行着的，我们就到controller机器上ps一下：123[root@controller wil]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5653c87be989 alpine:latest &quot;ping docker.com&quot; 4 minutes ago Up 3 minutes helloworld.1.dvjcrjztbqkj3x5e8fggraw0f 4. 服务扩容对于后台服务，很多时候我们是部署多个，然后再前面部署负载均衡的。我们可以通过swarm的命令轻松的将我们的服务进行扩容：12345678910111213141516[root@controller wil]# docker service scale helloworld=5helloworld scaled to 5[root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdvjcrjztbqkj3x5e8fggraw0f helloworld.1 alpine controller Running Running 5 minutes ago 5x3rsxhw31w5prpwhvh5bzn8m helloworld.2 alpine dnode2 Running Preparing 2 minutes ago 1ymr6qzfzvpjc86lwx7c7p938 helloworld.3 alpine dnode1 Running Preparing 16 seconds ago 3h36sr8m8c4f7utei50ss9tav helloworld.4 alpine dnode1 Running Preparing 16 seconds ago eneop6ooju630tzpxt8vlftjw helloworld.5 alpine controller Running Preparing less than a second ago [root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORdvjcrjztbqkj3x5e8fggraw0f helloworld.1 alpine controller Running Running 7 minutes ago 5x3rsxhw31w5prpwhvh5bzn8m helloworld.2 alpine dnode2 Running Running 3 minutes ago 1ymr6qzfzvpjc86lwx7c7p938 helloworld.3 alpine dnode1 Running Running about a minute ago 3h36sr8m8c4f7utei50ss9tav helloworld.4 alpine dnode1 Running Running about a minute ago eneop6ooju630tzpxt8vlftjw helloworld.5 alpine controller Running Running about a minute ago 可以看到，实现了扩容，自动将container重命名，并且分布到了不同的节点之上。 5. 删除服务1234[root@controller wil]# docker service rm helloworldhelloworld[root@controller wil]# docker service ps helloworldError: No such service: helloworld 虽然service很快就被删除了，但是其实container是需要一些时间才能清理干净的。开始的时候docker ps还能看到部分，稍等一下就没有了：123456789[root@controller wil]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES5653c87be989 alpine:latest &quot;ping docker.com&quot; 10 minutes ago Removal In Progress helloworld.1.dvjcrjztbqkj3x5e8fggraw0ffb3c81306678 a6a1f5eb98f2 &quot;bash /entrypoint.sh &quot; 3 hours ago Exited (1) 3 hours ago composetest_nodemanager_1c191fc0db545 a6a1f5eb98f2 &quot;bash /entrypoint.sh &quot; 19 hours ago Exited (137) 3 hours ago composetest_resourcemanager_1[root@controller wil]# docker ps -aCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESfb3c81306678 a6a1f5eb98f2 &quot;bash /entrypoint.sh &quot; 3 hours ago Exited (1) 3 hours ago composetest_nodemanager_1c191fc0db545 a6a1f5eb98f2 &quot;bash /entrypoint.sh &quot; 19 hours ago Exited (137) 3 hours ago composetest_resourcemanager_1 5. 节点维护有的时候我们要临时维护几台节点，就需要把这个节点标记下线，为了不影响服务质量，需要把正在运行的服务转移到其他节点上。swarm会自动为我们做这些工作，我们只需要标记某个节点为drain就可以了。1234567891011121314151617181920212223242526272829303132[root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORciw4f1ibuwuqjv2hvn6s1mqs9 helloworld.1 alpine dnode1 Running Running about a minute ago 0ezba4uegtfmajlkw6kbw66q5 helloworld.2 alpine dnode2 Running Running 4 minutes ago dnb7d4cjwes3ubywn6iv2vpiu helloworld.3 alpine controller Running Running about a minute ago dyidqdhh7n04m0sk3k0fy74yp helloworld.4 alpine controller Running Running about a minute ago 66lmmu30fywqqey7lpirz5o6s helloworld.5 alpine dnode1 Running Running about a minute ago [root@controller wil]# docker node update --availability drain dnode1dnode1[root@controller wil]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS0c67km60csfft6paxdsodp8ss dnode2 Ready Active 759vbg6zxxzffj4i3pw21249y dnode1 Ready Drain efpood09fyouiyib1f78y5oa6 * controller Ready Active Leader[root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9sxv9bi0j7yy341rtp6hkrm0e helloworld.1 alpine dnode2 Ready Preparing 3 minutes ago ciw4f1ibuwuqjv2hvn6s1mqs9 \_ helloworld.1 alpine dnode1 Shutdown Running 2 minutes ago 0ezba4uegtfmajlkw6kbw66q5 helloworld.2 alpine dnode2 Running Running 4 minutes ago dnb7d4cjwes3ubywn6iv2vpiu helloworld.3 alpine controller Running Running about a minute ago dyidqdhh7n04m0sk3k0fy74yp helloworld.4 alpine controller Running Running about a minute ago 8ozxlg29d4ynko3plauupuuu5 helloworld.5 alpine dnode2 Ready Preparing 3 minutes ago 66lmmu30fywqqey7lpirz5o6s \_ helloworld.5 alpine dnode1 Shutdown Running 2 minutes ago [root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9sxv9bi0j7yy341rtp6hkrm0e helloworld.1 alpine dnode2 Running Preparing 3 minutes ago ciw4f1ibuwuqjv2hvn6s1mqs9 \_ helloworld.1 alpine dnode1 Shutdown Shutdown 19 seconds ago 0ezba4uegtfmajlkw6kbw66q5 helloworld.2 alpine dnode2 Running Running 5 minutes ago dnb7d4cjwes3ubywn6iv2vpiu helloworld.3 alpine controller Running Running about a minute ago dyidqdhh7n04m0sk3k0fy74yp helloworld.4 alpine controller Running Running about a minute ago 8ozxlg29d4ynko3plauupuuu5 helloworld.5 alpine dnode2 Running Preparing 3 minutes ago 66lmmu30fywqqey7lpirz5o6s \_ helloworld.5 alpine dnode1 Shutdown Shutdown 20 seconds ago 当维护完成之后，再把node状态更新回来：12345678910111213141516[root@controller wil]# docker node update --availability active dnode1dnode1[root@controller wil]# docker service ps helloworldID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR9sxv9bi0j7yy341rtp6hkrm0e helloworld.1 alpine dnode2 Running Running 4 minutes ago ciw4f1ibuwuqjv2hvn6s1mqs9 \_ helloworld.1 alpine dnode1 Shutdown Shutdown 2 minutes ago 0ezba4uegtfmajlkw6kbw66q5 helloworld.2 alpine dnode2 Running Running 6 minutes ago dnb7d4cjwes3ubywn6iv2vpiu helloworld.3 alpine controller Running Running 3 minutes ago dyidqdhh7n04m0sk3k0fy74yp helloworld.4 alpine controller Running Running 3 minutes ago 8ozxlg29d4ynko3plauupuuu5 helloworld.5 alpine dnode2 Running Running 4 minutes ago 66lmmu30fywqqey7lpirz5o6s \_ helloworld.5 alpine dnode1 Shutdown Shutdown 2 minutes ago [root@controller wil]# docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS0c67km60csfft6paxdsodp8ss dnode2 Ready Active 759vbg6zxxzffj4i3pw21249y dnode1 Ready Active efpood09fyouiyib1f78y5oa6 * controller Ready Active Leader 但是服务如果不出问题的话，不会自动迁移回dnode1了。 通常我们可以让manager节点是drain的，以保证它负载低，集群能正常运行。 6. 路由所有的swarm节点都加入了一个ingress的routing mesh。 1234567[root@controller wil]# docker network lsNETWORK ID NAME DRIVER SCOPE6461c2e9e64e bridge bridge local 5d5d0be2f5a6 docker_gwbridge bridge local e2bbfd5c0b62 host host local 0r5dk7ja8vw7 ingress overlay swarm 01c12cd7bba7 none null local routing mesh会把所有对于public port的请求转发到所有对应service的节点的container上。 先搞一个有public接口的service:12345678910[root@controller wil]# docker service create \&gt; --name my-web \&gt; --publish 8080:80 \&gt; --replicas 2 \&gt; nginx2lsi4py6pbcu96ttnjyqj7u4i[root@controller wil]# docker service ps my-webID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR84xjxx4kth0idnzuob4186cn7 my-web.1 nginx dnode1 Running Preparing 20 seconds ago 0fqk2zwyp9u2ohi9iu91qli50 my-web.2 nginx dnode2 Running Preparing 3 minutes ago 这个命令会在所有的node上都开放8080端口，swarm的load balancer会把对于任何一个node的8080接口的请求路由给active的container【这里就是dnode1和dnode2】。 也可以为正在运行的服务添加开放的端口映射：123$ docker service update \ --publish-add &lt;PUBLISHED-PORT&gt;:&lt;TARGET-PORT&gt; \ &lt;SERVICE&gt; 查看某service开放的端口：12[root@controller wil]# docker service inspect --format=&quot;&#123;&#123;json .Endpoint.Spec.Ports&#125;&#125;&quot; my-web[&#123;&quot;Protocol&quot;:&quot;tcp&quot;,&quot;TargetPort&quot;:80,&quot;PublishedPort&quot;:8080&#125;] 单独指定开放tcp/udp端口，默认就是TCP：12$ docker service create --name dns-cache -p 53:53/tcp dns-cache$ docker service create --name dns-cache -p 53:53/udp dns-cache 因为目前是有三个node的8080都可以提供服务，所以我们也可以在他们之外再加个HAProxy做负载均衡。其实如果swarm自己的调度器能够均匀分发请求给不同的container的话，也没有必要弄外部的LB了。 7. 其他另外，创建服务的时候和compose类似，也是有很多参数可以指定的123456789101112131415161718192021222324252627282930313233343536[root@controller wil]# docker service create --helpUsage: docker service create [OPTIONS] IMAGE [COMMAND] [ARG...]Create a new serviceOptions: --constraint value Placement constraints (default []) --container-label value Container labels (default []) --endpoint-mode string Endpoint mode (vip or dnsrr) -e, --env value Set environment variables (default []) --help Print usage -l, --label value Service labels (default []) --limit-cpu value Limit CPUs (default 0.000) --limit-memory value Limit Memory (default 0 B) --log-driver string Logging driver for service --log-opt value Logging driver options (default []) --mode string Service mode (replicated or global) (default &quot;replicated&quot;) --mount value Attach a mount to the service --name string Service name --network value Network attachments (default []) -p, --publish value Publish a port as a node port (default []) --replicas value Number of tasks (default none) --reserve-cpu value Reserve CPUs (default 0.000) --reserve-memory value Reserve Memory (default 0 B) --restart-condition string Restart when condition is met (none, on-failure, or any) --restart-delay value Delay between restart attempts (default none) --restart-max-attempts value Maximum number of restarts before giving up (default none) --restart-window value Window used to evaluate the restart policy (default none) --stop-grace-period value Time to wait before force killing a container (default none) --update-delay duration Delay between updates --update-failure-action string Action on update failure (pause|continue) (default &quot;pause&quot;) --update-parallelism uint Maximum number of tasks updated simultaneously (0 to update all at once) (default 1) -u, --user string Username or UID --with-registry-auth Send registry authentication details to swarm agents -w, --workdir string Working directory inside the container 需要注意的是：慎重使用mount。原因有以下三点： 如果mount到host path上，那么需要每个node都有同样的path和文件。 需要确保所有node上的这个文件一直是可用的状态 这种方式完全就是反设计的。因为不能保证生产和测试的mount文件是一致的，从而导致运行出问题。 参考：https://docs.docker.com/engine/swarm/services/ 可以通过demote和promote改变node的角色。 重大发现： docker是有python的API的，可以用API管理集群： https://docker-py.readthedocs.io/en/stable/services.html。 坑1.12版本的swarm模式里并不能获取已经失败了的service的container的日志： 1234567[root@controller wil]# docker service ps yarnID NAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORc317x0ovq833c6g83hi4iwzz4 yarn.1 willcup/yarn:0.1 controller Running Running 2 seconds ago 2w5k9c4efh8h188ompgnfvh57 \_ yarn.1 willcup/yarn:0.1 dnode1 Shutdown Failed about a minute ago &quot;task: non-zero exit (127)&quot;4jvdu7nz8kxvnd9wa9qzmy1pw \_ yarn.1 willcup/yarn:0.1 controller Shutdown Failed about a minute ago &quot;task: non-zero exit (127)&quot;5yu69ugxmndhfdetvd9kthel2 \_ yarn.1 willcup/yarn:0.1 dnode1 Shutdown Failed 3 minutes ago &quot;task: non-zero exit (127)&quot;7qyx81ptxouv6883h1o1pbiti \_ yarn.1 willcup/yarn:0.1 controller Shutdown Failed 5 minutes ago &quot;task: non-zero exit (127)&quot; 上面失败的就都找不到了。而且rm service以后，那些container都会被自动清理掉。 对于某些功能支持的不完善. 参考： http://www.jianshu.com/p/3efee6927833 https://github.com/docker/docker/issues/24812]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparkR安装历程]]></title>
    <url>%2F2017%2F01%2F09%2FSparkR%E5%AE%89%E8%A3%85%E5%8E%86%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[在centos上安装一下R的基础包。 123yum install -y RR CMD javareconf 安装Rstudio server 123wget https://download2.rstudio.org/rstudio-server-rhel-1.0.136-x86_64.rpmyum install --nogpgcheck rstudio-server-rhel-1.0.136-x86_64.rpmrstudio-server verify-installation 为Rstudio server添加用户，默认是使用linux用户的 12useradd rtestpasswd rtest R安装依赖为R安装sparkR的库 1R CMD INSTALL /usr/hdp/current/spark-client/R/lib/SparkR/ 安装forecast的时候报错, g++版本低，通过安装devtools-2解决：1234sudo rpm --import http://ftp.scientificlinux.org/linux/scientific/5x/x86_64/RPM-GPG-KEYs/RPM-GPG-KEY-cernwget -O /etc/yum.repos.d/slc6-devtoolset.repo http://linuxsoft.cern.ch/cern/devtoolset/slc6-devtoolset.reposudo yum install devtoolset-2scl enable devtoolset-2 bash 这样版本就已经好了。1234567891011$ gcc --versiongcc (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)...$ g++ --versiong++ (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)...$ gfortran --versionGNU Fortran (GCC) 4.8.2 20140120 (Red Hat 4.8.2-15)... 参考： https://gist.github.com/stephenturner/e3bc5cfacc2dc67eca8b 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286&gt; sc&lt;-sparkR.init(master = &quot;yarn-client&quot;)Launching java with spark-submit command spark-submit sparkr-shell /tmp/RtmpERrwOX/backend_port628c57c68104 17/01/10 15:28:50 INFO SparkContext: Running Spark version 1.5.217/01/10 15:28:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/01/10 15:28:54 INFO SecurityManager: Changing view acls to: zhangjiayi17/01/10 15:28:54 INFO SecurityManager: Changing modify acls to: zhangjiayi17/01/10 15:28:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhangjiayi); users with modify permissions: Set(zhangjiayi)17/01/10 15:29:00 INFO Slf4jLogger: Slf4jLogger started17/01/10 15:29:00 INFO Remoting: Starting remoting17/01/10 15:29:01 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.1.5.65:34109]17/01/10 15:29:01 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 34109.17/01/10 15:29:01 INFO SparkEnv: Registering MapOutputTracker17/01/10 15:29:01 INFO SparkEnv: Registering BlockManagerMaster17/01/10 15:29:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ce31222a-c674-4391-be3c-43b075e758d417/01/10 15:29:02 INFO MemoryStore: MemoryStore started with capacity 530.0 MB17/01/10 15:29:02 INFO HttpFileServer: HTTP File server directory is /tmp/spark-c2196ae2-3c1b-46e0-97dd-583cd7a240a3/httpd-af4e2720-d455-4d5a-93de-6a0e7231da3417/01/10 15:29:02 INFO HttpServer: Starting HTTP Server17/01/10 15:29:03 INFO Server: jetty-8.y.z-SNAPSHOT17/01/10 15:29:03 INFO AbstractConnector: Started SocketConnector@0.0.0.0:4470817/01/10 15:29:03 INFO Utils: Successfully started service &apos;HTTP file server&apos; on port 44708.17/01/10 15:29:03 INFO SparkEnv: Registering OutputCommitCoordinator17/01/10 15:29:03 INFO Server: jetty-8.y.z-SNAPSHOT17/01/10 15:29:03 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:404017/01/10 15:29:03 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.17/01/10 15:29:03 INFO SparkUI: Started SparkUI at http://10.1.5.65:404017/01/10 15:29:04 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.spark.yarn.driver.memoryOverhead is set but does not apply in client mode.17/01/10 15:29:05 INFO TimelineClientImpl: Timeline service address: http://slavenode4:8188/ws/v1/timeline/17/01/10 15:29:06 INFO RMProxy: Connecting to ResourceManager at slavenode3/10.1.5.64:805017/01/10 15:29:11 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.17/01/10 15:29:12 INFO Client: Requesting a new application from cluster with 5 NodeManagers17/01/10 15:29:12 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (5120 MB per container)17/01/10 15:29:12 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead17/01/10 15:29:12 INFO Client: Setting up container launch context for our AM17/01/10 15:29:12 INFO Client: Setting up the launch environment for our AM container17/01/10 15:29:12 INFO Client: Preparing resources for our AM container17/01/10 15:29:12 INFO Client: Uploading resource file:/usr/hdp/2.3.4.0-3485/spark/lib/spark-assembly-1.5.2.2.3.4.0-3485-hadoop2.7.1.2.3.4.0-3485.jar -&gt; hdfs://masternode:8020/user/zhangjiayi/.sparkStaging/application_1483512430911_0002/spark-assembly-1.5.2.2.3.4.0-3485-hadoop2.7.1.2.3.4.0-3485.jar17/01/10 15:29:22 INFO Client: Uploading resource file:/tmp/spark-c2196ae2-3c1b-46e0-97dd-583cd7a240a3/__spark_conf__1393429344923620787.zip -&gt; hdfs://masternode:8020/user/zhangjiayi/.sparkStaging/application_1483512430911_0002/__spark_conf__1393429344923620787.zip17/01/10 15:29:22 INFO SecurityManager: Changing view acls to: zhangjiayi17/01/10 15:29:22 INFO SecurityManager: Changing modify acls to: zhangjiayi17/01/10 15:29:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhangjiayi); users with modify permissions: Set(zhangjiayi)17/01/10 15:29:22 INFO Client: Submitting application 2 to ResourceManager17/01/10 15:29:22 INFO YarnClientImpl: Submitted application application_1483512430911_000217/01/10 15:29:22 INFO YarnExtensionServices: Starting Yarn extension services with app application_1483512430911_0002 and attemptId None17/01/10 15:29:22 INFO YarnHistoryService: Starting YarnHistoryService for application application_1483512430911_0002 attempt None; state=1; endpoint=http://slavenode4:8188/ws/v1/timeline/; bonded to ATS=false; listening=false; batchSize=10; flush count=0; total number queued=0, processed=0; attempted entity posts=0 successful entity posts=0 failed entity posts=0; events dropped=0; app start event received=false; app end event received=false;17/01/10 15:29:22 INFO YarnHistoryService: Spark events will be published to the Timeline service at http://slavenode4:8188/ws/v1/timeline/17/01/10 15:29:22 INFO TimelineClientImpl: Timeline service address: http://slavenode4:8188/ws/v1/timeline/17/01/10 15:29:22 INFO YarnHistoryService: History Service listening for events: YarnHistoryService for application application_1483512430911_0002 attempt None; state=1; endpoint=http://slavenode4:8188/ws/v1/timeline/; bonded to ATS=true; listening=true; batchSize=10; flush count=0; total number queued=0, processed=0; attempted entity posts=0 successful entity posts=0 failed entity posts=0; events dropped=0; app start event received=false; app end event received=false;17/01/10 15:29:22 INFO YarnExtensionServices: Service org.apache.spark.deploy.yarn.history.YarnHistoryService started17/01/10 15:29:23 INFO Client: Application report for application_1483512430911_0002 (state: ACCEPTED)17/01/10 15:29:23 INFO Client: client token: N/A diagnostics: N/A ApplicationMaster host: N/A ApplicationMaster RPC port: -1 queue: default start time: 1484033362415 final status: UNDEFINED tracking URL: http://slavenode3:8088/proxy/application_1483512430911_0002/ user: zhangjiayi17/01/10 15:29:24 INFO Client: Application report for application_1483512430911_0002 (state: ACCEPTED)17/01/10 15:29:25 INFO Client: Application report for application_1483512430911_0002 (state: ACCEPTED)17/01/10 15:29:26 INFO Client: Application report for application_1483512430911_0002 (state: ACCEPTED)17/01/10 15:29:27 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as AkkaRpcEndpointRef(Actor[akka.tcp://sparkYarnAM@10.1.5.66:34209/user/YarnAM#-468278157])17/01/10 15:29:27 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -&gt; slavenode3, PROXY_URI_BASES -&gt; http://slavenode3:8088/proxy/application_1483512430911_0002), /proxy/application_1483512430911_000217/01/10 15:29:27 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter17/01/10 15:29:27 INFO Client: Application report for application_1483512430911_0002 (state: RUNNING)17/01/10 15:29:27 INFO Client: client token: N/A diagnostics: N/A ApplicationMaster host: 10.1.5.66 ApplicationMaster RPC port: 0 queue: default start time: 1484033362415 final status: UNDEFINED tracking URL: http://slavenode3:8088/proxy/application_1483512430911_0002/ user: zhangjiayi17/01/10 15:29:27 INFO YarnClientSchedulerBackend: Application application_1483512430911_0002 has started running.17/01/10 15:29:27 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 44133.17/01/10 15:29:27 INFO NettyBlockTransferService: Server created on 4413317/01/10 15:29:27 INFO BlockManagerMaster: Trying to register BlockManager17/01/10 15:29:27 INFO BlockManagerMasterEndpoint: Registering block manager 10.1.5.65:44133 with 530.0 MB RAM, BlockManagerId(driver, 10.1.5.65, 44133)17/01/10 15:29:27 INFO BlockManagerMaster: Registered BlockManager17/01/10 15:29:28 INFO YarnHistoryService: Application started: SparkListenerApplicationStart(SparkR,Some(application_1483512430911_0002),1484033330463,zhangjiayi,None,None)17/01/10 15:29:28 INFO YarnHistoryService: About to POST entity application_1483512430911_0002 with 3 events to timeline service http://slavenode4:8188/ws/v1/timeline/17/01/10 15:29:34 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000(ms)&gt; 17/01/10 15:29:35 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@slavenode3:41731/user/Executor#207450695]) with ID 117/01/10 15:29:35 INFO BlockManagerMasterEndpoint: Registering block manager slavenode3:45665 with 530.0 MB RAM, BlockManagerId(1, slavenode3, 45665)17/01/10 15:29:39 INFO YarnClientSchedulerBackend: Registered executor: AkkaRpcEndpointRef(Actor[akka.tcp://sparkExecutor@slavenode2:42294/user/Executor#1616446929]) with ID 217/01/10 15:29:39 INFO BlockManagerMasterEndpoint: Registering block manager slavenode2:42279 with 530.0 MB RAM, BlockManagerId(2, slavenode2, 42279)&gt; sqlcontext&lt;-sparkRSQL.init(sc)&gt; df&lt;-createDataFrame(sqlContext = sqlcontext,data = iris)17/01/10 15:29:54 INFO SparkContext: Starting job: collectPartitions at NativeMethodAccessorImpl.java:-217/01/10 15:29:54 INFO DAGScheduler: Got job 0 (collectPartitions at NativeMethodAccessorImpl.java:-2) with 1 output partitions17/01/10 15:29:54 INFO DAGScheduler: Final stage: ResultStage 0(collectPartitions at NativeMethodAccessorImpl.java:-2)17/01/10 15:29:54 INFO DAGScheduler: Parents of final stage: List()17/01/10 15:29:54 INFO DAGScheduler: Missing parents: List()17/01/10 15:29:54 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at RRDD.scala:454), which has no missing parents17/01/10 15:30:03 INFO MemoryStore: ensureFreeSpace(1280) called with curMem=0, maxMem=55575576517/01/10 15:30:03 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1280.0 B, free 530.0 MB)17/01/10 15:30:03 INFO MemoryStore: ensureFreeSpace(854) called with curMem=1280, maxMem=55575576517/01/10 15:30:03 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 854.0 B, free 530.0 MB)17/01/10 15:30:03 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.1.5.65:44133 (size: 854.0 B, free: 530.0 MB)17/01/10 15:30:03 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:86117/01/10 15:30:03 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at RRDD.scala:454)17/01/10 15:30:03 INFO YarnScheduler: Adding task set 0.0 with 1 tasks17/01/10 15:30:03 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, slavenode2, PROCESS_LOCAL, 16553 bytes)17/01/10 15:30:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on slavenode2:42279 (size: 854.0 B, free: 530.0 MB)17/01/10 15:30:05 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1661 ms on slavenode2 (1/1)17/01/10 15:30:05 INFO DAGScheduler: ResultStage 0 (collectPartitions at NativeMethodAccessorImpl.java:-2) finished in 1.786 s17/01/10 15:30:05 INFO DAGScheduler: Job 0 finished: collectPartitions at NativeMethodAccessorImpl.java:-2, took 11.381818 s17/01/10 15:30:05 INFO YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool 17/01/10 15:30:06 INFO YarnHistoryService: About to POST entity application_1483512430911_0002 with 10 events to timeline service http://slavenode4:8188/ws/v1/timeline/Warning messages:1: In FUN(X[[i]], ...) : Use Sepal_Length instead of Sepal.Length as column name2: In FUN(X[[i]], ...) : Use Sepal_Width instead of Sepal.Width as column name3: In FUN(X[[i]], ...) : Use Petal_Length instead of Petal.Length as column name4: In FUN(X[[i]], ...) : Use Petal_Width instead of Petal.Width as column name&gt; &gt; typeof(df)[1] &quot;S4&quot;&gt; a&lt;-&apos;sdfsd&apos;&gt; typeof(a)[1] &quot;character&quot;&gt; head(df)17/01/10 15:31:24 INFO SparkContext: Starting job: dfToCols at NativeMethodAccessorImpl.java:-217/01/10 15:31:24 INFO DAGScheduler: Got job 1 (dfToCols at NativeMethodAccessorImpl.java:-2) with 1 output partitions17/01/10 15:31:24 INFO DAGScheduler: Final stage: ResultStage 1(dfToCols at NativeMethodAccessorImpl.java:-2)17/01/10 15:31:24 INFO DAGScheduler: Parents of final stage: List()17/01/10 15:31:24 INFO DAGScheduler: Missing parents: List()17/01/10 15:31:24 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at dfToCols at NativeMethodAccessorImpl.java:-2), which has no missing parents17/01/10 15:31:24 INFO MemoryStore: ensureFreeSpace(9176) called with curMem=2134, maxMem=55575576517/01/10 15:31:24 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 530.0 MB)17/01/10 15:31:24 INFO MemoryStore: ensureFreeSpace(3690) called with curMem=11310, maxMem=55575576517/01/10 15:31:24 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 530.0 MB)17/01/10 15:31:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.1.5.65:44133 (size: 3.6 KB, free: 530.0 MB)17/01/10 15:31:24 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:86117/01/10 15:31:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at dfToCols at NativeMethodAccessorImpl.java:-2)17/01/10 15:31:24 INFO YarnScheduler: Adding task set 1.0 with 1 tasks17/01/10 15:31:24 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, slavenode3, PROCESS_LOCAL, 16553 bytes)17/01/10 15:31:24 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on slavenode3:45665 (size: 3.6 KB, free: 530.0 MB)17/01/10 15:31:35 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, slavenode3): java.net.SocketTimeoutException: Accept timed out at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:426) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/01/10 15:31:35 INFO TaskSetManager: Starting task 0.1 in stage 1.0 (TID 2, slavenode2, PROCESS_LOCAL, 16553 bytes)17/01/10 15:31:35 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on slavenode2:42279 (size: 3.6 KB, free: 530.0 MB)17/01/10 15:31:37 WARN TaskSetManager: Lost task 0.1 in stage 1.0 (TID 2, slavenode2): java.io.IOException: Cannot run program &quot;Rscript&quot;: error=13, Permission denied at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.spark.api.r.RRDD$.createRProcess(RRDD.scala:407) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:423) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: error=13, Permission denied at java.lang.UNIXProcess.forkAndExec(Native Method) at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:248) at java.lang.ProcessImpl.start(ProcessImpl.java:134) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 20 more17/01/10 15:31:37 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 3, slavenode3, PROCESS_LOCAL, 16553 bytes)17/01/10 15:31:47 WARN TaskSetManager: Lost task 0.2 in stage 1.0 (TID 3, slavenode3): java.net.SocketTimeoutException: Accept timed out at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:426) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/01/10 15:31:47 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 4, slavenode2, PROCESS_LOCAL, 16553 bytes)17/01/10 15:31:47 WARN TaskSetManager: Lost task 0.3 in stage 1.0 (TID 4, slavenode2): java.io.IOException: Cannot run program &quot;Rscript&quot;: error=13, Permission denied at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.spark.api.r.RRDD$.createRProcess(RRDD.scala:407) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:423) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)Caused by: java.io.IOException: error=13, Permission denied at java.lang.UNIXProcess.forkAndExec(Native Method) at java.lang.UNIXProcess.&lt;init&gt;(UNIXProcess.java:248) at java.lang.ProcessImpl.start(ProcessImpl.java:134) at java.lang.ProcessBuilder.start(ProcessBuilder.java:1029) ... 20 more17/01/10 15:31:47 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job17/01/10 15:31:47 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 17/01/10 15:31:47 INFO YarnScheduler: Cancelling stage 117/01/10 15:31:47 INFO DAGScheduler: ResultStage 1 (dfToCols at NativeMethodAccessorImpl.java:-2) failed in 23.005 s17/01/10 15:31:47 INFO DAGScheduler: Job 1 failed: dfToCols at NativeMethodAccessorImpl.java:-2, took 23.056582 s17/01/10 15:31:47 ERROR RBackendHandler: dfToCols on org.apache.spark.sql.api.r.SQLUtils failed17/01/10 15:31:47 INFO YarnHistoryService: About to POST entity application_1483512430911_0002 with 10 events to timeline service http://slavenode4:8188/ws/v1/timeline/Error in invokeJava(isStatic = TRUE, className, methodName, ...) : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, slavenode2): java.io.IOException: Cannot run program &quot;Rscript&quot;: error=13, Permission denied at java.lang.ProcessBuilder.start(ProcessBuilder.java:1048) at org.apache.spark.api.r.RRDD$.createRProcess(RRDD.scala:407) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:423) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD17/01/10 16:30:49 INFO ContextCleaner: Cleaned accumulator 117/01/10 16:30:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on slavenode3:45665 in memory (size: 3.6 KB, free: 530.0 MB)17/01/10 16:30:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on slavenode2:42279 in memory (size: 3.6 KB, free: 530.0 MB)17/01/10 16:30:49 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 10.1.5.65:44133 in memory (size: 3.6 KB, free: 530.0 MB)17/01/10 16:30:49 INFO ContextCleaner: Cleaned accumulator 217/01/10 16:30:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 10.1.5.65:44133 in memory (size: 854.0 B, free: 530.0 MB)17/01/10 16:30:49 INFO BlockManagerInfo: Removed broadcast_0_piece0 on slavenode2:42279 in memory (size: 854.0 B, free: 530.0 MB) 搜了一下没有理想答案，所以只能看源码了,下面是报错的代码位置：1234567891011121314151617181920212223242526private def createRProcess(port: Int, script: String): BufferedStreamThread = &#123; // "spark.sparkr.r.command" is deprecated and replaced by "spark.r.command", // but kept here for backward compatibility. val sparkConf = SparkEnv.get.conf var rCommand = sparkConf.get("spark.sparkr.r.command", "Rscript") rCommand = sparkConf.get("spark.r.command", rCommand) val rOptions = "--vanilla" val rLibDir = RUtils.sparkRPackagePath(isDriver = false) val rExecScript = rLibDir(0) + "/SparkR/worker/" + script print(rCommand) print(rOptions) print(rExecScript) val pb = new ProcessBuilder(Arrays.asList(rCommand, rOptions, rExecScript)) // Unset the R_TESTS environment variable for workers. // This is set by R CMD check as startup.Rs // (http://svn.r-project.org/R/trunk/src/library/tools/R/testing.R) // and confuses worker script which tries to load a non-existent file pb.environment().put("R_TESTS", "") pb.environment().put("SPARKR_RLIBDIR", rLibDir.mkString(",")) pb.environment().put("SPARKR_WORKER_PORT", port.toString) pb.redirectErrorStream(true) // redirect stderr into stdout val proc = pb.start() val errThread = startStdoutThread(proc) errThread &#125; 上面的print语句是我加上去的。 忘了一个前置条件，使用sparkR的时候不指定master，也就是local模式的话是可以成功的。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697&gt;sc&lt;-sparkR.init()Launching java with spark-submit command spark-submit sparkr-shell /tmp/RtmpRKsp2x/backend_port306c2e180534 17/01/10 18:20:53 INFO SparkContext: Running Spark version 1.5.217/01/10 18:20:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable17/01/10 18:20:54 INFO SecurityManager: Changing view acls to: zhangjiayi17/01/10 18:20:54 INFO SecurityManager: Changing modify acls to: zhangjiayi17/01/10 18:20:54 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(zhangjiayi); users with modify permissions: Set(zhangjiayi)17/01/10 18:20:59 INFO Slf4jLogger: Slf4jLogger started17/01/10 18:20:59 INFO Remoting: Starting remoting17/01/10 18:21:01 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.1.5.65:45017]17/01/10 18:21:01 INFO Utils: Successfully started service &apos;sparkDriver&apos; on port 45017.17/01/10 18:21:01 INFO SparkEnv: Registering MapOutputTracker17/01/10 18:21:02 INFO SparkEnv: Registering BlockManagerMaster17/01/10 18:21:02 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a7f2bf0d-c363-4220-ade9-5210fe54f9f717/01/10 18:21:03 INFO MemoryStore: MemoryStore started with capacity 530.0 MB17/01/10 18:21:03 INFO HttpFileServer: HTTP File server directory is /tmp/spark-d5f07559-6482-4523-aa40-8b99b7fbc84b/httpd-00d9c253-b8f8-41c2-ae1e-ad5bac726bf117/01/10 18:21:03 INFO HttpServer: Starting HTTP Server17/01/10 18:21:04 INFO Server: jetty-8.y.z-SNAPSHOT17/01/10 18:21:04 INFO AbstractConnector: Started SocketConnector@0.0.0.0:3534917/01/10 18:21:04 INFO Utils: Successfully started service &apos;HTTP file server&apos; on port 35349.17/01/10 18:21:04 INFO SparkEnv: Registering OutputCommitCoordinator17/01/10 18:21:05 INFO Server: jetty-8.y.z-SNAPSHOT17/01/10 18:21:05 INFO AbstractConnector: Started SelectChannelConnector@0.0.0.0:404017/01/10 18:21:05 INFO Utils: Successfully started service &apos;SparkUI&apos; on port 4040.17/01/10 18:21:05 INFO SparkUI: Started SparkUI at http://10.1.5.65:404017/01/10 18:21:05 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.17/01/10 18:21:05 INFO Executor: Starting executor ID driver on host localhost17/01/10 18:21:05 INFO Utils: Successfully started service &apos;org.apache.spark.network.netty.NettyBlockTransferService&apos; on port 38751.17/01/10 18:21:05 INFO NettyBlockTransferService: Server created on 3875117/01/10 18:21:05 INFO BlockManagerMaster: Trying to register BlockManager17/01/10 18:21:05 INFO BlockManagerMasterEndpoint: Registering block manager localhost:38751 with 530.0 MB RAM, BlockManagerId(driver, localhost, 38751)17/01/10 18:21:05 INFO BlockManagerMaster: Registered BlockManager&gt; &gt; sqlcontext&lt;-sparkRSQL.init(sc)scError: unexpected symbol in &quot;sqlcontext&lt;-sparkRSQL.init(sc)sc&quot;&gt; sqlcontext&lt;-sparkRSQL.init(sc)&gt; df&lt;-createDataFrame(sqlContext = sqlcontext,data = iris)17/01/10 18:23:34 INFO SparkContext: Starting job: collectPartitions at NativeMethodAccessorImpl.java:-217/01/10 18:23:34 INFO DAGScheduler: Got job 0 (collectPartitions at NativeMethodAccessorImpl.java:-2) with 1 output partitions17/01/10 18:23:34 INFO DAGScheduler: Final stage: ResultStage 0(collectPartitions at NativeMethodAccessorImpl.java:-2)17/01/10 18:23:34 INFO DAGScheduler: Parents of final stage: List()17/01/10 18:23:34 INFO DAGScheduler: Missing parents: List()17/01/10 18:23:34 INFO DAGScheduler: Submitting ResultStage 0 (ParallelCollectionRDD[0] at parallelize at RRDD.scala:454), which has no missing parents17/01/10 18:23:35 INFO MemoryStore: ensureFreeSpace(1280) called with curMem=0, maxMem=55575576517/01/10 18:23:35 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1280.0 B, free 530.0 MB)17/01/10 18:23:35 INFO MemoryStore: ensureFreeSpace(854) called with curMem=1280, maxMem=55575576517/01/10 18:23:35 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 854.0 B, free 530.0 MB)17/01/10 18:23:35 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:38751 (size: 854.0 B, free: 530.0 MB)17/01/10 18:23:35 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:86117/01/10 18:23:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (ParallelCollectionRDD[0] at parallelize at RRDD.scala:454)17/01/10 18:23:35 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks17/01/10 18:23:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 16553 bytes)17/01/10 18:23:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)17/01/10 18:23:36 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 15464 bytes result sent to driver17/01/10 18:23:36 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 249 ms on localhost (1/1)17/01/10 18:23:36 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 17/01/10 18:23:36 INFO DAGScheduler: ResultStage 0 (collectPartitions at NativeMethodAccessorImpl.java:-2) finished in 0.300 s17/01/10 18:23:36 INFO DAGScheduler: Job 0 finished: collectPartitions at NativeMethodAccessorImpl.java:-2, took 1.618354 sWarning messages:1: In FUN(X[[i]], ...) : Use Sepal_Length instead of Sepal.Length as column name2: In FUN(X[[i]], ...) : Use Sepal_Width instead of Sepal.Width as column name3: In FUN(X[[i]], ...) : Use Petal_Length instead of Petal.Length as column name4: In FUN(X[[i]], ...) : Use Petal_Width instead of Petal.Width as column name&gt; &gt; head(df)17/01/10 18:23:45 INFO SparkContext: Starting job: dfToCols at NativeMethodAccessorImpl.java:-217/01/10 18:23:45 INFO DAGScheduler: Got job 1 (dfToCols at NativeMethodAccessorImpl.java:-2) with 1 output partitions17/01/10 18:23:45 INFO DAGScheduler: Final stage: ResultStage 1(dfToCols at NativeMethodAccessorImpl.java:-2)17/01/10 18:23:45 INFO DAGScheduler: Parents of final stage: List()17/01/10 18:23:45 INFO DAGScheduler: Missing parents: List()17/01/10 18:23:45 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at dfToCols at NativeMethodAccessorImpl.java:-2), which has no missing parents17/01/10 18:23:45 INFO MemoryStore: ensureFreeSpace(9176) called with curMem=2134, maxMem=55575576517/01/10 18:23:45 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KB, free 530.0 MB)17/01/10 18:23:45 INFO MemoryStore: ensureFreeSpace(3690) called with curMem=11310, maxMem=55575576517/01/10 18:23:45 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.6 KB, free 530.0 MB)17/01/10 18:23:45 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:38751 (size: 3.6 KB, free: 530.0 MB)17/01/10 18:23:45 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:86117/01/10 18:23:45 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at dfToCols at NativeMethodAccessorImpl.java:-2)17/01/10 18:23:45 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks17/01/10 18:23:45 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, PROCESS_LOCAL, 16553 bytes)17/01/10 18:23:45 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)17/01/10 18:23:51 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1794 bytes result sent to driver17/01/10 18:23:51 INFO DAGScheduler: ResultStage 1 (dfToCols at NativeMethodAccessorImpl.java:-2) finished in 5.469 s17/01/10 18:23:51 INFO DAGScheduler: Job 1 finished: dfToCols at NativeMethodAccessorImpl.java:-2, took 5.493984 s17/01/10 18:23:51 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5468 ms on localhost (1/1)17/01/10 18:23:51 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool Sepal_Length Sepal_Width Petal_Length Petal_Width Species1 5.1 3.5 1.4 0.2 setosa2 4.9 3.0 1.4 0.2 setosa3 4.7 3.2 1.3 0.2 setosa4 4.6 3.1 1.5 0.2 setosa5 5.0 3.6 1.4 0.2 setosa6 5.4 3.9 1.7 0.4 setosa 单机 :12-----------will---------------------will mid----------zhangjiayi-----------will-end----------rCommand is : RscriptrOptions is : --vanillarExecScript is : /usr/hdp/2.3.4.0-3485/spark/R/lib/SparkR/worker/daemon.R 用户是zhangjiayi ,执行的系统xi 集群是打印在yarn的container的stdout里的：123-----------will---------------------will mid----------yarn-----------will-end----------rCommand is : RscriptrOptions is : --vanillarExecScript is : /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/sparkr/SparkR/worker/daemon.R-----------will---------------------will mid----------yarn-----------will-end----------rCommand is : RscriptrOptions is : --vanillarExecScript is : /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/sparkr/SparkR/worker/daemon.R 结果找不到：123456789101112[root@slavenode5 ~]# ll /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/sparkr/SparkR/worker/daemon.Rls: cannot access /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/sparkr/SparkR/worker/daemon.R: No such file or directory[root@slavenode5 ~]# ll /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/sparkr/SparkR/worker/daemon.Rls: cannot access /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/sparkr/SparkR/worker/daemon.R: No such file or directory[root@slavenode5 ~]# ll /server/hadoop/yarn/local/usercache/zhangjiayi/appcache/application_1483512430911_0006/container_e18_1483512430911_0006_01_000002/total 24-rw-r--r--. 1 yarn hadoop 88 Jan 11 12:25 container_tokens-rwx------. 1 yarn hadoop 687 Jan 11 12:25 default_container_executor_session.sh-rwx------. 1 yarn hadoop 741 Jan 11 12:25 default_container_executor.sh-rwx------. 1 yarn hadoop 4042 Jan 11 12:25 launch_container.shlrwxrwxrwx. 1 yarn hadoop 122 Jan 11 12:25 __spark__.jar -&gt; /server/hadoop/yarn/local/usercache/zhangjiayi/filecache/19/spark-assembly-1.5.2.2.3.4.0-3485-hadoop2.7.1.2.3.4.0-3485.jardrwx--x---. 2 yarn hadoop 4096 Jan 11 12:25 tmp 此时控制台的报错只有： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717217/01/11 12:25:42 INFO TaskSetManager: Starting task 0.2 in stage 1.0 (TID 3, slavenode5, PROCESS_LOCAL, 16553 bytes)17/01/11 12:25:52 WARN TaskSetManager: Lost task 0.2 in stage 1.0 (TID 3, slavenode5): java.net.SocketTimeoutException: Accept timed out at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:446) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/01/11 12:25:52 INFO TaskSetManager: Starting task 0.3 in stage 1.0 (TID 4, slavenode1, PROCESS_LOCAL, 16553 bytes)17/01/11 12:26:02 WARN TaskSetManager: Lost task 0.3 in stage 1.0 (TID 4, slavenode1): java.net.SocketTimeoutException: Accept timed out at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:446) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) at org.apache.spark.scheduler.Task.run(Task.scala:88) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)17/01/11 12:26:02 ERROR TaskSetManager: Task 0 in stage 1.0 failed 4 times; aborting job17/01/11 12:26:02 INFO YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool 17/01/11 12:26:02 INFO YarnScheduler: Cancelling stage 117/01/11 12:26:02 INFO DAGScheduler: ResultStage 1 (dfToCols at NativeMethodAccessorImpl.java:-2) failed in 42.286 s17/01/11 12:26:02 INFO DAGScheduler: Job 1 failed: dfToCols at NativeMethodAccessorImpl.java:-2, took 42.314808 s17/01/11 12:26:02 ERROR RBackendHandler: dfToCols on org.apache.spark.sql.api.r.SQLUtils failedError in invokeJava(isStatic = TRUE, className, methodName, ...) : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 4 times, most recent failure: Lost task 0.3 in stage 1.0 (TID 4, slavenode1): java.net.SocketTimeoutException: Accept timed out at java.net.PlainSocketImpl.socketAccept(Native Method) at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409) at java.net.ServerSocket.implAccept(ServerSocket.java:545) at java.net.ServerSocket.accept(ServerSocket.java:513) at org.apache.spark.api.r.RRDD$.createRWorker(RRDD.scala:446) at org.apache.spark.api.r.BaseRRDD.compute(RRDD.scala:62) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300) at org.apache.spark.rdd.RDD.iterator(RDD.scala:264) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) 可以看到是createRWorker里而不是createRProcess报错了。。 123456789101112131415161718192021222324Launching java with command /server/java/jdk1.8.0_60/bin/java -Xmx512m -cp &apos;/usr/lib64/R/library/SparkR/sparkr-assembly-0.1.jar:&apos; edu.berkeley.cs.amplab.sparkr.SparkRBackend /tmp/Rtmp34rELP/backend_port4e6c126d914 createSparkContext on edu.berkeley.cs.amplab.sparkr.RRDD failed with java.lang.NullPointerExceptionjava.lang.NullPointerException at edu.berkeley.cs.amplab.sparkr.SparkRBackendHandler.handleMethodCall(SparkRBackendHandler.scala:111) at edu.berkeley.cs.amplab.sparkr.SparkRBackendHandler.channelRead0(SparkRBackendHandler.scala:58) at edu.berkeley.cs.amplab.sparkr.SparkRBackendHandler.channelRead0(SparkRBackendHandler.scala:19) at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244) at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308) at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294) at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846) at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131) at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511) at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468) at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382) at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354) at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111) at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137) at java.lang.Thread.run(Thread.java:745)]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[R]]></title>
    <url>%2F2017%2F01%2F09%2FR%2F</url>
    <content type="text"><![CDATA[123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051library(SparkR)library(tseries)library(forecast)library(magrittr)sc&lt;-sparkR.init()hivecontext&lt;-sparkRHive.init(sc)sqlcontext&lt;-sparkRSQL.init(sc)dateline&lt;-seq.Date(from = as.Date(&quot;2015-05-01&quot;),to=as.Date(&quot;2017-01-04&quot;),by=&quot;day&quot;)%&gt;%data.frame(.)colnames(dateline)&lt;-&quot;log_day&quot;dateline&lt;-createDataFrame(sqlContext=sqlcontext,data=dateline)registerTempTable(dateline,&quot;premium_date&quot;) # Query the premium log of the users who are still remained for 30d and not labeled as brush.z0&lt;-&quot;select a.userid, from_unixtime(unix_timestamp(c.log_day,&apos;yyyyMMdd&apos;),&apos;yyyy-MM-dd&apos;) as log_day, c.premium_finalfrom dim_jlc.dim_user_info aleft join app_jlc.brush_user b on a.userid=b.userid and b.log_day=&apos;2017-01-04&apos;left join fact_jlc.premium_record c on a.userid=c.useridwhere b.userid is null and datediff(&apos;2017-01-04&apos;,to_date(a.invest1st_time))&gt;30&quot;z1&lt;-&quot;select a.userid, to_date(a.invest1st_time) as invest1st_dayfrom dim_jlc.dim_user_info aleft join app_jlc.brush_user b on a.userid=b.useridwhere b.userid is null and datediff(&apos;2017-01-04&apos;,to_date(a.invest1st_time))&gt;30&quot;result&lt;-sql(hivecontext,z0)userinfo&lt;-sql(hivecontext,z1)colnames(result)&lt;-c(&quot;userid&quot;,&quot;log_day&quot;,&quot;premium&quot;)cache(result)cache(userinfo)user_num&lt;-dim(userinfo)[1]for(i in 1:user_num)&#123; userid&lt;- start_day&lt;-select(userinfo,&quot;invest1st_day&quot;) dateline&lt;-seq()&#125;#rawdata&lt;-fread(&quot;raw0104.csv&quot;)#rawdata_df&lt;-tbl_df(rawdata)#colnames(rawdata_df)&lt;-c(&quot;userid&quot;,&quot;log_day&quot;,&quot;premium&quot;)userinfo_df&lt;-collect(userinfo)userinfo_df&lt;-tbl_df(userinfo_df)]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[rancher上手与问题]]></title>
    <url>%2F2017%2F01%2F02%2Francher%E4%B8%8A%E6%89%8B%E4%B8%8E%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[rancher里的host能不能同时属于env1和env2，测试了一下，貌似一个host上只能启动一个agent。 先把host1和host2添加到env1后，添加了wordpress的service stack，之后将所有服务都停掉，并deactivate两个hosts。 新建env2，将host1、host2同样的方法添加到env2中，布置mesos集群。 此时切回env1，再去激活host，发现已经不行了，host上提示disconnected。查看下面的container里，是有rancher agent的。有可能这个agent是在env2的那个把，所以不能连接到env1。 那：就是不能同时属于两个env咯？ 遇到get ip timeout问题，一般升级这个组件的版本就可以了 如果我们升级了某个组件的版本，再去调整这个组件的scale，它竟然会先自动降级，然后再调整scale。然后版本就又恢复成低版本的了。 Orz….]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack自动化运维实施（二）]]></title>
    <url>%2F2016%2F12%2F30%2Fsaltstack%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%AE%9E%E6%96%BD%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[我们安装saltstack的痛点在于文件同步，与分布式命令执行。 这一章我们弄一下文件同步的事情.编辑master的配置文件1file_recv: True 重启master。 从一个minion向master推送一个文件：12 参考： http://docs.saltstack.cn/ref/file_server/all/index.html http://docs.saltstack.cn/ref/file_server/all/salt.fileserver.minionfs.html#module-salt.fileserver.minionfs]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[saltstack自动化运维实施（一）]]></title>
    <url>%2F2016%2F12%2F30%2Fsaltstack%E8%87%AA%E5%8A%A8%E5%8C%96%E8%BF%90%E7%BB%B4%E5%AE%9E%E6%96%BD%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[安装1pip install salt 配置master配置文件/etc/salt/master, 打开注释1autosign_file: /etc/salt/autosign.conf 更多参考： https://docs.saltstack.com/en/latest/ref/configuration/master.html minion一个是/etc/salt/minion_id, 一般可以取为hostname 还有/etc/salt/minion, 指定master的host1master: 10.0.1.110 更多参考： https://docs.saltstack.com/en/latest/ref/configuration/minion.html 启动mastersalt-master restart minionsalt-minion restart 认证手动认证默认是没有开启自动认证的，需要人工手动判断哪些id是允许的。123456789101112131415[root@datanode21 ~]# salt-key -L # 列出所有idAccepted Keys:datanode19datanode20datanode21Denied Keys:Unaccepted Keys:Rejected Keys:[root@datanode21 ~]# salt-key -A # 通过所有认证[root@datanode21 ~]# salt-key -d datanode21 # 删除指定idThe following keys are going to be deleteed:Accepted Keys:datanode21Proceed? [N/y] yKey for minion datanode21 deleteed. 官网有说把master.pub放到minion里，但是我使用的时候并没有发现什么卵用~~~ 自动认证我弄了一种并不是很安全，但是相对比较安全的方式，使用hostname来自动认证.编辑/etc/salt/master文件, 打开注释1autosign_file: /etc/salt/autosign.conf 之后编辑/etc/salt/autosign.conf1datanode??.will.com 这里是正则匹配id的，如果minion的id能满足这里的正则表达式，就可以自动通过认证。 因为修改了/etc/salt/master文件，所以需要重启一下，如果只修改匹配规则文件是不需要重启的。 参考：http://www.it610.com/article/3325439.htm 测试在master的机器上执行1234567[root@datanode21 ~]# salt &apos;*&apos; test.pingdatanode21.will.com: Truedatanode19.will.com: Truedatanode20.will.com: True 分发脚本先安装Fabric. 编辑fabfile.py：123456789101112131415161718192021222324#!/usr/bin/env python# encoding: utf-8#from fabric.api import local,cd,run,env,putfrom fabric.api import *env.hosts=['172.16.4.74']env.password = 'Yinker.com'@paralleldef salt_install():# run('/etc/init.d/iptables stop') #远程操作用run# run('mkdir /server') run('yum -y install openssh-clients') #远程操作用run run('yum -y install https://repo.saltstack.com/yum/redhat/salt-repo-latest-1.el6.noarch.rpm') #远程操作用run run('yum -y install python-devel') #远程操作用run run('yum -y install salt-minion') #远程操作用run run('test -f /etc/salt/minion &amp;&amp; rm -rf /etc/salt/minion') #远程操作用run put('/root/qian/minion','/etc/salt/minion') run('echo `hostname` &gt;/etc/salt/minion_id') put('/root/qian/selinux','/etc/sysconfig/selinux') run('setenforce 0') run('/etc/init.d/salt-minion restart') 调用1/usr/local/bin/fab -f fabfile.py salt_install]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单点登出问题]]></title>
    <url>%2F2016%2F12%2F28%2F%E5%8D%95%E7%82%B9%E7%99%BB%E5%87%BA%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[表现登陆后短时间内刷新页面，就会提示重新登陆。 后台对应的log：12345678910112016-12-28 14:32:53,094 INFO [org.jasig.inspektr.audit.support.Slf4jLoggingAuditTrailManager] - Audit trail record BEGIN=============================================================WHO: audit:unknownWHAT: TGT-************************************************OgnMZDqjiE-cas01.example.orgACTION: TICKET_GRANTING_TICKET_CREATEDAPPLICATION: CASWHEN: Wed Dec 28 14:32:53 CST 2016CLIENT IP ADDRESS: 10.1.5.83SERVER IP ADDRESS: 10.0.1.62============================================================= 正常的登陆log:123456789102016-12-28 14:32:53,093 INFO [org.jasig.inspektr.audit.support.Slf4jLoggingAuditTrailManager] - Audit trail record BEGIN=============================================================WHO: adminWHAT: Supplied credentials: [admin]ACTION: AUTHENTICATION_SUCCESSAPPLICATION: CASWHEN: Wed Dec 28 14:32:53 CST 2016CLIENT IP ADDRESS: 10.1.5.83SERVER IP ADDRESS: 10.0.1.62============================================================= 参考：]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hdfs-rebalance引发的问题]]></title>
    <url>%2F2016%2F12%2F23%2Fhdfs-rebalance%E5%BC%95%E5%8F%91%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[原来的集群节点是8台datanode…….HDFS的使用总量已经到了 1Slow waitForAckedSeqno took 179279ms (threshold=30000ms) 1Bad response ERROR for block BP-1029563541-10.0.1.72-1463106887558:blk_1077490973_3758370 from datanode DatanodeInfoWithStorage[10.0.1.98:50010,DS-a387140d-7 1Got error, status message , ack with firstBadLink as 10.0.1.101:50010 12java.io.IOException: Bad response ERROR for block BP-1029563541-10.0.1.72-1463106887558:blk_1077490815_3758189 from datanode DatanodeInfoWithStorage[10.0.1.101:50010,DS-8786adb7-d8ae-4b29-a8af-4cb73a8a5e72,DISK] at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:785) map失败log信息：1234567892016-12-24 22:19:01,865 WARN [main] org.apache.hadoop.metrics2.impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-maptask.properties,hadoop-metrics2.properties2016-12-24 22:19:05,431 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).2016-12-24 22:19:05,431 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system started2016-12-24 22:19:05,442 INFO [main] org.apache.hadoop.mapred.YarnChild: Executing with tokens:2016-12-24 22:19:05,442 INFO [main] org.apache.hadoop.mapred.YarnChild: Kind: mapreduce.job, Service: job_1482504625052_0529, Ident: (org.apache.hadoop.mapreduce.security.token.JobTokenIdentifier@57a78e3)2016-12-24 22:20:33,408 INFO [main] org.apache.hadoop.mapred.YarnChild: Sleeping for 0ms before retrying again. Got null now.2016-12-24 22:20:46,112 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping MapTask metrics system...2016-12-24 22:20:46,119 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system stopped.2016-12-24 22:20:46,119 INFO [main] org.apache.hadoop.metrics2.impl.MetricsSystemImpl: MapTask metrics system shutdown complete. MR任务之间有一段空白期123456789101112131415161718192021222016-12-24 21:29:50,046 Stage-1 map = 100%, reduce = 99%, Cumulative CPU 4516.43 sec2016-12-24 21:30:04,499 Stage-1 map = 100%, reduce = 100%, Cumulative CPU 4528.6 secMapReduce Total cumulative CPU time: 0 days 1 hours 15 minutes 28 seconds 600 msecEnded Job = job_1482504625052_0527Launching Job 2 out of 3Number of reduce tasks not specified. Defaulting to jobconf value of: 30In order to change the average load for a reducer (in bytes):set hive.exec.reducers.bytes.per.reducer=In order to limit the maximum number of reducers:set hive.exec.reducers.max=In order to set a constant number of reducers:set mapreduce.job.reduces=Starting Job = job_1482504625052_0528, Tracking URL = http://datanode02.will.com:8088/proxy/application_1482504625052_0528/Kill Command = /usr/hdp/2.4.2.0-258/hadoop/bin/hadoop job -kill job_1482504625052_0528Hadoop job information for Stage-5: number of mappers: 7; number of reducers: 302016-12-24 21:54:09,652 Stage-5 map = 0%, reduce = 0%2016-12-24 21:54:19,037 Stage-5 map = 14%, reduce = 0%, Cumulative CPU 7.53 sec2016-12-24 21:54:20,077 Stage-5 map = 29%, reduce = 0%, Cumulative CPU 15.38 sec2016-12-24 21:54:36,677 Stage-5 map = 39%, reduce = 0%, Cumulative CPU 44.43 sec2016-12-24 21:54:37,759 Stage-5 map = 71%, reduce = 0%, Cumulative CPU 52.41 sec2016-12-24 21:54:48,153 Stage-5 map = 71%, reduce = 1%, Cumulative CPU 53.96 sec2016-12-24 21:54:49,186 Stage-5 map = 71%, reduce = 2%, Cumulative CPU 56.57 sec 21:30到21:54,这段时间namenode的log里的一些内容：1234567891011121314152016-12-24 21:49:01,939 INFO hdfs.StateChange (FSNamesystem.java:completeFile(3545)) - DIR* completeFile: /apps/hbase/data/data/default/KYLIN_U1V82JA44K/8c474006d2f4332bdaaee58105bb465a/.tmp/79460c3b99ba48f8b67cfadba06d277e is closed by DFSClient_NONMAPREDUCE_-90235743_12016-12-24 21:49:02,045 INFO hdfs.StateChange (FSNamesystem.java:logAllocatedBlock(3652)) - BLOCK* allocate blk_1077490355_3757707, replicas=10.0.1.86:50010, 10.0.1.95:50010, 10.0.1.84:50010 for /apps/hbase/data/data/default/KYLIN_U1V82JA44K/9674d0c2aec6aa86cb092088d9461163/.tmp/7d5802bddb694c25a84cdd2263bcca422016-12-24 21:42:49,930 INFO hdfs.StateChange (FSNamesystem.java:completeFile(3545)) - DIR* completeFile: /apps/hbase/data/data/default/KYLIN_V5P8H4CZJF/2c1b15e259130ca37d1c4808aade03e6/.tmp/d702471e4c784abba520d494c29b03f2 is closed by DFSClient_NONMAPREDUCE_434452785_1namenode.FSNamesystem (FSNamesystem.java:updatePipeline(6535)) - updatePipeline(blk_1077490302_3757650, newGS=3757651, newLength=114137522, newNodes=[10.0.1.96:50010, 10.0.1.83:50010], client=DFSClient_NONMAPREDUCE_829342269_1)2016-12-24 21:42:49,567 INFO namenode.FSNamesystem (FSNamesystem.java:updatePipeline(6554)) - updatePipeline(blk_1077490302_3757650 =&gt; blk_1077490302_3757651) successchooseUnderReplicatedBlocks selected 1 blocks at priority level 2; Total=1 Reset bookmarks? truenamenode.FSEditLog (FSEditLog.java:printStatistics(699)) - Number of transactions: 106401 Total time for transactions(ms): 961 Number of transactions batched in Syncs: 3666 Number of syncs: 87634 SyncTimes(ms): 239376 failed的reducer的提示note：1AttemptID:attempt_1482504625052_0537_r_000003_0 Timed out after 300 secs Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 Container exited with a non-zero exit code 143 balance完成之后，运行了几天，每天的任务从2小时、1个小时40分钟、1小时30分钟、1小时20分钟递减~~也很少出现这种问题了……诡异 参考下面的资料，基本思想就是timeout的时间内，没有完成某个任务，所以把timeout延长就可以了。还没有实践测试….就好了 还是看一下这附近的源码比较好一些~ 参考： http://www.superwu.cn/2014/11/13/1334/ http://blog.sina.com.cn/s/blog_72827fb1010198j7.html http://blog.163.com/zhengjiu_520/blog/static/3559830620130743644473/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mesos的docker化安装部署]]></title>
    <url>%2F2016%2F12%2F22%2Fmesos%E7%9A%84docker%E5%8C%96%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[首先，还是从源头说起，用阿里云的yum源，下载安装都会快一些。zookeeper1docker run -d --net=&quot;host&quot; -e SERVER_ID=3 -e ADDITIONAL_ZOOKEEPER_1=server.1=10.1.5.129:2888:3888 -e ADDITIONAL_ZOOKEEPER_2=server.2=10.1.5.180:2888:3888 -e ADDITIONAL_ZOOKEEPER_3=server.3=10.1.5.190:2888:3888 --name zk garland/zookeeper master123456789101112131415161718192021222324252627282930313233343536docker run --net=&quot;host&quot; \ -p 5050:5050 \ -e &quot;MESOS_HOSTNAME=10.1.5.129&quot; \ -e &quot;MESOS_IP=10.1.5.129&quot; \ -e &quot;MESOS_ZK=zk://10.1.5.129:2181,10.1.5.180:2181,10.1.5.190:2181/mesos&quot; \ -e &quot;MESOS_PORT=5050&quot; \ -e &quot;MESOS_LOG_DIR=/var/log/mesos&quot; \ -e &quot;MESOS_QUORUM=1&quot; \ -e &quot;MESOS_REGISTRY=in_memory&quot; \ -e &quot;MESOS_WORK_DIR=/var/lib/mesos&quot; \ -d --name mesos-master garland/mesosphere-docker-mesos-master docker run --net=&quot;host&quot; \ -p 5050:5050 \ -e &quot;MESOS_HOSTNAME=10.1.5.180&quot; \ -e &quot;MESOS_IP=10.1.5.180&quot; \ -e &quot;MESOS_ZK=zk://10.1.5.129:2181,10.1.5.180:2181,10.1.5.190:2181/mesos&quot; \ -e &quot;MESOS_PORT=5050&quot; \ -e &quot;MESOS_LOG_DIR=/var/log/mesos&quot; \ -e &quot;MESOS_QUORUM=1&quot; \ -e &quot;MESOS_REGISTRY=in_memory&quot; \ -e &quot;MESOS_WORK_DIR=/var/lib/mesos&quot; \ -d --name mesos-master garland/mesosphere-docker-mesos-master docker run --net=&quot;host&quot; \ -p 5050:5050 \ -e &quot;MESOS_HOSTNAME=10.1.5.190&quot; \ -e &quot;MESOS_IP=10.1.5.190&quot; \ -e &quot;MESOS_ZK=zk://10.1.5.129:2181,10.1.5.180:2181,10.1.5.190:2181/mesos&quot; \ -e &quot;MESOS_PORT=5050&quot; \ -e &quot;MESOS_LOG_DIR=/var/log/mesos&quot; \ -e &quot;MESOS_QUORUM=1&quot; \ -e &quot;MESOS_REGISTRY=in_memory&quot; \ -e &quot;MESOS_WORK_DIR=/var/lib/mesos&quot; \ -d --name mesos-master garland/mesosphere-docker-mesos-master marathon1234docker run \ -d \ -p 8080:8080 \ garland/mesosphere-docker-marathon --master zk://10.1.5.129:2181,10.1.5.180:2181,10.1.5.190:2181/mesos --zk zk://10.1.5.129:2181,10.1.5.180:2181,10.1.5.190:2181/marathon slave123456docker run -d \ --entrypoint=&quot;mesos-slave&quot; \ -e &quot;MESOS_MASTER=zk://10.1.5.129:2181,10.1.5.180:2181,10.1.5.190:2181/mesos&quot; \ -e &quot;MESOS_LOG_DIR=/var/log/mesos&quot; \ -e &quot;MESOS_LOGGING_LEVEL=INFO&quot; \ garland/mesosphere-docker-mesos-master:latest 参考： http://www.cnblogs.com/ee900222/p/docker_2.html https://mesosphere.com/blog/2014/07/17/mesosphere-package-repositories/# http://www.jingyuyun.com/article/8062.html http://dockone.io/article/493 http://www.mesoscn.cn/document/runing-Mesos/Configuration.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一致性协议zab与raft]]></title>
    <url>%2F2016%2F12%2F09%2F%E4%B8%80%E8%87%B4%E6%80%A7%E5%8D%8F%E8%AE%AEzab%E4%B8%8Eraft%2F</url>
    <content type="text"><![CDATA[zookeeper的zab1.0协议raft协议重视看来一下zookeeper的选举过程，并不是原来说的大家都排队拿一个号，谁小谁当。而是依次比较epoch(选举轮数)、zxid(处理的事务id)、serverid(my.ini里的server.id)，可能最后的server.id是原来那样说的原因吧。zookeeper的实现是通过一个MessageSender和一个MessageReceiver来实现 raft算法中，follower跟leader不一致的地方会被leader覆盖掉。zookeeper同样也是这样，TRUNC类型的message的处理。 raft不同于zookeeper的地方：选举的时候serverid的地方并不一样，是通过随机选举超时时间，防止瓜分选票 有一个难点，假设一共5个server，如果某个leader在将所有的logid/zxid同步到了大多数机器，也就是3台，term或者epoch为2。此时挂掉，选举了一个新的leader，它对于这三台机器中的新的log/zxid的处理方式。raft会判断是否多数已经同步，如果是，就接受并发送出去，同步给其他server。zookeeper并没有提及 —— TODO 源码确认 ####参考： https://cwiki.apache.org/confluence/display/ZOOKEEPER/Zab1.0 http://www.tuicool.com/articles/IfQR3u3 https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md-http://www.cnblogs.com/yuyijq/p/4116365.html-http://iwinit.iteye.com/blog/1773531]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[微服务（翻译）]]></title>
    <url>%2F2016%2F12%2F05%2F%E5%BE%AE%E6%9C%8D%E5%8A%A1%EF%BC%88%E7%BF%BB%E8%AF%91%EF%BC%89%2F</url>
    <content type="text"><![CDATA[新架构角度的定义微服务这个名词最近几年比较火，它代表一套独立部署的应用服务的软件设计。然而对于这种架构方式并没有精确的定义，微观组织的业务能力有几个共同的特征：自动化部署、智能终端、语言和数据的分散控制。 微服务是众多软件架构中的一种。以前我们只是轻蔑的瞥一眼，最近几年来微服务的使用效果让我们见证到了它的威力。而且有些学校已经把它当做企业编程系统模式来教学。可悲的是，并没有人明确指出应该怎样构建一个微服务架构~ 简单来说，微服务就是提供一套服务来开发一个应用，其中每个服务都运行在自己的进程里，它们之间使用轻量级的协议进行沟通，多数是http。这些服务是围绕业务构建的，而且各自独立地被自动化部署。最低限度地集中管理这些服务，因为这些服务可能是各种编程语言写的，也有可能使用各自不同的存储技术。 下面对比整体编程架构来介绍一下微服务架构。一般的应用由三部分构成：前端、数据库、后端服务。后端服务负责处理http请求，执行业务逻辑，接收并更新数据到数据库，选择适当的视图返回给客户端。这样的后端服务就是一个整体编程架构。任何修改都要重新发布整个后端服务。 上面提到的整体架构是构建系统的一个很自然的方式。处理请求的所有的业务逻辑都在一个单一的进程里，你可以使用语言特性讲不通的功能分配给不同的类、函数、命名空间。对于个别关心的东西，你可以在开发人员的笔记本上运行测试。使用一个发布pipeline来保证改变的东西已经被测试过了，然后发布到生产环境。你可以在负载均衡后面运行多个app实例来横向扩展。 整体架构其实比较成功，只是人们慢慢感觉到有些受挫，尤其是越来越多的应用被发布到云中。修改环节紧紧联系在一起，对应用的一小部分进行修改，就需要整体应用都重新构建与发布。随着时间推移，要想保持一个很好的模块化架构变得特别困难。扩展的话也必须扩展整体应用，而不能紧紧扩展指定的一些模块，这样就耗费了很多资源。 这些困难催生了微服务架构：把app当做服务组来构建。每个服务不但独立部署和扩展，他们还提供严格的模块边界，甚至不同的服务还可以使用不同的编程语言来实现，也可以由不同的团队负责开发与管理。 我们并不是说微服务是创新的，至少它回归到了Unix的设计原则。我们就是认为使用微服务架构思想编程的人还比较少，其实使用它会让我们的软件开发更好。 微服务架构的特点 While we authors have been active members of this rather loose community, our intention is to attempt a description of what we see in our own work and in similar efforts by teams we know of. In particular we are not laying down some definition to conform to. 我们并不能对微服务下一个官方定义，但是我们可以尝试描述一下微服务的几个共同特点。并不是所有的微服务架构都有这些特点，但是我们希望多数的微服务都展示出了这些特点的大部分。 组件化服务软件工业中我们一般把系统设计成可以将component组装起来的架构，多数是使用公用的libraries实现。 提到component，我们看一下它的定义，component是软件的一个单元，独立、可替换、可升级。 Microservice architectures will use libraries, but their primary way of componentizing their own software is by breaking down into services. We define libraries as components that are linked into a program and called using in-memory function calls, while services are out-of-process components who communicate with a mechanism such as a web service request, or remote procedure call. (This is a different concept to that of a service object in many OO programs [3].) 微服务架构把软件打散成不同的服务，也是用libraries。libraries是被链接到程序中，在内存中调用函数的component，而服务是进程之外的component，使用类似web service request进行沟通，或者RPC。 One main reason for using services as components (rather than libraries) is that services are independently deployable. If you have an application [4] that consists of a multiple libraries in a single process, a change to any single component results in having to redeploy the entire application. But if that application is decomposed into multiple services, you can expect many single service changes to only require that service to be redeployed. That’s not an absolute, some changes will change service interfaces resulting in some coordination, but the aim of a good microservice architecture is to minimize these through cohesive service boundaries and evolution mechanisms in the service contracts. 把服务组件化的一个主要理由就是服务可以独立部署。如果你有一个单进程中包含多个libraries的应用，修改任何一个component都需要重新部署整个应用。但是如果你的应用解耦成为多个服务，你可以只重新部署修改到的地方。当然这并不是绝对的，如果修改了服务接口，肯定也要重新修改部署协作的服务，但是微服务的目标就是通过黏着的服务边界和服务规范的进化机制最小化这个修改范围。 Another consequence of using services as components is a more explicit component interface. Most languages do not have a good mechanism for defining an explicit Published Interface. Often it’s only documentation and discipline that prevents clients breaking a component’s encapsulation, leading to overly-tight coupling between components. Services make it easier to avoid this by using explicit remote call mechanisms. 另一个服务组件化的结果就是更灵活的component接口。大多数语言都不能定义一个明确详实的公共接口。通常只能通过文档和规范来约束客户端遵循component的封装，导致组件之间过度依赖。组件化服务可以使用明确的远程调用机制避免这些。 Using services like this does have downsides. Remote calls are more expensive than in-process calls, and thus remote APIs need to be coarser-grained, which is often more awkward to use. If you need to change the allocation of responsibilities between components, such movements of behavior are harder to do when you’re crossing process boundaries. 使用这样的服务也是有缺点的。远程多用通常比进程内调用代价昂贵，因此远程API需要是粗粒度的。如果你必须修改component之间的职责分配，这样的行为移动非常难弄，尤其是跨进程边界的时候。 At a first approximation, we can observe that services map to runtime processes, but that is only a first approximation. A service may consist of multiple processes that will always be developed and deployed together, such as an application process and a database that’s only used by that service. 第一印象，我们可以把这样的服务看做是运行时进程。注意，这只是为了好理解一些，事实上一个service可以包含多个进程，比如一个应用进程和这个进程使用的数据库进程。 业务管理When looking to split a large application into parts, often management focuses on the technology layer, leading to UI teams, server-side logic teams, and database teams. When teams are separated along these lines, even simple changes can lead to a cross-team project taking time and budgetary approval. A smart team will optimise around this and plump for the lesser of two evils - just force the logic into whichever application they have access to. Logic everywhere in other words. This is an example of Conway’s Law[5] in action. 当把一个大的应用切分成多个部分的时候，通常管理就集中在技术层面，出现UI团队，后端逻辑团队，数据库团队。当各个团队分成这些战线之后，即便是特别小的改变也是一个跨团队项目，需要时间和预算。一个牛逼的团队会优化这些东西，取这两个恶魔中比较少的那个。强制把业务逻辑放到一个必须存在的地方。换句话说导出都写逻辑。这是Conway’s Law in action的一个实例。 Any organization that designs a system (defined broadly) will produce a design whose structure is a copy of the organization’s communication structure.– Melvyn Conway, 1967 任何组织的系统都是组织结构沟通结构的复制。 The microservice approach to division is different, splitting up into services organized around business capability. Such services take a broad-stack implementation of software for that business area, including user-interface, persistant storage, and any external collaborations. Consequently the teams are cross-functional, including the full range of skills required for the development: user-experience, database, and project management. 微服务的切分不同，它是围绕业务切分服务。这样的服务包含特定业务领域系统的所有技术栈的实现，包含UI、持久存储、外部协作。结果这样的团队就跨功能，包含开发所需的所有的技术：UI、数据库、项目管理。 One company organised in this way is www.comparethemarket.com. Cross functional teams are responsible for building and operating each product and each product is split out into a number of individual services communicating via a message bus. www.comparethemarket.com就是这样组织的一个公司。跨功能团队负责构建和操作每个产品，每个产品也都被切分成几个单独的服务，这些服务之间通过message bus沟通。 Large monolithic applications can always be modularized around business capabilities too, although that’s not the common case. Certainly we would urge a large team building a monolithic application to divide itself along business lines. The main issue we have seen here, is that they tend to be organised around too many contexts. If the monolith spans many of these modular boundaries it can be difficult for individual members of a team to fit them into their short-term memory. Additionally we see that the modular lines require a great deal of discipline to enforce. The necessarily more explicit separation required by service components makes it easier to keep the team boundaries clear. 大的整体应用通常也可以围绕业务进行模块化，尽管还不是通用案例。当然我们需要推动一个大的团队构建一个整体应用根据业务线进行切割。我们在这里遇到的主要的问题是，需要这里面的组织会包含太多的context。如果这个整体应用贯穿了所有模块边界，那么就很难让团队里的个人成员把他们短期记忆住。另外我们知道模块线需要一个准则。 服务组价需要一定更明确的分离，这样能够让团队边界也更清晰。 产品而不是项目Most application development efforts that we see use a project model: where the aim is to deliver some piece of software which is then considered to be completed. On completion the software is handed over to a maintenance organization and the project team that built it is disbanded. 我们见的多数的应用开发都是使用项目模型：目标就是交付软件的某些片段。完成后，软件就转交给运维，然后项目团队就解散了。 Microservice proponents tend to avoid this model, preferring instead the notion that a team should own a product over its full lifetime. A common inspiration for this is Amazon’s notion of “you build, you run it” where a development team takes full responsibility for the software in production. This brings developers into day-to-day contact with how their software behaves in production and increases contact with their users, as they have to take on at least some of the support burden. 微服务支持者要避免这种情况，一个团队应该负责产品的整个生命周期。亚马逊的口号：谁构建，谁运行。一个开发团队对于生产线上的软件要全权负责。这样开发者每天都会考虑他的软件在生成环境运行的情况咋样，也多跟他的用户沟通，他们必须多少承担一些事情。 The product mentality, ties in with the linkage to business capabilities. Rather than looking at the software as a set of functionality to be completed, there is an on-going relationship where the question is how can software assist its users to enhance the business capability. 产品是与业务线贴合的。除了把软件当做一组完成的功能，还有一个持续的关系，就是软件怎样辅助它的用户来加强业务能力。 Smart endpoints and dumb pipesWhen building communication structures between different processes, we’ve seen many products and approaches that stress putting significant smarts into the communication mechanism itself. A good example of this is the Enterprise Service Bus (ESB), where ESB products often include sophisticated facilities for message routing, choreography, transformation, and applying business rules. 当为不同的进程构建沟通结构的时候，我们见过很多产品都强调吧关键的smart放进沟通机制本身中。一个例子就是ESB。ESB产品通常都包含复杂的设备来支持消息路由、编排、转化、应用业务规则。 The microservice community favours an alternative approach: smart endpoints and dumb pipes. Applications built from microservices aim to be as decoupled and as cohesive as possible - they own their own domain logic and act more as filters in the classical Unix sense - receiving a request, applying logic as appropriate and producing a response. These are choreographed using simple RESTish protocols rather than complex protocols such as WS-Choreography or BPEL or orchestration by a central tool. 微服务组织最喜欢另一种方式：smart endpoints and dumb pipes. 使用微服务构建的应用旨在尽可能解耦，边界尽可能清洗-他们都有自己的逻辑，表现的像个经典Unix派感觉的filter - 接收一个请求，应用业务逻辑，然后产生一个response。他们被使用简单的REST系列协议编排，而不是类似WS-Choreography或者BPRL啥的复杂的协议。 The two protocols used most commonly are HTTP request-response with resource API’s and lightweight messaging[6]. The best expression of the first is 两个最常用的协议是带有resource API的HTTP request-reponse，还有轻量级消息协议。前者最好的表达是： Be of the web, not behind the web– Ian Robinson Microservice teams use the principles and protocols that the world wide web (and to a large extent, Unix) is built on. Often used resources can be cached with very little effort on the part of developers or operations folk. 不会翻译。 The second approach in common use is messaging over a lightweight message bus. The infrastructure chosen is typically dumb (dumb as in acts as a message router only) - simple implementations such as RabbitMQ or ZeroMQ don’t do much more than provide a reliable asynchronous fabric - the smarts still live in the end points that are producing and consuming messages; in the services. 后者通常通过一个轻量级的message bug来使用。基础设施通常选用dumb，简单的实现例如RabbitMQ、ZeroMQ，并没有为一个可靠的异步fabric多做了些什么 - smart还是在生成和消费message的终端上。 In a monolith, the components are executing in-process and communication between them is via either method invocation or function call. The biggest issue in changing a monolith into microservices lies in changing the communication pattern. A naive conversion from in-memory method calls to RPC leads to chatty communications which don’t perform well. Instead you need to replace the fine-grained communication with a coarser -grained approach. 在一个整体应用中，component是运行在同一个进程内的，可以通过方法触发或函数调用来相互沟通。切分成为微服务后，一个比较大的问题就是沟通模型。从内存中的方法调用到RPC导致了啰嗦的沟通机制。除此之外，你必须使用细粒度沟通替换为粗粒度的。 Decentralized Governance （分权治理/去中心化）One of the consequences of centralised governance is the tendency to standardise on single technology platforms. Experience shows that this approach is constricting - not every problem is a nail and not every solution a hammer. We prefer using the right tool for the job and while monolithic applications can take advantage of different languages to a certain extent, it isn’t that common. 中心化治理的结果是标准趋向于单一技术平台。经验表明这种方式是不太好的，通常不是一个方案就能解决掉所有问题。我们更愿意使用合适的工具来做对应的工作，整体应用可以利用不同语言解决某个特定的范围，但是这并不常见。 Splitting the monolith’s components out into services we have a choice when building each of them. You want to use Node.js to standup a simple reports page? Go for it. C++ for a particularly gnarly near-real-time component? Fine. You want to swap in a different flavour of database that better suits the read behaviour of one component? We have the technology to rebuild him. 把整体应用切分成组件化服务的话，我们可以选择啥时候构建每个组件。想用nodejs创建一个简单的表单页？做呗。想用c++弄一个实时处理组件？好啊。你想在不同的数据库支持之间切换，来适应个别组件的读取？我们可以重新构建它。 Of course, just because you can do something, doesn’t mean you should - but partitioning your system in this way means you have the option. 当然，并不是你可以做，你就应该做。只是把你当系统以这种方式拆分后，你可以选择做或者不做。 Teams building microservices prefer a different approach to standards too. Rather than use a set of defined standards written down somewhere on paper they prefer the idea of producing useful tools that other developers can use to solve similar problems to the ones they are facing. These tools are usually harvested from implementations and shared with a wider group, sometimes, but not exclusively using an internal open source model. Now that git and github have become the de facto version control system of choice, open source practices are becoming more and more common in-house . 各个团队构建微服务的方式也不尽相同。他们会设计对生产有用的工具，供其他开发者重用，解决类似的问题，而不是使用一系列定义好的标准写在纸上。这些工具通常从一些实现或者在一个共享组里获得的。现在git和github都成为了版本控制系统，开源的练习也越来越多见。 Netflix is a good example of an organisation that follows this philosophy. Sharing useful and, above all, battle-tested code as libraries encourages other developers to solve similar problems in similar ways yet leaves the door open to picking a different approach if required. Shared libraries tend to be focused on common problems of data storage, inter-process communication and as we discuss further below, infrastructure automation. netflix是追随这个哲学的代表组织。分享有用的精炼的代码，作为libraries提供给其他的开发者使用，解决类似的问题。共享的libraries会慢慢在数据存储、进程内沟通、基础设置自动化问题上被注意到。 For the microservice community, overheads are particularly unattractive. That isn’t to say that the community doesn’t value service contracts. Quite the opposite, since there tend to be many more of them. It’s just that they are looking at different ways of managing those contracts. Patterns like Tolerant Reader and Consumer-Driven Contracts are often applied to microservices. These aid service contracts in evolving independently. Executing consumer driven contracts as part of your build increases confidence and provides fast feedback on whether your services are functioning. Indeed we know of a team in Australia who drive the build of new services with consumer driven contracts. They use simple tools that allow them to define the contract for a service. This becomes part of the automated build before code for the new service is even written. The service is then built out only to the point where it satisfies the contract - an elegant approach to avoid the ‘YAGNI’[9] dilemma when building new software. These techniques and the tooling growing up around them, limit the need for central contract management by decreasing the temporal coupling between services. 对于微服务社区，一般费用是不太惹人注意的。不是说社区不重视服务规范。正相反，很重视服务规范。只是在各种方式来管理这些规范。诸如 Tolerant Reader和Consumer-Driven的规范模型经常被应用于微服务。这帮助服务规范独立展开。 Perhaps the apogee of decentralised governance is the build it / run it ethos popularised by Amazon. Teams are responsible for all aspects of the software they build including operating the software 24/7. Devolution of this level of responsibility is definitely not the norm but we do see more and more companies pushing responsibility to the development teams. Netflix is another organisation that has adopted this ethos[11]. Being woken up at 3am every night by your pager is certainly a powerful incentive to focus on quality when writing your code. These ideas are about as far away from the traditional centralized governance model as it is possible to be. 获取去中心化治理的至高点是Amazon号召的构建/运行。各种团队也24/7的负责了软件的不同方面。责任的转义肯定不是规范，但是我们看到了越来越多的公司把责任推给了开发团队。netflix是融合了这种号召的另一类组织。你的手机每天教你半夜3点起来肯定一个很牛逼的动力，让你专注于写代码的质量。 Decentralized Data Management（数据管理去中心化）Decentralization of data management presents in a number of different ways. At the most abstract level, it means that the conceptual model of the world will differ between systems. This is a common issue when integrating across a large enterprise, the sales view of a customer will differ from the support view. Some things that are called customers in the sales view may not appear at all in the support view. Those that do may have different attributes and (worse) common attributes with subtly different semantics. 数据管理的去中心化代表着几种不同的方式。最抽象的层次上讲，它表示概念模型在系统之间是不同的。在整个大公司的系统的时候这个问题很常见，销售视图不同于支持视图。有一些customer在销售视图里，而在支持视图没有。还有有不同的属性和相同的属性，但是有不同的语义。 This issue is common between applications, but can also occur within applications, particular when that application is divided into separate components. A useful way of thinking about this is the Domain-Driven Design notion of Bounded Context. DDD divides a complex domain up into multiple bounded contexts and maps out the relationships between them. This process is useful for both monolithic and microservice architectures, but there is a natural correlation between service and context boundaries that helps clarify, and as we describe in the section on business capabilities, reinforce the separations. 这个问题在应用之间是很常见的，在app内部也会发生，尤其是应用拆分成多个component之后。考虑这个一个很好的方式是把它看做bounded contexts的Domain-Driven设计概念。DDD气氛一个复杂的domain到多个bounded contexts，然后映射这些关系。这个过程对整体应用和微服务架构都有用，但是在service和context边界之间有一层自然的关联，这层关联帮助清晰化、加强了分离界限。 As well as decentralizing decisions about conceptual models, microservices also decentralize data storage decisions. While monolithic applications prefer a single logical database for persistant data, enterprises often prefer a single database across a range of applications - many of these decisions driven through vendor’s commercial models around licensing. Microservices prefer letting each service manage its own database, either different instances of the same database technology, or entirely different database systems - an approach called Polyglot Persistence. You can use polyglot persistence in a monolith, but it appears more frequently with microservices. 像概念模型的去中心化一样，微服务也对数据存储方案去中心化。整体应用通常是一个逻辑数据库来持久化数据，企业通常使用一个单独的数据库给多个应用。微服务更原因让每个服务管理自己的数据库，甚至整个数据库系统-Polyglot Persistence。你可以在整体应用中使用Polyglot Persistence，但是它更常见与微服务架构中。 参考：http://martinfowler.com/articles/microservices.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari故障记录]]></title>
    <url>%2F2016%2F12%2F05%2Fambari%E6%95%85%E9%9A%9C%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[问题 ambari的metric界面上全都load不出来 hive命令行进不去，通过beeline连接成功，但是执行命令卡住 观察ambari-server.log里：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697989910010110210310410510610710810911011111211311427 May 2016 06:00:16,468 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|229f66ed]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - Task com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@13d18137 (in deadlocked PoolThread) failed to complete in maximum time 60000ms. Trying interrupt().27 May 2016 06:00:16,481 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|229f66ed]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - Task com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@28fdf8 (in deadlocked PoolThread) failed to complete in maximum time 60000ms. Trying interrupt().27 May 2016 06:00:16,481 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|229f66ed]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - Task com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@7a7239f6 (in deadlocked PoolThread) failed to complete in maximum time 60000ms. Trying interrupt().27 May 2016 06:32:26,157 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - com.mchange.v2.async.ThreadPoolAsynchronousRunner$DeadlockDetector@40bbff68 -- APPARENT DEADLOCK!!! Creating emergency threads for unassigned pending tasks!27 May 2016 06:32:26,171 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - com.mchange.v2.async.ThreadPoolAsynchronousRunner$DeadlockDetector@40bbff68 -- APPARENT DEADLOCK!!! Complete Status: Managed Threads: 3 Active Threads: 3 Active Tasks: com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@5383aba6 on thread: C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-HelperThread-#2 com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@38f907f5 on thread: C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-HelperThread-#1 com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@204f2da on thread: C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-HelperThread-#0 Pending Tasks: com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@1ddb0d4d com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@534b5bcbPool thread stack traces: Thread[C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-HelperThread-#2,5,main] java.net.SocketInputStream.socketRead0(Native Method) java.net.SocketInputStream.socketRead(SocketInputStream.java:116) java.net.SocketInputStream.read(SocketInputStream.java:170) java.net.SocketInputStream.read(SocketInputStream.java:141) com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:100) com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:143) com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:173) com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2911) com.mysql.jdbc.MysqlIO.readPacket(MysqlIO.java:559) com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1013) com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2239) com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2270) com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2069) com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:794) com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:44) sun.reflect.GeneratedConstructorAccessor186.newInstance(Unknown Source) sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) java.lang.reflect.Constructor.newInstance(Constructor.java:422) com.mysql.jdbc.Util.handleNewInstance(Util.java:389) com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:399) com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:325) com.mchange.v2.c3p0.DriverManagerDataSource.getConnection(DriverManagerDataSource.java:175) com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:220) com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:206) com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:203) com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1138) com.mchange.v2.resourcepool.BasicResourcePool.doAcquireAndDecrementPendingAcquiresWithinLockOnSuccess(BasicResourcePool.java:1125) com.mchange.v2.resourcepool.BasicResourcePool.access$700(BasicResourcePool.java:44) com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask.run(BasicResourcePool.java:1870) com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:696) Thread[C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-HelperThread-#1,5,main] java.net.SocketInputStream.socketRead0(Native Method) java.net.SocketInputStream.socketRead(SocketInputStream.java:116) java.net.SocketInputStream.read(SocketInputStream.java:170) java.net.SocketInputStream.read(SocketInputStream.java:141) com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:100) com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:143) com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:173) com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2911) com.mysql.jdbc.MysqlIO.readPacket(MysqlIO.java:559) com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1013) com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2239) com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2270) com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2069) com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:794) com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:44) sun.reflect.GeneratedConstructorAccessor186.newInstance(Unknown Source) sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) java.lang.reflect.Constructor.newInstance(Constructor.java:422) com.mysql.jdbc.Util.handleNewInstance(Util.java:389) com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:399) com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:325) com.mchange.v2.c3p0.DriverManagerDataSource.getConnection(DriverManagerDataSource.java:175) com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:220) com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:206) com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:203) com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1138) com.mchange.v2.resourcepool.BasicResourcePool.doAcquireAndDecrementPendingAcquiresWithinLockOnSuccess(BasicResourcePool.java:1125) com.mchange.v2.resourcepool.BasicResourcePool.access$700(BasicResourcePool.java:44) com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask.run(BasicResourcePool.java:1870) com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:696) Thread[C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-HelperThread-#0,5,main] java.net.SocketInputStream.socketRead0(Native Method) java.net.SocketInputStream.socketRead(SocketInputStream.java:116) java.net.SocketInputStream.read(SocketInputStream.java:170) java.net.SocketInputStream.read(SocketInputStream.java:141) com.mysql.jdbc.util.ReadAheadInputStream.fill(ReadAheadInputStream.java:100) com.mysql.jdbc.util.ReadAheadInputStream.readFromUnderlyingStreamIfNecessary(ReadAheadInputStream.java:143) com.mysql.jdbc.util.ReadAheadInputStream.read(ReadAheadInputStream.java:173) com.mysql.jdbc.MysqlIO.readFully(MysqlIO.java:2911) com.mysql.jdbc.MysqlIO.readPacket(MysqlIO.java:559) com.mysql.jdbc.MysqlIO.doHandshake(MysqlIO.java:1013) com.mysql.jdbc.ConnectionImpl.coreConnect(ConnectionImpl.java:2239) com.mysql.jdbc.ConnectionImpl.connectOneTryOnly(ConnectionImpl.java:2270) com.mysql.jdbc.ConnectionImpl.createNewIO(ConnectionImpl.java:2069) com.mysql.jdbc.ConnectionImpl.&lt;init&gt;(ConnectionImpl.java:794) com.mysql.jdbc.JDBC4Connection.&lt;init&gt;(JDBC4Connection.java:44) sun.reflect.GeneratedConstructorAccessor186.newInstance(Unknown Source) sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) java.lang.reflect.Constructor.newInstance(Constructor.java:422) com.mysql.jdbc.Util.handleNewInstance(Util.java:389) com.mysql.jdbc.ConnectionImpl.getInstance(ConnectionImpl.java:399) com.mysql.jdbc.NonRegisteringDriver.connect(NonRegisteringDriver.java:325) com.mchange.v2.c3p0.DriverManagerDataSource.getConnection(DriverManagerDataSource.java:175) com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:220) com.mchange.v2.c3p0.WrapperConnectionPoolDataSource.getPooledConnection(WrapperConnectionPoolDataSource.java:206) com.mchange.v2.c3p0.impl.C3P0PooledConnectionPool$1PooledConnectionResourcePoolManager.acquireResource(C3P0PooledConnectionPool.java:203) com.mchange.v2.resourcepool.BasicResourcePool.doAcquire(BasicResourcePool.java:1138) com.mchange.v2.resourcepool.BasicResourcePool.doAcquireAndDecrementPendingAcquiresWithinLockOnSuccess(BasicResourcePool.java:1125) com.mchange.v2.resourcepool.BasicResourcePool.access$700(BasicResourcePool.java:44) com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask.run(BasicResourcePool.java:1870) com.mchange.v2.async.ThreadPoolAsynchronousRunner$PoolThread.run(ThreadPoolAsynchronousRunner.java:696) 27 May 2016 06:33:26,173 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - Task com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@5383aba6 (in deadlocked PoolThread) failed to complete in maximum time 60000ms. Trying interrupt().27 May 2016 06:33:26,173 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - Task com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@38f907f5 (in deadlocked PoolThread) failed to complete in maximum time 60000ms. Trying interrupt().27 May 2016 06:33:26,174 WARN [C3P0PooledConnectionPoolManager[identityToken-&gt;2rvy2w9g1rvd7t51qlbfjd|74e262f6]-AdminTaskTimer] ThreadPoolAsynchronousRunner:220 - Task com.mchange.v2.resourcepool.BasicResourcePool$ScatteredAcquireTask@204f2da (in deadlocked PoolThread) failed to complete in maximum time 60000ms. Trying interrupt(). 尝试重启后，log如下12345678927 May 2016 10:24:07,167 INFO [main] Configuration:791 - Reading password from existing file27 May 2016 10:24:07,197 INFO [main] Configuration:1128 - Hosts Mapping File null27 May 2016 10:24:07,197 INFO [main] HostsMap:60 - Using hostsmap file null27 May 2016 10:24:08,493 INFO [main] ControllerModule:195 - Detected MYSQL as the database type from the JDBC URL27 May 2016 10:24:08,522 INFO [main] ControllerModule:237 - Using c3p0 ComboPooledDataSource as the EclipsLink DataSource27 May 2016 10:24:08,582 INFO [MLog-Init-Reporter] MLog:212 - MLog clients using slf4j logging.27 May 2016 10:24:08,963 INFO [main] C3P0Registry:212 - Initializing c3p0-0.9.5.2 [built 08-December-2015 22:06:04 -0800; debug? true; trace: 10]27 May 2016 10:24:11,656 INFO [main] ControllerModule:596 - Binding and registering notification dispatcher class org.apache.ambari.server.notifications.dispatchers.SNMPDispatcher27 May 2016 10:24:11,665 INFO [main] ControllerModule:596 - Binding and registering notification dispatcher class org.apache.ambari.server.notifications.dispatchers.AlertScriptDispatcher 想了一下会不会是mysql的问题，然后就尝试连接一下mysql —— 端口通，但是连接不上。找运维看了一下，mysql所在的机器的cpu爆满，有一个线程给打满了。 运维重启了，昨天集群跑任务的时候也是资源没有变，但是cpu被打满，以往同样的资源根本不会耗时那么多。个人怀疑是被人攻击了。 hive meta库重启之后，通过beeline是可以成功连接hive并查询的。但是hive命令行还是进不去，卡在:1234[hive@datanode01 root]$ hiveWARNING: Use &quot;yarn jar&quot; to launch YARN applications.Logging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.properties 查看hive的log:12342016-05-27 11:33:14,153 INFO [main]: SessionState (SessionState.java:printInfo(953)) - Logging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.properties2016-05-27 11:33:14,653 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(382)) - Trying to connect to metastore with URI thrift://datanode03.will.com:90832016-05-27 11:33:14,736 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(478)) - Connected to metastore. 正常命令行log：1234567891011121314151617181920212223242526272016-05-19 10:40:59,299 INFO [main]: SessionState (SessionState.java:printInfo(953)) -Logging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.properties2016-05-19 10:41:00,519 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(382)) - Trying to connect to metastore with URI thrift://datanode03.will.com:90832016-05-19 10:41:01,125 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(478)) - Connected to metastore.2016-05-19 10:41:04,652 INFO [main]: session.SessionState (SessionState.java:createPath(633)) - Created local directory: /tmp/1d1a2120-6339-40c1-b63b-5dbd4dc71074_resources2016-05-19 10:41:04,664 INFO [main]: session.SessionState (SessionState.java:createPath(633)) - Created HDFS directory: /tmp/hive/hive/1d1a2120-6339-40c1-b63b-5dbd4dc710742016-05-19 10:41:04,666 INFO [main]: session.SessionState (SessionState.java:createPath(633)) - Created local directory: /tmp/hive/1d1a2120-6339-40c1-b63b-5dbd4dc710742016-05-19 10:41:04,669 INFO [main]: session.SessionState (SessionState.java:createPath(633)) - Created HDFS directory: /tmp/hive/hive/1d1a2120-6339-40c1-b63b-5dbd4dc71074/_tmp_space.db2016-05-19 10:41:04,717 INFO [main]: sqlstd.SQLStdHiveAccessController (SQLStdHiveAccessController.java:&lt;init&gt;(95)) - Created SQLStdHiveAccessController for session context : HiveAuthzSessionContext [sessionString=1d1a2120-6339-40c1-b63b-5dbd4dc71074, clientType=HIVECLI]2016-05-19 10:41:04,720 INFO [main]: hive.metastore (HiveMetaStoreClient.java:isCompatibleWith(296)) - Mestastore configuration hive.metastore.filter.hook changed from org.apache.hadoop.hive.metastore.DefaultMetaStoreFilterHookImpl to org.apache.hadoop.hive.ql.security.authorization.plugin.AuthorizationMetaStoreFilterHook2016-05-19 11:21:13,947 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(382)) - Trying to connect to metastore with URI thrift://datanode03.will.com:90832016-05-19 11:21:13,951 INFO [main]: hive.metastore (HiveMetaStoreClient.java:open(478)) - Connected to metastore.2016-05-19 11:21:14,674 INFO [main]: SessionState (SessionState.java:printInfo(953)) - Added [/server/app/hive/will_hive_udf.jar] to class path2016-05-19 11:21:14,674 INFO [main]: SessionState (SessionState.java:printInfo(953)) - Added resources: [/server/app/hive/will_hive_udf.jar]2016-05-19 11:21:14,858 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG method=Driver.run from=org.apache.hadoop.hive.ql.Driver&gt;2016-05-19 11:21:14,858 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG method=TimeToSubmit from=org.apache.hadoop.hive.ql.Driver&gt;2016-05-19 11:21:14,858 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG method=compile from=org.apache.hadoop.hive.ql.Driver&gt;2016-05-19 11:21:14,923 INFO [main]: ql.Driver (Driver.java:compile(420)) - We are setting the hadoop caller context from to hive_20160519112114_e4649b5a-c094-40ff-a9b1-aa6ab595c9672016-05-19 11:21:14,928 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG method=parse from=org.apache.hadoop.hive.ql.Driver&gt;2016-05-19 11:21:14,937 INFO [main]: parse.ParseDriver (ParseDriver.java:parse(185)) - Parsing command: create temporary function strstart as &apos;com.will.common.hive.StrStart&apos;2016-05-19 11:21:16,408 INFO [main]: parse.ParseDriver (ParseDriver.java:parse(209)) - Parse Completed2016-05-19 11:21:16,411 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(162)) - &lt;/PERFLOG method=parse start=1463628074928 end=1463628076411 duration=1483 from=org.apache.hadoop.hive.ql.Driver&gt;2016-05-19 11:21:16,412 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(135)) - &lt;PERFLOG method=semanticAnalyze from=org.apache.hadoop.hive.ql.Driver&gt;2016-05-19 11:21:17,249 INFO [main]: parse.FunctionSemanticAnalyzer (FunctionSemanticAnalyzer.java:analyzeInternal(66)) - analyze done2016-05-19 11:21:17,249 INFO [main]: ql.Driver (Driver.java:compile(471)) - Semantic Analysis Completed 可以看到是createPath那卡住了，建立临时会话相关文件。同事那边要得急，就直接把hive metastore重启了，就恢复正常了。这样看来如果hive metastore和mysql meta库失去连接的话，即便mysql恢复正常，启动hive命令行的时候，HiveMetaStoreClient也会有问题。 这个可以修复一下，然后commit给hive官网。。。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[cas-4.2.6-server配置与http支持]]></title>
    <url>%2F2016%2F12%2F05%2Fcas-4.2.6-server%E9%85%8D%E7%BD%AE%E4%B8%8Ehttp%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"><![CDATA[下载下载cas-server-webapp-4.2.6.war，放置到tomcat9的webapps下。 开始使用的tomcat7，结果报错：Unable to process Jar entry。查了一下是tomcat版本过低的原因。 启动tomcat，确认war正常解压，cas server正常启动。关闭tomcat。 编辑cas的主要配置文件在于deployerConfigContext.xml。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:p="http://www.springframework.org/schema/p" xmlns:c="http://www.springframework.org/schema/c" xmlns:aop="http://www.springframework.org/schema/aop" xmlns:tx="http://www.springframework.org/schema/tx" xmlns:util="http://www.springframework.org/schema/util" xmlns:sec="http://www.springframework.org/schema/security" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd http://www.springframework.org/schema/aop http://www.springframework.org/schema/aop/spring-aop.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/security http://www.springframework.org/schema/security/spring-security.xsd http://www.springframework.org/schema/util http://www.springframework.org/schema/util/spring-util.xsd"&gt; &lt;util:map id="authenticationHandlersResolvers"&gt; &lt;entry key-ref="proxyAuthenticationHandler" value-ref="proxyPrincipalResolver" /&gt; &lt;entry key-ref="primaryAuthenticationHandler" value-ref="primaryPrincipalResolver" /&gt; &lt;/util:map&gt; &lt;util:list id="authenticationMetadataPopulators"&gt; &lt;ref bean="successfulHandlerMetaDataPopulator" /&gt; &lt;ref bean="rememberMeAuthenticationMetaDataPopulator" /&gt; &lt;/util:list&gt; &lt;bean id="attributeRepository" class="org.jasig.services.persondir.support.NamedStubPersonAttributeDao" p:backingMap-ref="attrRepoBackingMap" /&gt; &lt;!-- 设置数据源 --&gt; &lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"&gt;&lt;/property&gt; &lt;property name="url" value="jdbc:mysql://127.0.0.1:3306/metamap1?useUnicode=true&amp;amp;characterEncoding=utf8"&gt;&lt;/property&gt; &lt;property name="username" value="root"&gt;&lt;/property&gt; &lt;property name="password" value=""&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id="primaryAuthenticationHandler" class="com.will.cas.auth.WillAuthenticationHandler" p:dataSource-ref="dataSource" /&gt; &lt;!-- &lt;alias name="acceptUsersAuthenticationHandler" alias="primaryAuthenticationHandler" /&gt; --&gt; &lt;alias name="personDirectoryPrincipalResolver" alias="primaryPrincipalResolver" /&gt; &lt;util:map id="attrRepoBackingMap"&gt; &lt;entry key="uid" value="uid" /&gt; &lt;entry key="eduPersonAffiliation" value="eduPersonAffiliation" /&gt; &lt;entry key="groupMembership" value="groupMembership" /&gt; &lt;entry&gt; &lt;key&gt;&lt;value&gt;memberOf&lt;/value&gt;&lt;/key&gt; &lt;list&gt; &lt;value&gt;faculty&lt;/value&gt; &lt;value&gt;staff&lt;/value&gt; &lt;value&gt;org&lt;/value&gt; &lt;/list&gt; &lt;/entry&gt; &lt;/util:map&gt; &lt;alias name="serviceThemeResolver" alias="themeResolver" /&gt; &lt;!-- &lt;alias name="jsonServiceRegistryDao" alias="serviceRegistryDao" /&gt; --&gt; &lt;!-- 注册服务 --&gt; &lt;bean id="serviceRegistryDao" class="org.jasig.cas.services.InMemoryServiceRegistryDaoImpl" p:registeredServices-ref="registeredServicesList" /&gt; &lt;util:list id="registeredServicesList"&gt; &lt;bean class="org.jasig.cas.services.RegexRegisteredService" p:id="0" p:name="HTTP and IMAP" p:description="Allows HTTP(S) and IMAP(S) protocols" p:serviceId="^(https?|http?|imaps?)://.*" p:evaluationOrder="10000001" /&gt; &lt;/util:list&gt; &lt;alias name="defaultTicketRegistry" alias="ticketRegistry" /&gt; &lt;alias name="ticketGrantingTicketExpirationPolicy" alias="grantingTicketExpirationPolicy" /&gt; &lt;alias name="multiTimeUseOrTimeoutExpirationPolicy" alias="serviceTicketExpirationPolicy" /&gt; &lt;alias name="anyAuthenticationPolicy" alias="authenticationPolicy" /&gt; &lt;alias name="acceptAnyAuthenticationPolicyFactory" alias="authenticationPolicyFactory" /&gt; &lt;bean id="auditTrailManager" class="org.jasig.inspektr.audit.support.Slf4jLoggingAuditTrailManager" p:entrySeparator="$&#123;cas.audit.singleline.separator:|&#125;" p:useSingleLine="$&#123;cas.audit.singleline:false&#125;"/&gt; &lt;alias name="neverThrottle" alias="authenticationThrottle" /&gt; &lt;util:list id="monitorsList"&gt; &lt;ref bean="memoryMonitor" /&gt; &lt;ref bean="sessionMonitor" /&gt; &lt;/util:list&gt; &lt;alias name="defaultPrincipalFactory" alias="principalFactory" /&gt; &lt;alias name="defaultAuthenticationTransactionManager" alias="authenticationTransactionManager" /&gt; &lt;alias name="defaultPrincipalElectionStrategy" alias="principalElectionStrategy" /&gt; &lt;alias name="tgcCipherExecutor" alias="defaultCookieCipherExecutor" /&gt;&lt;/beans&gt; cas提供了几种连接数据库验证的实现，但是对于我们验证django用户的需求不适合，因为django用户的密码的加密salt是动态的。如果是简单的数据库用户验证，可以参考：https://apereo.github.io/cas/4.2.x/installation/Database-Authentication.html 这里我们实现了一个自己的AuthenticationHandler，主要代码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204package com.will.cas.auth;/* Example implementation of password hasher similar on Django's PasswordHasher * Requires Java8 (but should be easy to port to older JREs) * Currently it would work only for pbkdf2_sha256 algorithm * * Django code: https://github.com/django/django/blob/1.6.5/django/contrib/auth/hashers.py#L221 */import org.jasig.cas.authentication.HandlerResult;import org.jasig.cas.authentication.PreventedException;import org.jasig.cas.authentication.UsernamePasswordCredential;import org.jasig.cas.authentication.handler.support.AbstractUsernamePasswordAuthenticationHandler;import org.springframework.jdbc.core.JdbcTemplate;import javax.crypto.SecretKey;import javax.crypto.SecretKeyFactory;import javax.crypto.spec.PBEKeySpec;import javax.security.auth.login.FailedLoginException;import javax.sql.DataSource;import javax.validation.constraints.NotNull;import java.nio.charset.Charset;import java.security.GeneralSecurityException;import java.security.NoSuchAlgorithmException;import java.security.spec.InvalidKeySpecException;import java.security.spec.KeySpec;import java.util.Base64;class WillAuthenticationHandler extends AbstractUsernamePasswordAuthenticationHandler &#123; private JdbcTemplate jdbcTemplate; private DataSource dataSource; /** * Method to set the datasource and generate a JdbcTemplate. * * @param dataSource the datasource to use. */ public void setDataSource(@NotNull final DataSource dataSource) &#123; this.jdbcTemplate = new JdbcTemplate(dataSource); this.dataSource = dataSource; &#125; /** * Method to return the jdbcTemplate. * * @return a fully created JdbcTemplate. */ protected final JdbcTemplate getJdbcTemplate() &#123; return this.jdbcTemplate; &#125; protected final DataSource getDataSource() &#123; return this.dataSource; &#125; public final Integer DEFAULT_ITERATIONS = 10000; public final String algorithm = "pbkdf2_sha256"; public WillAuthenticationHandler() &#123; &#125; protected HandlerResult authenticateUsernamePasswordInternal(UsernamePasswordCredential credential) throws GeneralSecurityException, PreventedException &#123; String username = credential.getUsername(); String pwd = credential.getPassword(); System.out.println("Got username : " + username + " and password : " + pwd); int count = this.jdbcTemplate.queryForObject("select count(1) from auth_user where username=?", Integer.class, username); if (count == 0) &#123; System.out.println(username + " not found with SQL query."); throw new FailedLoginException(username + " not found with SQL query."); &#125; String encyptedPassword = this.jdbcTemplate.queryForObject("select password from auth_user where username=?", String.class, username); System.out.println("Got encyptedPassword : " + encyptedPassword); if (checkPassword(pwd, encyptedPassword)) &#123; System.out.println(" auth success for user : " + username); return createHandlerResult(credential, this.principalFactory.createPrincipal(username), null); &#125; throw new FailedLoginException(username + "'s password is wrong for : " + pwd); &#125; public String getEncodedHash(String password, String salt, int iterations) &#123; // Returns only the last part of whole encoded password SecretKeyFactory keyFactory = null; try &#123; keyFactory = SecretKeyFactory.getInstance("PBKDF2WithHmacSHA256"); &#125; catch (NoSuchAlgorithmException e) &#123; System.err.println("Could NOT retrieve PBKDF2WithHmacSHA256 algorithm"); System.exit(1); &#125; KeySpec keySpec = new PBEKeySpec(password.toCharArray(), salt.getBytes(Charset.forName("UTF-8")), iterations, 256); SecretKey secret = null; try &#123; secret = keyFactory.generateSecret(keySpec); &#125; catch (InvalidKeySpecException e) &#123; System.out.println("Could NOT generate secret key"); e.printStackTrace(); &#125; byte[] rawHash = secret.getEncoded(); byte[] hashBase64 = Base64.getEncoder().encode(rawHash); return new String(hashBase64); &#125; public String encode(String password, String salt, int iterations) &#123; // returns hashed password, along with algorithm, number of iterations and salt String hash = getEncodedHash(password, salt, iterations); return String.format("%s$%d$%s$%s", algorithm, iterations, salt, hash); &#125; public String encode(String password, String salt) &#123; return this.encode(password, salt, this.DEFAULT_ITERATIONS); &#125; public boolean checkPassword(String password, String hashedPassword) &#123; // hashedPassword consist of: ALGORITHM, ITERATIONS_NUMBER, SALT and // HASH; parts are joined with dollar character ("$") String[] parts = hashedPassword.split("\\$"); if (parts.length != 4) &#123; // wrong hash format return false; &#125; Integer iterations = Integer.parseInt(parts[1]); String salt = parts[2]; String hash = encode(password, salt, iterations); return hash.equals(hashedPassword); &#125; // Following examples can be generated at any Django project: // // &gt;&gt;&gt; from django.contrib.auth.hashers import make_password // &gt;&gt;&gt; make_password('mystery', hasher='pbkdf2_sha256') # salt would be randomly generated // 'pbkdf2_sha256$10000$HqxvKtloKLwx$HdmdWrgv5NEuaM4S6uMvj8/s+5Yj+I/d1ay6zQyHxdg=' // &gt;&gt;&gt; make_password('mystery', salt='mysalt', hasher='pbkdf2_sha256') // 'pbkdf2_sha256$10000$mysalt$KjUU5KrwyUbKTGYkHqBo1IwUbFBzKXrGQgwA1p2AuY0=' // // // mystery // pbkdf2_sha256$10000$qx1ec0f4lu4l$3G81rAm/4ng0tCCPTrx2aWohq7ztDBfFYczGNoUtiKQ= // // s3cr3t // pbkdf2_sha256$10000$BjDHOELBk7fR$xkh1Xf6ooTqwkflS3rAiz5Z4qOV1Jd5Lwd8P+xGtW+I= // // puzzle // pbkdf2_sha256$10000$IFYFG7hiiKYP$rf8vHYFD7K4q2N3DQYfgvkiqpFPGCTYn6ZoenLE3jLc= // // riddle // pbkdf2_sha256$10000$A0S5o3pNIEq4$Rk2sxXr8bonIDOGj6SU4H/xpjKHhHAKpFXfmNZ0dnEY= public static void main(String[] args) &#123; runTests(); &#125; private static void runTests() &#123; System.out.println("==========================="); System.out.println("= Testing password hasher ="); System.out.println("==========================="); System.out.println(); System.out.println(); passwordShouldMatch("admin", "pbkdf2_sha256$12000$g3UeiuUEnwfp$Qr/Qb6cCntnCflkBcJ+OenxWubIu6O84xLWGgWqNftw="); passwordShouldMatch("will@2016", "pbkdf2_sha256$30000$tTJpdOsd3eoN$GcBF0YoHg8eV7s6NBqjX1i3VllEIeYuMUsyH5stzuw4="); System.out.println(); passwordShouldNotMatch("foo", ""); passwordShouldNotMatch("mystery", "pbkdf2_md5$10000$qx1ec0f4lu4l$3G81rAm/4ng0tCCPTrx2aWohq7ztDBfFYczGNoUtiKQ="); passwordShouldNotMatch("mystery", "pbkdf2_sha1$10000$qx1ec0f4lu4l$3G81rAm/4ng0tCCPTrx2aWohq7ztDBfFYczGNoUtiKQ="); passwordShouldNotMatch("mystery", "pbkdf2_sha256$10001$Qx1ec0f4lu4l$3G81rAm/4ng0tCCPTrx2aWohq7ztDBfFYczGNoUtiKQ="); passwordShouldNotMatch("mystery", "pbkdf2_sha256$10001$qx1ec0f4lu4l$3G81rAm/4ng0tCCPTrx2aWohq7ztDBfFYczGNoUtiKQ="); passwordShouldNotMatch("mystery", "pbkdf2_sha256$10000$qx7ztDBfFYczGNoUtiKQ="); passwordShouldNotMatch("s3cr3t", "pbkdf2_sha256$10000$BjDHOELBk7fR$foobar"); passwordShouldNotMatch("puzzle", "pbkdf2_sha256$10000$IFYFG7hiiKYP$rf8vHYFD7K4q2N3DQYfgvkiqpFPGCTYn6ZoenLE3jLcX"); &#125; private static void passwordShouldMatch(String password, String expectedHash) &#123; WillAuthenticationHandler willAuthenticationHandler = new WillAuthenticationHandler(); if (willAuthenticationHandler.checkPassword(password, expectedHash)) &#123; System.out.println(" =&gt; OK"); &#125; else &#123; String[] parts = expectedHash.split("\\$"); if (parts.length != 4) &#123; System.out.printf(" =&gt; Wrong hash provided: '%s'\n", expectedHash); return; &#125; String salt = parts[2]; String resultHash = willAuthenticationHandler.encode(password, salt); String msg = " =&gt; Wrong! Password '%s' hash expected to be '%s' but is '%s'\n"; System.out.printf(msg, password, expectedHash, resultHash); &#125; &#125; private static void passwordShouldNotMatch(String password, String expectedHash) &#123; WillAuthenticationHandler willAuthenticationHandler = new WillAuthenticationHandler(); if (willAuthenticationHandler.checkPassword(password, expectedHash)) &#123; System.out.printf(" =&gt; Wrong (password '%s' did '%s' match but were not supposed to)\n", password, expectedHash); &#125; else &#123; System.out.println(" =&gt; OK (password didn't match)"); &#125; &#125;&#125; 基本是抄袭了AbstractJdbcUsernamePasswordAuthenticationHandler的内容，然后执行了自己的算法实现。本来是可以直接继承AbstractJdbcUsernamePasswordAuthenticationHandler，重写方法实现的，但是考虑到我们要兼容很多个系统的数据库用户，所以需要多个dataSource和jdbcTemplate，就放弃了。 对于cas配置的切入点在于authenticationHandlersResolvers，以前版本的都是有个authenticationManager，然后编辑他的构造参数来定义认证处理器的，4.2.6貌似把它内置了。 1234567891011121314151617 &lt;util:map id="authenticationHandlersResolvers"&gt; &lt;entry key-ref="proxyAuthenticationHandler" value-ref="proxyPrincipalResolver" /&gt; &lt;entry key-ref="primaryAuthenticationHandler" value-ref="primaryPrincipalResolver" /&gt; &lt;/util:map&gt;&lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;property name="driverClassName" value="com.mysql.jdbc.Driver"&gt;&lt;/property&gt; &lt;property name="url" value="jdbc:mysql://127.0.0.1:3306/metamap1?useUnicode=true&amp;amp;characterEncoding=utf8"&gt;&lt;/property&gt; &lt;property name="username" value="root"&gt;&lt;/property&gt; &lt;property name="password" value=""&gt;&lt;/property&gt; &lt;/bean&gt; &lt;bean id="primaryAuthenticationHandler" class="com.will.cas.auth.WillAuthenticationHandler" p:dataSource-ref="dataSource" /&gt; 如此，便指定了自定义的认证处理器。 问题在app1配置cas server为当前cas server后，跳转到登录页面，结果提示： 未认证的授权服务 不允许使用CAS来认证您访问的目标应用 查了一下解决方案，同样是在deployerConfigContext.xml中配置服务注册的DAO serviceRegistryDao为内存实现类，然后使用正则表达式表示这个cas server接收的服务：1&lt;alias name="jsonServiceRegistryDao" alias="serviceRegistryDao" /&gt; 改为：12345678&lt;bean id="serviceRegistryDao" class="org.jasig.cas.services.InMemoryServiceRegistryDaoImpl" p:registeredServices-ref="registeredServicesList" /&gt; &lt;util:list id="registeredServicesList"&gt; &lt;bean class="org.jasig.cas.services.RegexRegisteredService" p:id="0" p:name="HTTP and IMAP" p:description="Allows HTTP(S) and IMAP(S) protocols" p:serviceId="^(https?|http?|imaps?)://.*" p:evaluationOrder="10000001" /&gt; &lt;/util:list&gt; 其实理论上应该也可以在默认的jsonServiceRegistryDao配置的，但是找了一会儿没有找到json配置文件的位置。注意了一下貌似它是属于cas-addons的，有些插件的意思，后面再探查一下。 参考： https://github.com/Unicon/cas-addons/wiki/Configuring-JSON-Service-Registry http://www.cnphp6.com/archives/133139]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mondrian-定义schema]]></title>
    <url>%2F2016%2F12%2F05%2Fmondrian-%E5%AE%9A%E4%B9%89schema%2F</url>
    <content type="text"><![CDATA[一个schema定义了一个多维数据库。它包含一个逻辑模型，这个逻辑模型中包含了多个立方体、层级、成员、以及将这个模型映射到物理模型的映射。 逻辑模型包含了MDX语言中的构造组件：: cubes, dimensions, hierarchies, levels, and members.物理模型是物理数据源。一般是星型模型，就是RDBMS中的一堆表。 Scheme 文件mondrian的schema是定义在xml文件中的。有一个比较全面的例子demo/FoodMart.xml。目前，创建schema只能通过编辑xml文件来实现。下面是xml结构：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;Schema&gt;&lt;Cube&gt;&lt;Table&gt;&lt;AggName&gt;aggElements&lt;AggPattern&gt;aggElements&lt;Dimension&gt;&lt;Hierarchy&gt;relation&lt;Closure/&gt;&lt;Level&gt;&lt;KeyExpression&gt;&lt;SQL/&gt;&lt;NameExpression&gt;&lt;SQL/&gt;&lt;CaptionExpression&gt;&lt;SQL/&gt;&lt;OrdinalExpression&gt;&lt;SQL/&gt;&lt;ParentExpression&gt;&lt;SQL/&gt;&lt;Property&gt;&lt;PropertyExpression&gt;&lt;SQL/&gt;&lt;DimensionUsage&gt;&lt;Measure&gt;&lt;MeasureExpression&gt;&lt;SQL/&gt;&lt;CalculatedMemberProperty/&gt;&lt;CalculatedMember&gt;&lt;Formula/&gt;&lt;CalculatedMemberProperty/&gt;&lt;NamedSet&gt;&lt;Formula/&gt;&lt;VirtualCube&gt;&lt;CubeUsages&gt;&lt;CubeUsage&gt;&lt;VirtualCubeDimension&gt;&lt;VirtualCubeMeasure&gt;&lt;Role&gt;&lt;SchemaGrant&gt;&lt;CubeGrant&gt;&lt;DimensionGrant&gt;&lt;HierarchyGrant&gt;&lt;MemberGrant/&gt;&lt;Union&gt;&lt;RoleUsage/&gt;&lt;UserDefinedFunction/&gt;&lt;Parameter/&gt;relation ::=&lt;Table&gt;&lt;SQL/&gt;&lt;View&gt;&lt;SQL/&gt;&lt;InlineTable&gt;&lt;ColumnDefs&gt;&lt;ColumnDef&gt;&lt;Rows&gt;&lt;Row&gt;&lt;Value&gt;&lt;Join&gt;relationaggElement ::=&lt;AggExclude&gt;&lt;AggFactCount&gt;&lt;AggIgnoreColumn&gt;&lt;AggForeignKey&gt;&lt;AggMeasure&gt;&lt;AggLevel&gt; 注意：xml元素的顺序。 必须在 , , and 元素后面。 2.1 注解主要的元素(schema, cube, virtual cube, shared dimension, dimension, hierarchy, level, measure, calculated member)都支持注解。注解能够把用户定义的属性和元数据元素联系起来，允许工具在不继承mondrian schema的前提下添加元数据。 下面例子，为Schema添加了 Author和Date两个注解。1234567&lt;Schema name="Rock Sales"&gt;&lt;Annotations&gt;&lt;Annotation name="Author"&gt;Fred Flintstone&lt;/Annotation&gt;&lt;Annotation name="Date"&gt;10,000 BC&lt;/Annotation&gt;&lt;/Annotations&gt;&lt;Cube name="Sales"&gt;... 3 逻辑模型最重要的三个组件： cubes, measures, dimensions。 cube。 维度和指标在指定子区域的集合。 measure。量化的指标。 dimension。可切分指标的属性。 下面是一个简单的schema:12345678910111213141516171819202122232425&lt;Schema&gt; &lt;Cube name="Sales"&gt; &lt;Table name="sales_fact_1997"/&gt; &lt;Dimension name="Gender" foreignKey="customer_id"&gt; &lt;Hierarchy hasAll="true" allMemberName="All Genders" primaryKey="customer_id"&gt; &lt;Table name="customer"/&gt; &lt;Level name="Gender" column="gender" uniqueMembers="true"/&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Dimension name="Time" foreignKey="time_id"&gt; &lt;Hierarchy hasAll="false" primaryKey="time_id"&gt; &lt;Table name="time_by_day"/&gt; &lt;Level name="Year" column="the_year" type="Numeric" uniqueMembers="true"/&gt; &lt;Level name="Quarter" column="quarter" uniqueMembers="false"/&gt; &lt;Level name="Month" column="month_of_year" type="Numeric" uniqueMembers="false"/&gt; &lt;/Hierarchy&gt; &lt;/Dimension&gt; &lt;Measure name="Unit Sales" column="unit_sales" aggregator="sum" formatString="#,###"/&gt; &lt;Measure name="Store Sales" column="store_sales" aggregator="sum" formatString="#,###.##"/&gt; &lt;Measure name="Store Cost" column="store_cost" aggregator="sum" formatString="#,###.00"/&gt; &lt;CalculatedMember name="Profit" dimension="Measures" formula="[Measures].[Store Sales] - [Measures].[Store Cost]"&gt; &lt;CalculatedMemberProperty name="FORMAT_STRING" value="$#,##0.00"/&gt; &lt;/CalculatedMember&gt; &lt;/Cube&gt;&lt;/Schema&gt; 这个schema只有一个立方体，叫做’sales’。两个维度，time和gender。四个指标。 我们可以基于这个schema写出如下MDX查询：1234SELECT &#123;[Measures].[Unit Sales], [Measures].[Store Sales]&#125; ON COLUMNS, &#123;descendants([Time].[1997].[Q1])&#125; ON ROWSFROM [Sales]WHERE [Gender].[F] 3.1 cubecube是指标和维度的集合。指标和维度的共同处之一就是都在事实表上，这里就是 “sales_fact_1997表。指标基于事实表里的字段计算，事实表里也包含所有的维度。 事实表使用元素定义。如果事实表不在默认的schema中，你可以使用schema属性明确指定：1&lt;Table schema=" dmart" name="sales_fact_1997"/&gt; 还可以使用构造更复杂的SQL语句。事实表不支持。 3.2 measureSales cube定义了 “Unit Sales” 和 “Store Sales”几个指标。12&lt;Measure name="Unit Sales" column="unit_sales" aggregator="sum" datatype="Integer" formatString="#,###"/&gt;&lt;Measure name="Store Sales" column="store_sales" aggregator="sum" datatype="Numeric" formatString="#,###.00"/&gt; datatype是可选的属性，它指示着这个指标在mondrian缓存中存在的形式，也决定这些指标怎样通过XML返回。(“String”, “Integer”, “Numeric”, “Boolean”, “Date”, “Time”, “Timestamp”) 。 除了”count”或者 “distinct-count” 是”Integer”，其他的aggregator的默认值都是”Numeric”。 formatString也是可选的，它代表指标被打印的形式。更多参考 获取某个指标的名称可以调用 Member.getCaption() 。 指标使用cell reader，而不是读取column，或者也能使用sql表达式来计算。 “Promotion Sales”就是通过sql表达式计算的：1234567&lt;Measure name="Promotion Sales" aggregator="sum" formatString="#,###.00"&gt; &lt;MeasureExpression&gt; &lt;SQL dialect="generic"&gt; (case when sales_fact_1997.promotion_id = 0 then 0 else sales_fact_1997.store_sales end) &lt;/SQL&gt; &lt;/MeasureExpression&gt;&lt;/Measure&gt; 要格式化cell 值，请参考 3.3 dimension, hierarchy, level先看一些定义： member。 指定维度中的一个点。性别hierarchy包含M和F两个member。 hierarchy。一组形成某个结构的member。store hierarchy包含name, city, state, nation。这个hierarchy允许出现中间层次，一个state的子聚合（sub-total）是这个state的所有city的所有子聚合(sub-total)之和。 level。同一个level的member距离hierarchy最上层root的距离相等，也就是说在同一层上 dimemsion。一组hierarchy的集合，用来区分同一个事实表中的属性。 例子：123456&lt;Dimension name="Gender" foreignKey="customer_id"&gt; &lt;Hierarchy hasAll="true" primaryKey="customer_id"&gt; &lt;Table name="customer"/&gt; &lt;Level name="Gender" column="gender" uniqueMembers="true"/&gt; &lt;/Hierarchy&gt;&lt;/Dimension&gt; 这个维度包含了一个hierarchy，这个hierarchy包含了一个level。这个维度的值来源于customer表的gender字段。 对于任意一个订单，gender维度是这个顾客的性别，应该是事实表 “sales_fact_1997.customer_id” 和维度表”customer.customer_id” join的结果。 3.3.1 将维度和hierarchy映射到表一个维度通过一对儿字段join进入一个cube，一个字段来自事实表，另一个来自维度表。 元素有一个foreignKey属性，它是事实表里字段的名字。元素有一个primaryKey属性。 如果hierarchy涉及到多个表，我们可以使用primaryKey来区分。 column属性定义了level的key。必须是在level表中的名字。如果key是一个表达式，可以在level中使用元素。下面是一个例子12345678910&lt;Dimension name="Gender" foreignKey="customer_id"&gt; &lt;Hierarchy hasAll="true" primaryKey="customer_id"&gt; &lt;Table name="customer"/&gt; &lt;Level name="Gender" column="gender" uniqueMembers="true"&gt; &lt;KeyExpression&gt; &lt;SQL dialect="generic"&gt;customer.gender&lt;/SQL&gt; &lt;/KeyExpression&gt; &lt;/Level&gt; &lt;/Hierarchy&gt;&lt;/Dimension&gt; , , 还有一些嵌套属性：Parent element | 属性 | Equivalent nested element | 描述—|—|—|— | column | | Key of level.]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[公平调度器-capacity-scheduler]]></title>
    <url>%2F2016%2F12%2F05%2F%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6%E5%99%A8-capacity-scheduler%2F</url>
    <content type="text"><![CDATA[概览公平调度器是Hadoop上的一个可配置的调度器，支持多租户共享集群资源，最大化利用集群资源。传统来说，每个组织都有自己的私有资源，这些资源能够在一些极端峰值状态足够满足自己部门的SLA。这样其实是有问题的，所有的集群都需要管理，而且每个部门的资源平均使用率并不高。集群共享其实是一个比较划算的事情，每个部门既能完成工作，又不用去单独管理各自的集群。但是，各部门又有些担心如果共享集群资源的话，其他部门的东西会不会对自己的SLA造成影响。CapacityScheduler就是解决这个问题而生的，它可以让多个部门共享一个大的集群资源，而且各个部门的资源不会被其他部门使用，这保证了部门之间资源共享的灵活性，而且很划算。 多个组织之间共享集群资源，很重要的一点是保证每个组织有一个自保的机制，保证自己的资源不受其他部门资源调度的影响。CapacityScheduler提供了一套严格的保证机制来限制应用、用户、queue等不能无限制地访问集群资源。另外，CapacityScheduler也限制了一个用户或者queue的应用的初始化与pending资源来保证集群的稳定性和公平性。 CapacityScheduler提供的主要概念是queue的概念。一般queue都是管理员创建的，反映了共享集群的各个经济单体。 为了进一步的控制资源共享情况，CapacityScheduler支持纵向队列，也就是资源还可以在sub-queue子队列里进行分配。To provide further control and predictability on sharing of resources, the CapacityScheduler supports hierarchical queues to ensure resources are shared among the sub-queues of an organization before other queues are allowed to use free resources, there-by providing affinity for sharing free resources among applications of a given organization. 特性如下: Hierarchical Queues 纵向队列 -在其他queue使用空闲资源之前，支持资源可被当前组织的sub-queue优先抢占，以此来提供更高的可控性和可预测性。 保证容量 - 保证总资源的某个百分比的容量随时听候某queue的调遣。所有提交到这个queue的应用都可以访问这部分资源。 管理员可以灵活配置soft limit和hard limit限制每个queue的容量占比。 安全 - 每个queue都有严格的ACLs策略控制谁可以提交应用到哪个queue。另外，也不能让某些普通用户看到或者修改运行在其他queue里的应用。 管理员角色粒度可以对应一个queue或整个scheduler系统。 弹性 - 任何队列资源不够的话，都可以使用空闲资源区的资源。 When there is demand for these resources from queues running below capacity at a future point in time, as tasks scheduled on these resources complete, they will be assigned to applications on queues running below the capacity (pre-emption is not supported). This ensures that resources are available in a predictable and elastic manner to queues, thus preventing artifical silos of resources in the cluster which helps utilization. 多租户 - Comprehensive set of limits are provided to prevent a single application, user and queue from monopolizing resources of the queue or the cluster as a whole to ensure that the cluster isn’t overwhelmed.Operability 运行时配置优先 - The queue definitions and properties such as capacity, ACLs can be changed, at runtime, by administrators in a secure manner to minimize disruption to users. Also, a console is provided for users and administrators to view current allocation of resources to various queues in the system. Administrators can add additional queues at runtime, but queues cannot be deleted at runtime. 清空应用 - .如果一个queue是STOPPED状态，就不能向它或者它的任何sub-queue提交应用了。 正在运行的应用会继续运行到完成，因此queue可以被优雅的清空。管理员也可以启动或者停掉某个queue。 基于资源的调度 - 可以多分配一些配置，主要是为了支持一些资源敏感的应用。Queue Mapping based on User or Group - This feature allows users to map a job to a specific queue based on the user or group.配置 配置ResourceManager使用CapacityScheduler conf/yarn-site.xml:| 属性 | 值 ||—|—|| yarn.resourcemanager.scheduler.class |org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler || | || | | 配置queues etc/hadoop/capacity-scheduler.xml是CapacityScheduler的主配置文件.CapacityScheduler又一个预定义的queue: root, 所有的队列都是这个队列的子队列sub-queue.其他的队列通过设置 yarn.scheduler.capacity.root.queues属性，弄一个逗号隔开的字符串就行.CapacityScheduler使用queue path配置queue的层次. queue path是queue层级的全路径, 以root开始, 使用. (点)作为分隔符.指定queue的所有sub-queue可以这样指定: yarn.scheduler.capacity..queues. 子队列并不自动继承queue的属性。下面是一个例子，a\b\c为root的子队列，然后a\b还有自己的子队列:1234567891011121314151617181920&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.queues&lt;/name&gt; &lt;value&gt;a,b,c&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.a.queues&lt;/name&gt; &lt;value&gt;a1,a2&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.root.b.queues&lt;/name&gt; &lt;value&gt;b1,b2,b3&lt;/value&gt; &lt;description&gt;The queues at the this level (root is the root queue). &lt;/description&gt;&lt;/property&gt; queue属性Resource AllocationProperty Descriptionyarn.scheduler.capacity..capacity Queue capacity in percentage (%) as a float (e.g. 12.5). The sum of capacities for all queues, at each level, must be equal to 100. Applications in the queue may consume more resources than the queue’s capacity if there are free resources, providing elasticity.yarn.scheduler.capacity..maximum-capacity Maximum queue capacity in percentage (%) as a float. This limits the elasticity for applications in the queue. Defaults to -1 which disables it.yarn.scheduler.capacity..minimum-user-limit-percent Each queue enforces a limit on the percentage of resources allocated to a user at any given time, if there is demand for resources. The user limit can vary between a minimum and maximum value. The the former (the minimum value) is set to this property value and the latter (the maximum value) depends on the number of users who have submitted applications. For e.g., suppose the value of this property is 25. If two users have submitted applications to a queue, no single user can use more than 50% of the queue resources. If a third user submits an application, no single user can use more than 33% of the queue resources. With 4 or more users, no user can use more than 25% of the queues resources. A value of 100 implies no user limits are imposed. The default is 100. Value is specified as a integer.yarn.scheduler.capacity..user-limit-factor The multiple of the queue capacity which can be configured to allow a single user to acquire more resources. By default this is set to 1 which ensures that a single user can never take more than the queue’s configured capacity irrespective of how idle th cluster is. Value is specified as a float.yarn.scheduler.capacity..maximum-allocation-mb The per queue maximum limit of memory to allocate to each container request at the Resource Manager. This setting overrides the cluster configuration yarn.scheduler.maximum-allocation-mb. This value must be smaller than or equal to the cluster maximum.yarn.scheduler.capacity..maximum-allocation-vcores The per queue maximum limit of virtual cores to allocate to each container request at the Resource Manager. This setting overrides the cluster configuration yarn.scheduler.maximum-allocation-vcores. This value must be smaller than or equal to the cluster maximum.Running and Pending Application LimitsThe CapacityScheduler supports the following parameters to control the running and pending applications:Property Descriptionyarn.scheduler.capacity.maximum-applications / yarn.scheduler.capacity..maximum-applications Maximum number of applications in the system which can be concurrently active both running and pending. Limits on each queue are directly proportional to their queue capacities and user limits. This is a hard limit and any applications submitted when this limit is reached will be rejected. Default is 10000. This can be set for all queues withyarn.scheduler.capacity.maximum-applications and can also be overridden on a per queue basis by setting yarn.scheduler.capacity..maximum-applications. Integer value expected.yarn.scheduler.capacity.maximum-am-resource-percent /yarn.scheduler.capacity..maximum-am-resource-percent Maximum percent of resources in the cluster which can be used to run application masters - controls number of concurrent active applications. Limits on each queue are directly proportional to their queue capacities and user limits. Specified as a float - ie 0.5 = 50%. Default is 10%. This can be set for all queues with yarn.scheduler.capacity.maximum-am-resource-percent and can also be overridden on a per queue basis by setting yarn.scheduler.capacity..maximum-am-resource-percentQueue Administration &amp; PermissionsThe CapacityScheduler supports the following parameters to the administer the queues:Property Descriptionyarn.scheduler.capacity..state The state of the queue. Can be one of RUNNING or STOPPED. If a queue is in STOPPED state, new applications cannot be submitted to itself or any of its child queues. Thus, if the root queue is STOPPEDno applications can be submitted to the entire cluster. Existing applications continue to completion, thus the queue can be drained gracefully. Value is specified as Enumeration.yarn.scheduler.capacity.root..acl_submit_applications The ACL which controls who can submit applications to the given queue. If the given user/group has necessary ACLs on the given queue or one of the parent queues in the hierarchy they can submit applications. ACLs for this property are inherited from the parent queue if not specified.yarn.scheduler.capacity.root..acl_administer_queue The ACL which controls who can administer applications on the given queue. If the given user/group has necessary ACLs on the given queue or one of the parent queues in the hierarchy they can administer applications. ACLs for this property are inherited from the parent queue if not specified.Note: An ACL is of the form user1, user2spacegroup1, group2. The special value of implies anyone. The special value of space implies no one. The default is for the root queue if not specified.Queue Mapping based on User or GroupThe CapacityScheduler supports the following parameters to configure the queue mapping based on user or group:Property Descriptionyarn.scheduler.capacity.queue-mappings This configuration specifies the mapping of user or group to aspecific queue. You can map a single user or a list of users to queues. Syntax: [u or g]:[name]:[queue_name][,next_mapping]*. Here, u or gindicates whether the mapping is for a user or group. The value is u for user and g for group. name indicates the user name or group name. To specify the user who has submitted the application, %user can be used. queue_name indicates the queue name for which the application has to be mapped. To specify queue name same as user name, %user can be used. To specify queue name same as the name of the primary group for which the user belongs to, %primary_group can be used.yarn.scheduler.capacity.queue-mappings-override.enable This function is used to specify whether the user specified queues can be overridden. This is a Boolean value and the default value is false.Example:12345678910&lt;property&gt; &lt;name&gt;yarn.scheduler.capacity.queue-mappings&lt;/name&gt; &lt;value&gt;u:user1:queue1,g:group1:queue2,u:%user:%user,u:user2:%primary_group&lt;/value&gt; &lt;description&gt; Here, &lt;user1&gt; is mapped to &lt;queue1&gt;, &lt;group1&gt; is mapped to &lt;queue2&gt;, maps users to queues with the same name as user, &lt;user2&gt; is mapped to queue name same as &lt;primary group&gt; respectively. The mappings will be evaluated from left to right, and the first valid mapping will be used. &lt;/description&gt;&lt;/property&gt; 其他属性Resource CalculatorProperty Descriptionyarn.scheduler.capacity.resource-calculator The ResourceCalculator implementation to be used to compare Resources in the scheduler. The default i.e. org.apache.hadoop.yarn.util.resource.DefaultResourseCalculator only uses Memory while DominantResourceCalculator uses Dominant-resource to compare multi-dimensional resources such as Memory, CPU etc. A Java ResourceCalculator class name is expected.Data LocalityProperty Descriptionyarn.scheduler.capacity.node-locality-delay Number of missed scheduling opportunities after which the CapacityScheduler attempts to schedule rack-local containers. Typically, this should be set to number of nodes in the cluster. By default is setting approximately number of nodes in one rack which is 40. Positive integer value is expected.Reviewing the configuration of the CapacityScheduler Once the installation and configuration is completed, you can review it after starting the YARN cluster from the web-ui.Start the YARN cluster in the normal manner.Open the ResourceManager web UI.The /scheduler web-page should show the resource usages of individual queues.Changing Queue Configuration Changing queue properties and adding new queues is very simple. You need to edit conf/capacity-scheduler.xml and run yarn rmadmin -refreshQueues.$ vi $HADOOP_CONF_DIR/capacity-scheduler.xml$ $HADOOP_YARN_HOME/bin/yarn rmadmin -refreshQueuesNote: Queues cannot be deleted, only addition of new queues is supported - the updated queue configuration should be a valid one i.e. queue-capacity at each level should be equal to 100%.]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[metamap部署]]></title>
    <url>%2F2016%2F12%2F05%2Fmetamap%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[vituralEnv先创建执行环境1virtualenv metamap 这个命令会在当前目录下生成一个metamap目录，后来查实了一下，其实后面是指定要生成的目标记录的(virtualenv /path/to/tartget/metamap)，便于统一管理所有的env环境。 进入我们刚刚创建的环境:12root@will-vm:/usr/local/metamap# source metamap/bin/activate(metamap) root@will-vm:/usr/local/metamap# 我这里没有使用绝对路径，所以就成了当前目录。目录结构如下：1234567891011121314151617181920212223242526272829(metamap) root@will-vm:/usr/local/metamap# tree -L 2 metamapmetamap├── bin│ ├── activate│ ├── activate.csh│ ├── activate.fish│ ├── activate_this.py│ ├── django-admin│ ├── django-admin.py│ ├── django-admin.pyc│ ├── easy_install│ ├── easy_install-2.7│ ├── pip│ ├── pip2│ ├── pip2.7│ ├── python│ ├── python2 -&gt; python│ ├── python2.7 -&gt; python│ ├── python-config│ └── wheel├── include│ └── python2.7 -&gt; /usr/include/python2.7├── lib│ └── python2.7├── local│ ├── bin -&gt; /usr/local/metamap/metamap/bin│ ├── include -&gt; /usr/local/metamap/metamap/include│ └── lib -&gt; /usr/local/metamap/metamap/lib└── pip-selfcheck.json 另外还会在新的env中安装三个基础依赖包：1234(metamap) root@will-vm:/usr/local/metamap# pip listpip (8.1.2)setuptools (25.1.3)wheel (0.29.0) 使用pip freeze &gt; requirements.txt来导出当前环境中的所有python依赖。 然后使用pip install -r path/to/requirements.txt安装，发现有一些是跟os有关的，比如包含Ubuntu的，根本就没有必要安装。所以就只保留了有印象的几个依赖。 最后requirements.txt只剩下：1234tlr4-python2-runtime==4.5.3Django==1.9.7MySQL-python==1.2.5pyhs2==0.6.0 如果要退出virtualenv，就使用deactivate。 Gunicorn在vituralenv里安装1pip install gunicorn 进入我们的django项目根目录，也就是manage.py所在的目录：12345678910111213141516171819202122232425262728293031323334353637(metamap) root@will-vm:/usr/local/metamap/metamap_django# gunicorn_django -b 0.0.0.0:8089!!!!!! WARNING: This command is deprecated.!!! !!! You should now run your application with the WSGI interface!!! installed with your project. Ex.:!!! !!! gunicorn myproject.wsgi:application!!! !!! See https://docs.djangoproject.com/en/1.8/howto/deployment/wsgi/gunicorn/!!! for more info.!!![2016-08-03 19:13:32 +0000] [13020] [INFO] Starting gunicorn 19.6.0[2016-08-03 19:13:32 +0000] [13020] [INFO] Listening at: http://0.0.0.0:8089 (13020)[2016-08-03 19:13:32 +0000] [13020] [INFO] Using worker: sync[2016-08-03 19:13:32 +0000] [13025] [INFO] Booting worker with pid: 13025[2016-08-03 19:13:32 +0000] [13025] [ERROR] Exception in worker processTraceback (most recent call last): File &quot;/usr/local/metamap/metamap/local/lib/python2.7/site-packages/gunicorn/arbiter.py&quot;, line 557, in spawn_worker worker.init_process() File &quot;/usr/local/metamap/metamap/local/lib/python2.7/site-packages/gunicorn/workers/base.py&quot;, line 126, in init_process self.load_wsgi() File &quot;/usr/local/metamap/metamap/local/lib/python2.7/site-packages/gunicorn/workers/base.py&quot;, line 136, in load_wsgi self.wsgi = self.app.wsgi() File &quot;/usr/local/metamap/metamap/local/lib/python2.7/site-packages/gunicorn/app/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;/usr/local/metamap/metamap/local/lib/python2.7/site-packages/gunicorn/app/djangoapp.py&quot;, line 105, in load mod = util.import_module(&quot;gunicorn.app.django_wsgi&quot;) File &quot;/usr/lib/python2.7/importlib/__init__.py&quot;, line 37, in import_module __import__(name) File &quot;/usr/local/metamap/metamap/local/lib/python2.7/site-packages/gunicorn/app/django_wsgi.py&quot;, line 21, in &lt;module&gt; from django.core.management.validation import get_validation_errorsImportError: No module named validation[2016-08-03 19:13:32 +0000] [13025] [INFO] Worker exiting (pid: 13025)[2016-08-03 19:13:32 +0000] [13020] [INFO] Shutting down: Master[2016-08-03 19:13:32 +0000] [13020] [INFO] Reason: Worker failed to boot. 查了一下，是版本问题，改用12345(metamap) root@will-vm:/usr/local/metamap/metamap_django# gunicorn metamap_django.wsgi:application -b 0.0.0.0:8089[2016-08-03 19:16:10 +0000] [13097] [INFO] Starting gunicorn 19.6.0[2016-08-03 19:16:10 +0000] [13097] [INFO] Listening at: http://0.0.0.0:8089 (13097)[2016-08-03 19:16:10 +0000] [13097] [INFO] Using worker: sync[2016-08-03 19:16:10 +0000] [13102] [INFO] Booting worker with pid: 13102 wsgi为我们生成django项目时，跟settings.py同目录的wsgi.py。然后就可以到浏览器访问了。 看一下我们django自动生成的这个文件内容，可以看到还有：123456789101112131415161718import osfrom django.core.wsgi import get_wsgi_applicationos.environ.setdefault("DJANGO_SETTINGS_MODULE", "metamap_django.settings")application = get_wsgi_application()def get_wsgi_application(): """ The public interface to Django's WSGI support. Should return a WSGI callable. Allows us to avoid making django.core.handlers.WSGIHandler public API, in case the internal WSGI implementation changes or moves in the future. """ django.setup() return WSGIHandler() 晚上整体浏览了一遍Gunicorn官网，整理以下几点注意的地方： 可以指定django的settings文件 gunicorn 官网中指定的几个管理master和worker的signal其实都是Linux的kill命令的参数 kill -TERM cat /tmp/xx.pid worker和thread是不一样的概念，前者是进程，后者是线程，worker中包含thread。具体并发数需要压测结果来确定 为了防止出现内存泄露问题，可以为worker指定类似max_request之类的参数在一定阶段重启当前worker Gunicorn可以动态增加worker实现调优 命令行可以加入运行目录并生成pid 环境中添加python插件setproctitle之后就可以为gunicorn进程命名了 12345678910 (metamap) root@will-vm:/usr/local/metamap/metamap_django# ps -ef | grep gunicornroot 2258 10709 0 11:38 pts/26 00:00:00 tailf /tmp/gunicorn_error.logroot 2494 2264 0 11:38 pts/27 00:00:00 tailf /tmp/gunicorn_access.logroot 3190 2444 0 11:57 ? 00:00:00 gunicorn: master [will&apos;s metamap] root 3195 3190 2 11:57 ? 00:00:00 gunicorn: worker [will&apos;s metamap] root 3198 3190 2 11:57 ? 00:00:00 gunicorn: worker [will&apos;s metamap] root 3199 3190 2 11:57 ? 00:00:00 gunicorn: worker [will&apos;s metamap] root 3205 3190 2 11:57 ? 00:00:00 gunicorn: worker [will&apos;s metamap] root 3207 3190 2 11:57 ? 00:00:00 gunicorn: worker [will&apos;s metamap] root 3223 17309 0 11:57 pts/25 00:00:00 grep --color=auto gunicorn –statsd-host=localhost:8125会启动一个针对gunicorn的监控系统 master只负责管理worker，不处理请求，worker是真正处理请求的进程 可接受配置文件-c, 具体可配置项，参考这里 1234567891011121314151617181920# !/usr/bin/env python# -*- coding: utf-8 -*'''created by will'''import multiprocessingprint('gunicorn config is running....')bind = "0.0.0.0:8088"workers = multiprocessing.cpu_count() * 2 + 1pidfile = '/tmp/gunicorn.pid'accesslog = '/tmp/gunicorn_access.log'errorlog = '/tmp/gunicorn_error.log'loglevel = 'info'capture_output = Truestatsd_host = '0.0.0.0:8888'proc_name = 'will\'s metamap'daemon = True 然后运行的时候指定此文件的位置即可。 注意： 启动指定wsgi位置的时候，需要是gunicorn metamap_django.wsgi:application，要注意的是metamap_django与wsgi之间是.，而不是能是文件路径/，否则启动gunicorn的时候报错:import by filename is not supported gunicorn 执行 kill -HUP cat /tmp/xx/pid来管理master和worker进程 如果要使用supervisord管理进程的话，最好就不要设置daemon为True了 有些东西跟官网说的不一致，比如: statsd_host，django_settings指定settings文件位置，access-log等 参考： http://ju.outofmemory.cn/entry/105202 http://zqpythonic.qiniucdn.com/data/20130901152951/index.html Django注意一下几点： 为django创建多个settings文件，如prod.py,test.py等,之后在gunicorn的配置文件里指定需要的配置文件即可 123456789101112131415161718192021222324# !/usr/bin/env python# -*- coding: utf-8 -*'''created by will'''import multiprocessing, osprint('gunicorn config is running....')bind = "0.0.0.0:8088"workers = multiprocessing.cpu_count() * 2 + 1pidfile = '/tmp/gunicorn.pid'# accesslog = '/tmp/gunicorn_access.log'errorlog = '/tmp/gunicorn_error.log'loglevel = 'info'capture_output = True# statsd_host = 'localhost:8077'proc_name = 'will\'s metamap'daemon = True# 测试确定，这里设置这个参数不生效# django_settings = 'metamap.config.prod'os.environ.setdefault("DJANGO_SETTINGS_MODULE", "metamap.config.prod") 修改settings里的几个选项 参数 解释 DEBUG 改为False，不然程序出错的方法栈会直接展示在前端用户那里，不友好。 ALLOWED_HOSTS 添加可以访问你的项目的几个域名或主机地址，防止 HTTP Host header attacks ADMINS 当程序出错的时候django会发送错误给这些邮件列表 参考：https://docs.djangoproject.com/es/1.9/topics/settings/ 确认有处理view里抛出的异常的逻辑，如果没有的话，自定义一个处理异常的middleware，注册到setting离去。 参考：https://docs.djangoproject.com/ja/1.9/topics/http/middleware/ 将django的所有的静态文件整理到指定文件夹在settings.py里设置static_root，然后执行./manage.py collectstatic，这个命令就会把所有的静态资源整理到static_root里面。 参考：https://docs.djangoproject.com/en/1.10/howto/static-files/#deployment Nginxnginx的主要功能是分发动态请求，以及处理静态资源请求。 配置文件1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#user nobody;worker_processes 1;events &#123; worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #log_format main &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; # &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; # &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;; access_log /tmp/nginx.access.log combined; sendfile on; keepalive_timeout 65; #gzip on; upstream app_server &#123; # 指定后台地址 server 192.168.244.128:8088 fail_timeout=0; &#125; server &#123; listen 80; client_max_body_size 4G; # 指定域名 server_name 192.168.244.128; keepalive_timeout 5; # static文件夹所在的根目录 root /usr/local/metamap; location / &#123; # 如果是静态资源就自己处理，如果不是就发送给proxy_to_app # try_files-&gt;按顺序检查文件是否存在，返回第一个找到的文件或文件夹（结尾加斜线表示为文件夹），如果所有的文件或文件夹都找不到，会进行一个内部重定向到最后一个参数 try_files $uri @proxy_to_app; &#125; location @proxy_to_app &#123; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # enable this if and only if you use HTTPS # proxy_set_header X-Forwarded-Proto https; proxy_set_header Host $http_host; # we don&apos;t want nginx trying to do something clever with # redirects, we set the Host: header above already. proxy_redirect off; proxy_pass http://app_server; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html &#123; root html; &#125; &#125;&#125; 未解决的问题不定时出现找不到静态资源的问题：12342016-08-04 10:04:10,128 [MainThread:-1219164416] [django.request:182] [base:get_response] [WARNING]- Not Found: /static/js/bootstrap.js2016-08-04 10:04:10,126 [MainThread:-1219164416] [django.request:182] [base:get_response] [WARNING]- Not Found: /static/js/jquery.js2016-08-04 10:04:10,148 [MainThread:-1219164416] [django.request:182] [base:get_response] [WARNING]- Not Found: /static/css/bootstrap.css2016-08-04 10:04:10,140 [MainThread:-1219164416] [django.request:182] [base:get_response] [WARNING]- Not Found: /static/style.css]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery队列路由]]></title>
    <url>%2F2016%2F12%2F05%2Fcelery%E9%98%9F%E5%88%97%E8%B7%AF%E7%94%B1%2F</url>
    <content type="text"><![CDATA[同一个app里如果有多种不同的任务，也就是分由不同的task进行处理。假设只有一个worker的话，task1耗时特别长，有可能会阻塞短平快的task2、task3、task4的执行，也就是面临着小任务饿死的情况。 这时候我们就需要使用队列路由，将不同类型的任务路由至不同的queue，并且启动多个worker，分别负责消费指定queue的任务。这样，长少慢的任务就不会影响短频快的小任务的执行了。 自动路由CELERY_CREATE_MISSING_QUEUES开启后，如果queue没有在CELERY_QUEUES中的话会自动创建。 假设我们有两个server：x y。他俩负责普通的task。然后z server只处理特定的跟种子有关的task，那么可以这样配置：1CELERY_ROUTES = &#123;&apos;feed.tasks.import_feed&apos;: &#123;&apos;queue&apos;: &apos;feeds&apos;&#125;&#125; 所有feed.tasks.import_feed的任务都会被放进feeds队列，其他的就放进默认的队列【celery】。 然后启动z server的时候指定一下队列名称：1user@z:/$ celery -A proj worker -Q feeds 修改默认队列的名称123456from kombu import Exchange, QueueCELERY_DEFAULT_QUEUE = 'default'CELERY_QUEUES = ( Queue('default', Exchange('default'), routing_key='default'),) 队列定义celery隐藏了后面负责的AMQP协议，但是有人也想知道怎样声明队列。 下面是创建一个video队列：123&#123;&apos;exchange&apos;: &apos;video&apos;, &apos;exchange_type&apos;: &apos;direct&apos;, &apos;routing_key&apos;: &apos;video&apos;&#125; 非AMQP协议的后端不支持exchange。 手动配置还是上面一样，xy处理普通任务，z处理feed相关任务：12345678910from kombu import QueueCELERY_DEFAULT_QUEUE = 'default'CELERY_QUEUES = ( Queue('default', routing_key='task.#'), Queue('feed_tasks', routing_key='feed.#'),)CELERY_DEFAULT_EXCHANGE = 'tasks'CELERY_DEFAULT_EXCHANGE_TYPE = 'topic'CELERY_DEFAULT_ROUTING_KEY = 'task.default' CELERY_QUEUES里定义了一串队列的实例。如果没有设置exchange或者没给key设置exchange类型，会按照默认的CELERY_DEFAULT_EXCHANGE、CELERY_DEFAULT_EXCHANGE_TYPE。 要指定某个task的路由，需要在CELERY_ROUTES里添加一个入口123456CELERY_ROUTES = &#123; 'feeds.tasks.import_feed': &#123; 'queue': 'feed_tasks', 'routing_key': 'feed.import', &#125;,&#125; 也可以在代码调用的时候指定：1234&gt;&gt;&gt; from feeds.tasks import import_feed&gt;&gt;&gt; import_feed.apply_async(args=['http://cnn.com/rss'],... queue='feed_tasks',... routing_key='feed.import') 启动z：1user@z:/$ celery -A proj worker -Q feed_tasks --hostname=z@%h 还要配置x y启动的时候只消费默认队列default：12user@x:/$ celery -A proj worker -Q default --hostname=x@%huser@y:/$ celery -A proj worker -Q default --hostname=y@%h 这些队列也可以混搭在一起，如果你的server都能处理的话，也可以这么做：1user@z:/$ celery -A proj worker -Q feed_tasks,default --hostname=z@%h 要是想添加一个在其他exchange上的队列，自己定义好exchange以及exchange类型就行：12345678from kombu import Exchange, QueueCELERY_QUEUES = ( Queue('feed_tasks', routing_key='feed.#'), Queue('regular_tasks', routing_key='task.#'), Queue('image_tasks', exchange=Exchange('mediatasks', type='direct'), routing_key='image.compress'),) 要是对这部分内容有疑问，可以自行查阅一下AMQP。 初识AMQPmessagemessage包含header和body。celery使用head来存储内容类型和内容编码。内容类型用来序列化message。body中是要执行的task的name，id，参数以及其他元数据信息。 下面是一个例子：1234&#123;'task': 'myapp.tasks.add', 'id': '54086c5e-6193-4575-8308-dbab76798756', 'args': [4, 4], 'kwargs': &#123;&#125;&#125; Producers, consumers， brokers发送message的客户端叫做publisher，也叫producer。收到message的就叫做consumer。 broker是message所在的server，把message从producer路由到consumer。 Exchanges, queues， routing keys message被发送给exchange exchange路由message到一或多个队列。有多种exchange类型，提供不同的路由方式，实现不同的消息场景 message在队列中等着consumer消费 当message被ack之后就会从队列中移除 下面是发送与接收message的必要步骤： 创建一个exchange 创建一个队列 把队列绑定到exchange上 celery自动为CELERY_QUEUES创建了以上这些对象【除非你关闭了自动创建不存在队列的配置】。 下面是一个包含三个队列的配置，一个video，一个image，还一个默认的：12345678910from kombu import Exchange, QueueCELERY_QUEUES = ( Queue('default', Exchange('default'), routing_key='default'), Queue('videos', Exchange('media'), routing_key='media.video'), Queue('images', Exchange('media'), routing_key='media.image'),)CELERY_DEFAULT_QUEUE = 'default'CELERY_DEFAULT_EXCHANGE_TYPE = 'direct'CELERY_DEFAULT_ROUTING_KEY = 'default' Exchange类型标准的有：direct, topic, fanout, headers。非标准的就是一些rabbitMQ的插件了。 direct匹配精确的routing key，绑定到routing key video的队列只会接收带有video routing key的message。 topc支持对号隔开、带有通配符的routing key。 相关API123456789101112131415161718192021222324252627282930313233exchange.declare(exchange_name, type, passive,durable, auto_delete, internal)Declares an exchange by name.See amqp:Channel.exchange_declare.Parameters: passive – Passive means the exchange won’t be created, but you can use this to check if the exchange already exists. durable – Durable exchanges are persistent. That is - they survive a broker restart. auto_delete – This means the queue will be deleted by the broker when there are no more queues using it. queue.declare(queue_name, passive, durable, exclusive, auto_delete) Declares a queue by name.See amqp:Channel.queue_declareExclusive queues can only be consumed from by the current connection. Exclusive also implies auto_delete.queue.bind(queue_name, exchange_name, routing_key)Binds a queue to an exchange with a routing key.Unbound queues will not receive messages, so this is necessary.See amqp:Channel.queue_bindqueue.delete(name, if_unused=False, if_empty=False)Deletes a queue and its binding.See amqp:Channel.queue_deleteexchange.delete(name, if_unused=False)¶Deletes an exchange.See amqp:Channel.exchange_delete 使用API自带了一个工具 celery amqp，用来在命令行访问AMQP API，执行创建/删除队列或exchange的，清空队列或者正在发送的message。也能在没有AMQP broker的上面使用。 可以在celery amap后面指定参数来调用API, 或者不指定参数，就进入了shell模式：1234$ celery -A proj amqp-&gt; connecting to amqp://guest@localhost:5672/.-&gt; connected.1&gt; 这里的1是你执行命令个数的编号。可以输入help，这个模式也支持自动补全。 下面是创建队列的例子1234567$ celery -A proj amqp1&gt; exchange.declare testexchange directok.2&gt; queue.declare testqueueok. queue:testqueue messages:0 consumers:0.3&gt; queue.bind testqueue testexchange testkeyok. 这里队列名称是testqueue，使用routing key testkey绑定了exchange为testexchange。 现在开始所有发送到exchange testchange的message，只要她的routing key是testkey，那么他就会进入testqueue这个队列。我们可以使用basic.publish来发送一个message:124&gt; basic.publish &apos;This is a message!&apos; testexchange testkeyok. 现在message已经发送出去了，你可以使用basic.get命令抽取它了(这个命令指示用来维护task的，正常服务应该使用basic.consume替代)。 从队列中pop出一条message：123456785&gt; basic.get testqueue&#123;&apos;body&apos;: &apos;This is a message!&apos;, &apos;delivery_info&apos;: &#123;&apos;delivery_tag&apos;: 1, &apos;exchange&apos;: u&apos;testexchange&apos;, &apos;message_count&apos;: 0, &apos;redelivered&apos;: False, &apos;routing_key&apos;: u&apos;testkey&apos;&#125;, &apos;properties&apos;: &#123;&#125;&#125; AMQP使用ack来确认某个message被消费并且成功执行。如果message还没有被ack，consumer channel就关闭了，那么这个message就会被传递给其他的consumer。 注意传递的tag在上面结构中已经有了。在一个连接channel内，每个接收到的message都有唯一的传递tag，这个tag用来ack这个message。还要注意，传递tag在多个连接之中并不是唯一的，也就是说连接1中的连接tag为1的message是A，连接2中的连接tag为1的message有可能就是B。 使用basic.ack来加上这个标记：126&gt; basic.ack 1ok. 记得要在测试会话完成后清空刚才创建的message：12347&gt; queue.delete testqueueok. 0 messages deleted.8&gt; exchange.delete testexchangeok. Routing Tasks定义队列下面是一个包含三个队列定义的例子，一个是video，一个是images还有一个默认的队列1234567891011default_exchange = Exchange('default', type='direct')media_exchange = Exchange('media', type='direct')CELERY_QUEUES = ( Queue('default', default_exchange, routing_key='default'), Queue('videos', media_exchange, routing_key='media.video'), Queue('images', media_exchange, routing_key='media.image'))CELERY_DEFAULT_QUEUE = 'default'CELERY_DEFAULT_EXCHANGE = 'default'CELERY_DEFAULT_ROUTING_KEY = 'default' 若没有指定特定路由就都进入CELERY_DEFAULT_QUEUE里。 指定task目的地按照如下顺序决定message的目的地： 在CELERY_ROUTES中定义的路由 Task.apply_async()中指定的路由参数 在Task自身中定义的相关属性 最好不要硬编码这些配置，使用 Routers来读取配置，这是最灵活的方式。但是一些敏感的默认值还是可以被设置为任务属性。 RoutersRouter类是用来决定某个task的路由选项的。 我们只需要创建一个新的路由类，带有一个route_for_task方法：12345678class MyRouter(object): def route_for_task(self, task, args=None, kwargs=None): if task == 'myapp.tasks.compress_video': return &#123;'exchange': 'video', 'exchange_type': 'topic', 'routing_key': 'video.compress'&#125; return None 如果返回值中包含了 queue，那么会在CELERY_QUEUES里定义的配置上扩展，也就是会继承既有的配置。1&#123;&apos;queue&apos;: &apos;video&apos;, &apos;routing_key&apos;: &apos;video.compress&apos;&#125; 就变成了1234&#123;&apos;queue&apos;: &apos;video&apos;, &apos;exchange&apos;: &apos;video&apos;, &apos;exchange_type&apos;: &apos;topic&apos;, &apos;routing_key&apos;: &apos;video.compress&apos;&#125; 完事儿后把Router放进CELERY_ROUTES配置信息里就好了。1CELERY_ROUTES = (MyRouter(), ) 或者1CELERY_ROUTES = (&apos;myapp.routers.MyRouter&apos;, ) 其实对于上面那种简单的路由规则， 也可以直接写在配置里：1234CELERY_ROUTES = (&#123;&apos;myapp.tasks.compress_video&apos;: &#123; &apos;queue&apos;: &apos;video&apos;, &apos;routing_key&apos;: &apos;video.compress&apos; &#125;&#125;, ) 广播Celery也支持广播路由。12345from kombu.common import BroadcastCELERY_QUEUES = (Broadcast(&apos;broadcast_tasks&apos;), )CELERY_ROUTES = &#123;&apos;tasks.reload_cache&apos;: &#123;&apos;queue&apos;: &apos;broadcast_tasks&apos;&#125;&#125; 这样，tasks.reload_cache任务就可以发送给每个消费这个队列的worker了。 注意celery的结果并没有定义如果两个任务都有同一个task_id。如果同样的task被分发给多个worker，那么state历史可能也不会被保存。 这样的话，设置task.ignore_result是一个不错的选择。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ember-data---record操作]]></title>
    <url>%2F2016%2F12%2F05%2FEmber-data---record%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[查询record抽取单个record使用 store.findRecord()方法，按照id和type进行查询. 返回一个包含对应record的promise: 1var post = this.store.findRecord('post', 1); // =&gt; GET /posts/1 使用 store.peekRecord().跟上面的一样，但是只查询store中当前已经存在的record，不会向后端server请求: 1var post = this.store.peekRecord('post', 1); // =&gt; no network request 抽取多个Records使用 store.findAll()查询某个类型的所有record: 1var posts = this.store.findAll('post'); // =&gt; GET /posts 参上： store.peekAll(): 1var posts = this.store.peekAll('post'); // =&gt; no network request store.findAll() 返回的PromiseArray 包含RecordArray 。 store.peekAll直接返回RecordArray. 注意RecordArray并不是JavaScript array，而是一个实现了Ember.Enumerable的对象，所以如果要按照index访问数据，不能使用 []，而应该使用 objectAt(index) . 查询多个Records可以查一些满足某些范式的record。store.query()会发起一个带有查询参数的GET请求，返回值跟findAll一样是一个 PromiseArray. 找出所有名字叫Peter的person models : 1234// GET to /persons?filter[name]=Peterthis.store.query('person', &#123; filter: &#123; name: 'Peter' &#125; &#125;).then(function(peters) &#123; // Do something with `peters`&#125;); 查询单个Recordstore.queryRecord()返回一个promise。 查询邮箱是’tomster@example.com’ 的person model:tomster@example.com: 1234// GET to /persons?filter[email]=tomster@example.comthis.store.queryRecord('person', &#123; filter: &#123; email: 'tomster@example.com' &#125; &#125;).then(function(tomster) &#123; // do something with `tomster`&#125;); 创建、更新与删除创建createRecord() 1234store.createRecord('post', &#123; title: 'Rails is Omakase', body: 'Lorem ipsum'&#125;); 在controller或者route中直接调用 this.store就可以获得store对象. 更新1234this.store.findRecord('person', 1).then(function(tyrion) &#123; // ...after the record has loaded tyrion.set('firstName', "Yollo");&#125;); Ember中修改属性特别便捷。例如，你可以使用 Ember.Object的incrementProperty helper: 1person.incrementProperty('age'); // Happy birthday! 持久化执行Model的任意实例的save()方法都会触发网络请求. Ember Data会跟踪每个record的状态。这样当存储新创建的record和已经存在的record的时候就能区别对待。 Ember Data默认POST 新创建的record到它的type对应的url. 123456var post = store.createRecord('post', &#123; title: 'Rails is Omakase', body: 'Lorem ipsum'&#125;);post.save(); // =&gt; POST to '/posts' 已经存在的record使用HTTP的 PATCH动作. 1234567store.findRecord('post', 1).then(function(post) &#123; post.get('title'); // =&gt; "Rails is Omakase" post.set('title', 'A new post'); post.save(); // =&gt; PATCH to '/posts/1'&#125;); 你可以通过record的hasDirtyAttributes属性来辨别是否有没有提交的修改. 也可以使用changedAttributes()方法辨别，哪些属性修改了，哪些没有. changedAttributes返回一个对象，key是被修改的属性名称，值是[oldValue, newValue]. 12345person.get('isAdmin'); //=&gt; falseperson.get('hasDirtyAttributes'); //=&gt; falseperson.set('isAdmin', true);person.get('hasDirtyAttributes'); //=&gt; trueperson.changedAttributes(); //=&gt; &#123; isAdmin: [false, true] &#125; 这时，你可以调用save()方法持久化这些改变，也可以调用rollbackAttributes()回滚这些修改。如果这个record isNew【是新建的】，那么就会被从store中删除. 12345678person.get('hasDirtyAttributes'); //=&gt; trueperson.changedAttributes(); //=&gt; &#123; isAdmin: [false, true] &#125;person.rollbackAttributes();person.get('hasDirtyAttributes'); //=&gt; falseperson.get('isAdmin'); //=&gt; falseperson.changedAttributes(); //=&gt; &#123;&#125; 验证错误如果后端server尝试save的时候返回了验证错误，那么会体现在你的model的errors属性上。你可以通过下面的方法在你的模板中展示这些错误： 123456&#123;&#123;#each post.errors.title as |error|&#125;&#125; &lt;div class="error"&gt;&#123;&#123;error.message&#125;&#125;&lt;/div&gt;&#123;&#123;/each&#125;&#125;&#123;&#123;#each post.errors.body as |error|&#125;&#125; &lt;div class="error"&gt;&#123;&#123;error.message&#125;&#125;&lt;/div&gt;&#123;&#123;/each&#125;&#125; Promisessave() 返回一个promise。下面是典型操作: 12345678910111213141516171819var post = store.createRecord('post', &#123; title: 'Rails is Omakase', body: 'Lorem ipsum'&#125;);var self = this;function transitionToPost(post) &#123; self.transitionToRoute('posts.show', post);&#125;function failure(reason) &#123; // handle the error&#125;post.save().then(transitionToPost).catch(failure);// =&gt; POST to '/posts'// =&gt; transitioning to posts.show route 删除删除跟创建一样直接。 调用一个实例的deleteRecord()方法. 之后会标记record为 isDeleted. 使用save()持久化删除操作. 或者使用destroyRecord 方法直接物理删除. 12345678910store.findRecord('post', 1).then(function(post) &#123; post.deleteRecord(); post.get('isDeleted'); // =&gt; true post.save(); // =&gt; DELETE to /posts/1&#125;);// ORstore.findRecord('post', 2).then(function(post) &#123; post.destroyRecord(); // =&gt; DELETE to /posts/2&#125;); 事先推送record进入storecontroller或者route请求store中没有的store时，store还要向adapter发起网络请求。除了等待app请求一个record，你也可以提前向store中push record。 如果你知道你的用户下一步会使用什么record，那这就很有用。当他们请求的时候，会飞速返回。Ember可以自动重新渲染模板。 另外一个用例是与后端建立streaming connection的时候。当一个record被创建或者修改后，你应该马上更新UI。 推送调用store的 push() 方法. 假设当app刚启动的时候，我们要预加载一些数据到store中。 我们使用route:application来做这项工作. route:application 是route层级结构中的最顶级route，他的 model hook 只在app启动的时候被调用一次. 12345678import Model from &apos;ember-data/model&apos;;import attr from &apos;ember-data/attr&apos;;export default Model.extend(&#123; title: attr(), artist: attr(), songCount: attr()&#125;); 12345678910111213141516171819202122232425export default Ember.Route.extend(&#123; model() &#123; this.store.push(&#123; data: [&#123; id: 1, type: &apos;album&apos;, attributes: &#123; title: &apos;Fewer Moving Parts&apos;, artist: &apos;David Bazan&apos;, songCount: 10 &#125;, relationships: &#123;&#125; &#125;, &#123; id: 2, type: &apos;album&apos;, attributes: &#123; title: &apos;Calgary b/w I Can\&apos;t Make You Love Me/Nick Of Time&apos;, artist: &apos;Bon Iver&apos;, songCount: 2 &#125;, relationships: &#123;&#125; &#125;] &#125;); &#125;&#125;); store的 push() 方法是一个低级API，接收JSON API文档，使用JSONAPISerializer进行解析 。JSON API终端 type名称必须跟model的严格对应。（这个例子里就是album，因为model就是app/models/album.js）。属性和关系的大小写也必须和model定义中完全一致。 如果你想在推送进store前标准化model默认的serializer，可使用store.pushPayload() 方法. 12345678910import RestSerializer from &apos;ember-data/serializers/rest&apos;;export default RestSerializer.extend(&#123; normalize(typeHash, hash) &#123; hash[&apos;songCount&apos;] = hash[&apos;song_count&apos;] delete hash[&apos;song_count&apos;] return this._super(typeHash, hash); &#125;&#125;) 1234567891011121314151617181920export default Ember.Route.extend(&#123; model() &#123; this.store.pushPayload(&#123; albums: [ &#123; id: 1, title: &apos;Fever Moving Parts&apos;, artist: &apos;David Bazan&apos;, song_count: 10 &#125;, &#123; id: 2, title: &apos;Calgary b/w I Can\&apos;t Make You Love Me/Nick Of Time&apos;, artist: &apos;Bon Iver&apos;, song_count: 2 &#125; ] &#125;); &#125;&#125;); push() 方法在于复杂终端协同的时候也很重要。你的应用终端有时会执行一些业务逻辑，创建多个record。这对于Ember Data的save() API并不是完全对应，后者只能操作单条record。这是，你应该使用自己的ajax请求推送结果数据到store中，供其他UI页面使用。 1234567891011121314export default Ember.Route.extend(&#123; actions: &#123; confirm: function(data) &#123; $.ajax(&#123; data: data, method: &apos;POST&apos;, url: &apos;process-payment&apos; &#125;).then((digitalInventory) =&gt; &#123; this.store.pushPayload(digitalInventory); this.transitionTo(&apos;thank-you&apos;); &#125;); &#125; &#125;&#125;);]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari上hive-view添加用户]]></title>
    <url>%2F2016%2F12%2F05%2Fambari%E4%B8%8Ahive-view%E6%B7%BB%E5%8A%A0%E7%94%A8%E6%88%B7%2F</url>
    <content type="text"><![CDATA[问题描述在ambari 2.2.0.0中添加完用户和用户组之后，需要为这个用户单独给定一次ambari dashboard的只读权限以后才能正常访问hive view，否则就会报错：1E090 RA040 I/O error while requesting Ambari [AmbariApiException] 还有一个问题是，hive view里的每个用户都需要在/user/hive下有自己的文件夹才行，并且把文件夹的owner设置生当前用户才行。否则报错1E090 HDFS020 Could not write file query.hql [HdfsApiException] 以前总结a. 给新用户开通dashboard的只读权限和指定view的使用权限【解决ambari的IO问题AmbariApiException】; b. 到hdfs上创建/user/xxx目录，并将此目录的owner设置为xxx【解决hql写入问题HdfsApiException】 c. 使用没有问题的话，就可以把这个xxx用户的dashboard权限去掉了。 回想上面给用户readonly权限感觉就跟跑堂的似的，那么它实际是执行了什么操作呢？我怀疑它会在hdfs上创建一个user，但是并不存在于Linux中。 问题来了： hdfs和linux到底用的是不是同一套用户呢？还是说只是用的Linux的POSIX体系？12345678[root@datanode04 ~]# groups willwill : will hdfs[root@datanode04 ~]# hdfs groups willwill :[will@datanode04 root]$ hdfs dfs -ls /apps/hivedrwxrwxrw- - hive hdfs 0 2016-05-30 10:47 /apps/hive/warehouse[will@datanode04 root]$ hdfs dfs -ls /apps/hive/warehousels: Permission denied: user=will, access=READ_EXECUTE, inode=&quot;/apps/hive/warehouse&quot;:hive:hdfs:drwxrwxrw- 看这样子，no！hdfs并没有使用Linux上已有的用户组和用户的关系。可以看到hdfs上的文件夹/apps/hive/warehouse对于all并没有x权限，也就是说不能进去。/apps/hive/warehouse这个文件夹是属于hive用户，hdfs组。而will不能访问，代表他在hdfs系统里并不属于hdfs组。 找了hdfs的官网命令，没有找到关于在hdfs里添加用户的命令。回头儿想了一下，有个失误的地方：我只在一个node上添加了will用户，其他node上并没有。参考http://dongxicheng.org/mapreduce/hadoop-permission-management/，hdfs就是使用的Linux用户和用户组，但是需要每个node分别创建。 1scp -v /etc/passwd /etc/group datanode05.will.com:/etc 每个机器都scp过之后，再次查看hdfs里的效果：12[root@datanode04 ~]# hdfs groups willwill : will hdfs 好了。那么为了验证上面给新用户dashboard的readonly权限是为了添加用户，我们再去ambari里添加普通用户will。还是报错，老套路，添加用户对集群的read权限，发现以下几个借口： 为用户组添加dashboard的只读权限PUT: http://namenode01.will.com:8080/api/v1/clusters/datacenter/privileges123456789[ &#123; "PrivilegeInfo": &#123; "permission_name": "CLUSTER.READ", "principal_name": "analyst", "principal_type": "GROUP" &#125; &#125;] curl -u admin:admin -H ‘X-Requested-By:ambari’ -X PUT -d ‘[{“PrivilegeInfo”:{“permission_name”:”CLUSTER.READ”,”principal_name”:”testgroup”,”principal_type”:”GROUP”}}]’ http://namenode01.will.com:8080/api/v1/clusters/datacenter/privileges 为用户添加dashboard的只读权限PUT：http://namenode01.will.com:8080/api/v1/clusters/datacenter/privileges123456789[ &#123; "PrivilegeInfo": &#123; "permission_name": "CLUSTER.READ", "principal_name": "will", "principal_type": "USER" &#125; &#125;] 收回用户/用户组dashboard的只读权限PUT：http://namenode01.will.com:8080/api/v1/clusters/datacenter/privileges1[] 更新成最新的权限组合就行了【这种就存在并发问题了，如果操作的是group会有相互影响，user就好些】 为用户/用户组添加hive view的只读权限PUT：http://namenode01.will.com:8080/api/v1/views/HIVE/versions/1.0.0/instances/AUTO_HIVE_INSTANCE/privileges123456789[ &#123; "PrivilegeInfo": &#123; "permission_name": "VIEW.USE", "principal_name": "will", "principal_type": "USER" &#125; &#125;] curl -u admin:admin -H ‘X-Requested-By:ambari’ -X PUT -d ‘[{“PrivilegeInfo”:{“permission_name”:”VIEW.USE”,”principal_name”:”testgroup”,”principal_type”:”GROUP”}}]’ http://namenode01.will.com:8080/api/v1/views/HIVE/versions/1.0.0/instances/AUTO_HIVE_INSTANCE/privileges 以上接口移除权限的时候同样适用PUT方法，只是json为空：[]。 ambari删除用户DELETE: http://namenode01.will.com:8080/api/v1/users/will1&#123;"id":"will"&#125; ambari添加用户POST: http://namenode01.will.com:8080/api/v1/users123456&#123; "Users/user_name": "will", "Users/password": "1234qwer", "Users/active": true, "Users/admin": false&#125; curl -u admin:admin -H ‘X-Requested-By:ambari’ -X POST -d ‘{“Users/user_name”:”testuser”,”Users/password”:”1234qwer”,”Users/active”:true,”Users/admin”:false}’ http://namenode01.will.com:8080/api/v1/users ambari添加用户组POST: http://namenode01.will.com:8080/api/v1/groups1&#123;"Groups/group_name":"tt"&#125; curl -u admin:admin -H ‘X-Requested-By:ambari’ -X POST -d ‘{“Groups/group_name”:”testgroup”}’ http://namenode01.will.com:8080/api/v1/groups ambari删除用户组DELETE: http://namenode01.will.com:8080/api/v1/groups/tt啥都不用传 ambari为用户指定用户组POST: http://namenode01.will.com:8080/api/v1/groups/**analyst**/members/**will** curl -u admin:admin -H ‘X-Requested-By:ambari’ -X POST http://namenode01.will.com:8080/api/v1/groups/testgroup/members/testuser 这个参数在url里 综上总结过程如下：1234567891011121314151617181920212223242526272829303132#! /bin/bashuser=$1group=$2pri_grp=$3# 1.在namenode上执行添加用户以及指定用户组的Linux命令useradd -G $group $usergroups $user# 2.检查hdfs上的用户组hdfs groups $user# 3.ambari上添加用户curl -u admin:admin -H 'X-Requested-By:ambari' -X POST -d '&#123;"Users/user_name":"$user","Users/password":"1234qwer","Users/active":true,"Users/admin":false&#125;' http://namenode01.will.com:8080/api/v1/users# 4.ambari 调整用户到用户组curl -u admin:admin -H 'X-Requested-By:ambari' -X POST http://namenode01.will.com:8080/api/v1/groups/$group/members/$user# 为用户添加dashboard的只读权限curl -u admin:admin -H 'X-Requested-By:ambari' -X PUT -d '[&#123;"PrivilegeInfo":&#123;"permission_name":"CLUSTER.READ","principal_name":"$user","principal_type":"USER"&#125;&#125;]' http://namenode01.will.com:8080/api/v1/clusters/datacenter/privileges# 模拟用户登录，访问一下hive view登录，这个过程会自动创建/user/$user文件夹# 用chrome找了半天，么有找到登录的API，看看源码吧# 模拟登录用户，访问hive view# 删除普通用户对dashboard的所有权限curl -u admin:admin -H 'X-Requested-By:ambari' -X PUT -d '[]' http://namenode01.will.com:8080/api/v1/clusters/datacenter/privileges# 6. 检查一下用户文件夹，以此作为终点hdfs dfs -ls /user/$user 思考可以修改一下ambari添加用户和用户组的地方，后台自动针对namenode的Linux系统进行相应操作，就不用单独再去执行命令行了。这样ambari、hdfs、Linux的用户以及用户组就会是一致的，后面的依赖前面的。 hive view加载过程中会请求一个URL：http://namenode01.will.com:8080/api/v1/clusters/datacenter/hosts?fields=Hosts%2Fpublic_host_name%2Chost_components%2FHostRoles%2Fcomponent_name 这个URL需要对集群有可读权限，奇怪的是使用chrome看下面的网络请求里压根儿没有这个。它是内嵌在某个接口调用里的。 延迟问题：12345678910111213141516171819202122232425262728293031[root@namenode01 will]# useradd -G hdfs will1[root@namenode01 will]# groups will1will1 : will1 hdfs[root@namenode01 will]# hdfs groups will1will1 : will1 hdfs[root@namenode01 will]# userdel will1[root@namenode01 will]# groups will1groups: will1: No such user[root@namenode01 will]# hdfs groups will1will1 : will1 hdfs[root@namenode01 will]# cat /etc/group | grep willhdfs:x:511:hdfs,willwill:x:1003:[root@namenode01 will]# hdfs groups will1will1 : will1 hdfs[root@namenode01 will]# hdfs groups will2will2 :[root@namenode01 will]# useradd -G sqoop will1[root@namenode01 will]# groups will1will1 : will1 sqoop[root@namenode01 will]# hdfs groups will1will1 : will1 hdfs [root@namenode01 will]# su will1[will1@namenode01 will]$ hdfs dfs -ls /apps/hive/warehouseFound 2 itemsdrwxrwxrw- - hive hdfs 0 2016-05-30 10:40 /apps/hive/warehouse/battingdrwxrwxrw- - hive hdfs 0 2016-05-30 10:47 /apps/hive/warehouse/batting2[will1@namenode01 will]$ hdfs groups will1will1 : will1 hdfs[will1@namenode01 will]$ groups will1will1 : will1 sqoop 上面看到，在namenode上添加用户后就可以生效了。但是假设我分配组分配错了，重新修改用户组后，hdfs并没有跟着修改，will1用户还是可以访问到hdfs组权限的文件。过了一段时间后，才可以了。目测盯了一下，差不多4分钟左右才生效。 可是如果Linux删除了这个用户，hdfs能够也跟着删除么？1234[root@namenode01 will]# hdfs groups will1will1 : will1 hdfs[root@namenode01 will]# grep will /etc/passwd[root@namenode01 will]# grep will /etc/group 刚删除同样没有生效，过一会儿再看。12[root@namenode01 will]# hdfs groups will1will1 : 也是会删除的。但是如果权限给错就不能立即收回，这是个问题。 参考：https://issues.apache.org/jira/browse/AMBARI-12732https://docs.hortonworks.com/HDPDocuments/Ambari-2.2.0.0/bk_ambari_views_guide/content/troubleshooting.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[openLDAP-安装配置]]></title>
    <url>%2F2016%2F12%2F05%2FopenLDAP-%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[介绍LDAP是一个许多传统高级系统管理员喜欢的服务，但是门槛高。这里及你进介绍一些实用的规则，最下面会有一些参考。 LDAP在局域网中的角色openLDAP实现了Lightweight Directory Access Protocol协议。目录本身是一个树结构的、可读的数据库。 我们使用openLDAP提供了一个网络验证中心，每个用户登录的时候都要到这里验证身份，如果是第一次的就自动创建他们的home目录。 这篇文章会提到怎样使用openLDAP作为验证中心，并为用户提供元数据。但是，文中使用明文用户名密码在wire中传播的方式是不安全的。因此推荐大家结合Kerberos使用openLDAP。 下面介绍LDAP的安装。 技术角度来看，LDAP目录是由一系列的有层级的entry组成。每个entry都属于某个指定的Object Classes，而且每个entry都包多含个kv键值对，也就是attribute。entry是使用DN(distinguished name)来区分彼此的。DN由一系列组件构成，这些组件之间用逗号隔开，表示从树的top节点到这个entry的全路径。例如，公司：dc=example,dc=com。员工:cn=person,ou=People,dc=example,dc=com。 entry的objectClasses决定哪些attribute必须有，哪些可以没有。 上面的cn=persion，就是键值对的形式。上面的键(cn,ou,dc)分别代表着Common Name, Organizational Unit以及Domain Component。这是一些常用的术语。 先说位一下我们安装的几个注意点： LDAP与传统的系统用户或者其他数据无关。但是，我们安装过程中会存储一部分信息在/etc/paswd和/etc/group中，然后让这些信息在一个网络中心位置共享。 可以配置LDAP存储用户的密码。密码主要用来验证用户是否有权限访问指定的目录，还有就是验证用户是否知道正确的密码。当某个用户打开一个LDAP client来查看目录的时候，他的DB和密码就用来验证他的权限。当LDAP用来验证用户的时候，他的DB和密码只是用来建立LDAP目录的连接。成功连接就意味着用户知道正确的密码。 粘结层：将LDAP于系统软件整合起来NSSPAMConventions【约定】 Debian GNU平台，或者Ubuntu。 有sudo免密命令。 安装过程中会有很多提示问题，运行：sudo dpkg-reconfigure debconf。然后舒服interface=Dialog，还有priority=low 查看日志： cd /var/log; sudo tail -F daemon.log sulog user.log auth.log debug kern.log syslog dmesg messages kerberos/{krb5kdc,kadmin,krb5lib}.log 我们的测试系统叫做monarch.spinlock.hr，ip地址是192.168.7.12。server和client都会安在同一个机器上，为了明确区分client相对于monarch.spinlock.hr，server对应ldap1.spinlock.hr。按照下面配置/etc/hosts： 192.168.7.12 monarch.spinlock.hr monarch krb1.spinlock.hr krb1 ldap1.spinlock.hr ldap1注意有的机器域名会被设置成127.0.0.1，这可能会出现问题。需要保证/etc/hosts里只能是: 127.0.0.1 localhost 最后，测试是不是成功了。1234567891011 ping -c1 localhostPING localhost (127.0.0.1) 56(84) bytes of data.....ping -c1 monarchPING monarch.spinlock.hr (192.168.7.12) 56(84) bytes of data.....ping -c1 ldap1PING ldap1.spinlock.hr (192.168.7.12) 56(84) bytes of data..... 2. OpenLDAP2.1 server安装openLDAP的server叫做slapd。 sudo apt-get install slapd ladp-tuils Debconf如下：123456789101112131415Omit OpenLDAP server configuration? NoDNS domain name: spinlock.hrOrganization name? spinlock.hrAdministrator password: PASSWORDConfirm password: PASSWORDDatabase backend to use: HDBDo you want the database to be removed when slapd is purged? NoAllow LDAPv2 protocol? No 安装完成后slapd会自动启动。 2.1.1初始配置server端的配置主要包含ObjectClasses、attribute、syntax、matching rules以及其他的LDAP数据结构的细节。配置文件目录是/etc/ldap/slapd.d/，每次启动的时候都会加载这些配置文件。 这些配置文件是以LDIF的形式保存的，预期是想使用标准的LDAP数据修改工具进行修改，然后slapd就会把新的配置存储在LDIF文件中，以保证下次重启生效。其实手动修改也是可以的。这种只能在运行时修改配置的方式叫做olc(on-line configuration)，也叫cn=config以及slapd.d。 以前版本的openLDAP是使用标准的txt配置文件/etc/ladp/slapd.conf，虽然目前也还是支持的，但是并不推荐使用这个文件。因为修改之后需要重启才能生效，cn=config是当前默认的方式。 注意 如果你的系统上游/etc/ldap/slapd.conf文件，但是没有/etc/ldap/slapd.d目录的话，运行以下命令： sudo slaptest -f /etc/ldap/slapd.conf -F /etc/ldap/slapd.d 然后确认你的slapd是使用-F /etc/ldap/slapd.d启动的，而不是-f /etc/ldap/slapd.conf grep -e -F /etc/init.d/slapdps aux | grep slapd 2.1.1.1 CN=CONFIG配置cn=config是一个特殊的LDAP数据库，它保存着OpenLDAP server的配置，运行时可读可写。对它的修改直接影响当前运行的server，也会修改/etc/ldap/slapd.d/下对应的配置文件。 修改配置的时候我们需要利用已经存在的默认的cn=config配置，能让本地root用户连接到/var/run/slapd/ldapi的slapd socket修改cn=config数据库。 有了这个途径，我们先验证一下所有的LDAP schema定义(core,cosine,nis,inetorgperson)都已经加载成功。在这一步出现重复entry问题的话不要紧张，目的在于知道我们的client以local user的身份验证身份，然后连接到slapd socket “-Y EXTERNAL -H ldapi:///“:1234sudo ldapadd -c -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/core.ldifsudo ldapadd -c -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/cosine.ldifsudo ldapadd -c -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/nis.ldifsudo ldapadd -c -Y EXTERNAL -H ldapi:/// -f /etc/ldap/schema/inetorgperson.ldif 然后我们把默认的olcLogLevel: none替换成olcLogLevel: 256:123456echo &quot;dn: cn=configchangetype: modifyreplace: olcLogLevelolcLogLevel: 256&quot; &gt; /var/tmp/loglevel.ldifsudo ldapmodify -Y EXTERNAL -H ldapi:/// -f /var/tmp/loglevel.ldif 然后为attribute uid添加索引”eq”:12345678echo &quot;dn: olcDatabase=&#123;1&#125;hdb,cn=configchangetype: modifyadd: olcDbIndexolcDbIndex: uid eq&quot; &gt; /var/tmp/uid_eq.ldifsudo ldapmodify -Y EXTERNAL -H ldapi:/// -f /var/tmp/uid_eq.ldif 最后，以同样的方式允许LDAP管理员用户对于cn=config可读可写，dc=spinlock,dc=hr数据树就也可写了。只要用户知道正确的管理员DN和密码，就可以很方便快捷地修改slapd的配置。使用图形化的LDAP client来查看和修改配置对于初学者来说会更好上手一些：123456echo &quot;dn: olcDatabase=&#123;0&#125;config,cn=configchangetype: modifyadd: olcAccessolcAccess: to * by dn=&quot;cn=admin,dc=spinlock,dc=hr&quot; write&quot; &gt; /var/tmp/access.ldifsudo ldapmodify -c -Y EXTERNAL -H ldapi:/// -f /var/tmp/access.ldif 2.1.1.2 spapd.conf配置【失效】2.1.1.3 client端配置server已经可以运行了。下面配置所有LDAP client的配置文件 /etc/ldap/ldap.conf。12BASE dc=spinlock, dc=hr URI ldap://192.168.7.12/ 2.1.2 initial test 初始测试可以测试一下我们安装的成果了。我们的openLDAP server只包含基本的读取操作。从LDAP的角度来看，读取操作被成为search。要使用命令行工具执行search，我们需要安装ldapsearch和slapcat。 ldapsearch使用LDAP协议执行在线操作。 SLAPCAT执行离线操作，直接修改本地文件系统里的文件。因此，这些命令只能在openLDAP server的本机执行，而且还需要特殊权限。向数据库写数据之前，需要先停掉openLDAP server。 在以上两个命令的输出中，一会注意到有两个LDAP entry，一个代表树的top节点，另一个代表LDAP 管理员的entry。在slapcat的输出中，不会像ldapsearch一样把额外的attributes打印出来。12345678910111213141516171819202122232425262728293031ldapsearch -x# extended LDIF## LDAPv3# base &lt;dc=spinlock, dc=hr&gt; (default) with scope subtree# filter: (objectclass=*)# requesting: ALL## spinlock.hrdn: dc=spinlock,dc=hrobjectClass: topobjectClass: dcObjectobjectClass: organizationo: spinlock.hrdc: spinlock# admin, spinlock.hrdn: cn=admin,dc=spinlock,dc=hrobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administrator# search resultsearch: 2result: 0 Success# numResponses: 3# numEntries: 2 12345678910111213141516171819202122232425262728sudo slapcatdn: dc=spinlock,dc=hrobjectClass: topobjectClass: dcObjectobjectClass: organizationo: spinlock.hrdc: spinlockstructuralObjectClass: organizationentryUUID: 350a2db6-87d3-102c-8c1c-1ffeac40db98creatorsName:modifiersName:createTimestamp: 20080316183324ZmodifyTimestamp: 20080316183324ZentryCSN: 20080316183324.797498Z#000000#000#000000dn: cn=admin,dc=spinlock,dc=hrobjectClass: simpleSecurityObjectobjectClass: organizationalRolecn: admindescription: LDAP administratoruserPassword:: e2NyeXB0fVdSZDJjRFdRODluNHM=structuralObjectClass: organizationalRoleentryUUID: 350b330a-87d3-102c-8c1d-1ffeac40db98creatorsName:modifiersName:createTimestamp: 20080316183324ZmodifyTimestamp: 20080316183324Z 创建基本树结构]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban源码追踪(1)-——-上传zip文件]]></title>
    <url>%2F2016%2F12%2F05%2Fazkaban%E6%BA%90%E7%A0%81%E8%BF%BD%E8%B8%AA(1)-%E2%80%94%E2%80%94-%E4%B8%8A%E4%BC%A0zip%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[相关类 ProjectMangerServlet ProjectManager ValidatorManager DirectoryFlowLoader JdbcProjectLoader 对应的流程log12345678910111213141516171819202122232016/06/14 15:39:18.537 +0800 INFO [ProjectManagerServlet] [Azkaban] Uploading file willjob.zip2016/06/14 15:39:23.861 +0800 INFO [ProjectManager] [Azkaban] Uploading files to mytestProject2016/06/14 15:40:16.454 +0800 WARN [XmlValidatorManager] [Azkaban] Validator directory validators does not exist or is not a directory2016/06/14 15:40:16.455 +0800 WARN [XmlValidatorManager] [Azkaban] Azkaban properties file does not contain the key project.validators2016/06/14 15:40:35.752 +0800 INFO [ProjectManager] [Azkaban] Validating project willjob.zip using the registered validators [Director2016/06/14 16:11:45.952 +0800 INFO [XmlValidatorManager] [Azkaban] Validation status of validator Directory Flow is PASS2016/06/14 16:56:51.338 +0800 INFO [ProjectManager] [Azkaban] Uploading file to db willjob.zip2016/06/14 16:56:51.338 +0800 INFO [JdbcProjectLoader] [Azkaban] Uploading to mytestProject version:1 file:willjob.zip2016/06/14 16:56:51.341 +0800 INFO [JdbcProjectLoader] [Azkaban] Creating message digest for upload willjob.zip2016/06/14 16:56:51.343 +0800 INFO [JdbcProjectLoader] [Azkaban] Md5 hash created2016/06/14 16:56:51.359 +0800 INFO [JdbcProjectLoader] [Azkaban] Read bytes for willjob.zip size:5602016/06/14 16:56:51.359 +0800 INFO [JdbcProjectLoader] [Azkaban] Running update for willjob.zip chunk 02016/06/14 16:56:51.360 +0800 INFO [JdbcProjectLoader] [Azkaban] Finished update for willjob.zip chunk 02016/06/14 16:56:51.362 +0800 INFO [JdbcProjectLoader] [Azkaban] Commiting upload willjob.zip2016/06/14 16:56:51.363 +0800 INFO [ProjectManager] [Azkaban] Uploading flow to db willjob.zip2016/06/14 16:56:51.363 +0800 INFO [JdbcProjectLoader] [Azkaban] Uploading flows2016/06/14 16:56:51.418 +0800 INFO [JdbcProjectLoader] [Azkaban] Flow upload will3 is byte size 2142016/06/14 16:56:51.432 +0800 INFO [JdbcProjectLoader] [Azkaban] Flow upload will2 is byte size 2382016/06/14 16:56:51.434 +0800 INFO [ProjectManager] [Azkaban] Changing project versions willjob.zip2016/06/14 16:56:51.436 +0800 INFO [ProjectManager] [Azkaban] Uploading Job properties2016/06/14 16:56:51.820 +0800 INFO [ProjectManager] [Azkaban] Uploading Props properties2016/06/14 16:56:51.822 +0800 INFO [ProjectManager] [Azkaban] Uploaded project files. Cleaning up temp files.2016/06/14 16:56:51.927 +0800 INFO [ProjectManager] [Azkaban] Cleaning up old install files older than -2 ProjectManager白话一下ProjectManager处理上传zip文件的过程： 将文件存到/tmp目录并解压 使用DirectoryFlowLoader解析文件，放置到这个loader的状态变量中。这些变量会一直保存在内存里，以供后面使用 遍历loader里的flowMap，把每个flow都加上最新的project_id和version 上传project信息 4.1 把zip文件弄成字节流上传到mysql的project_files表中 4.2 把最新版本的project信息上传到project_versions表中 上传flow: 把flow逐个序列化成json，再弄成byte字节流，再使用gzip压缩，上传至project_flows表中 更新projects里的相关记录，并修改内存中的project信息【修改时间等、最新版本的flows】 上传job的详细信息到project_properties中，job的详细信息也是json-&gt;byte[]-&gt;gzip的过程 上传props的详细信息到project_properties中，流程同job完全一样【这个debug的时候数据是空的】 将此次操作插入到project_event中，之后删除本地/tmp下的文件 使用project_id和当前version判断依次清除先前版本的project_flows的flow、project_properties、project_files、project_versions中的记录【会有最近版本保留数量设置project.version.retention，默认是3】 完成 文件解析DirectoryFlowLoader加载文件目录，解析所有的文件，进行解析，将信息存放在自己的状态变量中：123456789101112131415private Props props; // 环境、配置信息private HashSet&lt;String&gt; rootNodes; // 根节点，也就是实际执行的时候的起始节点private HashMap&lt;String, Flow&gt; flowMap; // 所有的flow，key是flowIdprivate HashMap&lt;String, Node&gt; nodeMap; // 所有的node，key是JobId或者embed的flowId// node之间的dependency，key是job名称，value是一系列的相关依赖job，一般是children，然后Edge指定从谁到谁private HashMap&lt;String, Map&lt;String, Edge&gt;&gt; nodeDependencies; private HashMap&lt;String, Props&gt; jobPropsMap; // 包含job的详细信息// flow之间的依赖关系【embedeed flow】private HashMap&lt;String, Set&lt;String&gt;&gt; flowDependencies;private ArrayList&lt;FlowProps&gt; flowPropsList;private ArrayList&lt;Props&gt; propsList;private Set&lt;String&gt; errors;private Set&lt;String&gt; duplicateJobs; 1234567891011121314151617181920212223242526272829// 临时解压目录 baseDirectory : temp/46769225// 项目信息 project: testPropublic void loadProjectFlow(Project project, File baseDirectory) &#123; propsList = new ArrayList&lt;Props&gt;(); flowPropsList = new ArrayList&lt;FlowProps&gt;(); jobPropsMap = new HashMap&lt;String, Props&gt;(); nodeMap = new HashMap&lt;String, Node&gt;(); flowMap = new HashMap&lt;String, Flow&gt;(); errors = new HashSet&lt;String&gt;(); duplicateJobs = new HashSet&lt;String&gt;(); nodeDependencies = new HashMap&lt;String, Map&lt;String, Edge&gt;&gt;(); rootNodes = new HashSet&lt;String&gt;(); flowDependencies = new HashMap&lt;String, Set&lt;String&gt;&gt;(); // 加载所有的properties、job文件，创建node对象 loadProjectFromDir(baseDirectory.getPath(), baseDirectory, null); jobPropertiesCheck(project); // Create edges and find missing dependencies resolveDependencies(); // 创建flows. 在此之后flowMap里就已经构建好了flow以及job之间的依赖了，node之间通过level来界定先后关系 buildFlowsFromDependencies(); // 处理embedded flows，就是嵌入的flow resolveEmbeddedFlows(); &#125; loadProjectFromDir方法详细123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private void loadProjectFromDir(String base, File dir, Props parent) &#123; // 找出所有的properties File[] propertyFiles = dir.listFiles(new SuffixFilter(PROPERTY_SUFFIX)); Arrays.sort(propertyFiles); for (File file : propertyFiles) &#123; String relative = getRelativeFilePath(base, file.getPath()); parent = new Props(parent, file); parent.setSource(relative); FlowProps flowProps = new FlowProps(parent); flowPropsList.add(flowProps); logger.info("Adding " + relative); propsList.add(parent); &#125; // 加载job文件 File[] jobFiles = dir.listFiles(new SuffixFilter(JOB_SUFFIX)); for (File file : jobFiles) &#123; String jobName = getNameWithoutExtension(file); if (!duplicateJobs.contains(jobName)) &#123; if (jobPropsMap.containsKey(jobName)) &#123; // 是否有重复 ... &#125; else &#123; Props prop = new Props(parent, file); String relative = getRelativeFilePath(base, file.getPath()); prop.setSource(relative); Node node = new Node(jobName); String type = prop.getString("type", null); if (type == null) &#123; errors.add("Job doesn't have type set '" + jobName + "'."); &#125; node.setType(type); // job类型 node.setJobSource(relative); // job相对路径 if (parent != null) &#123; node.setPropsSource(parent.getSource()); &#125; // Force root node if (prop.getBoolean(CommonJobProperties.ROOT_NODE, false)) &#123; rootNodes.add(jobName); &#125; // 把properties和node放进loader中的队列中 jobPropsMap.put(jobName, prop); nodeMap.put(jobName, node); &#125; &#125; &#125; // 检查是否有子目录 File[] subDirs = dir.listFiles(DIR_FILTER); for (File file : subDirs) &#123; loadProjectFromDir(base, file, parent); &#125;&#125; 常用的blob处理代码12345String propertyJSON = PropsUtils.toJSONString(props, true); byte[] data = propertyJSON.getBytes("UTF-8"); if (defaultEncodingType == EncodingType.GZIP) &#123; data = GZIPUtils.gzipBytes(data); &#125; 其他 level表示当前node所在的层次。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery上手]]></title>
    <url>%2F2016%2F12%2F05%2Fcelery%E4%B8%8A%E6%89%8B%2F</url>
    <content type="text"><![CDATA[首先我们需要有一个celery实例，也就是celery app。有了这个实例我们才能创建task、管理worker，只需要把这个实例作为moduel import进来就可以了。 创建实例，并创建task：1234567from celery import Celeryapp = Celery('tasks', broker='amqp://guest@localhost//')@app.taskdef add(x, y): return x + y 启动worker：1celery -A tasks worker --loglevel=info 12345678910111213-------------- celery@halcyon.local v3.1 (Cipater)---- **** -------- * *** * -- [Configuration]-- * - **** --- . broker: amqp://guest@localhost:5672//- ** ---------- . app: __main__:0x1012d8590- ** ---------- . concurrency: 8 (processes)- ** ---------- . events: OFF (enable -E to monitor this worker)- ** ----------- *** --- * --- [Queues]-- ******* ---- . celery: exchange:celery(direct) binding:celery--- ***** -----[2012-06-08 16:23:51,078: WARNING/MainProcess] celery@halcyon.local has started. concurrency 是worker进程的并发数据，默认是cpu个数。需要视情况而定，cpu密集型的就少一些，IO密集型的就多一些。除了默认的prefork 池，celery还支持eventlet、gevent以及threads。 event。 启用后celery就发送这个worker里的监控event。这可以被celery events、flower使用。 queue。代表这个worker会消费的队列。 调用task：12&gt;&gt;&gt; from tasks import add&gt;&gt;&gt; add.delay(4, 4) 使用delay()方法调用task，它是apply_async()方法的简写。然后我们可以去worker的console观察任务执行了。 调用task放回的对象是AsyncResult实例，它可以用来获取当前task的状态，等待task结束后，获取task的返回值。默认是没有启用的，要使用的话需要给celery配置一个result backend。 1app = Celery('tasks', backend='redis://localhost', broker='amqp://') 这里是使用redis作为task状态和结果的存储介质。再次调用12345678910111213141516171819&gt;&gt;&gt; result = add.delay(4, 4)&gt;&gt;&gt; result.ready()False# 一般并不会直接获取这个结果，这样就把异步调用弄成同步调用一样了&gt;&gt;&gt; result.get(timeout=1)8## 当出现异常的时候，get方法默认也会抛出这个异常，可以使用propagate来阻止&gt;&gt;&gt; result.get(propagate=False)## 获取异常堆栈&gt;&gt;&gt; result.traceback&gt;&gt;&gt; res.failed()True&gt;&gt;&gt; res.successful()False# 一个task的典型status历程：PENDING -&gt; STARTED -&gt; SUCCESS# 重试的task：PENDING -&gt; STARTED -&gt; RETRY -&gt; STARTED -&gt; RETRY -&gt; STARTED -&gt; SUCCESS&gt;&gt;&gt; res.state'FAILURE' 要使配置马上生效的话，可以调用update方法1234567app.conf.update( CELERY_TASK_SERIALIZER='json', CELERY_ACCEPT_CONTENT=['json'], # Ignore other content CELERY_RESULT_SERIALIZER='json', CELERY_TIMEZONE='Europe/Oslo', CELERY_ENABLE_UTC=True,) 要是大型项目，弄一个特定的配置module比较好。最好不要硬编码周期任务和task路由，把这些放在一个集中的位置，方便控制。比如系统管理员在系统故障的时候做一些简单的微调什么的。 使用app.config_from_object()方法，使celery实例使用一个配置module。1app.config_from_object('celeryconfig') celeryconfig.py:12345678BROKER_URL = 'amqp://'CELERY_RESULT_BACKEND = 'rpc://'CELERY_TASK_SERIALIZER = 'json'CELERY_RESULT_SERIALIZER = 'json'CELERY_ACCEPT_CONTENT=['json']CELERY_TIMEZONE = 'Europe/Oslo'CELERY_ENABLE_UTC = True 注意，这个module应该可以import到才对。配置文件中可以指定路由、注解等很多东西。1234567CELERY_ANNOTATIONS = &#123; 'tasks.add': &#123;'rate_limit': '10/m'&#125;&#125;CELERY_ROUTES = &#123; 'tasks.add': 'low-priority',&#125; 要是使用rabbitMQ或者Redis作为broker的话，还可以直接通过命令进行配置：1celery -A tasks control rate_limit tasks.add 10/m 实际使用celery项目结构：123proj/__init__.py /celery.py /tasks.py celery.py文件内容：12345678910111213141516from __future__ import absolute_importfrom celery import Celeryapp = Celery('proj', broker='amqp://', backend='amqp://', include=['proj.tasks'])# Optional configuration, see the application user guide.app.conf.update( CELERY_TASK_RESULT_EXPIRES=3600,)if __name__ == '__main__': app.start() 在celery中可以创建自己的实例，然后在其他地方import这个实例就可以使用了。 注意一下include，应该把自定义的task的module填写进来，这样celery实例启动的时候就会加载这些module。 我们的proj/tasks.py：123456789101112131415161718from __future__ import absolute_importfrom proj.celery import app@app.taskdef add(x, y): return x + y@app.taskdef mul(x, y): return x * y@app.taskdef xsum(numbers): return sum(numbers) 启动后要停掉worker的话，只需要直接ctrl + c就可以了。 生产环境中需要以守护进程的形式启动worker，使用celery multi任务就可以了，它可以启动一个或多个worker。 1234567891011121314151617celery multi start w1 -A proj -l infocelery multi v3.1.1 (Cipater)&gt; Starting nodes... &gt; w1.halcyon.local: OK$ celery multi restart w1 -A proj -l infocelery multi v3.1.1 (Cipater)&gt; Stopping nodes... &gt; w1.halcyon.local: TERM -&gt; 64024&gt; Waiting for 1 node..... &gt; w1.halcyon.local: OK&gt; Restarting node w1.halcyon.local: OKcelery multi v3.1.1 (Cipater)&gt; Stopping nodes... &gt; w1.halcyon.local: TERM -&gt; 64052 celery multi stop w1 -A proj -l info stop命令是异步的，这样就不用hang，等着worker停止了。为了我保证所有的task能够完成，也可以使用stopwait方法。1celery multi stopwait w1 -A proj -l info 下面是加上指定pid，和log的启动命令：1234$ mkdir -p /var/run/celery$ mkdir -p /var/log/celery$ celery multi start w1 -A proj -l info --pidfile=/var/run/celery/%n.pid \ --logfile=/var/log/celery/%n%I.log 12celery multi start 10 -A proj -l info -Q:1-3 images,video -Q:4,5 data \ -Q default -L:4,5 debug Canvas：工作流使用subtask搞定，它封装了单个task的参数，可以传递给后面的方法。 12&gt;&gt;&gt; add.subtask((2, 2), countdown=10)tasks.add(2, 2) 可以替换为:12&gt;&gt;&gt; add.s(2, 2)tasks.add(2, 2) 比较牛逼的地方在于，他竟然可以定义偏函数12345678910111213141516&gt;&gt;&gt; s1 = add.s(2, 2)&gt;&gt;&gt; res = s1.delay()&gt;&gt;&gt; res.get()4# 空的是前面的参数，后面的是2# incomplete partial: add(?, 2)&gt;&gt;&gt; s2 = add.s(2)# resolves the partial: add(8, 2)&gt;&gt;&gt; res = s2.delay(8)&gt;&gt;&gt; res.get()10&gt;&gt;&gt; s3 = add.s(2, 2, debug=True)&gt;&gt;&gt; s3.delay(debug=False) # debug is now False. primitive group map chain starmap chord chunks这些primitive本身就是subtask，所以它们可以组合成工作流。 groupsgroup并行调用多个task，返回一串结果。12345&gt;&gt;&gt; from celery import group&gt;&gt;&gt; from proj.tasks import add&gt;&gt;&gt; group(add.s(i, i) for i in xrange(10))().get()[0, 2, 4, 6, 8, 10, 12, 14, 16, 18] 结合偏函数使用group：123&gt;&gt;&gt; g = group(add.s(i) for i in xrange(10))&gt;&gt;&gt; g(10).get()[10, 11, 12, 13, 14, 15, 16, 17, 18, 19] chain有序调用一系列task123456&gt;&gt;&gt; from celery import chain&gt;&gt;&gt; from proj.tasks import add, mul# (4 + 4) * 8&gt;&gt;&gt; chain(add.s(4, 4) | mul.s(8))().get()64 偏函数：1234# (? + 4) * 8&gt;&gt;&gt; g = chain(add.s(4) | mul.s(8))&gt;&gt;&gt; g(4).get()64 12&gt;&gt;&gt; (add.s(4, 4) | mul.s(8))().get()64 chord带有一个回调函数的一组task。12345&gt;&gt;&gt; from celery import chord&gt;&gt;&gt; from proj.tasks import add, xsum&gt;&gt;&gt; chord((add.s(i, i) for i in xrange(10)), xsum.s())().get()90 被chain到其他task的group自动转为chord：12&gt;&gt;&gt; (group(add.s(i, i) for i in xrange(10)) | xsum.s())().get()90 结合多种subtask使用：1&gt;&gt;&gt; upload_document.s(file) | group(apply_filter.s() for filter in filters) 路由主要是针对consumer/worker和producer各自的队列 远程控制使用RabbitMQ、或者Redis作为broker的话可以在运行时控制与查看worker的状态。1$ celery -A proj inspect active 这是查看当前正在运行的task。 参考： http://docs.celeryproject.org/en/latest/getting-started/first-steps-with-celery.html http://docs.celeryproject.org/en/latest/getting-started/next-steps.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ember-data-定制化adapter]]></title>
    <url>%2F2016%2F12%2F05%2FEmber-data-%E5%AE%9A%E5%88%B6%E5%8C%96adapter%2F</url>
    <content type="text"><![CDATA[adapter在Ember Data中决定数据怎样被持久化到后端数据存储层。比如处理URL格式、REST API的header等。(数据格式决定于serializer.)Ember Data的默认adapter有一些内置的假设规范，参看：REST API should look。如果你的后端不满足这些规范，就集成默认的adapter，修改一下它的方法。 一些典型的应用场景是在你的urls中使用underscores_case , 使用中介而不是直接使用你后端的server，甚至直接使用local storage backend. 继承adapter在Ember Data中是很轻松的过程。 如果你的后端你有一致性要求，你可以定义一个adapter:application. adapter:application 的优先级高于所有其他的默认Adapter, 但是低于model内指定的Adapters. 12345import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; // Application specific overrides go here&#125;); 如果你有一个model相对于其他的model在与后台沟通的时候需要额外的规则，那么你就可以创建一个Model指定的Adapter：ember generate adapter adapter-name.例如，运行ember generate adapter post 会创建下面的文件: 12345import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; namespace: &apos;api/v1&apos;&#125;); Ember Data默认有几个内置的adapter。 Adapter 是最基础的adapter，不好喊任何功能。如果你要创建一个不同于其他的adapter功能的adapter，通常它是要给不错的起点。 JSONAPIAdapterJSONAPIAdapter是遵循默认JSON API规范的adapter，负责通过XHR传递JSON与http server沟通。 RESTAdapterRESTAdapter 通过XHR传递JSON允许你的store与http server直接沟通。2.0版本之前，这个是默认的adapter。 定制化JSONAPIAdapterJSONAPIAdapter有一些很好用的hooks来适应非标准的后台。 URL规范JSONAPIAdapter可以通过mode的名字来判断 URLs.比如，如果你通过ID查询Post: 12store.findRecord('post', 1).then(function(post) &#123;&#125;); JSON API adapter会自动发送一个GET 请求到/posts/1. JSON API adapter中的操作与url对应如下: ActionHTTP VerbURL FindGET/posts/123 Find AllGET/posts UpdatePATCH/posts/123 CreatePOST/posts DeleteDELETE/posts/123 Pluralization CustomizationTo facilitate pluralizing model names when generating route urls EmberData comes bundled withEmber Inflector, aActiveSupport::Inflector compatible library for inflecting wordsbetween plural and singular forms. Irregular or uncountablepluralizations can be specified via Ember.Inflector.inflector.A common way to do this is（上面的没咋看懂，就是url中处理model负数问题的）: 12// sets up Ember.Inflectorimport &apos;./models/custom-inflector-rules&apos;; 123456789import Inflector from &apos;ember-inflector&apos;;const inflector = Inflector.inflector;inflector.irregular(&apos;formula&apos;, &apos;formulae&apos;);inflector.uncountable(&apos;advice&apos;);// Meet Ember Inspector&apos;s expectation of an exportexport default &#123;&#125;; 这告诉JSON API adapter对于formula的请求应该去/formulae/1 而不是 /formulas/1, 对于advice的请求应该去 /advice/1 而不是 /advices/1. Endpoint Path Customizationnamespace 属性可以作为指定url的前缀. 12345import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; namespace: &apos;api/1&apos;&#125;); 请求person 被指向 http://emberjs.com/api/1/people/1. Host Customization默认adapter会向当前域发送请求。如果要指定新的域名需要设置adapter的host属性。 12345import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; host: &apos;https://api.example.com&apos;&#125;); 请求person 会指向 https://api.example.com/people/1. Path Customization默认 JSONAPIAdapter会处理model的名字为复数，生成path。如果这并不适用于你的后台，那就override pathForType 方法. 例如，如果你不想复数化你的model名称，而是要把你的model名字从驼峰转为下滑线，你可以这边写pathForType 方法: 1234567import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; pathForType: function(type) &#123; return Ember.String.underscore(type); &#125;&#125;); 请求 person 被定向到 /person/1.请求 user-profile定向到 /user_profile/1. Headers customization又的API必须要有一些header才行，如果提供一个key。在JSONAPIAdapter的 headers对象中我们可以设置任意的header信息。Ember Data会在发送每个ajax 请求的时候带上它们. 12345678import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; headers: &#123; &apos;API_KEY&apos;: &apos;secret key&apos;, &apos;ANOTHER_HEADER&apos;: &apos;Some header value&apos; &#125;&#125;); headers也可以使用 computed property来支持动态header. 下面例子中是基于sessionservice中的authToken属性的. 1234567891011import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; session: Ember.inject.service(&apos;session&apos;), headers: Ember.computed(&apos;session.authToken&apos;, function() &#123; return &#123; &apos;API_KEY&apos;: this.get(&apos;session.authToken&apos;), &apos;ANOTHER_HEADER&apos;: &apos;Some header value&apos; &#125;; &#125;)&#125;); In some cases, your dynamic headers may require data from someobject outside of Ember’s observer system (for exampledocument.cookie). You can use thevolatilefunction to set the property into a non-cached mode causing the headers tobe recomputed with every request. 12345678910import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; headers: Ember.computed(function() &#123; return &#123; &apos;API_KEY&apos;: Ember.get(document.cookie.match(/apiKey\=([^;]*)/), &apos;1&apos;), &apos;ANOTHER_HEADER&apos;: &apos;Some header value&apos; &#125;; &#125;).volatile()&#125;); Authoring AdaptersdefaultSerializer 可以用来指定adapter的serializer. 只有model需要指定的serializer或者么有定义serializer:application的时候才会用到。 在app中, 指定serializer:application通常很容易. 但是，如果你是一个社区adapter的作者，记住一定要设置这个属性，以免你的用户忘记没有指定这个属性，Ember就报错了。. 12345import JSONAPIAdapter from &apos;ember-data/adapters/json-api&apos;;export default JSONAPIAdapter.extend(&#123; defaultSerializer: &apos;-default&apos;&#125;); 社区的Adapters可以从下面的地址中找到社区贡献的adapter，或许有适合你的哦: Ember Observer GitHub Bower]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[jenkins]]></title>
    <url>%2F2016%2F12%2F05%2Fjenkins%2F</url>
    <content type="text"><![CDATA[安装插件 问题：每次在线安装插件的时候，Jenkins默认都是使用www.google.com去验证网络是否通畅，如果不通畅下面指定的插件还是会去下载，但是根本就不会安装。(比较让人费解的是，都下了还不安，要么就别下了也) 期望：使用www.baidu.com来验证网络，或者跳过验证。 方案: 修改/var/lib/jenkins/updates/default.json里的connectionCheckUrl，这个文件特别大， 但是这个connectionCheckUrl一般都在最开始的地方。 启用用户验证系统管理-&gt; Configure Global Security maven相关安装maven插件 maven仓库主要是为了能够让每个maven项目build的时候都使用一个公共的maven地址，而又不想去默认的~/.m2/repository(我们这边/的硬盘不是很大). 安装一个插件：config file provider plugin在系统管理-&gt;Managed files里添加一个maven的settings文件，可以把自己线上的maven配置信息弄过来。 maven构建maven项目配置的时候，记得以下几点 在goals和options里指定maven构建目标，反正我是二逼的忘掉了。这一条只为二逼的自己 root pom指定相对根目录的pom.xml的位置。 修改config file provider plugin的settings.xml指定的localRepository对于Jenkins用户的可读可写可执行的权限，懒货如我，直接整个repo都777了。 发布到tomcat期望：build成功后直接使用shell将war包发布到tomcat，并重启之，等待一段时间后，测试一系列接口，确认后台正常服务。 问题：jenkins的job进程创建的、包括调用脚本创建的进程都会随着Jenkins job进程的退出而退出。 参考：ProcessTreeKiller 方案：下载Deploy to container Plugin，在指定project下配置构建完成后操作，配置好tomcat的相关信息之后，尝试build。 发现报错:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758Deploying /var/lib/jenkins/jobs/metamap/workspace/metamap_java/target/metamap.war to container Tomcat 7.x RemoteERROR: Build step failed with exceptionorg.codehaus.cargo.container.ContainerException: Failed to redeploy [/var/lib/jenkins/jobs/metamap/workspace/metamap_java/target/metamap.war] at org.codehaus.cargo.container.tomcat.internal.AbstractTomcatManagerDeployer.redeploy(AbstractTomcatManagerDeployer.java:189) at hudson.plugins.deploy.CargoContainerAdapter.deploy(CargoContainerAdapter.java:73) at hudson.plugins.deploy.CargoContainerAdapter$1.invoke(CargoContainerAdapter.java:116) at hudson.plugins.deploy.CargoContainerAdapter$1.invoke(CargoContainerAdapter.java:103) at hudson.FilePath.act(FilePath.java:990) at hudson.FilePath.act(FilePath.java:968) at hudson.plugins.deploy.CargoContainerAdapter.redeploy(CargoContainerAdapter.java:103) at hudson.plugins.deploy.DeployPublisher.perform(DeployPublisher.java:61) at hudson.tasks.BuildStepMonitor$3.perform(BuildStepMonitor.java:45) at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:782) at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:723) at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.post2(MavenModuleSetBuild.java:1037) at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:668) at hudson.model.Run.execute(Run.java:1763) at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:529) at hudson.model.ResourceController.execute(ResourceController.java:98) at hudson.model.Executor.run(Executor.java:410)Caused by: org.codehaus.cargo.container.tomcat.internal.TomcatManagerException: The username you provided is not allowed to use the text-based Tomcat Manager (error 403) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.invoke(TomcatManager.java:555) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.list(TomcatManager.java:686) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.getStatus(TomcatManager.java:699) at org.codehaus.cargo.container.tomcat.internal.AbstractTomcatManagerDeployer.redeploy(AbstractTomcatManagerDeployer.java:174) ... 16 moreCaused by: java.io.IOException: Server returned HTTP response code: 403 for URL: http://10.0.1.62:8080//manager/text/list at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1839) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.invoke(TomcatManager.java:544) ... 19 moreorg.codehaus.cargo.container.tomcat.internal.TomcatManagerException: The username you provided is not allowed to use the text-based Tomcat Manager (error 403) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.invoke(TomcatManager.java:555) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.list(TomcatManager.java:686) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.getStatus(TomcatManager.java:699) at org.codehaus.cargo.container.tomcat.internal.AbstractTomcatManagerDeployer.redeploy(AbstractTomcatManagerDeployer.java:174) at hudson.plugins.deploy.CargoContainerAdapter.deploy(CargoContainerAdapter.java:73) at hudson.plugins.deploy.CargoContainerAdapter$1.invoke(CargoContainerAdapter.java:116) at hudson.plugins.deploy.CargoContainerAdapter$1.invoke(CargoContainerAdapter.java:103) at hudson.FilePath.act(FilePath.java:990) at hudson.FilePath.act(FilePath.java:968) at hudson.plugins.deploy.CargoContainerAdapter.redeploy(CargoContainerAdapter.java:103) at hudson.plugins.deploy.DeployPublisher.perform(DeployPublisher.java:61) at hudson.tasks.BuildStepMonitor$3.perform(BuildStepMonitor.java:45) at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:782) at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:723) at hudson.maven.MavenModuleSetBuild$MavenModuleSetBuildExecution.post2(MavenModuleSetBuild.java:1037) at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:668) at hudson.model.Run.execute(Run.java:1763) at hudson.maven.MavenModuleSetBuild.run(MavenModuleSetBuild.java:529) at hudson.model.ResourceController.execute(ResourceController.java:98) at hudson.model.Executor.run(Executor.java:410)Caused by: java.io.IOException: Server returned HTTP response code: 403 for URL: http://10.0.1.62:8080//manager/text/list at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1839) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1440) at org.codehaus.cargo.container.tomcat.internal.TomcatManager.invoke(TomcatManager.java:544) ... 19 moreBuild step &apos;Deploy war/ear to a container&apos; marked build as failure 修改对应tomcat的conf/tomcat-users.xml，添加tomcat-script角色，并赋予给当前的tomcat管理员用户。 123456build pom done.....Deploying /var/lib/jenkins/jobs/metamap/workspace/metamap_java/target/metamap.war to container Tomcat 7.x Remote Redeploying [/var/lib/jenkins/jobs/metamap/workspace/metamap_java/target/metamap.war] Undeploying [/var/lib/jenkins/jobs/metamap/workspace/metamap_java/target/metamap.war] Deploying [/var/lib/jenkins/jobs/metamap/workspace/metamap_java/target/metamap.war]Finished: SUCCESS 使用root用户因为会部署很多任务放在jenkins上，而各个任务都有自己的权限目录，使用jenkins用户执行任务会有些不方便，总是遇到权限问题。 结合我们既有环境考虑，使用root用户执行jenkins任务并不会造成额外的危险。（操作jenksin 的人很有限，而且各自有账户） 具体步骤:12345678vim /etc/sysconfig/jenkins$JENKINS_USER=&quot;root&quot;chown -R root:root /var/lib/jenkinschown -R root:root /var/cache/jenkinschown -R root:root /var/log/jenkinsservice jenkins restart 先在jenkins配置文件里指定jenkins系统执行任务的用户，之后修改jenkins既有目录的权限，重启服务。 问题python脚本不及时输出，最后一股子出来。在执行python命令前，先添加环境变量：1export PYTHONUNBUFFERED=1 https://stackoverflow.com/questions/11631951/jenkins-console-output-not-in-realtime]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mondrian架构]]></title>
    <url>%2F2016%2F12%2F05%2Fmondrian%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[1 mondrain系统的层次 表现层(the presentation layer) 维度层(the dimensional layer) 聚合层(the star layer) 存储层(the storage layer) 1.1 表现层(the presentation layer)表现层决定了最终用户将在他们的显示器上看到什么, 及他们如何同系统产生交互。有许多方法可以用来向用户显示多维数据集, 有 pivot 表 (一种交互式的表), pie, line 和图表(bar charts)。它们可以用Swing 或 JSP来实现。表现层以多维”文法(grammar)(维、度量、单元)”的形式发出查询，然后OLAP服务器返回结果。 1.2 维度层(the dimensional layer)维度层用来解析、验证和执行MDX查询请求。一个MDX查询要通过几个阶段来完成：首先是计算坐标轴（axes），再者计算坐标轴axes 中cell的值。 为了提高效率，维度层把要求查询的单元成批发送到集合层，查询转换器接受操作现有查询的请求，而不是对每个请求都建立一个MDX 声明。 1.3 聚合层(the star layer)集合层负责维护和创建聚合缓存，一个聚合是在内存中缓存一组单元值， 这些单元值由一组维的值来确定。维度层对这些单元发出查询请求，如果所查询的单元值不在缓存中，则聚合管理器(aggregation manager)会向存储层发出查询请求 1.4 存储层(the storage layer)存储层是一个关系型数据库(RDBMS)。它负责创建集合的单元数据，和提供维表的成员。在后面后说一下直接使用RDBMS而不另外开发存储的原因。 这些层次组件可以都在同一个机器上，也可以分布开。第二层和第三层组成mondrian server，他俩必须在一个机器上。 2 存储与聚合策略OLAP server通常都基于他们存储数据的方式被归为以下两类： MOLAP（multidimensional OLAP） server。为了便于多维访问，它将所有的数据结构化存储在磁盘上。一般数据都是存储在稠密的数据中，每个cell只需要4或8个字节 ROLAP(relational OLAP) server。直接将数据存在RDBMS中。事实表里的每一行的字段都同时包含了维度和指标。 我们需要存储三种数据：事实表数据（事务记录）、聚合数据、维度。 MOLAP数据库是以多维格式存储事实数据的，但是如果维度很多的话，数据就会稀疏，这种存储格式表现就不太好了。 HOLAP (hybrid OLAP)系统的解决了这个问题，它在关系数据库中存储到最细粒度的数据，把聚合结果存在多维格式下。 对于大数据集来说，否则有些查询可能会需要遍历事实表里的所有数据。 MOLAP的聚合表通常是内存数据结构的一个快照，存储在磁盘上。ROLAP 聚合表是存在表里的。在一些 ROLAP 系统中，这些都需要OLAP server详细管理。对其他系统来说，这些表只是物化视图而已，只有在OLAP server被访问到指定查询时才会用到。 最极端的聚合策略就是缓存了，存在内存里。如果缓存里数据集在比较低的聚合level上，还可以上卷。 缓存可能是聚合策略中最重要的部分了，因为他是可以适配的。很难在不适用大量磁盘的前提下，选择聚合维度并预计算。要是维度较多，或者用户提交了某些一些不可预测的查询就更糟糕了。在数据实时变化的系统中，维护预计算聚合几乎是不可实现的。一个缓存的合理大小应该能够允许系统执行一些不可预估的查询，不适用任何聚合。 Mondrian的聚合策略如下： 事实数据存在RDBMS中。我们没必要自己再开发一个存储了 通过提交group by查询，将聚合数据读取进入缓存。既然RDBMS有了聚合操作，我们也直接使用 如果RDBMS支持物化视图，数据库管理员为某些聚合创建了物化视图，然后mondrian就可以使用它们咯。理想情况下，mondrian的聚合管理员应该知道这些物化视图是否存在，知道怎样高效计算，甚至能够给数据库管理员一些调优建议。 总的原则就是能委托给数据库的就给数据库做。数据库的负载虽然增加了一些，但是数据库的客户端获益很大。多维存储应该减少IO，才能更快。 另外，因为mondrian并不需要什么存储，所以只要把jar文件加入classpath就可以运行它了。没有多余的数据管理，数据加载过程也很简单，mondrian很适合数据实时变化的OLAP。 3 APImondrian为app提供了客户端api。 因为对于执行OLAP查询并没有统一规范，所以mondrian也是自己独有的API，但是熟悉jdbc的朋友们都会很容易上手。主要区别在于查询语言是MDX(‘Multi-Dimensional eXpressions’)，而JDBC是使用标准SQL的。 下面的java代码是连接mondrian，执行查询，然后打印结果：12345678910111213141516import mondrian.olap.*;import java.io.PrintWriter;Connection connection = DriverManager.getConnection( "Provider=mondrian;" + "Jdbc=jdbc:odbc:MondrianFoodMart;" + "Catalog=/WEB-INF/FoodMart.xml;", null, false);Query query = connection.parseQuery( "SELECT &#123;[Measures].[Unit Sales], [Measures].[Store Sales]&#125; on columns," + " &#123;[Product].children&#125; on rows " + "FROM [Sales] " + "WHERE ([Time].[1997].[Q1], [Store].[CA].[San Francisco])");Result result = connection.execute(query);result.print(new PrintWriter(System.out)); 使用DriverManger创建Connection。Query对应于jdbc的Statement，通过解析MDX字符串创建。 Result对应于jdbc的ResultSet。既然我们这里处理的是多维数据，这个结果里就包含了坐标轴axes和单元格cell，而不是行row和列column了。还有，OLAP是用于数据探查的，你可以执行一些类似下钻drillDown、排序sort的操作修改parse tree，然后重新执行查询。 还有一些对象： Schema, Cube, Dimension, Hierarchy, Level, Member. 可以看一下mondrian的API了解详情。 4 MDX 4.1 参数参数是在MDX查询中内嵌的一个变量。1Parameter(&lt;name&gt;, &lt;type&gt;, &lt;defaultValue&gt;[, &lt;description&gt;]) name ,在query中唯一 type， NUMERIC, STRING或者层次名称之一 defaultValue。 默认值，是一个表达式，应该跟type类型一致，如果是层次，就得是层次的某个成员 desc。 要是想在同一个query中使用某个参数多次，就使用：1ParamRef(&lt;name&gt;) 下面的查询会查出California的前10名品牌，我们可以修改Count参数为前5名，或者修改Region参数为西雅图：123456SELECT &#123;[Measures].[Unit Sales]&#125; on columns, TopCount([Product].[Brand].members, Parameter(&quot;Count&quot;, NUMERIC, 10, &quot;Number of products to show&quot;), (Parameter(&quot;Region&quot;, [Store], [Store].[USA].[CA]), [Measures].[Unit Sales])) on rowsFROM Sales 可以调用Query.getParameters()来获取某个查询的所有参数，然后调用Query.setParameter(String name, String value)修改参数值。 4.2 内置函数12StrToSet(&lt;String Expression&gt;[, &lt;Hierarchy&gt;])StrToTuple(&lt;String Expression&gt;[, &lt;Hierarchy&gt;])]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ember异常记录]]></title>
    <url>%2F2016%2F12%2F05%2Fember%E5%BC%82%E5%B8%B8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[ember-mermaid要画流程图，对比了一下，决定使用ember-mermaid，但是github里说的安装方式运行的话，会报错：123456789101112131415root@will-vm:/usr/local/metamap/metamap_js# ember install ember-mermaidDEPRECATION: Overriding init without calling this._super is deprecated. Please call this._super(), addon: `ember-mermaid` at Function.Addon.lookup (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addon.js:896:27)The addon `ember-cli-htmlbars` requires an Ember CLI version of 0.1.2 or above, but you are running null.Error: The addon `ember-cli-htmlbars` requires an Ember CLI version of 0.1.2 or above, but you are running null. at Function.deprecatedAssertAbove [as assertAbove] (/usr/local/metamap/metamap_js/node_modules/ember-cli-htmlbars/node_modules/ember-cli-version-checker/index.js:143:18) at CoreObject.module.exports.init (/usr/local/metamap/metamap_js/node_modules/ember-cli-htmlbars/ember-addon-main.js:13:13) at CoreObject.superWrapper [as init] (/usr/local/metamap/metamap_js/node_modules/ember-cli/node_modules/core-object/lib/assign-properties.js:32:18) at CoreObject.Class (/usr/local/metamap/metamap_js/node_modules/ember-cli/node_modules/core-object/core-object.js:33:38) at /usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addons-factory.js:48:21 at visit (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/utilities/DAG.js:23:3) at DAG.topsort (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/utilities/DAG.js:82:7) at CoreObject.extend.initializeAddons (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addons-factory.js:44:11) at CoreObject.extend.initializeAddons (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addon.js:226:38) at setupRegistryForEachAddon (/usr/local/metamap/metamap_js/node_modules/ember-cli/node_modules/ember-cli-preprocess-registry/preprocessors.js:18:10) 搜了一下没找到相关资料，索性直接暴力解决,修改报错文件里检查版本的部分/usr/local/metamap/metamap_js/node_modules/ember-cli-htmlbars/node_modules/ember-cli-version-checker/index.js:143:18。看到上面是验证版本的地方出了问题123456if (!dependencyChecker.satisfies(comparison)) &#123; var error = new Error(message); error.suppressStacktrace = true; throw error;&#125; 注释掉上面这些代码后成功安装。12345678910111213141516171819202122root@will-vm:/usr/local/metamap/metamap_js# ember install ember-mermaidDEPRECATION: Overriding init without calling this._super is deprecated. Please call this._super(), addon: `ember-mermaid` at Function.Addon.lookup (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addon.js:896:27)DEPRECATION: Node v0.10.45 is no longer supported by Ember CLI. Please update to a more recent version of NodeCould not start watchman; falling back to NodeWatcher for file system events.Visit http://ember-cli.com/user-guide/#watchman for more info.Installed packages for tooling via npm.DEPRECATION: Overriding init without calling this._super is deprecated. Please call this._super(), addon: `ember-mermaid` at Function.Addon.lookup (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addon.js:896:27)installing ember-mermaid install bower package mermaid not-cached https://github.com/knsv/mermaid.git#^0.5.8 progress 接收对象中: 26% (58/222) progress 接收对象中: 67% (149/222), 1.98 MiB | 166.00 KiB/s progress 接收对象中: 68% (151/222), 1.98 MiB | 166.00 KiB/s resolved https://github.com/knsv/mermaid.git#0.5.8 conflict Unable to find suitable version for mermaid 1) mermaid ^0.5.8 2) mermaid ^6.0.0? Answer 2Installed browser packages via Bower.Installed addon package. 既然成功安装了，为防有其他差错，就把这个检查版本的文件恢复回去了。然后启动ember server又出现了这个问题123456789101112131415root@will-vm:/usr/local/metamap/metamap_js# ember serveDEPRECATION: Overriding init without calling this._super is deprecated. Please call this._super(), addon: `ember-mermaid` at Function.Addon.lookup (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addon.js:896:27)The addon `ember-cli-htmlbars` requires an Ember CLI version of 0.1.2 or above, but you are running null.Error: The addon `ember-cli-htmlbars` requires an Ember CLI version of 0.1.2 or above, but you are running null. at Function.deprecatedAssertAbove [as assertAbove] (/usr/local/metamap/metamap_js/node_modules/ember-cli-htmlbars/node_modules/ember-cli-version-checker/index.js:143:18) at CoreObject.module.exports.init (/usr/local/metamap/metamap_js/node_modules/ember-cli-htmlbars/ember-addon-main.js:13:13) at CoreObject.superWrapper [as init] (/usr/local/metamap/metamap_js/node_modules/ember-cli/node_modules/core-object/lib/assign-properties.js:32:18) at CoreObject.Class (/usr/local/metamap/metamap_js/node_modules/ember-cli/node_modules/core-object/core-object.js:33:38) at /usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addons-factory.js:48:21 at visit (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/utilities/DAG.js:23:3) at DAG.topsort (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/utilities/DAG.js:82:7) at CoreObject.extend.initializeAddons (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addons-factory.js:44:11) at CoreObject.extend.initializeAddons (/usr/local/metamap/metamap_js/node_modules/ember-cli/lib/models/addon.js:226:38) at setupRegistryForEachAddon (/usr/local/metamap/metamap_js/node_modules/ember-cli/node_modules/ember-cli-preprocess-registry/preprocessors.js:18:10) 仔细看了一下官方github上给出的解释，说是要把ember-font-awesome升级到2.1.1就可以了。查了一下，压根就没有安装这个东西12root@will-vm:/usr/local/metamap/metamap_js# find . -name &apos;ember-font-awesome&apos;root@will-vm:/usr/local/metamap/metamap_js# 看一下ember-mermaid的官网吧，到ember addon仓库搜索了一把https://emberobserver.com/addons/ember-mermaid，发现人家的ember-cli的版本是2.4.1，最近支持的是2.5.0.群里很多人都说2.6不稳定，所以一直没有升级。。索性我就降级吧。1234npm rm -g ember-clinpm install -g ember-cli@2.5ember -vember new testPro 上面使用ember 2.5版本生成了一个临时项目，为了把依赖的版本copy到现有的2.6的项目里去，覆盖掉2.6的那些版本依赖。主要针对bower.json和package.json两个文件里与ember-xxx相关的版本配置。 之后就成功启动了，也能够使用mermaid了，再删掉临时项目就可以了。 参考 ： https://github.com/crodriguez1a/ember-mermaid https://knsv.github.io/mermaid/#usage http://www.jqueryscript.net/chart-graph/Simple-SVG-Flow-Chart-Plugin-with-jQuery-flowSVG.html https://jsplumbtoolkit.com/community/demo/flowchart/index.html https://www.erp5.com/javascript-10.Flow.Chart https://github.com/ember-cli/ember-cli/issues/5973]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[antlr简记]]></title>
    <url>%2F2016%2F12%2F05%2Fantlr%E7%AE%80%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[要素 source code：面向编程人员的语言代码； Lexical Analyser ：词汇解析。将source code解析成为某个语言能理解的有意义的形式； Syntax Analyser: 语法解析，检查语言的正确性。 Semantic Analyser: 语义解析； Generate/Improve Code: 生成机器能够读懂的执行代码。 参考：https://www.youtube.com/watch?v=PooQrbFrd_U 环境准备环境准备使用eclipse进行开发。 IDE环境准备：eclipse classic 3.5.0 eclipse 插件 GEF-ALL-3.8.2 dltk-core-S-3.0.1-201108261011 antlr3. http://antlrv3ide.sourceforge.net/updates 插件设置主要就是preferences里的antlr-&gt;builder里添加一个installed package，只需要一个jar包即可，但是规范是把这个jar包放到某个文件夹里。 我的地址是：E:\tools\antlr3.2。里面放置了antlr-3.2.jar文件。还可以在code generator配置一下代码产生的位置。 IDE简介grammer编辑界面有些自动提示啥的，保存后会自动编译，并在前面指定位置生成lexer和parser代码。 测试执行界面完成gramemr的编辑之后，可以编辑一些测试脚本，验证grammer的正确性。跟测试连接的真紧密啊，貌似antlr想不测试驱动开发都难，哈哈~注意 : 有的时候直接点击右上角的执行按钮，并不能成功解析。可以再试一下执行的下来按钮里的Run(java)的方式，它等同于执行以下代码：1234567891011121314151617181920212223242526272829303132333435```##### RailRoad 视图展示每个rule的路线图。点击左边的rule，右边就自动呈现出来了。可以形象化的展示我们定义的每个rule。![antlr RailRoad界面](C:\Users\will\Pictures\antlr-railroad-eclipse.png) #### 创建项目创建普通java项目，之后转为antlr项目，.project：```xml&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;projectDescription&gt; &lt;name&gt;anltr-learning&lt;/name&gt; &lt;comment&gt;&lt;/comment&gt; &lt;projects&gt; &lt;/projects&gt; &lt;buildSpec&gt; &lt;buildCommand&gt; &lt;name&gt;org.eclipse.dltk.core.scriptbuilder&lt;/name&gt; &lt;arguments&gt; &lt;/arguments&gt; &lt;/buildCommand&gt; &lt;buildCommand&gt; &lt;name&gt;org.eclipse.jdt.core.javabuilder&lt;/name&gt; &lt;arguments&gt; &lt;/arguments&gt; &lt;/buildCommand&gt; &lt;/buildSpec&gt; &lt;natures&gt; &lt;nature&gt;org.eclipse.jdt.core.javanature&lt;/nature&gt; &lt;nature&gt;org.deved.antlride.core.nature&lt;/nature&gt; &lt;/natures&gt;&lt;/projectDescription&gt; 在根目录创建一个lib文件夹，然后把刚才那个install package里的jar包copy到lib里来，加入当前project的build path。 概念逻辑流程图物理流程图 Lexer解析出重要的Token发送给Parser Parser生成AST Tree， TreeParser负责结合AST Tree里的context生成代码 创建antlr文件创建Sample.g文件，开始真正编辑antlr文件。经测试，.g文件并不支持中文注释，会报某种NullPointer的错误。下面的注释只为解释，如果copy的话，自行注意去除。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107grammar Sample;// 类似类名似的，必须跟文件同名options &#123; language = Java;&#125;@header &#123; package com.will;&#125;@lexer::header &#123; package com.will;&#125;// 以下这些在antlr里都是rule了。就是定义一些语法规则。// IDENT其实也可以看成rule，只是只包含常量的rule而已。// 这个program是最外层的rule了，其实也可以叫pppp，随便。program : &apos;program&apos; IDENT &apos;=&apos; (constrant | variable)* &apos;begin&apos; statement* &apos;end&apos; IDENT &apos;.&apos; ;constrant : &apos;constrant&apos; IDENT &apos;:&apos; type &apos;:=&apos; Integer &apos;;&apos; ; type : &apos;Integer&apos; ; variable : &apos;var&apos; IDENT (&apos;,&apos; IDENT)* &apos;:&apos; type &apos;;&apos; ; // fun time// 运算符优先级设计实现，低优先级expression调用高优先级的expressionterm : Integer | &apos;(&apos; expression &apos;)&apos; | IDENT ;negation : &apos;not&apos;* term ; unary : (&apos;+&apos; |&apos;-&apos;)* negation ;mult : unary ((&apos;*&apos; | &apos;/&apos; | &apos;mode&apos;) unary)* ; add : mult ((&apos;+&apos; | &apos;-&apos;) mult)* ; relation : add ((&apos;=&apos; | &apos;/=&apos; | &apos;&lt;&apos; | &apos;&lt;=&apos; | &apos;&gt;&apos; | &apos;&gt;=&apos;) add)* ;expression : relation ((&apos;and&apos; | &apos;or&apos;) relation)* ;statement : assignmentStatement | ifStatement | loopStatement ;exitStatement : &apos;exit&apos; &apos;when&apos; expression ;assignmentStatement : IDENT &apos;:=&apos; expression &apos;;&apos; ; ifStatement : &apos;if&apos; expression &apos;then&apos; statement+ (&apos;elif&apos; expression &apos;then&apos; statement+)* (&apos;else&apos; statement+)? &apos;end&apos; &apos;if&apos; &apos;;&apos; ; loopStatement : &apos;loop&apos; statement* exitStatement &apos;end&apos; &apos;loop&apos; &apos;;&apos; ; // 静态常量ruleInteger: &apos;0&apos;..&apos;9&apos;+;IDENT : (&apos;a&apos;..&apos;z&apos; | &apos;A&apos;..&apos;Z&apos; | &apos;0&apos;..&apos;9&apos;)+;WS:(&apos; &apos; | &apos;\t&apos; |&apos;\n&apos;|&apos;\r&apos;|&apos;\f&apos;)+ &#123;$channel = HIDDEN;&#125;;// commentCOMMENT: &apos;//&apos; .* (&apos;\n&apos; | &apos;\r&apos;) &#123;$channel = HIDDEN;&#125;;MUTICOMMENT: &apos;/*&apos; .* &apos;*/&apos; &#123;$channel = HIDDEN;&#125;; 上面的文件中有个要提示的地方，就是优先级设置：直接copy的人家的图，大概就是从上到下优先级递减。在grammer中的实现就是低优先级的expression调用高优先级的expression，这样在调用某个expression的时候，就先执行位于叶子节点的最高优先级的expression了。 对应的测试代码：1234567891011121314151617181920212223242526272829program XLSample1 = constrant one:Integer :=1;constrant two:Integer :=2;var x, y, z:Integer;beginx := 30 + 2 * 9 -10;y := 10; if x=2 then y:= 9 * 7 -1; if 9 &gt; d then x:= 90; else x:=0; end if; elif x&gt;9 and 8 &lt; 2 then// x:=90; y:=100; else x:= 99; z:=100; y:=100; end if;loop x:=0; y:=99; exit when x =0end loop;end XLSample1. 虽然就这么几个简单的句子，但是跑出来的AST简直是太大了，就不截图了，感兴趣自己运行了看吧。 小结动态的rule使用小写，静态常量rule就只用大写字母。 表达式基本跟正则差不太多。表达式 | 解释—|—(a | b)| 就是a和b可以以任意次序出现，不严格限定a先于b,或者b先于a。(a | b) | 两者都可以有0或多个(a | b)+ | 两者至少有一个，多个的话，次序可乱| | 或者‘0’..’9’ | 从’0’至’9’，字母也一样，生成的java代码都是直接使用&gt; &lt;啥的限定范围的‘not’? term | ‘not’可有可无，但是有的话只能有一个，多个就+或者{$channel = HIDDEN;} | 设置当前元素在AST中不可见。应该有待挖掘，可能更多个性化的东西，都可以在这里处理]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[yarn-timeline-server]]></title>
    <url>%2F2016%2F12%2F05%2Fyarn-timeline-server%2F</url>
    <content type="text"><![CDATA[timeline server主要是为yarn管理应用的当前运行状态以及历史状态的，以前也叫history server。 负责管理以下两个事情： 已运行完成的应用基本信息这些信息包括ApplicationSubmissionContext中的队列名称、用户信息等。启动一个应用经历过的尝试次数，每次尝试的信息，每次尝试启用的container都有哪些，每个container的基本信息。这些基本信息被ResourceManager春出道了一个history-store(默认是一个文件系统)然后提供给web-UI来展示这些已完成的应用的这些基本信息。 正在运行或者已完成的应用的Per-framework informationPer-framework information是指针一个框架或者应用特有的信息。例如，Hadoop mapreduce框架会包含一些类似map task数量、reduce task数量、counter等信息。开发者可以自己通过TimeLineClient定制这些信息。这些信息可以通过rest api查询，也可以直接提供给某些特定UI进行渲染。 1. 当前状态Timeline sever 还正在开发中.基本信息和框架特定信息已经可以满足了基本的存储于查询，但是目前timeline server还不能在secure模式下正常工作。基本信息与框架指定信息是分别进行收集与呈现的，并没有很好的整合在一起。框架指定信息目前只能通过restful api访问，使用json形式，目前还不支持在yarn里安装框架。 2. 基本配置Basic Configuration用户必须先配置才能启动，下面是一个简单的示例，在yarn-site.xml里设置timeline server的主机名: 12345&lt;property&gt; &lt;description&gt;The hostname of the Timeline service web application.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.hostname&lt;/name&gt; &lt;value&gt;0.0.0.0&lt;/value&gt;&lt;/property&gt; 3. 高级配置In addition to the hostname, admins can also configure whether the service is enabled or not, the ports of the RPC and the web interfaces, and the number of RPC handler threads.1234567891011121314151617181920212223&lt;property&gt; &lt;description&gt;Address for the Timeline server to start the RPC server.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.address&lt;/name&gt; &lt;value&gt;$&#123;yarn.timeline-service.hostname&#125;:10200&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;The http address of the Timeline service web application.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.webapp.address&lt;/name&gt; &lt;value&gt;$&#123;yarn.timeline-service.hostname&#125;:8188&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;The https address of the Timeline service web application.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.webapp.https.address&lt;/name&gt; &lt;value&gt;$&#123;yarn.timeline-service.hostname&#125;:8190&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;Handler thread count to serve the client RPC requests.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.handler-thread-count&lt;/name&gt; &lt;value&gt;10&lt;/value&gt;&lt;/property&gt; 4. 基本信息相关的配置Users can specify whether the generic data collection is enabled or not, and also choose the storage-implementation class for the generic data. There are more configurations related to generic data collection, and users can refer to yarn-default.xml for all of them. 12345678910111213141516&lt;property&gt; &lt;description&gt;Indicate to ResourceManager as well as clients whether history-service is enabled or not. If enabled, ResourceManager starts recording historical data that Timelien service can consume. Similarly, clients can redirect to the history service when applications finish if this is enabled.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.generic-application-history.enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;Store class name for history store, defaulting to file system store&lt;/description&gt; &lt;name&gt;yarn.timeline-service.generic-application-history.store-class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore&lt;/value&gt;&lt;/property&gt; 5. 框架特有信息的相关配置Users can specify whether per-framework data service is enabled or not, choose the store implementation for the per-framework data, and tune the retention of the per-framework data. There are more configurations related to per-framework data service, and users can refer to yarn-default.xml for all of them. 12345678910111213141516171819202122232425&lt;property&gt; &lt;description&gt;Indicate to clients whether Timeline service is enabled or not. If enabled, the TimelineClient library used by end-users will post entities and events to the Timeline server.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;Store class name for timeline store.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.store-class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;Enable age off of timeline store data.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.ttl-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;description&gt;Time to live for timeline store data in milliseconds.&lt;/description&gt; &lt;name&gt;yarn.timeline-service.ttl-ms&lt;/name&gt; &lt;value&gt;604800000&lt;/value&gt;&lt;/property&gt; 6. 启动timeline serverAssuming all the aforementioned configurations are set properly, admins can start the Timeline server/history service with the following command:1$ yarn historyserver Or users can start the Timeline server / history service as a daemon:1$ yarn-daemon.sh start historyserver Accessing generic-data via command-line Users can access applications’ generic historic data via the command line as below. Note that the same commands are usable to obtain the corresponding information about running applications.12345$ yarn application -status &lt;Application ID&gt;$ yarn applicationattempt -list &lt;Application ID&gt;$ yarn applicationattempt -status &lt;Application Attempt ID&gt;$ yarn container -list &lt;Application Attempt ID&gt;$ yarn container -status &lt;Container ID&gt; Publishing of per-framework data by applications Developers can define what information they want to record for their applications by composing TimelineEntity and TimelineEvent objects, and put the entities and events to the Timeline server via TimelineClient. Below is an example: 1234567891011121314151617// Create and start the Timeline clientTimelineClient client = TimelineClient.createTimelineClient();client.init(conf);client.start();TimelineEntity entity = null;// Compose the entitytry &#123; TimelinePutResponse response = client.putEntities(entity);&#125; catch (IOException e) &#123; // Handle the exception&#125; catch (YarnException e) &#123; // Handle the exception&#125;// Stop the Timeline clientclient.stop();]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JPARepository札记]]></title>
    <url>%2F2016%2F12%2F05%2FJPARepository%E6%9C%AD%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[join时报错报错1QuerySyntaxException: expecting IDENT, found &apos;*&apos; near line 1 原始sql1hql:select a.* from Archive a,Employee e,Department d where a.contractEndDate = :date and a.status = 'A' and a.empId = e.id and e.status = 'A' and e.type = 'A' 改成1select a from Archive a,Employee e,Department d where a.contractEndDate = :date and a.status = 'A' and a.empId = e.id and e.status = 'A' and e.type = 'A' select 后面为对象 a .而非 a.* 参考：http://blog.sina.com.cn/s/blog_4bf2e5550100n2gh.html SQL面向Entity指的是在JPARepository的子类中自定义的函数123456789101112131415161718192021222324252627public interface TblBloodRepository extends JpaRepository&lt;TblBlood, Integer&gt;&#123; TblBlood findByTblName(String tblName); @Query("select a from " + "(select blood from TblBlood blood where valid = 1) a" + " left outer join " + "(select distinct parentTbl from TblBlood where valid = 1) b" + " on a.tblName = b.parentTbl" + " where b.parentTbl is null") List&lt;TblBlood&gt; selectAllLeaf(); @Modifying @Query("update TblBlood set valid = 0 where tblName=:tblName") void makePreviousInvalid(@Param("tblName")String tblName); @Query("select b from" + " TblBlood a join TblBlood b" + " on a.parentTbl = b.tblName and b.valid = 1" + " where a.valid = 1 and a.tblName = :tblName") List&lt;TblBlood&gt; selectParentByTblName(@Param("tblName")String tblName); @Query("from TblBlood where valid = 1 and parentTbl=:tblName") public List&lt;TblBlood&gt; selectByParentTblName(@Param("tblName")String tblName); @Query("from TblBlood where valid = 1 and tblName=:tblName") List&lt;TblBlood&gt; selectByTblName(@Param("tblName")String tblName);&#125; TblBlood就是对象的类名，而不是真正的表名。 自join12345@Query("select b from" + " TblBlood a join TblBlood b" + " on a.parentTbl = b.tblName and b.valid = 1" + " where a.valid = 1 and a.tblName = ?1") List&lt;TblBlood&gt; selectParentByTblName(String tblName); 报错123456789101112131415161718 Path expected for join! at org.hibernate.hql.internal.ast.HqlSqlWalker.createFromJoinElement(HqlSqlWalker.java:378) at org.hibernate.hql.internal.antlr.HqlSqlBaseWalker.joinElement(HqlSqlBaseWalker.java:3858) at org.hibernate.hql.internal.antlr.HqlSqlBaseWalker.fromElement(HqlSqlBaseWalker.java:3644) at org.hibernate.hql.internal.antlr.HqlSqlBaseWalker.fromElementList(HqlSqlBaseWalker.java:3522) org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;tblBloodRepository&apos;: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Validation failed for query for method public abstract java.util.List com.will.hivesolver.repositories.TblBloodRepository.selectParentByTblName(java.lang.String)! at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ...Caused by: java.lang.IllegalArgumentException: Validation failed for query for method public abstract java.util.List com.will.hivesolver.repositories.TblBloodRepository.selectParentByTblName(java.lang.String)! at org.springframework.data.jpa.repository.query.SimpleJpaQuery.validateQuery(SimpleJpaQuery.java:84) .....Caused by: java.lang.IllegalStateException: No data type for node: org.hibernate.hql.internal.ast.tree.IdentNode \-[IDENT] IdentNode: &apos;b&apos; &#123;originalText=b&#125; at org.hibernate.hql.internal.ast.tree.SelectClause.initializeExplicitSelectClause(SelectClause.java:174) at org.hibernate.hql.internal.ast.HqlSqlWalker.useSelectClause(HqlSqlWalker.java:923) ... 不能更新12严重: Servlet.service() for servlet [api] in context with path [/metamap] threw exception [Request processing failed; nested exception is org.springframework.dao.InvalidDataAccessApiUsageException: Executing an update/delete query; nested exception is javax.persistence.TransactionRequiredException: Executing an update/delete query] with root causejavax.persistence.TransactionRequiredException: Executing an update/delete query Spring Data JPA，事务导致的异常 需要在springMVC的xml里添加,这里只扫描Controller所在的位置里的Controller1234&lt;context:component-scan base-package="com.will.hivesolver.controller" use-default-filters="false"&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;context:include-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice"/&gt; &lt;/context:component-scan&gt; 在applicationContext.xml中添加，这里必须不能扫描上面扫描过的Controller：1234&lt;context:component-scan base-package="com.will.hivesolver"&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice"/&gt; &lt;/context:component-scan&gt; 参考：https://github.com/springside/springside4/wiki/Spring-MVC No property jobId found for type Execution!惨案Could not determine type for at table 紧接着Could not determine type for: java.util.Set 参考： http://stackoverflow.com/questions/15845030/no-property-u-found-for-type-com-models-entities-orderentity http://stackoverflow.com/questions/15849278/org-hibernate-mappingexception-could-not-determine-type-for-java-util-set-at failed to lazily initialize a collection of role: , could not initialize proxy - no Session著名的延迟加载问题。原因是One加载完之后，session就已经关闭了，此时再去请求many，就么有session了。最终采用的方案是：在web.xml里添加spring的OpenSessionInViewFilter。 1234567891011121314&lt;filter&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;filter-class&gt; org.springframework.orm.hibernate3.support.OpenSessionInViewFilter &lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;sessionFactoryBeanName&lt;/param-name&gt; &lt;param-value&gt;sessionfactory&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;OpenSessionInViewFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; No persistence units parsed from {classpath*:META-INF/persistence.xml 参考： http://blog.csdn.net/elfenliedef/article/details/6011892 http://lazycat0102.blog.51cto.com/8938702/1584779 http://www.cnblogs.com/luxh/archive/2012/05/24/2516282.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive中的权限-SQL-Standard-Based-Hive-Authorization]]></title>
    <url>%2F2016%2F12%2F05%2Fhive%E4%B8%AD%E7%9A%84%E6%9D%83%E9%99%90-SQL-Standard-Based-Hive-Authorization%2F</url>
    <content type="text"><![CDATA[hive 0.13之前默认hive的权限机制并不是为了防止一些恶意用户访问一些他们不该看的东西的，只是为了防止用户误操作。这套机制很不完善，甚至对于grant这种语句也不会做权限检查。权限检查是在hive query的编译阶段进行的。由于用户能够操作dfs、执行自定义函数、还能操作shell，跳过客户端的安全检查也不是不可能的。 hive还支持存储层的权限验证，主要用来给metastore server api添加权限验证（参考 Storage Based Authorization in the Metastore Server）。0.12版本之后，在client端也可以使用了。这样metastore就被保护起来了，但是对于细粒度访问权限还是控制不了（例如行、列）。 一般通过创建view来解决这个问题，只让用户访问到view层。 0.13之后基于sql标准的检查机制,推荐使用此种方式。这种认证方式可以在metastore server上结合storage based authorization一起使用。和目前目前hive的默认认证机制一样也算在hive query的编译阶段进行验证的。为了保证安全性，我们需要确认client端是安全的，让用户通过hiveserver2来访问数据，约束用户代码和非法sql命令。检查是针对提交query给hive server2的用户的，但是实际执行查询的时候使用hive server2用户执行，目录和文件也都只对hive server2的用户开放。对于不需要进行检查约束的用户，则可以直接开放给他们hive命令行执行权限，也就是查询的时候就使用提交的那个用户。 这项工作是尽量像SQL标准看齐的，不过也有一些有差异的地方。出发点一般都是为了让既有用户能够平滑的迁移到认证机制，或者考虑到易用性。 在这种认证机制下，所有可以访问到hive命令行、HDFS命令行、pig命令行、Hadoop jar等的用户都被认为是特权用户。团队中只有做ETL工作的人才能拥有这些权利。这些工具都不通过hive server访问数据，也就是说他们不会被检查认证情况。对于通过hive 客户端、pig、MR等访问hive table的用户可以控制他们启用metastore server的storage based authorization。 类似数据分析等使用sql或者jdbc通过hive server2访问数据的用户可以使用此套认证机制。 hive 命令和语句的限制启用此认证机制后 dfs\add\delete\compile\reset等命令都不可用。 hive运行时的set的配置参数只有少数能够被设置， hive.security.authorization.sqlstd.confwhitelist 控制这个东西，这里面是可以配置的参数的白名单。 只有admin角色才能添加和删除function和macro。admin角色需要为普通用户添加permanent function，这样其他用户才能使用自定义的一些function。 transform clause不可用 权限 select insert update delete all 对象 table和view。不支持database database所有权只对应部分动作 hive里还有特殊的对象URI。上面的权限对URI不适用，URI是指向系统的某个文件的。认证机制是通过用户的文件权限进行认证的。 对象所有权针对每个操作，都会检查当前用户对于当前对象是否有权限。创建的人就是owner。对于table和view来说owner有所有权限。某个role也可以是一个database的owner，通过alter database来修改database的owner。 user 和 role权限可以给user，也可以给role。一个user可以有一个或多个role。两个特殊的role：public以及admin。所有的user都是public role。You use this role in your grant statement to grant a privilege to all users. 当一个user运行hive命令的时候，就会检查这个用户的role以及它目前拥有的所有权限。使用”show current roles;”命令来查看当前用户的role。除了admin其他用户都可以在current role里查到，而且可以使用”set role xxx”命令来给当前用户设置一个role。 database的管理者应该使用admin role。这些用户可以执行类似”create role””drop role”等命令，也可以访问一切资源。但是，这些用户在成为admin role之前，必须先执行”set role admin”，因为admin默认是不再current roles里的。 role相关命令123456789101112131415161718192021222324252627282930313233-- 只有admin角色才能执行的命令CREATE ROLE role_name;DROP ROLE role_name;SHOW ROLES;-- Show Principals 查看role的权限SHOW PRINCIPALS role_name;SHOW CURRENT ROLES;SET ROLE (role_name|ALL|NONE);--All 是当有新的role被赋予给当前user的时候，刷新一下current roles--None 移除所有的current roles-- Grant roleGRANT role_name [, role_name] ...TO principal_specification [, principal_specification] ...[ WITH ADMIN OPTION ]; principal_specification : USER user | ROLE role-- 如果指定了'with admin option',那么这个用户就也能将这个role赋予其他user/role。如果出现role之间grant循环，就会直接报错。 -- Revoke RoleREVOKE [ADMIN OPTION FOR] role_name [, role_name] ...FROM principal_specification [, principal_specification] ... ; principal_specification : USER user | ROLE role-- Show Role Grant 只能查看自己的role的权限SHOW ROLE GRANT (USER|ROLE) principal_name; 管理对象权限1234567891011121314151617181920212223-- 授予权限GRANT priv_type [, priv_type ] ... ON table_or_view_name TO principal_specification [, principal_specification] ... [WITH GRANT OPTION]; -- 收回权限REVOKE [GRANT OPTION FOR] priv_type [, priv_type ] ... ON table_or_view_name FROM principal_specification [, principal_specification] ... ; principal_specification : USER user | ROLE role priv_type : INSERT | SELECT | UPDATE | DELETE | ALL -- Show GrantSHOW GRANT [principal_name] ON (ALL| ([TABLE] table_or_view_name)show grant user hive on all 注意revoke语句不能drop任何有依赖的权限。详见Postgres revoke documentation. 参考 SQL Standards Based Authorization in HiveServer2 https://cwiki.apache.org/confluence/display/Hive/SQL+Standard+Based+Hive+Authorization https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization#LanguageManualAuthorization-2SQLStandardsBasedAuthorizationinHiveServer2]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari-hive-storage-based-authorization]]></title>
    <url>%2F2016%2F12%2F05%2Fambari-hive-storage-based-authorization%2F</url>
    <content type="text"><![CDATA[概要hive使用hdfs的文件系统权限来约束用户对database/table/partition的访问权限，可以让用户属于多个group，不同权限的数据属于不同group就可以随意组合用户的权限了。 ambari配置参数123456789101112131415161718192021222324252627&lt;property&gt; &lt;name&gt;hive.security.metastore.authorization.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.authorization.DefaultHiveMetastoreAuthorizationProvider&lt;/value&gt; &lt;description&gt;authorization manager class name to be used in the metastore for authorization. The user defined authorization class should implement interface org.apache.hadoop.hive.ql.security.authorization.HiveMetastoreAuthorizationProvider. &lt;/description&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.security.metastore.authenticator.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.security.HadoopDefaultMetastoreAuthenticator&lt;/value&gt; &lt;description&gt;authenticator manager class name to be used in the metastore for authentication. The user defined authenticator should implement interface org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.metastore.pre.event.listeners&lt;/name&gt; &lt;value&gt; &lt;/value&gt; &lt;description&gt;pre-event listener classes to be loaded on the metastore side to run code whenever databases, tables, and partitions are created, altered, or dropped. Set to org.apache.hadoop.hive.ql.security.authorization.AuthorizationPreEventListener if metastore-side authorization is desired. &lt;/description&gt;&lt;/property&gt; 测试时间12345678910hive&gt; create database willtest;hive&gt; CREATE TABLE `batting`( &gt; `id` int, &gt; `dtdontquery` string, &gt; `name` string) &gt; PARTITIONED BY ( &gt; `dt` string); hive&gt; insert into willtest.batting partition(dt=&apos;20150409&apos;) values(12, &apos;will test string&apos;, &apos;will&apos;); 创建测试库willtest 创建测试表willtest.batting 插入一条测试数据，使用ambari用户maming到hive view查询此表，正常。 观察一下hdfs上的文件.12345[hdfs@data-test02 root]$ hdfs dfs -ls /apps/hive/warehousedrwxrwxrwx - hive hdfs 0 2016-05-27 15:22 /apps/hive/warehouse/willtest.db[hdfs@data-test02 root]$ hdfs dfs -ls /apps/hive/warehouse/willtest.dbdrwxrwxrwx - hive hdfs 0 2016-05-27 15:23 /apps/hive/warehouse/willtest.db/batting 现在这个库和表的权限都是谁都可以随便操作的，而且owner是hive，group是hdfs。 修改batting表的权限，期望我们的ambari用户maming不能查询；123[hdfs@data-test02 root]$ hdfs dfs -chmod -R 770 /apps/hive/warehouse/willtest.db/batting[hdfs@data-test02 root]$ hdfs dfs -ls /apps/hive/warehouse/willtest.dbdrwxrwx--- - hive hdfs 0 2016-05-27 15:23 /apps/hive/warehouse/willtest.db/batting 先把group和all的权限约束一下，这里因为maming本来就不是hdfs group的，所以就暂时不用修改group了。 再使用ambari用户amming查询willtest.batting，报错： 12 org.apache.ambari.view.hive.client.HiveErrorStatusException: H110 Unable to submit statement. Error while compiling statement: FAILED: SemanticException Unable to fetch table batting. java.security.AccessControlException: Permission denied: user=maming, access=READ, inode=&quot;/apps/hive/warehouse/willtest.db/batting&quot;:hive:hdfs:drwxrwx---... 补充一下maming等几个用户的权限 123456[hdfs@data-test02 root]$ hdfs groups mamingmaming : maming[hdfs@data-test02 root]$ hdfs groups sqoopsqoop : hadoop[hdfs@data-test02 root]$ hdfs groups hdfshdfs : hadoop hdfs 将maming暂时加入hdfs这个group之后再试一下，成功 12 测试成功。 延伸好，基本需求能够实现。那么如果我们为某个group分配了某个db的权限。如果我们新增一个表batting2呢，再看一下文件属性123[hdfs@data-test02 root]$ hdfs dfs -ls /apps/hive/warehouse/willtest.dbdrwxrwx--- - hive hdfs 0 2016-05-27 15:23 /apps/hive/warehouse/willtest.db/battingdrwxrwxrwx - hive hdfs 0 2016-05-27 15:51 /apps/hive/warehouse/willtest.db/batting2 问题出现，我们需要修改创建数据仓库文件的默认权限才行，不然我们就需要每次增加新库都得改一下table文件权限。 找到两个相关参数|参数 |含义 ||—|—|| hive.warehouse.subdir.inherit.perms |true/false 是否 继承上级文件夹的umask || hive.files.umask.value | 0770 解释说是被上面那个参数替换了 |那就设置hive.warehouse.subdir.inherit.perms为true，重启一下hive，再去创建表看一下文件夹的权限。12345678910[hdfs@data-test02 root]$ hdfs dfs -chmod -R 770 /apps/hive/warehouse/[hdfs@data-test02 root]$ hdfs dfs -ls /apps/hive/warehouse/Found 2 itemsdrwxrwx--- - hive hdfs 0 2016-05-27 15:10 /apps/hive/warehouse/battingdrwxrwx--- - hive hdfs 0 2016-05-27 16:26 /apps/hive/warehouse/willtest.db[hdfs@data-test02 root]$ hdfs dfs -ls /apps/hive/warehouse/willtest.dbFound 4 itemsdrwxrwx--- - hive hdfs 0 2016-05-27 15:23 /apps/hive/warehouse/willtest.db/battingdrwxrwx--- - hive hdfs 0 2016-05-27 15:51 /apps/hive/warehouse/willtest.db/batting2drwxrwx--- - hive hdfs 0 2016-05-27 16:27 /apps/hive/warehouse/willtest.db/batting4 先把willtest.db的文件夹权限改成了770，然后在willtest库里创建了一个新的表batting4，可以看到hdfs里的权限已经跟父目录保持一致了。这样我们给指定了某个db的文件夹权限之后，下面的table的新增与变化，权限都会稍微固定一些。 参考https://cwiki.apache.org/confluence/display/Hive/Storage+Based+Authorization+in+the+Metastore+Serverhttps://cwiki.apache.org/confluence/display/Hive/HCatalog+Authorization]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django模板语言]]></title>
    <url>%2F2016%2F12%2F05%2Fdjango%E6%A8%A1%E6%9D%BF%E8%AF%AD%E8%A8%80%2F</url>
    <content type="text"><![CDATA[本文从技术角度解释一下Django template系统，看一下它是怎样运行的，以及怎样继承它。 希望你已经看了前面的文章。 概览在python中使用template系统包含三个步骤： 配置一个Engine； 把template代码编译成Template； 在Context中渲染这个Template Django项目一般都是使用高级API来实现上面的步骤。 对于TEMPLATES配置的每个 DjangoTemplates，Django都实例化一个Engine. DjangoTemplates包装了一个Engine，使它能够适配后台的通用模板API。 django.template.loader moduel有一个get_template()方法用来加载template。返回一个包装了django.template.Template的django.template.backends.django.Template. 上一步骤中的Template有一个render()方法，它又包含一个context，有时也包含一个针对Context的request，负责在指定Template进行渲染。 配置engineclass Engine(dirs=None, app_dirs=False, allowed_include_roots=None, context_processors=None, debug=False, loaders=None, string_if_invalid=’’, file_charset=’utf-8’, libraries=None, builtins=None)source dirs template文件所在的文件夹们，默认是空 loaders 多个template loader类。详见 app_dirs 只在loaders里配置了’django.template.loaders.app_directories.Loader’ 才有效果 libraries 加载Template创建Template的推荐方式是调用Engine的get_template(), select_template()或者from_string(). 在django项目里是只能配置一个Template的，但我们也可以脱离配置直接使用Template。123from django.template import Templatetemplate = Template("My name is &#123;&#123; my_name &#125;&#125;.") 渲染context编译好Template之后，就可以用它来渲染任意的context了。12345678910&gt;&gt;&gt; from django.template import Context, Template&gt;&gt;&gt; template = Template("My name is &#123;&#123; my_name &#125;&#125;.")&gt;&gt;&gt; context = Context(&#123;"my_name": "Adrian"&#125;)&gt;&gt;&gt; template.render(context)"My name is Adrian."&gt;&gt;&gt; context = Context(&#123;"my_name": "Dolores"&#125;)&gt;&gt;&gt; template.render(context)"My name is Dolores." 调用Template的render方法来渲染context。 变量变量只能是字母、数字、下划线、点。点有个特殊的含义，叫做lookup。每次遇到点的时候，template系统就会按照以下此次序进行lookup： 字典lookup。例子：foo[‘sdf’] 属性lookup。例子：foo.bar 数组索引lookup。例子：frr[sdf]1234567891011121314151617&gt;&gt;&gt; from django.template import Context, Template&gt;&gt;&gt; t = Template("My name is &#123;&#123; person.first_name &#125;&#125;.")&gt;&gt;&gt; d = &#123;"person": &#123;"first_name": "Joe", "last_name": "Johnson"&#125;&#125;&gt;&gt;&gt; t.render(Context(d))"My name is Joe."&gt;&gt;&gt; class PersonClass: pass&gt;&gt;&gt; p = PersonClass()&gt;&gt;&gt; p.first_name = "Ron"&gt;&gt;&gt; p.last_name = "Nasty"&gt;&gt;&gt; t.render(Context(&#123;"person": p&#125;))"My name is Ron."&gt;&gt;&gt; t = Template("The first stooge in the list is &#123;&#123; stooges.0 &#125;&#125;.")&gt;&gt;&gt; c = Context(&#123;"stooges": ["Larry", "Curly", "Moe"]&#125;)&gt;&gt;&gt; t.render(c)"The first stooge in the list is Larry." 当某个变量可以被调用的时候，都会被尝试调用。例如123456&gt;&gt;&gt; class PersonClass2:... def name(self):... return "Samantha"&gt;&gt;&gt; t = Template("My name is &#123;&#123; person.name &#125;&#125;.")&gt;&gt;&gt; t.render(Context(&#123;"person": PersonClass2&#125;))"My name is Samantha." 可以被调用的变量比直接查找lookup的变量稍微复杂点儿。记住下面几条 如果返回异常的话，会被传递上去，除非设置了silent_variable_failure为True，设置这个属性的话，当异常发生时，就会使用engine配置的string_if_invalid选项； 123456789101112131415161718&gt;&gt;&gt; t = Template("My name is &#123;&#123; person.first_name &#125;&#125;.")&gt;&gt;&gt; class PersonClass3:... def first_name(self):... raise AssertionError("foo")&gt;&gt;&gt; p = PersonClass3()&gt;&gt;&gt; t.render(Context(&#123;"person": p&#125;))Traceback (most recent call last):...AssertionError: foo&gt;&gt;&gt; class SilentAssertionError(Exception):... silent_variable_failure = True&gt;&gt;&gt; class PersonClass4:... def first_name(self):... raise SilentAssertionError&gt;&gt;&gt; p = PersonClass4()&gt;&gt;&gt; t.render(Context(&#123;"person": p&#125;))"My name is ." 只能调用无参函数 显然如果前端能调用的话，就可能发生side effect，所以要确保措施。比如 要解决这个问题，就需要在敏感方法上加上alters_data属性，django默认动态生成的model对象都自动设置了这个属性123def sensitive_function(self): self.database_record.delete()sensitive_function.alters_data = True 有时我们想关闭这个属性，那么久告诉template系统不要调用。在这个属性上设置do_not_call_in_templates为True。跟上面的区别是，上面会直接返回string_if_invalid，而这个还是可以访问，但是不能调用，也就是当变量而不是当方法使用。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nodejs学习笔记]]></title>
    <url>%2F2016%2F12%2F05%2Fnodejs%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[v8的出现，还有commonjs的规范化使得nodejs提供后台服务成为可能。 两种启动方式： node app.js 不自动更新 supervisor app.js 自动更新，修改后不用重启 特点 默认全部都是异步操作 123456789var fs = require('fs');fs.readFile('test.file', 'utf-8', function(err, data)&#123; if (err) &#123; console.log(err); &#125; else &#123; console.log(data); &#125;&#125;);console.log('end'); // 这一行会先被打印出来 全部基于事件处理机制，nodejs的所有机制都是围绕事件回调机制完成的。事件的回调函数在执行的过程中，可能会发出IO请求或者emit事件，执行完毕后再返回事件循环 1234567891011121314// 声明事件对象var EventEmitter=require('events').EventEmiiter;var event = new EventEmiiter();//注册事件event.on('some_event', function()&#123; console.log("this is a custom event");&#125;);// 3秒钟后触发事件setTimeout(function()&#123; event.emit('some_event');&#125;, 3000); module和packagenodejs的module可以是js、json或者c++扩展。1var http = require('http'); // c++的module exports是模块对外开放的接口12345678var name;exports.setName = function(n)&#123; name = n;&#125;exports.say = function()&#123; console.log('name is : ' + name);&#125; require是外部引用模块的接口, 只会引入一次12345var modu1=require('./modu');modu1.setName('will');var modu2=require('./modu');modu2.setName('william');modu2.say(); // 会输出william 如果我们要每次都弄一个新的对象呢??1234567891011function hello() &#123; var name; this.setName = function(n)&#123; name = n; &#125; this.say = function()&#123; console.log('name is : ' + name); &#125;&#125;module.exports = hello; 对应的引入为：1234567var hello=require('./modu');var modu1 = new hello();modu1.setName('will');var modu2 = new hello();modu2.setName('william');modu1.say(); // 会输出willmodu2.say(); // 会输出william pacakgecommonJS规范后，可以发布在npm上 package.json在根目录下，没有的话就去找index.js 二进制文件在bin目录下 js代码在lib目录下 测试在test目录 …. 1234567// package.json&#123; "main" : "./lib/index.js", // 入口 "name" : "名", "repositories": "仓库托管地址数据，每个元素要包含type、url、path", "dependencies" : "依赖"&#125; 包管理器和代码调试包管理器 命令 介绍 npm init 初始化一个package目录，会自动生成规范目录与package.json npm install xxx 本地模式安装xxx，将package安装到node_modules目录下 npm list 查看已经安装的包 npm help 查看json的使用 npm publish 进入到package下执行，发布package到公共库（最好是通过npm init生成的目录规范） npm unpublish 取消发布 全局安装的包，不能直接require，不会搜索/usr/local/lib/node_modules。只有需要命令行使用的时候才安装全局模式 代码调试 node –debug-brk=5858 app.js, 会等待debug连上来 eclipse插件V8 vm里弄一个debug就行了 全局对象和全局变量global是所有全局变量的宿主。 consolelog\error\trace process描述当前进程状态，提供了与os交互的接口。|变量|描述||—|—||process.argv|命令行参数，第一个是node，第二是脚本名称，第三个才是参数||process.stdout|标准输出||process.stdin|标准输入，需要先process.stdin.resume()||process.nextTick(callback)|为事件循环设置一个任务|nodejs适合io密集型的应用，而不是计算密集型的应用。 12345678function doSth(args, callback)&#123; sthComplext(args); callback();&#125;;doSth('123', function onEnd()&#123; compute();&#125;); 有了process.nextTick后可以写成：1234function doSth(args, callback)&#123; sthComplext(args); process.nextTick(callback); // 把耗时的两个程序拆成两个事件，提高并行度&#125; util和EventEmitterutilities api 描述 inheris(constructor, superConstructor) 实现对象间原型继承 inspect() 将任意对象以字符串的形式打印出来 eventsnodejs本身架构就是事件驱动的。EventEmitter的核心就是事件发射与事件监听功能的封装。EventEmitter的每个事件由一个事件或若干个参数组成。对于某个事件，EventEmitter支持若干个事件监听器。事件发生时，事件监听器被依次调用，事件参数作为回调函数的参数传递。|api|描述||—|—||EventEmitter.on(event, listener)|为指定事件注册监听器||EventEmitter.emit()|发射event事件||EventEmitter.once(event, listener)|监听器只出发一次||EventEmitter.removeListener(event, listener)|移除监听器||EventEmitter.removeAllListener(event)|移除所有监听器| 特殊事件error，出错的时候，EventEmitter没有注册相应的监听器，就当做异常，退出程序，打印调用栈。 一般继承EventEmitter来使用它。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery笔记]]></title>
    <url>%2F2016%2F12%2F05%2Fcelery%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[需求需要异步调用后台执行时间长的任务，而且任务结束后回写一些status到数据库。 开始的时候，在django中使用自己的threadpool：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117# !/usr/bin/env python# -*- coding:utf-8 -*-'''created by will '''import Queueimport loggingimport subprocessimport threadingimport time, osfrom django.utils import timezonefrom subprocess import Popenfrom metamap.models import Executionsfrom metamap.utils import enumslogger = logging.getLogger('django')class WorkManager(object): def __init__(self, work_num=1000, thread_num=2): self.work_queue = Queue.Queue() self.threads = [] # self.__init_work_queue(work_num) self.__init_thread_pool(thread_num) """ 初始化线程 """ def __init_thread_pool(self, thread_num): for i in range(thread_num): self.threads.append(Work(self.work_queue)) """ 初始化工作队列 """ def __init_work_queue(self, jobs_num): for i in range(jobs_num): self.add_job(do_job, i) """ 添加一项工作入队 """ def add_job(self, func, *args): self.work_queue.put((func, list(args))) # 任务入队，Queue内部实现了同步机制 """ 检查剩余队列任务 """ def check_queue(self): return self.work_queue.qsize() """ 等待所有线程运行完毕 """ def wait_allcomplete(self): for item in self.threads: if item.isAlive(): item.join()class Work(threading.Thread): def __init__(self, work_queue): threading.Thread.__init__(self) self.work_queue = work_queue self.start() def run(self): # 死循环，从而让创建的线程在一定条件下关闭退出 while True: location = '' try: do, args = self.work_queue.get_nowait() # 任务异步出队，Queue内部实现了同步机制 logger.info('%s method ' % do) logger.info('%s find job ....... %s ' % (self.getName(), ''.join(args[0]))) returncode = do(args) self.work_queue.task_done() # 通知系统任务完成 location = ''.join(args[1]) execution = Executions.objects.get(logLocation=location) execution.end_time = timezone.now() logger.info('%s return code is %d' % (self.getName(), returncode)) if returncode == 0: execution.status = enums.EXECUTION_STATUS.DONE else: execution.status = enums.EXECUTION_STATUS.FAILED execution.save() except Queue.Empty: time.sleep(3) except Executions.DoesNotExist: logger.error('cannot find execution from db for location %s' % location) except Exception, e: logger.error('got error :%s' % str(e)) break# 具体要做的任务def do_job(*args): logger.debug(args) log = args[0][1] command = args[0][0] p = Popen([''.join(command)], stdout=open(log, 'a'), stderr=subprocess.STDOUT, shell=True, universal_newlines=True) p.wait() return p.returncodeif __name__ == '__main__': start = time.time() # work_manager = WorkManager(10, 3) # work_manager.add_job(do_job, "sh /usr/local/metamap/test.sh") # work_manager.add_job(do_job, "sh /usr/local/metamap/test.sh") print "doneeeeeeeeeeeeee" end = time.time() print "cost all time: %s" % (end - start) 遇到两个问题： django项目的manage.py运行的时候，连接池会随着view代码的加载而初始化运行，导致命令不退出，一直hang着 结合gunicorn使用的时候，即便已经完成了django的一次request和response过程，重定向到了任务的状态页面。但是gunicorn任务此次request并未结束，导致worker自认为请求超时自杀，另起worker。比如后台异步任务在连接池中需要执行400s，而gunicorn的请求超时时间是300s，那么到了300s的时候当前worker的进程就被kill，然后另起一个新的worker。那么后台的线程池里执行的这个任务就也随着worker的退出被中止了。 问了一些大神的意见， 推荐给我celery。尝试django-celery未成功，所以就使用了原生的celery，解决了以上问题。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django中使用视图]]></title>
    <url>%2F2016%2F12%2F05%2Fdjango%E4%B8%AD%E4%BD%BF%E7%94%A8%E8%A7%86%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[最近在做数据地图之类的东西，要对hive的meta库进行一些查询与分析之类的。但是hive的字段表、tbls表、dbs表设计的有些鸡肋。中间隔了sds、cds表，所以查询非常费劲，就考虑搞一个视图，然后利用django的models直接进行orm的只读操作。 12345678910111213141516CREATE OR REPLACE VIEW col_tbl_db_view AS SELECT 1 as id, db.DB_ID as db_id, db.`NAME` as db_name, a.TBL_ID as tbl_id, a.TBL_NAME as tbl_name, a.TBL_TYPE as tbl_type, d.TYPE_NAME as col_type_name, d.`COMMENT` as col_comment, d.COLUMN_NAME as col_nameFROM TBLS aLEFT JOIN SDS b ON a.SD_ID = b.SD_IDLEFT JOIN COLUMNS_V2 d ON b.CD_ID = d.CD_IDLEFT JOIN DBS db ON a.DB_ID = db.DB_ID model代码12345678910111213141516171819202122232425262728293031323334353637383940414243# !/usr/bin/env python# -*- coding: utf-8 -*'''created by will '''from django.db import modelsimport datetimefrom django.utils import timezoneclass DB(models.Model): db_id = models.BigIntegerField(max_length=20, primary_key=True) desc = models.CharField(max_length=4000) db_location_uri = models.CharField(max_length=4000) name = models.CharField(max_length=128) owner_name = models.CharField(max_length=128) owner_type = models.CharField(max_length=10) class Meta: managed = False db_table = 'DBS'class TBL(models.Model): tbl_id = models.BigIntegerField(max_length=20, primary_key=True) create_time = models.DateTimeField owner = models.CharField(max_length=767) tbl_name = models.CharField(max_length=128) class Meta: managed = False db_table = 'TBLS'class ColMeta(models.Model): ''' 字段对应meta ''' id = models.IntegerField(primary_key=True) db = models.ForeignKey(DB, on_delete=models.DO_NOTHING) tbl = models.ForeignKey(TBL, on_delete=models.DO_NOTHING) tbl_type = models.CharField(max_length=30, null=True) col_type_name = models.CharField(max_length=300, null=True) col_comment = models.CharField(max_length=300, null=True) col_name = models.CharField(max_length=300, null=True) class Meta: managed = False db_table = 'col_tbl_db_view' 注意几点： 由于是只读的model，所以设置了managed=false，这样就不会在migration中对数据库进行任何操作了。 对于hive的meta库我们只做查询不做修改，也是出于上面的原因，我们把这些代码放在read_models.py里面，这个文件里放置的都是对既有数据库的只读操作的model。 使用view的时候注意几点 django的模板遍历结果的时候会使用到model的主键，当model里面没有指定某个字段是主键的时候，会自动生成一个id字段作为主键。这时，view里并没有id这个字段就会报错了。所以我们需要一个row_number()之类的东西，但是mysql不支持此种东西。所以尝试了一下把所有的id都弄成1，然后在model里面指定primary_key为id，结果成功。以此，推测，django template在遍历context里的变量的时候并不会判断主键是否唯一，也不是以主键为索引去抽取数据。 在foreignkey的地方，一定要加上on_delete=models.DO_NOTHING。防止对表里的数据有任何的级联影响 指定db_table为view的名字 虽然明知会报错，但是不要调用save或者update之类的方法 在运行makemigrations的时候，并不会包含创建视图的sql语句，因为managed=false。我们需要手动加到0001_initial.py里面去。12345678910111213141516171819202122...migrations.RunSQL( """ CREATE OR REPLACE VIEW col_tbl_db_view AS SELECT 1 as id, db.DB_ID as db_id, db.`NAME` as db_name, a.TBL_ID as tbl_id, a.TBL_NAME as tbl_name, a.TBL_TYPE as tbl_type, d.TYPE_NAME as col_type_name, d.`COMMENT` as col_comment, d.COLUMN_NAME as col_name FROM TBLS a LEFT JOIN SDS b ON a.SD_ID = b.SD_ID LEFT JOIN COLUMNS_V2 d ON b.CD_ID = d.CD_ID LEFT JOIN DBS db ON a.DB_ID = db.DB_ID """),... 参考： https://blog.rescale.com/using-database-views-in-django-orm/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[celery数据库事务问题]]></title>
    <url>%2F2016%2F12%2F05%2Fcelery%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[我的场景跟官网的例子基本类似，就是单个事务中需要，先创建一个对象，之后执行异步任务，这个异步任务里又要更新这个新建的对象。 理论上，如果任何一步出问题的话，就应该回滚全部。但是官网给的解释，目前不是这样的： view代码：123456789101112131415161718192021222324252627282930313233from django import formsfrom django.http import HttpResponseRedirectfrom django.template.context import RequestContextfrom django.shortcuts import get_object_or_404, render_to_responsefrom blog import tasksfrom blog.models import Commentclass CommentForm(forms.ModelForm): class Meta: model = Commentdef add_comment(request, slug, template_name='comments/create.html'): post = get_object_or_404(Entry, slug=slug) remote_addr = request.META.get('REMOTE_ADDR') if request.method == 'post': form = CommentForm(request.POST, request.FILES) if form.is_valid(): # 这里创建对象，对DB有了插入操作 comment = form.save() # 异步方法的调用 tasks.spam_filter.delay(comment_id=comment.id, remote_addr=remote_addr) return HttpResponseRedirect(post.get_absolute_url()) else: form = CommentForm() context = RequestContext(request, &#123;'form': form&#125;) return render_to_response(template_name, context_instance=context) task代码：12345678910111213141516171819202122232425262728293031323334353637from celery import Celeryfrom akismet import Akismetfrom django.core.exceptions import ImproperlyConfiguredfrom django.contrib.sites.models import Sitefrom blog.models import Commentapp = Celery(broker='amqp://')@app.taskdef spam_filter(comment_id, remote_addr=None): logger = spam_filter.get_logger() logger.info('Running spam filter for comment %s', comment_id) # 可以看到异步方法调用依赖于前面对象的创建 comment = Comment.objects.get(pk=comment_id) current_domain = Site.objects.get_current().domain akismet = Akismet(settings.AKISMET_KEY, 'http://&#123;0&#125;'.format(current_domain)) if not akismet.verify_key(): raise ImproperlyConfigured('Invalid AKISMET_KEY') is_spam = akismet.comment_check(user_ip=remote_addr, comment_content=comment.comment, comment_author=comment.name, comment_author_email=comment.email_address) if is_spam: comment.is_spam = True # 异步方法调用完成后，会基于前面创建的对象，再次操作数据库对象 comment.save() return is_spam 这明显是已经破坏了这个事务的原子性的，虽然目前本人的代码也是这样的。 上面还有一段另外的写法：12345678910@transaction.commit_manuallydef create_article(request): try: article = Article.objects.create(…) except: transaction.rollback() raise else: transaction.commit() expand_abbreviations.delay(article.pk) 为什么要这样呢？为了避免race condition。也就是异步方法调用先于数据库插入操作的话，异步方法里就直接报错了，所以必须在调用异步方法之前就commit当前的事务。针对性的代码示例如下：123456from django.db import transaction@transaction.commit_on_successdef create_article(request): article = Article.objects.create(…) expand_abbreviations.delay(article.pk) 理论上来说，调用异步方法的时候这个article就是还没有创建的，因为是分离在两个进程的代码，所以异步方法中查不到这个article，就出现了race condition问题。 还有一点需要注意的是，在django中调用异步方法的时候，也不要以models里面的对象作为参数。如果需要这些对象，应该每次都去db里去查，否则也有可能面临race condition问题。 但是事务性怎么办！！！ 参考：http://docs.celeryproject.org/en/latest/userguide/tasks.html#database-transactions]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[djcelery定时任务坑记]]></title>
    <url>%2F2016%2F12%2F05%2Fdjcelery%E5%AE%9A%E6%97%B6%E4%BB%BB%E5%8A%A1%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[celery实现定时任务，原生是通过后台的配置文件settings里配置添加的。形如：123456789101112CELERYBEAT_SCHEDULE = &#123; 'add-every-30-seconds': &#123; 'task': 'tasks.add', 'schedule': timedelta(seconds=30), 'args': (16, 16) &#125;, 'add-every-30-seconds': &#123; 'task': 'tasks.add', 'schedule': timedelta(seconds=30), 'args': (16, 16) &#125;,&#125; 每增加或修改或删除都要重新修改这个文件，并且重启beat才能生效。不友好 djcelery结合mysql，将定时任务的配置放进mysql，然后自己实现了一个beat服务，每次定时扫描mysql里最新的定时任务信息，来调度任务，默认扫描间隔是5s。 挺好，它还提供了一个admin界面来添加定时任务。 我们需要的是一个自由增删改查任务的功能，暴露给编写任务并设定任务执行周期的数据分析人员。所以需要在djcelery的基础之上再稍微加一些自定义的实现：为了与现有任务融合，在periodTask模型上添加了一个我们自定义任务WillTask的外键。 中间遇到问题：123定义一个定时任务periodTask1，然后修改只修改cron到10分钟以后，结果到了时间竟然没有调度。到admin界面同样操作了一下，能够触发最新配置的调度。 观察beat的日志，发现它总是查询djcelery_periodictasks里的内容：12 然后就注意观察了一下，发现在admin里操作更改任务的cron的时候，djcelery_periodictasks里的last_update字段会更新。而我自己修改的话就不会更新。 修改逻辑，每次对task进行修改都去更新djcelery_periodictasks里的last_update字段为当前时间。 —— 成功。 找网友@周星星对了一下，他是使用的djcelery的master 3.2.0a分支，不存在这个问题。 他的代码：1234567target_task = PeriodicTask.objects.get(name='test_1338')crontab = CrontabSchedule.objects.create(minute='*/2')crontab.save()target_task.crontab = crontabtarget_task.args = '[1, 888888888, "1??99999???"]'result = target_task.save()return HttpResponse(result, content_type='text/json') 可以看到他是每次都新建的，中间我也尝试了新建cron，但是结果还是同上。鉴于程序严密性，又改回了修改cron。 我当前的代码：1234567891011121314151617task = WillDependencyTask.objects.get(pk=pk)httputils.post2obj(task, request.POST, 'id')task.save()cron_task = PeriodicTask.objects.get(willtask_id=pk)cron_task.name = task.namecron_task.save()cron = DjceleryCrontabschedule.objects.get(pk=cron_task.crontab_id)cron.minute, cron.hour, cron.day_of_month, cron.month_of_year, cron.day_of_week = cronhelper.cron_from_str( request.POST['cronexp'])cron.save()# 这里是多出来的地方tasks = DjceleryPeriodictasks.objects.get(ident=1)tasks.last_update = timezone.now()tasks.save() 所以初步确认为版本问题。我使用的是djcelery3.1.17。 参考： http://docs.celeryproject.org/en/latest/userguide/periodic-tasks.html https://github.com/celery/django-celery 传送长数据串出错1234567891011@transaction.atomicdef exec_job(request, sqoopid): sqoop = SqoopHive2Mysql.objects.get(id=sqoopid) location = AZKABAN_SCRIPT_LOCATION + dateutils.now_datetime() + '-sqoop-' + sqoop.name + '.log' command = etlhelper.generate_sqoop_hive2mysql(sqoop) execution = SqoopHive2MysqlExecutions(logLocation=location, job_id=sqoopid, status=0) execution.save() from metamap import tasks tasks.exec_sqoop.delay(command, location) return redirect('metamap:sqoop_execlog', execid=execution.id) command再这里打印是：1sqoop export --connect jdbc:mysql://120.55.176.18:5306/product?useCursorFetch=true&amp;dontTrackOpenResources=true&amp;defaultFetchSize=2000 --driver com.mysql.jdbc.Driver --username xmanread --password LtLUGkNbr84UWXglBFYe4GuMX8EJXeIG --input-fields-terminated-by &quot;\t&quot; --update-key create_date,period,type,channel_id,bank_name,platform,status_id,return_status,channel_id,bank_name,platform,status_id,return_status,number_order,amount_order --update-mode allowinsert --columns create_date,end_date,period,type,channel_id,bank_name,platform,status_id,return_status,number_order,amount_order --export-dir hdfs://namenode01.will.com:8020/apps/hive/warehouse/dim_payment.db/order_detail/log_type=1/log_period=1/log_create_date=2016-12-06 --table JLC_ORDER_DETAIL_APP --verbose 在task执行的时候打印出来是：1sqoop export --connect jdbc:mysql://120.55.176.18:5306/product?useCursorFetch=true&amp;dontTrackOpenResources=true&amp;defaultFetchSize=2000 --driver com.my xmanread --password LtLUGkNbr84UWXglBFYe4GuMX8EJXeIG --input-fields-terminated-by &quot;\t&quot; --update-key create_date,period,type,channel_id,bank_name,platform,status_id,return_status,channel_id,bank_name,platform,status_id,return_status,number_order,amount_order --update-mode allowinsert --columns create_date,end_date,period,type,channel_id,bank_name,platform,status_id,return_status,number_order,amount_order --export-dir hdfs://namenode01.will.com:8020/apps/hive/warehouse/dim_payment.db/order_detail/log_type=1/log_period=1/log_create_date=2016-12-06 --table JLC_ORDER_DETAIL_APP --verbose , location is /var/azkaban-metamap/20161208062420-sqoop-export_JLC_ORDER_DETAIL_APP.log driver那里莫名就消失了。 后来发现是由于windows里的换行符导致的。 celery beat不触发定时任务注意时区，妈蛋！！]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git--杂记]]></title>
    <url>%2F2016%2F12%2F05%2Fgit--%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[配置别名编辑~/.gitconfig文件, 添加1234567[alias] last = log -1 co = checkout ci = commit br = branch st = status lg = log --color --graph --pretty=format:&apos;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&apos; --abbrev-commit case有时本地修改了，然后直接checkout线上的某个branch:123Your local changes to the following files would be overwritten by checkout: app/models/VO/Request/ServiceSchedulerConfigExt.php app/models/purchase/InvoiceApplyModel.php 两个方案: 丢掉本地修改： git reset –hard, 之后重新执行 保留本地修改：git stash ： Stash the changes in a dirty working directory away。之后git pull,之后 git stash pop 说明：git stash是当我们对项目进行了修改，但是想在不丢失当前修改的前提下，到commit的某个版本【HEAD】那儿去。这时候调用这个命令，其实相当于调用了git stash save，它会将当前所有的修改存放到一个stash list里去，可以理解为一个队列。当我们到了【HEAD】那里，然后执行git stash pop，就会从stash list里移除最前面的修改信息，并把这个修改应用到当前情境之中，这个修改必须是前面刚刚git stash save存储的。 常用操作 取消对文件的修改。还原到最近的版本，废弃本地做的修改。git checkout – 取消已经暂存的文件。即，撤销先前”git add”的操作git reset HEAD … 修改最后一次提交。用于修改上一次的提交信息，或漏提交文件等情况。git commit –amend 回退所有内容到上一个版本git reset HEAD^ 回退a.py这个文件的版本到上一个版本 git reset HEAD^ a.py 向前回退到第3个版本 git reset –soft HEAD~3 将本地的状态回退到和远程的一样 git reset –hard origin/master 回退到某个版本 git reset 057d 回退到上一次提交的状态，按照某一次的commit完全反向的进行一次commit.(代码回滚到上个版本，并提交git)git revert HEAD回复git reset –hard之前的数据 使用git reflog查看所有的提交记录， 123456781d2b028 HEAD@&#123;0&#125;: reset: moving to 1d2b02818e8fa9510636 HEAD@&#123;1&#125;: commit: update pci driver and add network stuff1d2b028 HEAD@&#123;2&#125;: commit: update pci drivere4e042b HEAD@&#123;3&#125;: commit: add infrastructure of Linux pci drivereb7ac57 HEAD@&#123;4&#125;: commit: update i2c driver - add at24 device034034f HEAD@&#123;5&#125;: commit: update i2c linux driver - example of mini244017e0bfb HEAD@&#123;6&#125;: commit: add some diagram for i2c driver1f382d7 HEAD@&#123;7&#125;: commit: update i2c driver - overview 找到对应版本的commit后进行git reset 1d2b028 删除远程分支上的提交 12git reset --hard HEAD~1 // 在本地回退一次git提交，会提示回退到那个commit上了git push --force //将本地回退强制推送到远程]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django-celery坑记]]></title>
    <url>%2F2016%2F12%2F05%2Fdjango-celery%E5%9D%91%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[安装django-celery花了好长时间，因为网上一些资料都不是最新版本，缺失一些配置。这里自己再整理一下整体的步骤。 1.安装：1pip install django-celery 2. settings文件123456789101112131415161718192021INSTALLED_APPS = [ 'metamap', 'django.contrib.admin', 'django.contrib.auth', 'django.contrib.contenttypes', 'django.contrib.sessions', 'django.contrib.messages', 'django.contrib.staticfiles', 'djcelery',]import djcelerydjcelery.setup_loader()# 用于存放taskBROKER_URL = 'redis://localhost:6379'# Celery Beat 设置CELERYBEAT_SCHEDULER = 'djcelery.schedulers.DatabaseScheduler' 注意一下，这里的INSTALLED_APPS，在django1.9版本的官方文档中，是说写成metamap.apps.MetamapConfig的。但是写成这种的话，下面自动发现task的时候就会找不到。可以理解成是djcelery暂时还没有追上django版本的脚步，好在django兼容了以前版本，我们写成metamap也不会影响django1.9版本的运行。 3. celery启动启动celery的文件，有些博客少了这个主要部分。12345678910111213141516171819202122232425# !/usr/bin/env pythonfrom __future__ import absolute_importimport osfrom celery import Celery# set the default Django settings module for the 'celery' program.os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'metamap_django.settings')from django.conf import settingsapp = Celery('metamap_django')# Using a string here means the worker will not have to# pickle the object when using Windows.# 可以发现，这里有一个自动发现task的配置，就是针对settings里的INSTALLED_APPS，这里我们的目标是metamap# 找到metamap后，会自动找下面的tasks.py文件，扫描里面的taskapp.config_from_object('django.conf:settings')app.autodiscover_tasks(lambda: settings.INSTALLED_APPS) @app.task(bind=True)def debug_task(self): print('Request: &#123;0!r&#125;'.format(self.request)) 4. 启动celery在metamap/init.py里引入celery_app:12345from __future__ import absolute_import# This will make sure the app is always imported when# Django starts so that shared_task will use this app.from .celery import app as celery_app 5. tasks.py放到app的根目录下面，我的是metamap/tasks.py：1234567891011121314151617181920212223242526272829303132333435363738# !/usr/bin/env python# -*- coding: utf-8 -*'''created by will'''from __future__ import absolute_importimport loggingimport subprocessfrom celery import shared_task, taskfrom django.utils import timezonefrom metamap.models import ETL, Executionsfrom metamap.utils import enumsfrom celery.utils.log import get_task_loggerlogger = get_task_logger(__name__)@taskdef xx(): return 'sdfsdf'@shared_taskdef add(x, y): return x + y@shared_taskdef mul(x, y): return x * y@shared_taskdef xsum(numbers): return sum(numbers) 参考： http://www.weiguda.com/blog/73/ https://github.com/hardvic/djcelery_proj https://github.com/celery/django-celery]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flask笔记1]]></title>
    <url>%2F2016%2F12%2F05%2Fflask%E7%AC%94%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[instance文件夹有时你需要定义一些不能为人所知的配置变量。为此，你会想要把它们从config.py中的其他变量分离出来，并保持在版本控制之外。 你可能要隐藏类似数据库密码和API密钥的秘密，或定义特定于当前机器的参数。 为了让这更加轻松，Flask提供了一个叫instance文件夹的特性。 instance文件夹是根目录的一个子文件夹，包括了一个特定于当前应用实例的配置文件。我们不要把它提交到版本控制中。 以上是copy的别人的代码，引入instance中特定配置文件的方式是123app = Flask(__name__)app.config.from_object('config')app.config.from_pyfile('instance/config.py') 这样弄的话，就没有instance的概念了但是至少能找到这个配置文件可是flask本身是内置了又instance的概念的………… 我参考的网站都让我配置一个instance的相对路径instance_relative_config或者绝对路径instance_path.它们配置后的期望代码是123app = Flask(__name__, instance_relative_config=True)app.config.from_object('config')app.config.from_pyfile('config.py') 就是会自己把自己置于instance目录下，取相对的config.py文件。 其实不然，我使用flask版本是0.11.1，看的文档是0.10.1的，不知道是不是版本问题。 借助instance中的配置，我们可以在根路径下创建给config包，然后下面分别放置global.py、prod.py、dev.py、test.py，看名字就知道了，一个是全局的，flask加载的时候一定要弄进配置里。但是对于后面的prod.py、dev.py、test.py要加载哪一个？ 参考的资料是自己设置一个环境变量APP_CONFIG_FILE，就是在shell环境里去export.123456# yourapp/__init__.pyapp = Flask(__name__, instance_relative_config=True)app.config.from_object('config.default')app.config.from_pyfile('instance/config.py') # 从instance文件夹中加载配置app.config.from_envvar('APP_CONFIG_FILE') 最后这个app.config.from_envvar是从环境变量所代表的文件里加载配置。然后运行的时候12APP_CONFIG_FILE=/var/www/app/config/prod.pypython run.py 我认为也直接在instance里进行配置了。1234app = Flask(__name__)app.config.from_object('config')app.config.from_pyfile('instance/config.py')app.config.from_pyfile(app.config['APP_CONFIG_FILE']) 不错的学习网站： https://spacewander.github.io/explore-flask-zh/5-configuration.html http://docs.jinkan.org/docs/flask 蓝图参考： http://docs.jinkan.org/docs/flask/blueprints.html#url]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring-cloud杂记]]></title>
    <url>%2F2016%2F12%2F05%2Fspring-cloud%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Eureka云应用中，服务通常并不长久运行的。我们也不希望记录每个service的IP和port。它的功能有些类似一个DNS服务。 发现与定位service 负载均衡 中间件容错 看了一些文章， 跟我以前写的smart agent的master和plugin模式基本雷同，就是想master注册自己，挂掉的时候remove掉，定时发送心跳等等。 类似功能的还有Consul https://github.com/Netflix/eureka/wiki spring cloud配置文件默认service都是连接8888端口的configserver，要是想修改的话就需要使用bootstrap.properties外部配置文件。 1spring.cloud.config.uri: http://myconfigserver.com bootstrap的优先级高于configserver的application配置。 https://cloud.spring.io/spring-cloud-config/spring-cloud-config.html zuul地狱的守门怪兽 安全认证 动态路由 监视 压测 降载 静态资源处理 多区域 https://github.com/Netflix/zuul/wiki zipkin分布式中的神捕，对分布式调度的追踪，分布式追踪系统。可获取各个微服务接口调用的延时。基于Google论文dapper设计完成。 应用的执行耗时数据会保存在zipkin里。zipkin UI也会呈现出每个服务request之间的依赖关系图。特别便于最终错误或者延时问题。 本质其实跟以前我们在云智慧做的工作一样，在request上打标记。 存储可以选择内存、JDBC、Cassandra、ES。 http://zipkin.io/http://zipkin.io/pages/architecture.html oauth2分布式系统中，client与service是交叉访问的，需要解决一个问题：哪些client有权限访问哪些service？解决的方式就是单点登录：所有的请求某个service都需要一个token，这个token由一个统一的中心认证服务提供。我们这里就使用oauth2来做这个中心认证服务。 总结configserver为一切service服务，提供各个服务最新的配置信息。 eureka-service提供服务注册与发现。所有的client都来它这里找service，然后再执行RPC请求。 reservation-service是一个服务，通过configserver找到eureka，注册到eureka，提供服务。 reservation-client是一个客户端，先通过configserver找到eureka，找到要请求的reservation-service，再请求具体路径(整个请求可以通过zuul代理实现)。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django与gunicorn的关系]]></title>
    <url>%2F2016%2F12%2F05%2Fdjango%E4%B8%8Egunicorn%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[为什么需要有gunicorn呢？ 总的来说很简单：你需要一些东西来执行python，但是python并不是对于所有类型的request都很适合。 [说明一下: 我是 Gunicorn 的developer] 稍微复杂点儿解释:不管你用什么样的app server，都会有一些upstream都是你的django app不处理的，比如一些静态资源(images/css/js) 这就出现了经典的’三层架构’，可以Google一下‘three tier architecture’。就是webserver(比如nginx)负责处理许多静态资源的request。需要动态处理的request会被发送给app server(比如Gunicorn)。最内层的，也就是第三层就是数据层了。 纵观历史典型应用，每一层都应该在不同的主机上。（前两层或许还会占用多个主机，负载均衡之类，比如：5个web server分发请求到两个app server上，查询一个数据库） 最近来说，各种应用形态已经各有各的特点了。小项目其实并非完全用到这三层架构，或者也不用分别运行在很多机器上。也有些方案会把app server与web server集成起来(apache httpd + mod_wsgi, nginx + mod_uwsgi等)。 对于Gunicorn来说，是从Ruby的Unicorn抽取出来，只利用nginx里的代理功能。如果Gunicorn不会直接从互联网接收request，那么我们就不用担心客户端会比较慢。这意味着Gunicorn的处理模型十分的简单。 Gunicorn与nginx的功能分离也允许Gunicorn在不影响总体性能的前提下，使用纯python重写。 具体到底谁处理了HTTP request呢，就是Gunicorn了。其实是Nginx和Gunicorn一起处理的。Nginx接收request，如果他是一个动态的request，就传递给Gunicorn来处理，然后返回一个response给Nginx，然后Nginx再传递给client。 所以，最好是使用nginx和Gunicorn一起部署django项目。如果你只是想让nginx做代理，那么也可以使用Gunicorn、mod_uwsgi、CherryPy等作为django的依赖server。 参考： http://serverfault.com/questions/331256/why-do-i-need-nginx-and-something-like-gunicorn http://www.cnblogs.com/ArtsCrafts/p/gunicorn.html]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive中的权限]]></title>
    <url>%2F2016%2F12%2F05%2Fhive%E4%B8%AD%E7%9A%84%E6%9D%83%E9%99%90%2F</url>
    <content type="text"><![CDATA[用例 把hive当做存储工具。使用pig、MR等并行计算工具访问hive，hdfs权限通过文件认证来解决。metadata的访问需要通过hive配置的认证。 把hive作为一个查询引擎。又分为两种： a. hive命令行。这些用户可以有hdfs权限，也能看到hive的metastore，有点儿像1。 b. jdbc等通过hiveserver2访问。他们不能访问HDFS。 三种认证机制。 Storage Based Authorization in the Metastore Server在1和2.a里，用户可以直接访问到数据，hive没有控制这块。hdfs权限验证担当了主要任务。当你访问一个database、table、partition的时候，他会检查你是否有对应目录的权限。为了看到通过hiveserver2访问的用户的真实信息，需要设置hive.server2.enable.doAs为true。 SQL Standards Based AuthorizationSQL Standards Based Authorization in HiveServer2虽然Storage Based Authorization可以控制对database、table、partition的访问，但是他不能很好地控制对字段和view的访问，因为他是基于文件系统权限控制的。好的控制应该是可以控制用户访问的行和列。hiveserver2可以满足这种需求，可以只提供你的sql需要的行和列。 SQL Standards Based Authorization是基于sql标准的，可以使用revoke/grant语句控制权限。但是必须在hiveserver2的配置里设置一下。 注意如果启用了SQL Standards Based Authorization，那么2a这种方式就被禁用了。因为安全起见，不可能使用hive里的访问控制策略来控制hive命令行，用户可以直接访问HDFS，这样就直接绕过了认证机制。 Default Hive Authorization (Legacy Mode)这并不能完全控制好权限，会有很多漏洞。例如，没有统一的分配权限的用户，所有的用户都可以自己grant权限。 参考 https://cwiki.apache.org/confluence/display/Hive/Storage+Based+Authorization+in+the+Metastore+Server https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Authorization]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django中模型之间的关系]]></title>
    <url>%2F2016%2F12%2F05%2Fdjango%E4%B8%AD%E6%A8%A1%E5%9E%8B%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[Many-to-One使用django.db.models.ForeignKey来定义，像其他的字段类型一样，额外需要一个指定model类的位置。 例如，有一个Car model，它的生产厂家是Manufacturer，也就是一个厂家对应多个车，但是每个车只属于一个生产厂家。123456789from django.db import modelsclass Manufacturer(models.Model): # ... passclass Car(models.Model): manufacturer = models.ForeignKey(Manufacturer, on_delete=models.CASCADE) # ... 除此之外，也可以创建递归关系以及未定义模型的关系。详见此处。 建议把ForeignKey字段设置成目标model的小写形式，弄成别的也不会出错，但是最好规范一些。 对应数据库里面生成的字段名是manufacturer_id，就是在我们定义的属性的名字后面缀上_id。 123456class Car(models.Model): company_that_makes_it = models.ForeignKey( Manufacturer, on_delete=models.CASCADE, ) # ... 更多实例 Many-To-Many使用ManyToManyField关键字定义多对多关系。 例如，一个Pizza有多个Topping，一个Topping也可以放在多个pizza上。 123456789from django.db import modelsclass Topping(models.Model): # ... passclass Pizza(models.Model): # ... toppings = models.ManyToManyField(Topping) 以上无论哪个对象里放置ManyToManyFiled都可以，但是只能放在其中一个里面，不能都放。 两者的互相访问12pizza.toppingstopping.pizza_set 更多实例]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据挖据DW整理]]></title>
    <url>%2F2016%2F12%2F05%2F%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8D%AEDW%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[1234567891011【金奖】https://github.com/Honlan/ppd-magic-mirrorOMNIRank https://github.com/wang-haiyang/ppd_model 【银奖】https://github.com/palladino1/NiuwaBigData 【铜奖】https://github.com/miaofu/WebDataProduct【铜奖】https://github.com/wepe/PPD_RiskControlCompetition 【入围奖】https://github.com/Percyzhou/ppdmojing 文章列表 标题 链接 是否已阅 如何在一年内成为数据挖掘工程师 https://zhuanlan.zhihu.com/p/20766210 用贝叶斯判别分析再次预测股票涨跌情况 http://shujuren.org/article/164.html 自然语言处理实战之微博情感偏向分析 http://blog.csdn.net/baimafujinji/article/details/51153872 Udacity的Deep Learning，来自Google的讲师（附链接） http://t.cn/R5wsH9U http://t.cn/R5JA0oQ https://classroom.udacity.com/courses/ud730/lessons/6370362152/concepts/63798118150923 https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/udacity/1_notmnist.ipynb 用Python对金庸系列武侠小说进行文本挖掘 http://chuansong.me/n/399844947454 视频手把手教你用python做机器学习案例 http://www.duweiwen.com/wechat/11ff99ffdddd88.html 用贝叶斯判别分析再次预测股票涨跌情况 http://shujuren.org/article/164.html 文本挖掘分析《欢乐颂》到底谁和谁堪称好闺蜜、谁和谁又最为般配？ http://mp.weixin.qq.com/s?__biz=MzA3MDg0MjgxNQ==&amp;mid=2652389844&amp;idx=1&amp;sn=39e77ca6ea49f84474bf2a50c92a974f&amp;scene=0#wechat_redirect 数据挖掘工程师的面试问题与答题思路 http://www.idatacamp.com/2016/06/22/%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E5%B7%A5%E7%A8%8B%E5%B8%88%E7%9A%84%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%E4%B8%8E%E7%AD%94%E9%A2%98%E6%80%9D%E8%B7%AF/ 玩转数据系列一：图文教你！如何用机器学习玩转人口普查统计 http://toutiao.com/i6291804057988235778/ 注意这是系列啊 网站收集 http://shujuren.org/]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[NFS+autofs]]></title>
    <url>%2F2016%2F12%2F05%2FNFS%2Bautofs%2F</url>
    <content type="text"><![CDATA[server端安装服务1yum -y install nfs-utils rpcbind 设置开机启动，并启动服务12345chkconfig nfs onchkconfig rpcbind on/etc/init.d/rpcbind restart/etc/init.d/nfs restart 编辑nfs主配置文件/etc/exports，添加挂载目录1/server/will *(ro,no_root_squash,sync) 这里挂载了/server/will目录，任何机器远程挂载之后都可以访问此目录，只有普通的只读权限。 重新加载主配置文件12[root@namenode01 will]# exportfs -arvexporting *:/server/will client端安装服务1yum install nfs-utils autofs -y 启动服务12345chkconfig nfs onchkconfig rpcbind on/etc/init.d/rpcbind restart/etc/init.d/nfs restart 查看是否能成功连接NFS服务123[root@schedule ~]# showmount -e 10.0.1.72Export list for 10.0.1.72:/server/will * 这里可以看到可以成功访问。 配置autofs，在/etc/auto.master最后加入：1/server/will /etc/auto.nfs 编辑/etc/auto.nfs1script -fstype=nfs 10.0.1.72:/server/will 重启nfs123[root@schedule ~]# service autofs restartLoading autofs4: [ OK ]Starting automount: [ OK ] 测试12345678910[root@datanode01 ~]# dfFilesystem 1K-blocks Used Available Use% Mounted on/dev/mapper/vg_template-lv_root 16070076 6401092 8852652 42% /tmpfs 8167372 0 8167372 0% /dev/shm/dev/sda1 495844 31581 438663 7% /boot/dev/mapper/luoji-mail 707004512 423592108 247504948 64% /server10.0.1.72:/server/will 51606528 1049600 47935488 3% /server/will/script]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[express]]></title>
    <url>%2F2016%2F12%2F05%2Fexpress%2F</url>
    <content type="text"><![CDATA[app.set/get用来查看或者设置一些配置参数 express里变量的换行出不来12345678router.get('/get_log', function (req, res) &#123; worker.exec("cat " + req.query.location, function (error, stdout, stderr) &#123; console.log('stderr : ' + stderr); console.log('stdout : ' + stdout); console.log('error : ' + error); res.render('etls/log', &#123;stdout : stdout, log : req.query.location&#125;); &#125;); &#125;); 后面控制台的输出12345678stderr : stdout : # job for mymeta@my_table# author : will# create time : 19700118070840# pre settings delete from default.tblselect * from battingerror : null 模板文件1#&#123;stdout&#125; 前端输出1# job for mymeta@my_table # author : will # create time : 19700118070840 # pre settings delete from default.tbl select * from batting 控制台的stdout变量是换行的，但是前端就不换行。试了一下res.send()，结果也是这样。 模板修改为1!&#123;stdout.replace(/\n/g, &apos;&lt;br/&gt;&apos;)&#125; 如果使用res.send()的话，就res.send(stdout.replace(/\n/g, ‘‘))就行了。 这个换行支持起来那么难么？比较low的感觉。 body-parser安装了body parser之后就可以方便的在req中使用 req.body来获取form内容，使用req.query来获取querystring里的内容了。 但是一定要注意express的app.use是有前后次序的。踩过坑]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban文档翻译]]></title>
    <url>%2F2016%2F12%2F05%2Fazkaban%E6%96%87%E6%A1%A3%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[mysql的作用mysql被用来存储一些状态信息，azkaban的AzkabanWebServer和AzkabanExecutorServer都会访问mysql来获取与更新状态。 AzkabanWebServer使用Mysql做什么？ 项目管理。管理项目以及相关权限，还有上传的文件； 正在执行的flow的状态。追踪在执行的flow，并且查看那个executor正在执行这个flow。 查看以前的flow或者job。 调度器。保存调度完成的job的状态。 SLA。保存所有的sla规则。 AzkabanExecutorServer使用Mysql做什么？ 访问project。从db里抽取project文件。 执行flow/job。抽取并更新flow的数据。 日志。保存job和flow的执行日志到db里。 interflow的依赖。如果一个flow运行在不同的executor上，则应当把所有的状态存储到db里去。 AzkabanWebServer用来管理azkaban，管理项目、认证、调度、监控。azkaban使用*.job的kv值文件来定义flow中的task，dependencies用来定义job chain之间的依赖。这个job文件和相关的代码可以打成一个zip包通过web server上传。 AzkabanExecutorServer以前web server和executor server是在一起的，分离的原因是：我们预期要能够给executor自由快速扩展的能力。另外如果azkaban升级的话，对我们的用户的影响也更小，因为其实azkaban的发展还是很快的。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django使用记录]]></title>
    <url>%2F2016%2F12%2F05%2Fdjango%E4%BD%BF%E7%94%A8%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Ubuntu安装失败123456789101112131415root@will-vm:/usr/local/metamap/metamap_js# pip install DjangoDownloading/unpacking Django Downloading Django-1.9.7-py2.py3-none-any.whl (6.6MB): 6.6MB downloadedCleaning up...Exception:Traceback (most recent call last): File &quot;/usr/lib/python2.7/dist-packages/pip/basecommand.py&quot;, line 122, in main status = self.run(options, args) File &quot;/usr/lib/python2.7/dist-packages/pip/commands/install.py&quot;, line 278, in run requirement_set.prepare_files(finder, force_root_egg_info=self.bundle, bundle=self.bundle) File &quot;/usr/lib/python2.7/dist-packages/pip/req.py&quot;, line 1259, in prepare_files )[0]IndexError: list index out of rangeStoring debug log for failure in /root/.pip/pip.log 查了一下，貌似只有Ubuntu有这个问题，需要先安装distribute1pip install --no-use-wheel --upgrade distribute 之后再安装django就解决了12root@will-vm:/usr/local/metamap/metamap_js# python -c &quot;import django; print(django.get_version())&quot;1.9.7 参考：https://www.zhihu.com/question/38484255]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive杂记]]></title>
    <url>%2F2016%2F12%2F05%2Fhive%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Execution failed with exit status: 3使用mapjoin的时候出现map端内存溢出。通常有两个原因： 当面对压缩文件的时候，mapjoin基于的数据大小参数未必准确。或许解压开的数据会大得多。可以降低hive.smalltable.filesize来调整，或者增大hive.mapred.local.mem来让map任务获取更大一些的内存。 hive.mapred.local.mem没起作用。参考：https://issues.apache.org/jira/browse/HADOOP-10245 MapJoin的过程： 本地 从数据源读取数据 在内存构建hashtable 把hashtable写到本地磁盘 上传hashtable到hdfs 把hashtable添加到DistributeCache Map任务中 从DistributeCache读取到内存中 基于内存中的hashtable匹配每条记录的key join到合适的，写到output中 无reduce任务 参考： https://cwiki.apache.org/confluence/display/Hive/LanguageManual+JoinOptimization http://blog.csdn.net/yhao2014/article/details/42675011 org.apache.hadoop.hive.serde2.lazy.objectinspector.LazyListObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector=load data时确定了data的数据类型，load完成后如果改变hive表中列的类型后select会出错 grouping set 聚合函数多变量问题= 123456789101112131415161718192021222324select nvl(a.loan_day,&apos;-&apos;) loan_day, nvl(b.is_newuser, &apos;-&apos;) is_new, sum(case when to_date(a.create_time) &lt;=a.dt then a.apply_amount else 0 end) loan_apply_amount_all, count(case when to_date(a.create_time) &lt;=a.dt then a.id else null end) loan_apply_count_all, sum(case when to_date(a.loan_time) &lt;=a.dt then a.apply_amount else 0 end) loan_suc_amount_all, count(case when to_date(a.loan_time) &lt;=a.dt then a.id else null end) loan_suc_count_all, sum(case when to_date(a.create_time) =a.dt then a.apply_amount else 0 end) loan_apply_amount_inc, count(case when to_date(a.create_time) =a.dt then a.id else null end) loan_apply_count_inc, sum(case when to_date(a.loan_time) =a.dt then a.apply_amount else 0 end) loan_suc_amount_inc, count(case when to_date(a.loan_time) =a.dt then a.id else null end) loan_suc_count_inc, a.dt statistics_date, &apos;weibo&apos; channel ,a.dt dtfrom fact_level.fact_loan_info a left join ( select id,case when to_date(new_user_date)=dt then &apos;newuser&apos; else &apos;olduser&apos; end is_newuser from fact_level.fact_user_info where dt between &apos;2017-02-13&apos; and &apos;2017-03-27&apos; and is_new=1 )b on a.user_id=b.id where a.dt between &apos;2017-02-13&apos; and &apos;2017-03-27&apos; and a.loan_unit = &apos;002007001&apos; and to_date(a.create_time)&lt;=a.dt and to_date(a.create_time)&gt;=&apos;2017-02-13&apos; group by a.dt,a.loan_day,b.is_newuser grouping sets (a.dt,(a.dt,a.loan_day,b.is_newuser),(a.dt,a.loan_day),(a.dt,b.is_newuser)) 报错：1Error while compiling statement: FAILED: SemanticException [Error 10210]: Grouping sets aggregations (with rollups or cubes) are not allowed if aggregation function parameters overlap with the aggregation functions columns 字面意思上看就是grouping set的聚合函数里不能出现聚合函数的参数超过聚合函数的字段数。 对应上面，就是那些case when里都是传入了两个参数create_time和dt。所以不支持。下面的sql就能够正常运行123456789101112131415161718192021222324select nvl(a.loan_day,&apos;-&apos;) loan_day, nvl(b.is_newuser, &apos;-&apos;) is_new, sum(case when to_date(a.create_time) &lt;=&apos;2017-03-27&apos; then a.apply_amount else 0 end) loan_apply_amount_all, count(case when to_date(a.create_time) &lt;=&apos;2017-03-27&apos; then a.id else null end) loan_apply_count_all, sum(case when to_date(a.loan_time) &lt;=&apos;2017-03-27&apos; then a.apply_amount else 0 end) loan_suc_amount_all, count(case when to_date(a.loan_time) &lt;=&apos;2017-03-27&apos; then a.id else null end) loan_suc_count_all, sum(case when to_date(a.create_time) =&apos;2017-03-27&apos; then a.apply_amount else 0 end) loan_apply_amount_inc, count(case when to_date(a.create_time) =&apos;2017-03-27&apos; then a.id else null end) loan_apply_count_inc, sum(case when to_date(a.loan_time) =&apos;2017-03-27&apos; then a.apply_amount else 0 end) loan_suc_amount_inc, count(case when to_date(a.loan_time) =&apos;2017-03-27&apos; then a.id else null end) loan_suc_count_inc, a.dt statistics_date, &apos;weibo&apos; channel ,a.dt dtfrom fact_level.fact_loan_info a left join ( select id,case when to_date(new_user_date)=dt then &apos;newuser&apos; else &apos;olduser&apos; end is_newuser from fact_level.fact_user_info where dt between &apos;2017-02-13&apos; and &apos;2017-03-27&apos; and is_new=1 )b on a.user_id=b.id where a.dt between &apos;2017-02-13&apos; and &apos;2017-03-27&apos; and a.loan_unit = &apos;002007001&apos; and to_date(a.create_time)&lt;=a.dt and to_date(a.create_time)&gt;=&apos;2017-02-13&apos; group by a.dt,a.loan_day,b.is_newuser grouping sets (a.dt,(a.dt,a.loan_day,b.is_newuser),(a.dt,a.loan_day),(a.dt,b.is_newuser)) 因为上面这个sql就只有一个参数了create_time或者loan_time。然而我们是不想写死的，我们要的日期是要在where条件里面限制。那就通过嵌套的方式进行实现，只需要绕开grouping set不能支持聚合函数太多参数这个约束就行了。 12345678910111213141516171819202122232425selectxx.loan_day, xx.is_new, sum(xx.loan_apply_amoun) loan_apply_amoun_all, xx.dtfrom (select nvl(a.loan_day,&apos;-&apos;) loan_day, nvl(b.is_newuser, &apos;-&apos;) is_new, case when to_date(a.create_time) &lt;=a.dt then a.apply_amount else 0 end loan_apply_amoun, a.dt statistics_date, &apos;weibo&apos; channel ,a.dt dtfrom fact_level.fact_loan_info a left join ( select id,case when to_date(new_user_date)=dt then &apos;newuser&apos; else &apos;olduser&apos; end is_newuser from fact_level.fact_user_info where dt between &apos;2017-02-13&apos; and &apos;2017-03-27&apos; and is_new=1 )b on a.user_id=b.id where a.dt between &apos;2017-02-13&apos; and &apos;2017-03-27&apos; and a.loan_unit = &apos;002007001&apos; and to_date(a.create_time)&lt;=a.dt and to_date(a.create_time)&gt;=&apos;2017-02-13&apos; )xxgroup by xx.dt,xx.loan_day,xx.is_new grouping sets (xx.dt,(xx.dt,xx.loan_day,xx.is_new),(xx.dt,xx.loan_day),(xx.dt,xx.is_new)) PS: 并不明白为什么参数不能过多，理论上是可以支持的。有兴趣的同学可以参考一下hive 源码咯。 我写个邮件问问hive用户组的人。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[时间管理]]></title>
    <url>%2F2016%2F12%2F05%2F%E6%97%B6%E9%97%B4%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[时间管理 编号 简述 备注 1 早起 让光线唤醒自己，而不是冲击的铃声。【个人认为，开始的时候可以用逐渐增强的音乐作为闹铃，因为早起的同时不能有坏情绪，逐渐唤醒我们的大脑】. 2 晨间日记 推荐的是印象笔记之类的。关注个人的长期目标、中期目标、短期目标。列出当天的作息计划、行动计划。从容地进行昨日总结和今日计划 3 时间性价比 把有限的精力投入到最重要的事情中，其他的交给别人 4 做重要不紧急的事情 重要、紧急四象限 5 事情分情景 有些适合走路做、有些适合在电脑前的时候做、有些适合找一个安静的私密空间做… 6 TODO-List 大脑擅长的是思考而不是记忆，活在当下，把事情放在硬盘里，专心做当下的事情 7 推荐工具 focus系列 印象笔记或有道云笔记之类]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HUE维护记录]]></title>
    <url>%2F2016%2F12%2F05%2FHUE%E7%BB%B4%E6%8A%A4%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[应用管理工具tools/app_reg/app_reg.py 1234567891011121314151617181920A tool to manage Hue applications. This does not stop/restart arunning Hue instance.Usage: ./tools/app_reg/app_reg.py [flags] --install &lt;path_to_app&gt; [&lt;path_to_app&gt; ...] [--relative-paths] To register and install new application(s). Add &apos;--relative-paths&apos; to the end of the args list to force the app manager to register the new application using its path relative to the hue root. ./tools/app_reg/app_reg.py [flags] --remove &lt;application_name&gt; To unregister and remove an installed application. ./tools/app_reg/app_reg.py [flags] --list To list all registered applications. ./tools/app_reg/app_reg.py [flags] --sync Synchronize all registered applications with the Hue environment. Useful after a `make clean&apos;.Optional flags: --debug Turns on debugging output 脚本clean.sh定时关闭hue的session和查询1234export HIVE_CONF_DIR=/usr/hdp/2.4.2.0-258/hive/conf/cd /server/hue/huebuild/env/bin/hue close_queries 3build/env/bin/hue close_sessions 1 restart_hue.sh定时重启hue12345num=`ps -ef | grep hue | wc -l`if [ $num -lt 2 ];then echo &quot;restart hue at `date +%Y%m%d%H%M`&quot; /server/hue/hue/build/env/bin/supervisor &amp;fi 指定yarn队列beeline可以这样1jdbc:hive2://10.0.1.84:10000?mapreduce.job.queuename=root.default 但是hue没有提供额外设置hiveserver连接参数的地方。看来只能重新指定hive配置目录了。]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker-centos-6.4安装手记]]></title>
    <url>%2F2016%2F12%2F05%2Fdocker-centos-6.4%E5%AE%89%E8%A3%85%E6%89%8B%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[version Base not defined in file libdevmapper.so.1.02 with link time referenceyum install device-mapper-event-libs -y 参考： http://qicheng0211.blog.51cto.com/3958621/1582909 can’t initialize iptables table `nat’编译内核的时候没有选中iptable_nat module相关的组件。重新到内核文件夹中make GCC 4.8 or higher required参考：https://gist.github.com/stephenturner/e3bc5cfacc2dc67eca8b 参考： http://blog.csdn.net/scaleqiao/article/details/46633011]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban源码追踪(2)-——-手动触发flow调度]]></title>
    <url>%2F2016%2F12%2F05%2Fazkaban%E6%BA%90%E7%A0%81%E8%BF%BD%E8%B8%AA(2)-%E2%80%94%E2%80%94-%E6%89%8B%E5%8A%A8%E8%A7%A6%E5%8F%91flow%E8%B0%83%E5%BA%A6%2F</url>
    <content type="text"><![CDATA[对应的操作是进入到某个project中，然后单次执行下面的某个flow。会触发web-server这边与exec-server的远程调用去执行flow。 相关类 Web-server ExecutorServlet.doGet ExecutorManager exec-server ExecutorServlet.doGet FlowRunnerManager FlowRunner ExecutableFlowBase ExecutableNode JobRunner JobTypeManager Job AzkabanProcessBuilder ####]]></content>
      <tags>
        <tag>youdaonote</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（8）]]></title>
    <url>%2F2016%2F08%2F19%2Fakka-in-action-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%2F</url>
    <content type="text"><![CDATA[系统结构。 pipe和filter模型 scatter-gather模型 路由 Recipient list Aggregator Become/Unbecome 如果有很多工作单元并行运行的话，那么怎样协调它们的工作？有一些可以并行执行，有的任务A需要在前置任务B完成之后才能执行。 通过实现几个经典的Enterprise Integration Pattern，Akka使我们能够充分利用好它的并发功能。 先了解以下最简单的pipe和filter模型，它是基于消息传递的系统的默认模型，经典模型是顺序执行。然后介绍 Scatter-Gather模型，这种模型提供了各种使任务并行的方式。 8.1 pipe 和 filterpipe的定义就是，一个进程或者线程把它的结果交给下一个进程或线程作为输入。许多人都是从它的发源地unix了解到pipe模型的。这些pipe组件的组合，被称为pipeline。 8.1.1 Enterprise integration pattern Pipes and Filters]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（7）]]></title>
    <url>%2F2016%2F08%2F17%2Fakka-in-action-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%887%EF%BC%89%2F</url>
    <content type="text"><![CDATA[配置 日志 单点app web app 发布 只是了解actor和ActorSysten显然是不够的，一个完成的app在发布之前应该要包含其他必备功能。 7.1 配置akka使用了Typesafe的配置库。可以定义一些属性，然后直接在代码里引用。还有优雅的方式自动按照时间merge并override多个文件中的多个属性。对于配置还有一个比较重要的就是能够分test, dev, prod环境。 7.1.1 牛刀小试首先看一下Typesafe定义配置文件的实例： 我们使用ConfigurationFactory来加载配置文件；1val config = ConfigFactory.load() 它会按照以下顺序找默认的配置文件： application.properties application.json application.conf. 这种说明一下是HOCON格式的，基于json，但是可读性更强 也可以同时支持以上所有的文件类型：123456789hostname=&quot;localhost&quot; MyAppl &#123; version = 10 description = &quot;My application&quot; database &#123; connect=&quot;jdbc:mysql://localhost/mydata&quot; user=&quot;me&quot; &#125; &#125; 对于大多数app来说，这些配置就已经足够使用了。这种配置可读性强，易于分组管理属性，JDBC就是一个绝佳的例子。对于依赖注入的框架，需要把item写进控制他们的对象中去，这里有一个更简单的方法。 下面看一下怎样使用上面定义的属性：12val applicationVersion = config.getInt("MyAppl.version")val databaseConnectSting = config.getString("MyAppl.database.connect") 也可以分开获取连接串12val databaseCfg = configuration.getConfig("MyAppl.database")val databaseConnectSting = databaseCfg.getString("connect") 配置文件中包含的属性一般都是app名称、版本号等，一般会在程序中多次使用它们。 我们还可以使用环境变量来设置变量：1hostname=$&#123;?HOST_NAME&#125; ？代表我们会在系统环境变量中获取该变量的值。因为我们并不能保证有这个环境变量，所以还是先定义一个默认值：12hostname=&quot;localhost&quot;hostname=$&#123;?HOST_NAME&#125; 7.1.2 使用默认值还是看JDBC配置。开发环境我们可能配置地址为localhost，但是有人要看demo的话，肯定要改成别的地址。最简单的方法就是复制配置文件，重新命名这个文件，然后让代码读取这个文件的配置内容。问题是这样我们就在两个位置分别有了两个可以相互替代的配置文件。如果还要切换新环境的话，就成了3个了。 配置库包含一个fall-back机制，默认配置会被放进一个对象中，然后这个对象被作为fall-back机制的配置source。见下图： 注意：这个机制里使用到的属性必须在配置文件中存在默认值，如果找不到的话，就会报错了。 fall-back机制为我们提供默认的配置，那么我们怎样配置默认值呢？应该在jar文件的根目录下的reference.conf配置，每个库都应该包含自己的默认配置。配置库会找到所有的reference.conf文件，整个这些配置到fall-back结构中。这样，某个库所需要的所有的属性都有了默认值。 前面说到配置库是支持多种配置文件的，你可以在一个app中使用多个不同类型的配置文件。每个文件都可以作为下一个文件的fall-back。 下图展示了配置库使用多个文件构建配置tree的优先级： 多数app只需要一种配置文件就可以了。但是如果你定了多个，那么默认值之间的覆盖关系就是上图中，上面的覆盖下面的配置。 默认读取文件是 application.{conf,json,properties}。有两种方式自定义配置文件，一种是load的时候指定1val config = ConfigFactory.load("myapp") 这样它就会去找myapp.{conf,json,properties}。 另一种方式是使用系统变量。有时这种使用起来比较方便，我们可以构建bash脚本的时候给app指定(比去jar和war里面找要好一些)。 config.resource 此变量指定项目resource中配置文件完整名称，例如： application.conf config.file 文件系统中配置文件的全名 config.url 当使用load不传参的时候，就会使用系统属性了，使用系统属性指定的配置文件的时候，就不会再去搜索各种文件类型了：conf, json and properties。 7.1.3 akka配置上面介绍了怎样配置app的属性，但是如果我们想修改akka本身的一些配置怎么办呢？akka是怎样使用配置库的呢？有可能是有多个ActorSystem，每个Actorsystem都有自己对应的配置。当创建ActorSystem的时候没有指定配置的话，ActorSystem会使用默认值自己创建。 1val system = ActorSystem("mySystem") 这里内部调用了ConfigFactory.load()。 我们也可以指定配置文件位置：12val configuration = ConfigFactory.load("mysystem")val systemA = ActorSystem("mysystem",configuration) 这个配置就已经存在于这个app了，可以通过如下方式访问；123val mySystem = ActorSystem("myAppl")val config = mySystem.settings.configval applicationDescription = config.getString("myAppl.name") 7.1.4 多系统有时候，在单个实例上我们需要启动多个子系统，每个子系统应该有自己的配置。 假如我们使用了多个JVM，他们基于同一个jar文件。我们就是用系统变量来解决这个问题。每启动一个新的进程，就是用一个不同的配置文件。但是配置里多数都是相同的，只有少数几个配置需要修改。我们使用include来解决这个问题 下面是baseConfig.conf1234MyAppl &#123; version = 10 description = &quot;My application&quot; &#125; 我们使用一个共享变量version，还有不同的变量description。下面是子系统的配置文件subAppl.conf1234include &quot;baseConfig&quot;MyAppl &#123; description = &quot;Sub Application&quot;&#125; 注意include的位置。 如果子系统运行在同一个jvm里呢？那样我们就不能使用系统变量来读取其他的配置文件了。那我们就把两个系统的配置文件合二为一，指定app名称，下面是combined.conf123456789MyAppl &#123; version = 10 description = &quot;My application&quot;&#125;subApplA &#123; MyAppl &#123; description = &quot;Sub application&quot; &#125;&#125; 这里我们使用了subApplA的子树，然后把它放在了configuration chain的上端。This is called lifting a configuration, because the configuration path is shortened.。见下图； 当我们请求MyAppl.description的时候，返回”Sub application”因为它被设置在配置中的最高层级上，请求MyAppl.version获得10，因为配置的高层级上没有它的定义，所以它使用fall-back机制获取到了10. 下面代码展示了怎样同时使用lift和callback，注意fallback被chain了起来：1234val configuration = ConfigFactory.load("combined") val subApplACfg = configuration.getConfig("subApplA") // 把configuration作为fall-back val config = subApplACfg.withFallback(configuration) 说的不多，但是这些配置就已经够用了。 7.2 日志下一个app必备的功能就是写日志。日志库这个东西众口难调，akka实现了一个日志adapter支持多种日志框架。日志关心两个东西，定义log级别以及定义log内容。 7.2.1 Akka app中记录日志1234class MyActor extends Actor &#123; val log = Logging(context.system, this) ... &#125; 需要注意的是，创建logger的时候需要使用ActorSystem。 logging adapter使用ActorSystem的eventStream发送日志信息给eventhandler。eventStream是Akka中的订阅系统。eventhandler收到日志信息后，使用定义的日志框架记录下来。这样，每个actor都能记录日志，而只有一个eventHandler actor依赖指定的日志框架。另一个好处就是IO，IO在并发环境下会特别慢。在高性能的app里，我们不希望等待打日志的过程。使用eventHandler就不用任何等待。下面列出了默认创建event-handler需要的配置信息：1234567akka &#123; # Event handlers to register at boot time # (Logging$DefaultLogger logs to STDOUT) event-handlers = [&quot;akka.event.Logging$DefaultLogger&quot;] # Options: ERROR, WARNING, INFO, DEBUG loglevel = &quot;DEBUG&quot; &#125; eventHandler并没有使用任何log框架，而是把log到了标准输出。下面是一个自定义的eventHandler代码：123456789101112131415161718192021import akka.event.Logging.InitializeLoggerimport akka.event.Logging.LoggerInitializedimport akka.event.Logging.Errorimport akka.event.Logging.Warningimport akka.event.Logging.Infoimport akka.event.Logging.Debugclass MyEventListener extends Actor&#123; def receive = &#123; case InitializeLogger(_) =&gt; sender ! LoggerInitialized case Error(cause, logSource, logClass, message) =&gt; println( "ERROR " + message) case Warning(logSource, logClass, message) =&gt; println( "WARN " + message) case Info(logSource, logClass, message) =&gt; println( "INFO " + message) case Debug(logSource, logClass, message) =&gt; println( "DEBUG " + message) &#125;&#125; 这只是一个简单的示例。Akka内置了两个eventHandler的实现，一个是上面的输出到STDOUT的，还有一个就是SLF4J。需要在配置文件中添加：12345678akka &#123; event-handlers = [&quot;akka.event.slf4j.Slf4jEventHandler&quot;] # Options: ERROR, WARNING, INFO, DEBUG loglevel = &quot;DEBUG&quot; &#125; 7.2.2 使用日志功能重新定义logger：1234class MyActor extends Actor &#123; val log = Logging(context.system, this) ... &#125; 第二个参数就是日志的source。这里就是这个类的实例。可以mixin ActorLogging trait来简化代码。 打日志过程中还可以使用placeholder1log.debug("two parameters: &#123;&#125;, &#123;&#125;", "one","two") 7.2.3 控制日志功能有时我们想自由控制日志的输出内容，比如查看生产环境的某些信息。看一下这个配置12345678910111213141516171819202122232425262728293031323334353637akka &#123; # logging must be set to DEBUG to use any of the options below loglevel = DEBUG # Log the complete configuration at INFO level when the actor # system is started. This is useful when you are uncertain of # what configuration is used. log-config-on-start = on debug &#123; # logging of all user-level messages that are processed by # Actors that use akka.event.LoggingReceive enable function of # LoggingReceive, which is to log any received message at # DEBUG level # 把所有启用了akka.event.LoggingReceive的收到的user-leve的debug level的日志信息都打印出来 receive = on # enable DEBUG logging of all AutoReceiveMessages # (Kill, PoisonPill and the like) autoreceive = on # enable DEBUG logging of actor lifecycle changes # (restarts, deaths etc) lifecycle = on # enable DEBUG logging of all LoggingFSMs for events, # transitions and timers fsm = on # enable DEBUG logging of subscription (subscribe/unsubscribe) # changes on the eventStream event-stream = on &#125; remote &#123; # If this is &quot;on&quot;, Akka will log all outbound messages at # DEBUG level, if off then they are not logged log-sent-messages = on # If this is &quot;on,&quot; Akka will log all inbound messages at # DEBUG level, if off then they are not logged log-received-messages = on &#125;&#125; 对应使用logger的代码是：12345class MyActor extends Actor with ActorLogging &#123; def receive = LoggingReceive &#123; case ... =&gt; ... &#125;&#125; 现在，只要我们设置属性akka.debug.receive 为on，我们actor收到的所有message就都可以被log出来了。 好了。日志就介绍到这里。 7.3 发布基于Akka的app这一节介绍发布应用的两种方式，一种是独立app，另一种是基于play-min的web app。 7.3.1 独立app我们使用akka的插件MicroKernel创建独立app。先弄一个HelloWorld Actor，只接收message，恢复hello。123456789class HelloWorld extends Actor with ActorLogging &#123; def receive = &#123; case msg:String =&gt; val hello = "Hello %s".format(msg) sender ! hello log.info("Sent response &#123;&#125;",hello) &#125;&#125; 下面我们创建HelloWorldCaller，它负责调用上面的 HelloWorld actor。12345678910111213141516171819class HelloWorldCaller(timer:Duration, actor:ActorRef) extends Actor with ActorLogging &#123; case class TimerTick(msg:String) override def preStart() &#123; super.preStart() // 使用akka scheduler给自己发送message context.system.scheduler.schedule( timer, // 首次触发之前的时长 timer, // 周期 self, // 发往的actor，这里是自己 new TimerTick("everybody")) // 发出的message &#125; def receive = &#123; case msg: String =&gt; log.info("received &#123;&#125;",msg) case tick: TimerTick =&gt; actor ! tick.msg &#125;&#125; 这个actor使用内置的schuduler周期性的生成message。每次收到TimerTick，我们都给构造方法中传进来的actor发送一个这个tick的message。当接收到一个String的message的时候，就用日志记录下来。 下面使用akka内核来差un构建系统，这个内核包含了一个启动接口，我们就是要实现这个接口。1234567891011121314151617181920import akka.actor.&#123; Props, ActorSystem &#125;import akka.kernel.Bootableimport scala.concurrent.duration._// 继承Bootable trait，启动app的时候就会被调用class BootHello extends Bootable &#123; val system = ActorSystem("hellokernel") // 当app启动的时候，会调用这个接口方法 def startup = &#123; val actor = system.actorOf(Props[HelloWorld]) val config = system.settings.config val timer = config.getInt("helloWorld.timer") system.actorOf(Props(new HelloWorldCaller(timer millis, actor))) &#125; def shutdown = &#123; system.shutdown() &#125;&#125; 现在我们已构建好了系统，还需要一些资源让我们的app运行起来。我们先写一下配置文件reference.conf123helloWorld &#123; timer=5000 &#125; 我们的默认timer值是5000毫秒。注意reference.conf文件需要放在jar文件的根目录下。下面我们在application.conf里配置logger相关12345678akka &#123; event-handlers = [&quot;akka.event.slf4j.Slf4jEventHandler&quot;] # Options: ERROR, WARNING, INFO, DEBUG loglevel = &quot;DEBUG&quot; &#125; 万事俱备。下面使用 akka-sbt-plugin 来创建依赖配置文件, 编辑plugins.sbt1234resolvers += &quot;Typesafe Repository&quot; at &quot;http://repo.akka.io/releases/&quot; addSbtPlugin(&quot;com.typesafe.akka&quot; % &quot;akka-sbt-plugin&quot; % &quot;2.0.1&quot;) 为不熟悉sbt的同学解释一下。第一行添加了repository的地址。然后是一个空行，这个注意一下，它代表前一行完事儿了。下一行定义了我们需要的插件。 然后创建build文件， HelloKernelBuild.scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667import sbt._ import Keys._ import akka.sbt.AkkaKernelPlugin import akka.sbt.AkkaKernelPlugin.&#123; Dist, outputDirectory, distJvmOptions &#125; object HelloKernelBuild extends Build &#123; lazy val HelloKernel = Project( id = "hello-kernel-book", base = file("."), settings = defaultSettings ++ AkkaKernelPlugin.distSettings // 添加akka插件的功能 ++ Seq( // 构建需要的依赖 libraryDependencies ++= Dependencies.helloKernel, distJvmOptions in Dist := "-Xms256M -Xmx1024M", // dist输出目录 outputDirectory in Dist := file("target/helloDist") ) ) lazy val buildSettings = Defaults.defaultSettings ++ Seq( organization := "com.manning", version := "0.1-SNAPSHOT", scalaVersion := "2.9.1", crossPaths := false, organizationName := "Mannings", organizationHomepage := Some(url("http://www.mannings.com")) ) lazy val defaultSettings = buildSettings ++ Seq( resolvers += "Typesafe Repo" at "http://repo.typesafe.com/typesafe/releases/", // compile options scalacOptions ++= Seq("-encoding", "UTF-8", "-deprecation", "-unchecked"), javacOptions ++= Seq("-Xlint:unchecked", "-Xlint:deprecation") )&#125; // Dependencies object Dependencies &#123; import Dependency._ val helloKernel = Seq(akkaActor, akkaKernel, akkaSlf4j, lf4jApi, slf4jLog4j, Test.junit, Test.scalatest, Test.akkaTestKit) &#125;object Dependency &#123; // Versions object V &#123; val Scalatest = "1.6.1" val Slf4j = "1.6.4" val Akka = "2.0" &#125; // Compile val commonsCodec = "commons-codec" % "commons-codec"% "1.4" val commonsIo = "commons-io" % "commons-io" % "2.0.1" val commonsNet = "commons-net" % "commons-net" % "3.1" val slf4jApi = "org.slf4j" % "slf4j-api" % V.Slf4j val slf4jLog4j = "org.slf4j" % "slf4j-log4j12"% V.Slf4j val akkaActor = "com.typesafe.akka" % "akka-actor" % V.Akka val akkaKernel = "com.typesafe.akka" % "akka-kernel" % V.Akka val akkaSlf4j = "com.typesafe.akka" % "akka-slf4j" % V.Akka val scalatest = "org.scalatest" %% "scalatest" % V.Scalatest object Test &#123; val junit = "junit" % "junit" % "4.5" % "test" val scalatest = "org.scalatest" %% "scalatest" % V.Scalatest % "test" val akkaTestKit ="com.typesafe.akka" % "akka-testkit" % V.Akka % "test" &#125; &#125; 万事俱备了。进入项目根目录，启动sbt，执行dist命令12345678sbt[info] Loading projectdefinition from J:\boek\manningAkka\listings\kernel\project[info]Set current project to hello-kernel-book (in build file:/J:/boek/manningAkka/listings/kernel/)&gt; dist 完事儿后，查看 /target/helloDist目录，可看到4个子目录： bin 启动脚本所在的地方，包含linux和window的 config 运行app所需的配置文件 deploy jar文件所在的地方 lib 依赖的jar文件所在的地方 使用启动脚本，启动我们的app：1./start BootHello 观察log文件。 7.3.2 创建web app相比上面独立app，这里并不多做什么。 Play-mini是基于Play的扩展。我们要做的app只是提供接口，并不提供界面。有其他的工具可以，我们只是觉得 Play-mini来做例子1234object PlayMiniHello extends Application &#123; lazy val system = ActorSystem("webhello") lazy val actor = system.actorOf(Props[HelloWorld])&#125; 只要继承Application, 就集成了好多的功能，就像前面的kernel一样。web app中需要创建路由。1234567object PlayMiniHello extends Application &#123; def route = &#123; case GET(Path("/test")) =&gt; Action &#123; Ok("TEST @ %s\n".format(System.currentTimeMillis)) &#125; &#125;&#125; 在面向服务的架构里，整个app甚至就可以仅仅基于几个这样的服务映射。相比上面的kernel方式，这是唯一的区别。下面我们创建hello请求的路由，它接收一个name参数，当没有这个参数的时候就使用我们配置里的默认值。下面是从REST路径里获取参数的方式1val writeForm = Form("name" -&gt; text(1,10)) 这里使用了Play的From，约束长度只能在1到10之间。如果获取失败的话，就获取配置参数helloWorld.name。123456789101112131415case GET(Path("/hello")) =&gt; Action &#123; // 把request定已成implicit类型，方便下面进行变量绑定 implicit request =&gt; val name = try &#123; // 将request和我们的form进行绑定匹配 writeForm.bindFromRequest.get &#125; catch &#123; case ex:Exception =&gt; &#123; log.warning("no name specified") // 获取默认的配置参数 system.settings.config.getString("helloWorld.name") &#125; &#125; ...&#125; 现在有了name，我们可以发送信息给指定的actor。但是在创建response之前我们必须等着结果，可我们不希望阻塞。所以我们返回AsyncResult，它需要一个promise。promise很像Futrue，只是Futrue是在client端使用的。有了promise，结果就一直等在producer那儿，完成处理后producer会提供这个结果。关于Futrue和Promise的更多细节会在下一章涉及。123456789101112AsyncResult &#123; // 使用ask方法发送request val resultFuture = actor ? name // 使用futrue创建一个promise val promise = resultFuture.asPromise // 返回结果后，就创建response promise.map &#123; case res:String =&gt; &#123; Ok(res) &#125; &#125;&#125; 要使用 HelloWorld actor的response创建一个response，我们必须使用ask方法。ask方法返回一个Futrue，使用Future创建一个Promise。最后一步就是使用返回的结果填充进Promise。 如果超时没有收到response呢，这样map方法就不会被调用，promise里也不会被填充结果。要解决这个问题，我们继承Future，当失败的时候返回一个字符串。123456789val resultFuture = actor ? name recover &#123; case ex:AskTimeoutException =&gt; "Timeout" case ex:Exception =&gt; &#123; log.error("recover from "+ex.getMessage) "Exception:" + ex.getMessage &#125; &#125; 总结一下变化： 从resource文件中加载默认属性 抽取path中的参数 处理请求超时 并发地处理所有问题 用下面这个例子来总结一下这一章的所学：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061// 实现 Application traitobject PlayMiniHello extends Application &#123; lazy val system = ActorSystem("webhello") lazy val actor = system.actorOf(Props[HelloWorld]) implicit val timeout = Timeout(1000 milliseconds) val log = Logging(system,PlayMiniHello.getClass) def route = &#123; case GET(Path("/test")) =&gt; Action &#123; Ok("TEST @ %sn".format(System.currentTimeMillis)) &#125; // 使用 helloworld actor 创建一个request case GET(Path("/hello")) =&gt; Action &#123; // 设置request为implicit implicit request =&gt; val name = try &#123; // 获取name参数，具体参考Play文档 writeForm.bindFromRequest.get &#125; catch &#123; case ex:Exception =&gt; &#123; log.warning("no name specified") system.settings.config.getString("helloWorld.name") &#125; &#125; // 由于不想阻塞，我们返回AsyncResult AsyncResult &#123; // 担心超时，创建一个recover回调函数 val resultFuture = actor ? name recover &#123; case ex:AskTimeoutException =&gt; "Timeout" case ex:Exception =&gt; &#123; log.error("recover from "+ex.getMessage) "Exception:" + ex.getMessage &#125; &#125; val promise = resultFuture.asPromise promise.map &#123; case res:String =&gt; &#123; log.info("result "+res) Ok(res) &#125; case ex:Exception =&gt; &#123; log.error("Exception "+ex.getMessage) Ok(ex.getMessage) &#125; case _ =&gt; &#123; Ok("Unexpected message") &#125; &#125; &#125; &#125; &#125; val writeForm = Form("name" -&gt; text(1,10))&#125; 此外我们还要处理一些小事情，指定启动app的类，这里是”PlayMiniHello.”。1object Global extends com.typesafe.play.mini.Setup(ch04.PlayMiniHello) 继承了 play.api.GlobalSettings trait，这样我们就可以使用ActorSystem的onStart和onStop方法了。 编辑配置文件reference.conf123helloWorld &#123; name=world&#125; 编辑application.conf12345678helloWorld &#123; name=&quot;world!!!&quot;&#125;akka &#123; event-handlers = [&quot;akka.event.slf4j.Slf4jEventHandler&quot;] # Options: ERROR, WARNING, INFO, DEBUG loglevel = &quot;DEBUG&quot;&#125; 编辑项目构建文件Build.scala12345678910111213141516171819202122232425262728293031323334import sbt._import Keys._import PlayProject._object Build extends Build &#123; lazy val root = Project(id = "playminiHello", base = file("."), settings = Project.defaultSettings).settings( resolvers += "Typesafe Repo" at "http://repo.typesafe.com/typesafe/releases/", resolvers += "Typesafe Snapshot Repo" at "http://repo.typesafe.com/typesafe/snapshots/", libraryDependencies ++= Dependencies.hello, // 加上这一行，就可以支持sbt测试webapp了 mainClass in (Compile, run) := Some("play.core.server.NettyServer"))&#125;object Dependencies &#123; import Dependency._ val hello = Seq(akkaActor, akkaSlf4j, // slf4jLog4j, playmini )&#125;object Dependency &#123; // Versions object V &#123; val Slf4j = "1.6.4" val Akka = "2.0" &#125; // Compile val slf4jLog4j = "org.slf4j" % "slf4j-log4j12"% V.Slf4j val akkaActor = "com.typesafe.akka" % "akka-actor" % V.Akka val playmini = "com.typesafe" %% "play-mini" % "2.0-RC3" val akkaSlf4j = "com.typesafe.akka" % "akka-slf4j" % V.Akka&#125; 进入sbt，运行run。app会运行在9000端口上，可以使用curl进行测试了。 7.4 总结写的太绕口了。。不翻译了。。。。。akka各种牛逼。。。]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（6）]]></title>
    <url>%2F2016%2F08%2F15%2Fakka-in-action-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%886%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这一章我们介绍： scale out 分布式的卖票app 远程使用 测试分布式的actor系统 这一章中，我们会拿出第二章的例子来进行scale out，也就是在更多的机器上部署同一个app。【回忆一下scale up，在更多的jvm上部署同一个app】 这一章只介绍scale out，下一章介绍集群使用。13章会深入scale out的一些细节。 6.1 scale out不要指望一下子就能够把任何app扩展到几千台机器，分布式计算是众所周知的难点。akka只是让分布式计算简单一些，让并发编程简单一些，我们这里还是使用 GoTicks.com 项目，把它弄成分布式的。 许多网络技术都使用RPC，这种方式尽量掩饰本地调用与远程调用的不同，主要思想是既然本地调用很简单，那么就让编程人员像调用本地方法一样调用远程方法。这种通信方式对于点对点的server连接是有效的，但是对于大型网络并不是一个好的方案。面向message的中间件可以解决这个问题，但使app理解吸收message系统到app层的代价有些大。akka另辟蹊径，远程协作者相对透明，不用修改akka代码。 进入探讨之前，先看一下网络拓扑的例子和一些通用术语。要是你对这些已经很了解了就直接跳到6.2. 6.1.1 通用网络拓扑 Node: 在网络中运行着的一个app，是网络拓扑的一个连接点。一个server上可以有多个node。 Role： 每个node在分布式系统中都有自己指定的Role。它代表着这个node可以执行一些列的特定任务。 transport protocol ： node使用指定的传输协议来互相沟通，比如TCP/IP membership：当许多node在同一个分布式系统中的时候，他们公用一个group membership。这个membership可以是静态或动态的。静态是指node的数量和角色都是固定不变的。动态的是允许node自由加入或离开网络，也可能在不同时刻担任不同的角色。静态的最简单，也最脆弱。动态的则可以随着node的增多动态调整node之间的关系，可以应对一些错误问题。当有node加入或者离开网络的时候，需要有一种discovery机制。 6.1.2 为何要分布式编程模型扩展多n多node的第一步是县弄出一个本地的app支持分布逻辑？我们可以只定义抽象的处理逻辑，然某个牛叉的工具去处理这些转化细节么？答案是no。只指出分布式和本地环境的区别是不够的。根据论文 A Note on Distributed Computing所说，在本地编程和分布式编程，有4个重要的区别：延迟、内存访问、部分失败、并发。下面是一个简单的总结： 延迟：在协作者之间出现网络意味着每个message都需要更长的时间，可能由于各种网络原因造成延迟 部分失败：当系统的部分功能并不总是可见、有时消失有时又出现，我们很难确定整个系统都能正常工作。 内存访问： 在本地系统中获取一个内存对象的引用基本不会失败，但是对于分布式系统就失败概率就变大了。 并发： no one ‘owner’ of everything 由于以上因素，我们不能在分布式环境中使用本地编程模型。akka的切入点恰恰相反，只需要一个分布式编程模型，同时适用于本地系统和分布式系统。前面提到的论文也提到这个观点，但是有些关心会不会把编程难度提高到分布式编程的水平。 时代变了。前面章节我们介绍到了并发编程的易用性。我们已经熟用了异步迭代，部分失败处理，并发之间不共享任何状态，简化了多核编程，我们已经准备好弄一个分布式环境了。 6.2 scale out with remoting我们先选区一个最简单的网络拓扑模型：使用两个node组成client-server 静态网络拓扑。两个node一个是前端，一个是后端。REST接口运行在前端node上，BoxOffice和所有的TicketSeller运行在后端node上。node之间互相有一个静态的网络地址引用。如下图： 我们需要使用到akka-remote module。 在本地模式中，当有新的Event的时候，BoxOffice Actor就创建TicketSeller actor。在client-server拓扑中也是这样。只是我们可以看到使用akka-remote之后就可以远程创建与发布actor了。前端使用它知道的地址在后端找负责创建TicketSeller的BoxOffice actor。注意还有一个变化就是前端在后端远程部署了一个BoxOffice actor。 6.2.1 动手写程序在源码的chapter6文件夹里有一个对chapter2的升级版。 首先我们修改以下sbt文件，添加akka-remote和akka-mutlnode-testkit依赖：12&quot;com.typesafe.akka&quot; %% &quot;akka-remote&quot; % akkaVersion,&quot;com.typesafe.akka&quot; %% &quot;akka-multi-node-testkit&quot; % akkaVersion % &quot;test&quot;, 调用sbt update来pull这些依赖。 6.2.2 远程REPLakka提供两种方式来获取远程node上的actor引用。一种是通过path，另一种是创建actor，获取它的引用，然后远程发布。先介绍第一种。 REPL控制台是一个很好的交互工具。进入到chapter6目录下，执行sbt console进入REPL，我们需要开启两个session，也就是两个REPL。为了使远程生效，我们需要在src/main/resources下提供一个配置文件application.conf，REPL启动的时候会自动加载它。 在REPL中执行:paste后，贴入以下代码：12345678910111213141516val conf="""akka &#123; actor &#123; provider = "akka.remote.RemoteActorRefProvider" // 选择 Remote ActorRef Provider来启动远程 &#125; // 远程配置 remote &#123; enabled-transports = ["akka.remote.netty.tcp"] // 启用TCP netty.tcp &#123; hostname = "0.0.0.0" port = 2552 &#125; &#125;&#125;""" 我们会将这段配置信息加载进ActorSystem。需要注意的是它定义了一个ActorRefProvider来启动akka-remote module。顾名思义，他就是负责远程Actor的ActorRef。 下面代码首先引入必须的配置和actor包，然后把配置加载进了ActorSystem：1234567891011121314151617scala&gt; import com.typesafe.config._ import com.typesafe.config._ scala&gt; import akka.actor._ import akka.actor._// 解析字符串配置为对象config scala&gt; val config = ConfigFactory.parseString(conf) config: com.typesafe.config.Config = ....// 使用config对象创建ActorSystem scala&gt; val backend = ActorSystem("backend", config) [Remoting] Starting remoting ..... [Remoting] Remoting now listens on addresses: [akka.tcp://backend@0.0.0.0:2551] backend: akka.actor.ActorSystem = akka://backend 随着敲完最后一行，我们启动了一个启动了远程的ActorSystem。通过启动远程的config对象，我们启动了后端ActorSystem。如果忘了没有使用这个config，默认akka的application.conf里是不启用远程功能的。下面我们弄一个直接输出输入数据的小的actor：123456789101112scala&gt; :paste// Entering paste mode (ctrl-D to finish)class Simple extends Actor &#123;def receive = &#123;case m =&gt; println(s"received $m!")&#125;&#125;// Exiting paste mode, now interpreting.// 使用后端ActorSystem创建这个简单的Actor并命名为simplescala&gt; backend.actorOf(Props[Simple], "simple") 这个简单的actor已经运行在后端ActorSystem上了，并且有了名字simple，这样其他的node就可以找到它了。下面我们另起一个终端，开始编写前端：12345678910111213141516171819202122232425262728val conf = """akka &#123; actor &#123; provider = "akka.remote.RemoteActorRefProvider" &#125; remote &#123; enabled-transports = ["akka.remote.netty.tcp"] netty.tcp &#123; hostname = "0.0.0.0" port = 2551 // 注意端口不同 &#125; &#125;&#125;""" import com.typesafe.config._ import akka.actor._ val config = ConfigFactory.parseString(conf) val frontend= ActorSystem("frontend", config) [Remoting] Starting remoting ..... [Remoting] Remoting now listens on addresses: [akka.tcp://backend@0.0.0.0:2552] frontend: akka.actor.ActorSystem = akka://frontend config同样被加载进了前端的actorSystem。前端actorSystem已经启用了远程，而且启动起来了。下面我们想办法在前端获取Simple Actor的引用。我们首先构建一个actor path。下图是path的组成： 我们可以使用前端actorSystem的actorSelection方法通过path获取到后端ActorSystem的simple ActorRef：12345678scala&gt; :paste// Entering paste mode (ctrl-D to finish)val path = "akka.tcp://backend@0.0.0.0:2551/user/simple"val simple = frontend.actorSelection(path)// Exiting paste mode, now interpreting.path: String = akka.tcp://backend@0.0.0.0:2551/user/simplesimple: akka.actor.ActorSelection =ActorSelection[Actor[akka.tcp://backend@0.0.0.0:2551/]/user/simple] ctorSelection可以用来发送一些message给所有符合条件的actor。下面我们就在前端发送一个信息给后端：1scala&gt; simple ! "Hello Remote World!" 切换到后端的终端上，可以看到，我们收到并打印出了这条message：1scala&gt; received Hello Remote World!! 过程是message被序列化，发送给一个TCP socket，通过远程module收到，然后反序列化，再交给后端的Simple Actor处理。 6.2.3 远程查找我们并不会直接在RestInterface actor中直接创建BoxOffice actor，而是在后端node上找BoxOffice。如下图：前面版本的RestInterface是直接创建的BoxOffice actor：1val boxOffice = context.actorOf(Props[BoxOffice], "boxOffice") 这样调用的话二者就是直接的父子关系。为了让app更灵活一些，使之也能适应分布式系统，我们把这段代码放进一个trait，然后后面再mixin。12345678910111213141516// 这个trait必须被mixin进一个actor才能使用contexttrait BoxOfficeCreator &#123; this: Actor =&gt; def createBoxOffice:ActorRef = context.actorOf(Props[BoxOffice],"boxOffice")&#125;// RestApi trait包含了RestInterface actor的所有逻辑class RestInterface extends HttpServiceActor with RestApi &#123; def receive = runRoute(routes)&#125;trait RestApi extends HttpService with ActorLogging with BoxOfficeCreator &#123; actor: Actor =&gt; val boxOffice = createBoxOffice //使用BoxOfficeCreator trait中定义的方法创建BoxOffice ...... ..... 这样我们把创建BoxOffice的代码抽取到了一个trait中，然后在RestInterface把它设置为创建本地boxOffice的默认方法。 下面就定义三个入口方法：SingleNodeMain、FrontendMain、BackendMain。 123456789101112131415161718192021222324// 单点 val system = ActorSystem("singlenode", config) val restInterface = system.actorOf(Props[RestInterface], "restInterface")// FrontendMain val config = ConfigFactory.load("frontend") val system = ActorSystem("frontend", config) // mixin RemoteBoxOfficeCreator trait class FrontendRestInterface extends RestInterface with RemoteBoxOfficeCreator // 使用mixin的RemoteBoxOfficeCreator trait的FrontendRestInterface创建rest接口 val restInterface = system.actorOf(Props[FrontendRestInterface], "restInterface")// BackendMain val config = ConfigFactory.load("backend") val system = ActorSystem("backend", config) system.actorOf(Props[BoxOffice], "boxOffice") 以上三个入口分别加载的是singlenode.conf, frontend.conf ， backend.conf配置文件里的配置。frontend.conf相对多出一个部分来指定怎样找boxoffice actor。RemoteBoxOfficeCreator负责加载这部分配置：1234567backend &#123; host = &quot;0.0.0.0&quot; port = 2552 protocol = &quot;akka.tcp&quot; system = &quot;backend&quot; actor = &quot;user/boxOffice&quot;&#125; boxoffice actor的路径是从这个配置部分获取并构建的。下面就是像前面REPL一样使用Actor Selection获取远程Actor，一旦确认后端正常，我们就可以尝试发送message了。这次我们希望使用ActorRef而不是单节点那样使用，下面是RemoteBoxOfficeCreator的代码：123456789101112131415161718192021object RemoteBoxOfficeCreator &#123; val config = ConfigFactory.load("frontend").getConfig("backend") val host = config.getString("host") val port = config.getInt("port") val protocol = config.getString("protocol") val systemName = config.getString("system") val actorName = config.getString("actor")&#125;trait RemoteBoxOfficeCreator extends BoxOfficeCreator &#123; this:Actor =&gt; import RemoteBoxOfficeCreator._ def createPath:String = &#123; s"$protocol://$systemName@$host:$port/$actorName" &#125; override def createBoxOffice:ActorRef = &#123; val path = createPath context.actorOf(Props(classOf[RemoteLookup],path), "lookupBoxOffice") // 创建一个负责寻找boxoffice actor的actor，这个actor的参数是远程boxoffice的path &#125;&#125; RemoteBoxOfficeCreator创建了一个额外的actor RemoteLookup来寻找boxOffice。在前面版本的akka里你可能会使用actorFor方法来直接获取一个远程actor的ActorRef。但是这个方法目前已经过期了，因为如果actor本身挂掉的话，他返回的ActorRef的表现并不完全像本地actor一样。还有就是通过actorFor返回的ActorRef可以指向一个新建的远程actor实例，这对本地context来说绝无可能。再一个原因就是不能像本地actor那样监控它的终结message。所以把这个方法废除了。取而代之的是RemoteLookup actor： 后端ActorSystem可能还没有启动，或者暂时挂掉了，正在重启 boxOffice actor本身也可能挂掉重启中 理想状态下我们能先于前端启动后端 RemoteLookup负责处理以上这些场景。下路是RemoteLookup在RestInterface和BoxOffice的作用，它主要负责传递message给RestInterface： RemoteLookup actor是一个只包含两种状态的状态机：identify或者active。它使用become方法在这两种状态之间进行切换。当RemoteLookup想要获取一个BoxOffice的ActorRef的时候，如果它暂时还没有，就是在identify状态。不然它就是在active状态，传递所有的message到一个有效的BoxOffice ActorRef。 如果RemoteLookup发现BoxOffice已经被终结了，他会在接收不到message的一段时间后，重新获取一个有效的ActorRef。我们使用RemoteDeath监控这种情况。这东西我们并不陌生，跟普通的actor监控是一样的：123456789101112131415161718192021222324252627282930313233343536373839404142434445class RemoteLookup(path:String) extends Actor with ActorLogging &#123; // 若3秒内没有收到任何message，就发送一个ReceiveTimeout消息 context.setReceiveTimeout(3 seconds) // 马上请求actor的identify sendIdentifyRequest() def sendIdentifyRequest(): Unit = &#123; // 使用path获取actor val selection = context.actorSelection(path) // 向actorSelection发送一个Identify message selection ! Identify(path) &#125; def receive = identify def identify:Receive = &#123; case ActorIdentity(`path`, Some(actor)) =&gt; // 某个actor已经被确认(identified)了，返回它的ActorRef context.setReceiveTimeout(Duration.Undefined) // 既然当前actor已经是active了，就不用发送ReceiveTimeout了 log.info("switching to active state") context.become(active(actor)) // 修改状态 context.watch(actor) // 监视 case ActorIdentity(`path`, None) =&gt; // actor还不可用，后端连不上，或者还没有启动 log.error(s"Remote actor with path $path is not available.") case ReceiveTimeout =&gt; // 如果还没有收到message，就继续identify远程actor sendIdentifyRequest() case msg:Any =&gt; // 在identify接收状态下，不发送任何message log.error(s"Ignoring message $msg, remote actor is not ready yet.") &#125; def active(actor: ActorRef): Receive = &#123; case Terminated(actorRef) =&gt; // 如果远程的actor被终结，RemoteLookup应修改它的状态到identify接收状态 log.info("Actor $actorRef terminated.") log.info("switching to identify state") context.become(identify) context.setReceiveTimeout(3 seconds) sendIdentifyRequest() case msg:Any =&gt; actor forward msg // 当远程actor是active的时候，向下传递所有其他message &#125;&#125; 发送Identify message到ActorSelection来获取boxOffice的ActorRef。后端ActorSystem的远程module对应返回一个包含远程actor的ActorRef的 ActorIdentity message。 这样就完成了从一个单节点应用改到前后端node。除了能够远程通信，前端和后端还可以分开启动，前端会去找boxoffice，当boxoffice可用的时候就与之通信，不可用就采取其他措施。 最后就是运行FrontendMain和BackendMain。启动两个终端，分别使用sbt run指定入口类运行：12345678[info] ...[info] ... (sbt messages)[info] ...Multiple main classes detected, select one to run:[1] com.goticks.SingleNodeMain[2] com.goticks.FrontendMain[3] com.goticks.BackendMainEnter number: 测试一下如果kill掉后端，再重启。 远程event的生命周期默认是打在日志里的，可以帮主我们更快定位问题。我们还可以使用actorSystem的eventStream方法定于远程生命周期event，后面第十章会提到。考虑到连接管理，再考虑到我们可以像本地actor一样监视远程actor，我们就没必要再针对event做什么额外操作了。 重新看一下修改的地方： 抽出 BoxOfficeCreator trait。远程版本中向后端请求查找Boxoffice。 RemoteBoxOfficeCreator在RestInterface和BoxOffice之间添加了一个RemoteLookup actor。它负责传递他收到的所有的消息给boxoffice。它还identifyboxoffice的actorRef，并且远程监视boxoffice的actor。 6.2.4 远程发布前面说到akka提供了两种方式获取远程actor的ActorRef。下面我们介绍第二种，远程发布。 远程发布可以通过编程或配置实现。我们先弄一下配置方式的远程发布。它的好处就是不需要重新构建app就可以修改集群。标准的 BoxOfficeCreator trait 创建boxOffice，并把后者当做自己的一个子actor：1val boxOffice = context.actorOf(Props[BoxOffice], "boxOffice") local path是/restInterface/boxOffice，注意这里没有/user。当我们使用基于配置的远程发布的时候，我们只需要告诉前端actorSystem，创建/restInterface/boxOffice的actor的时候，应该是远程创建。通过如下配置实现：12345678910actor &#123; provider = &quot;akka.remote.RemoteActorRefProvider&quot; deployment &#123; /restInterface/boxOffice &#123; // 使用这个path的actor都应该被远程发布 remote = &quot;akka.tcp://backend@0.0.0.0:2552&quot; // 远程发布的目标地址，应该就是后端监听的端口 &#125; &#125;&#125; 基于编程的远程发布相比起来简直就没有存在的必要。大多数情况下我们都使用基于配置的远程发布。但是有时，假如你要通过CNAMES引用不同的node，写代码就方便些。当时用akka-cluster的时候，完全动态远程发布就很有意义了，因为它可以支持动态membership。下面是一个基于编程的远程发布实例：1234567val uri = "akka.tcp://backend@0.0.0.0:2552" val backendAddress = AddressFromURIString(uri) // 从uri创建一个后端地址 // 使用远程发布范围创建一个Props val props = Props[BoxOffice].withDeploy( Deploy(scope = RemoteScope(backendAddress)) ) context.actorOf(props, "boxOffice") 上面代码也是在后端远程创建与发布了一个boxOffice。 注意，远程发布并不是akka把实际的BoxOffice的class文件传送给远程server实现，而是远程server本身就有这个actor。如果后端远程ActorSystem挂掉并重启，这个ActorRef并不会自动关联到新的远程actor实例。这个实例是通过远程发布的，所以后端ActorSystem中不能是已经启动了的。基于这两点改变，下面是新的Main方法:12345678910111213141516171819202122232425262728// 前端不再创建boxOfficeobject BackendRemoteDeployMain extends App &#123; val config = ConfigFactory.load("backend") val system = ActorSystem("backend", config)&#125;// 后端不再mixin一些trait，使用默认的BoxOfficeCreatorobject FrontendRemoteDeployWatchMain extends App &#123; val config = ConfigFactory.load("frontend-remote-deploy") val host = config.getString("http.host") val port = config.getInt("http.port") val system = ActorSystem("frontend", config) class RestInterfaceWatch extends RestInterface with ConfiguredRemoteBoxOfficeDeployment val restInterface = system.actorOf(Props[RestInterfaceWatch], "restInterface") Http(system).manager ! Bind(listener = restInterface, interface = host, port =port)&#125; 分别在两个终端下运行这两个main方法，尝试使用httpie创建event，会看到下面的message在前端ActorSystem打印出来：1234// very long message, formatted in a couple of lines to fit.INFO [RestInterface]: Received new event Event(RHCP,10), sending toActor[akka.tcp://backend@0.0.0.0:2552/remote/akka.tcp/ frontend@0.0.0.0:2551/user/restInterface/boxOffice#-1230704641] 这是说前端向后端发送了一个message来远程发布了boxOffice。 目前来说一切都顺利。可是有个问题：如果前端尝试远程发布的时候后端没有启动会咋样，因为还是会获得一个ActorRef。即便后端ActorSystem一会儿启动了，这个ActorRef还是不可用。要解决这个问题，我们还是要对这个ActorRef添加监视。我们需要像前面RemoteLookup actor那样放一个actor在RestInterface和boxOffice之间，这次是RemoteBoxOfficeForwarder。 我们需要稍微修改以下配置文件：12345678910actor &#123; provider = &quot;akka.remote.RemoteActorRefProvider&quot; deployment &#123; /restInterface/forwarder/boxOffice &#123; remote = &quot;akka.tcp://backend@0.0.0.0:2552&quot; &#125; &#125;&#125; 下面是ConfiguredRemoteBoxOfficeDeployment和RemoteBoxOfficeForwarder的代码：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647trait ConfiguredRemoteBoxOfficeDeployment extends BoxOfficeCreator &#123; this:Actor =&gt; // 创建一个forwarder来负责监视和发布远程的BoxOffice override def createBoxOffice = &#123; context.actorOf(Props[RemoteBoxOfficeForwarder], "forwarder") &#125;&#125;class RemoteBoxOfficeForwarder extends Actor with ActorLogging &#123; context.setReceiveTimeout(3 seconds) // 远程发布并监视BoxOffice deployAndWatch() def deployAndWatch(): Unit = &#123; val actor = context.actorOf(Props[BoxOffice], "boxOffice") context.watch(actor) log.info("switching to maybe active state") context.become(maybeActive(actor)) context.setReceiveTimeout(Duration.Undefined) &#125; def receive = deploying def deploying:Receive = &#123; case ReceiveTimeout =&gt; deployAndWatch() case msg:Any =&gt; log.error(s"Ignoring message $msg, remote actor is not ready yet.") &#125; def maybeActive(actor:ActorRef): Receive = &#123; case Terminated(actorRef) =&gt; // 远程发布的actor被终结，那就重新远程发布一个 log.info("Actor $actorRef terminated.") log.info("switching to deploying state") context.become(deploying) context.setReceiveTimeout(3 seconds) deployAndWatch() case msg:Any =&gt; actor forward msg &#125;&#125; RemoteBoxOfficeForwarder和上文的RemoteLookup特别像，都有一个状态机，不过它是 ‘deploying’ 或 ‘maybe active’。没有actor selection，我们不能确认远程actor是否已经真的被发布了。 为RemoteBoxOfficeForwarder添加远程actorSelection的任务就交给读者你了，就是当前’maybe’的状态机。 前端的main类mixin了ConfiguredRemoteBoxOfficeDeployment。看一下FrontendRemoteDeployWatchMain：12345class RestInterfaceWatch extends RestInterface with ConfiguredRemoteBoxOfficeDeploymentval restInterface = system.actorOf(Props[RestInterfaceWatch],"restInterface") 重新启动前后端的程序，测试一把。 我们把lookup和远程发布两种方式都可以使程序特别有韧性。即便在只有两个node的时候，我们最好也能在最开始就弄的特别有韧性。 6.2.5 Multi-JVM testingsbt multi-jvm插件可以用来进行跨JVM的测试，这对于分布式系统是很重要的。需要在sbt配置文件中添加：1addSbtPlugin(&quot;com.typesafe.sbt&quot; % &quot;sbt-multi-jvm&quot; % &quot;0.3.5&quot;) 我们需要额外添加另一个sbt构件文件来使用它。 Multi-JVM插件只支持scala DSL版本的sbt下过目文件，是；uoyi我们需要在chapter6/project文件夹下添加GoTicksBuild.scala文件。Sbt会自动merge build.sbt和GoTicksBuild.scala。12345678910111213141516171819202122232425262728293031import sbt._import Keys._import com.typesafe.sbt.SbtMultiJvmimport com.typesafe.sbt.SbtMultiJvm.MultiJvmKeys.&#123; MultiJvm &#125;object GoTicksBuild extends Build &#123; lazy val buildSettings = Defaults.defaultSettings ++ multiJvmSettings ++ Seq( crossPaths := false ) lazy val goticks = Project( id = "goticks", base = file("."), settings = buildSettings ++ Project.defaultSettings ) configs(MultiJvm) lazy val multiJvmSettings = SbtMultiJvm.multiJvmSettings ++ Seq( // make sure that MultiJvm test are compiled by the default test compilation compile in MultiJvm &lt;&lt;= (compile in MultiJvm) triggeredBy (compile in Test), // disable parallel tests parallelExecution in Test := false, // make sure that MultiJvm tests are executed by the default test target executeTests in Test &lt;&lt;= ((executeTests in Test), (executeTests in MultiJvm)) map &#123; case ((_, testResults), (_, multiJvmResults)) =&gt; val results = testResults ++ multiJvmResults (Tests.overall(results.values), results) &#125; )&#125; 如果你对sbt不熟，不用担心这个文件的细节。上面基本就是配置了multi-jvm插件，确认multi-jvm测试使用普通单元测试执行。要是想看看sbt的话，可以参考SBT in Action。 多jvm的测试默认是应该写在 src/multi-jvm/scala 文件夹下。既然我们的项目已经正确配置了，那么我们可以开始写代码测试goticks的前端和后端了。 首先定义MultiNodeConfig描述测试的node的角色。1234object ClientServerConfig extends MultiNodeConfig &#123; val frontend = role("frontend") val backend = role("backend")&#125; 定义了两个role。role用来确认参与测试的node运行的代码，以便于针对性的测试。在写测试之前我们需要些一些基础设施代码： 1234567trait STMultiNodeSpec extends MultiNodeSpecCallbackswith WordSpec with MustMatchers with BeforeAndAfterAll &#123; override def beforeAll() = multiNodeSpecBeforeAll() override def afterAll() = multiNodeSpecAfterAll()&#125; 其中继承MultiNodeSpecCallbacks以获取回调函数，然后下面重写了几个回调函数。 下面我们创建MultiNodeSpec，并让他mixin刚刚定义的STMultiNodeSpec。两个版本的ClientServerSpec会运行在不同的两个jvm上。下面是ClientServerSpec：12345678class ClientServerSpecMultiJvmFrontend extends ClientServerSpecclass ClientServerSpecMultiJvmBackend extends ClientServerSpecclass ClientServerSpec extends MultiNodeSpec(ClientServerConfig)with STMultiNodeSpec with ImplicitSender &#123; // 参加到这次测试的node个数 def initialParticipants = roles.size ClientServerSpec使用了STMultiNodeSpec和ImplicitSender。后者设置testActor为所有message的默认sender，这样不用每次都设置了。123456789101112// import后我们才可以访问到后端roleimport ClientServerConfig._// 测试中使用TestRemoteBoxOfficeCreator，而不是RemoteBoxOfficeCreatortrait TestRemoteBoxOfficeCreator extends RemoteBoxOfficeCreator &#123; this:Actor =&gt; // 重写方法，返回测试的path override def createPath: String = &#123; val actorPath = node(backend) / "user" /"boxOffice" //node方法返回后端role node的地址。这个表达式是创建了一个ActorPath actorPath.toString &#125;&#125; 前端和后端role node默认是在随机端口上运行。我们使用TestRemoteBoxOfficeCreator替代RemoteBoxOfficeCreator，因为后者是使用的frontend.conf文件中的配置。我们在测试中需要找的是不一样的环境。 看一下测试代码：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849"A Client Server configured app" must &#123; "wait for all nodes to enter a barrier" in &#123; enterBarrier("startup") // 启动所有的node &#125; "be able to create an event and sell a ticket" in &#123; // 运行在前端jvm的代码 runOn(frontend) &#123; // 等待后端发布 enterBarrier("deployed") // 创建一个RestInterfaceMock val restInterface = system.actorOf(Props(new RestInterfaceMock with TestRemoteBoxOfficeCreator)) val path = node(backend) / "user" / "boxOffice" // 获取远程actor的actorSelection val actorSelection = system.actorSelection(path) // 向actorselection发送Identify消息 actorSelection.tell(Identify(path), testActor) // 等待boxoffice汇报它是否可用。RemoteLookup类会负责获取boxoffice的ActorRef val actorRef = expectMsgPF() &#123; case ActorIdentity(`path`, ref) =&gt; ref &#125; restInterface ! Event("RHCP", 1) expectMsg(EventCreated) restInterface ! TicketRequest("RHCP") expectMsg(Ticket("RHCP", 1)) &#125; // 运行在后端JVM上的代码 runOn(backend) &#123; // 创建boxOffice，并命名为boxOffice，这样RemoteLookup就可以找到它了 system.actorOf(Props[BoxOffice], "boxOffice") // 发送后端已经部署的信号 enterBarrier("deployed") &#125; // 测试结束 enterBarrier("finished") &#125; &#125; 这段测试可以分为4个部分。第一个是enterBarrier(“startup”)等待所有node启动。实际的测试会面就会指定前端和后端各自运行什么代码。前端会等后端启动成功的信号，然后执行测试。 后段代码只启动boxoffice，提供给前端使用。我们使用了RestInterfaceMock，TestRemoteBoxOfficeCreator。我们还是使用了RemoteLookup来等远程的actor被identify。 在发送message到远程boxoffice之前，我们先期望获得一个ActorIdentify message。 在项目目录下执行multi-jvm:test进行测试。 下图说明了测试的实际流程。 单节点运行和client server 模式的最大区别就是actorRef的获取与监视。在RestInterface和boxOffice 之间添加一个Remote Lookup，获取更大的灵活性，也能够更好地处理actor挂掉的问题。我们怎样等待远程actorRef可用呢？actorSelection和Identify message机制可以搞定。 6.3 总结记得本章开始的时候我们要分布式的理由么？ 除了我们要做一些改变之外，还有一些不变的事情： 我们使用ActorRef，不用关心它是远程actor还是本地actor 监控actor的API和local模式一样 除了协作者被网络分隔之外，我们通过中间actor，让RestInterface和BoxOffice互相通信。 我们学到的新东西： REPL提供了一个简单友好的方式来探索分布式拓扑 multi-node-testkit使得分布式系统的测试特别简单，不管这些分布式系统是基于akka-remote还是 akka-cluster 我们还没有处理在RemoteLookup和RemoteBoxOfficeForwarder的message丢失问题。下面的章节会介绍： 怎样在合作node之间创建爱你可信赖的proxy 当后端node挂掉的时候处理TicketSellers丢失的问题 状态怎样在集群间复制 但是。。在此之前，我们先看一下基于akka-cluster module的动态的node membership。]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（5）]]></title>
    <url>%2F2016%2F08%2F11%2Fakka-in-action-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这一章聊一下akka的Future。Actor提供了一个机制替代了并发object系统，Future提供了一个机制来替代并发function系统。 5.1 Futrue用例Actor被用来处理message，获取state，基于收到的不同message执行不同代码。借助于监管和监控，他们成为生命力极强的对象，即便出现错误也可以继续生存。 Future是用来不用object，而使用function就能解决问题的工具。它是一个function结果的占位符，在未来的某一个时刻生效，是一个异步结果。 Future是只读的，而且可以被读多次，不能被外界修改。 注意：Future并不是对java.util.concurrent.Futrue的封装。后者的get方法还是会引起block的。 Future对于pipeline的处理方式很友好，就是经过function1的结果传递给function2使用。 以买票的例子来说，假设我们创建了一个web页面，提供一些额外的信息，比如天气预报、交通状况、出行计划、停车等。下面是event的处理流程： 图中getEvent和getTraffic都是异步请求web服务的function，顺序执行。getTrafficInfo接收一个Event参数，当Future[Event]获得结果后就会触发getTrafficInfo的执行。这与在并发线程中执行getEvent方法等待返回结果是不同的。我们定义了一个一定会被执行的getTrafficInfo函数，而且不用任何等待。 下图展示了一个简单的例子： 可见异步的话只需要花费4和2中的最长时间，而同步就需要2+4。 下图展示了另一个用例，利用异步最快获取天气信息： 假设X服务出现超时问题的话，就直接不用管它的结果了。 这些场景使用actor并不是不可以，只是直接使用actor的话会特别麻烦。假设使用actor实现上面那个获取天气和交通信息的功能，需要包含以下步骤： 我们需要为天气和交通服务分别创建两个actor，这样他们才能被同时调用。调用方式被定义在TicketInfo Actor中。这里也并不能实现非阻塞。 如果应用包含以下特性，就可以考虑使用future： 不要block 调用一次某个function之后，在未来的某个时间点使用调用结果 组合许多一次调用的function结果 调用一些相互竞争的function，只使用其中的一部分结果，比如：最快的天气信息 当function出现错误的时候，需要返回默认值，以便后面的function能够继续调用 pipeline对结果相互依赖的function 5.2 Futrue中无阻塞是时候写一下TicketInfoService完成上面获取天气和交通信息了。 异步与同步的方法在与程序中的处理流，下面是一个同步的程序： 这里既没有使用futrue，也没有使用scala的懒加载，所以每行都需要等返回值才能继续。 再看一下异步的程序： callEventService实际上是在另一个线程里调用的，这个线程其实是阻塞的，后面我们会解决这个问题。futrue是将一段代码进行一部执行调用的Futrue.apply的简写。 以上代码其实是一个闭包，把request从main线程传递给一个新的线程去执行某个方法。 下面把交通信息串进来： 这段代码块只有在futrueEvent可用的时候才会执行。如果我们也需要这段程序有返回值咋办 也可以直接串起来。 重构一下getEvent和getRoute方法： 其实要真正的异步的话，callEventService和callTrafficService应该也是异步返回Futrue才对，这个可以借助spray-client，在后面的rest相关章节我们会介绍。 需要注意一个细节，就是钥匙用futrue的话需要引入对ExecutionContext的隐式转换，不然编译不过去。1import scala.concurrent.Implicits.global ExecutionContext是在某个线程池执行任务的抽象，可以类比java.util.concurrent.Executor。 这里介绍了成功调用的案例，下面说一下怎样从失败中恢复。 5.3 失败假如在异步处理的过程中出现了Exception咋办？启动一个scala REPL，执行以下代码123456789101112131415scala&gt; :paste// Entering paste mode (ctrl-D to finish)import scala.concurrent._import ExecutionContext.Implicits.globalval futureFail = future &#123; throw new Exception(&quot;error!&quot;)&#125;futureFail.foreach(value=&gt; println(value))// Exiting paste mode, now interpreting.futureFail: scala.concurrent.Future[Nothing] =scala.concurrent.impl.Promise$DefaultPromise@193cd8e1scala&gt; 可以看到并没有打印我们期望的结果，命令行也没有打印方法栈。由于前面没有执行成功，所以foreach代码没有执行。 我们可以使用OnComplete接口1234567891011121314151617scala&gt; :paste// Entering paste mode (ctrl-D to finish)import scala.util._import scala.util.control.NonFatalimport scala.concurrent._import ExecutionContext.Implicits.globalval futureFail = future &#123; throw new Exception("error!")&#125;futureFail.onComplete &#123;case Success(value) =&gt; println(value)case Failure(NonFatal(e)) =&gt; println(e)&#125;// Exiting paste mode, now interpreting.java.lang.Exception: error! OnComplete回调函数总是会被调用，而且返回值是Unit。 123futureFail.onFailure &#123;case NonFatal(e) =&gt; println(e)&#125; TicketInfo服务同样也应该保持一些信息，以便出现异常后记录抛出的异常信息。下图是TicketInfo里保存的事件信息： getEvent和getTraffic改用TicketInfo作为返回值与接收值，这样TicketInfo可以一路积攒信息，TicketInfo是一个case class。 注意：在使用futrue的过程中一定要全程使用不可变数据。 下图说明了GetTraffic调用失败的时候会发生什么： 我们可以定义recover方法，当出现异常的时候就返回这个方法的返回值。下面代码说明了怎样使用：123456val futureStep1 = getEvent(ticketNr)//返回Future[TicketInfo]val futureStep2 = futureStep1.flatMap &#123; ticketInfo =&gt; // 使用flatMap我们可以直接返回Future[TicketInfo]而不是一个TicketInfo getTraffic(ticketInfo).recover &#123; //返回Future[TicketInfo] case e:TrafficServiceException =&gt; ticketInfo //使用一个包含初始TicketInfo的Future来恢复 &#125;&#125; getTraffic啥的都是创建TicketInfo的copy。 还有一个recoverWith用来返回 Future[TicketInfo]而不是TicketInfo。注意上面的recover方法是同步调用的。 上面代码中还有一个问题，如果getEvent就失败了咋办？flatMap根本就不会执行。12345678val futureStep1 = getEvent(ticketNr)val futureStep2 = futureStep1.flatMap &#123; ticketInfo =&gt; getTraffic(ticketInfo).recover &#123; case e:TrafficServiceException =&gt; ticketInfo &#125;&#125;.recover &#123; case NonFatal(e) =&gt; TicketInfo(ticketNr)&#125; 如果getEvent失败的话，就直接返回包含ticketNr的空TicketInfo。 5.4 整合Futrue下面是买票服务中的所有的case class。12345678910111213141516171819202122232425262728package com.goticksimport org.joda.time.&#123;Duration, DateTime&#125;case class TicketInfo(ticketNr:String, userLocation:Location, event:Option[Event]=None, travelAdvice:Option[TravelAdvice]=None, weather:Option[Weather]=None, suggestions:Seq[Event]=Seq())case class Event(name:String,location:Location, time:DateTime)case class Artist(name:String, calendarUri:String)case class Location(lat:Double, lon:Double)// 简单起见，所有的route都弄成了一个简单的字符串case class RouteByCar(route:String, timeToLeave:DateTime, origin:Location, destination:Location, estimatedDuration:Duration, trafficJamTime:Duration)case class PublicTransportAdvice(advice:String, timeToLeave:DateTime, origin:Location, destination:Location, estimatedDuration:Duration)case class TravelAdvice(routeByCar:Option[RouteByCar]=None, publicTransportAdvice: Option[PublicTransportAdvice]=None)case class Weather(temperature:Int, precipitation:Boolean) 下图是TicketInfoService的处理流： combinators在图中以菱形表示。所有的future最终都被整合进了Future[TicketInfo]。 我们以最快选取天气信息先开始讲解，下面是这块逻辑使用combinator的地方： 两个天气服务都返回Future[Weather]，然后我们需要的是Future[TicketInfo]。下面是代码：12345678910def getWeather(ticketInfo:TicketInfo):Future[TicketInfo] = &#123; val futureWeatherX = callWeatherXService(ticketInfo).recover(withNone) // 恢复代码封装在withNone里了 val futureWeatherY = callWeatherYService(ticketInfo).recover(withNone) Future.firstCompletedOf(Seq(futureWeatherX, futureWeatherY)).map&#123; weatherResponse =&gt; ticketInfo.copy(weather = weatherResponse)// 复制到ticketInfo里 &#125;&#125; firstCompletedOf只返回第一个返回结果的服务的response，成功失败无所谓，也就是说加入先返回的是失败，那么后面即便有成功返回也不会生效了。【可以考虑firstSucceededOf】 下面是公共交通信息和行车路由服务，它们应该并行，然后等两者都结束调用就将结果combine进TravelAdvice。下图展示了这个combinator： getTraffic和getPublicTransport的Futrue各自返回一种类型数据：RouteByCar以及PublicTransportAdvice。这两个值先被放进一个tuple，然后把这个tuple map放进TravelAdvice里面。12case class TravelAdvice(routeByCar:Option[RouteByCar]=None, publicTransportAdvice: Option[PublicTransportAdvice]=None) 基于这个信息，用户就可以决定是使用公共交通工具出行，还是自驾游。下面是zip combinator的代码：12345678910111213 def getTravelAdvice(info:TicketInfo, event:Event):Future[TicketInfo] = &#123; val futureRoute = callTrafficService(info.userLocation, event.location, event.time).recover(withNone) val futurePublicTransport = callPublicTransportService(info.userLocation, event.location, event.time).recover(withNone)// 先把 Future[RouteByCar] 和 Future[PublicTransportAdvice] zip成tuple放入Future[(RouteByCar, PublicTransportAdvice)]// 然后再将这两者map放入Future[TicketInfo] futureRoute.zip(futurePublicTransport).map &#123; case(routeByCar, publicTransportAdvice) =&gt; val travelAdvice = TravelAdvice(routeByCar, publicTransportAdvice) info.copy(travelAdvice = Some(travelAdvice)) &#125; &#125; 也可以使用for替换map，有时可读性更强一些.123for((routeByCar, publicTransportAdvice) &lt;- futureRoute.zip(futurePublicTransport); travelAdvice = TravelAdvice(routeByCar, publicTransportAdvice)) yield info.copy(travelAdvice = Some(travelAdvice)) 下一个部分我们看一下推荐类似活动给用户。我们使用了两个web服务，一个是类似的艺术服务，返回用户感兴趣的类似艺术的信息。使用这个信息来调用一个日历服务，以便计划下一站的去向。123456789// 返回对每种艺术活动的计划列表 Future[Seq[Events]]def getSuggestions(event:Event):Future[Seq[Event]] = &#123; val futureArtists = callSimilarArtistsService(event).recover(withEmptySeq) // 返回类似艺术活动的列表：Future[Seq[Events]] for(artists &lt;- futureArtists.recover(withEmptySeq); // 'artists' evaluates at some point to a Seq[Artist] events &lt;- getPlannedEvents(event, artists).recover(withEmptySeq) // 'events' evaluates at some point to a Seq[Events], a planned event for every called artist. ) yield events&#125; getPlannedEvents使用Future.sequence方法把Seq[Future[Event]]构建成一个Future[Seq[Event]]作为返回值。也就是说把一串futrue放进一个futrue，这个futrue包含一串对象。下面是代码：1234def getPlannedEvents(event:Event, artists:Seq[Artist]) = &#123; val events = artists.map(artist=&gt; callArtistCalendarService(artist, event.location)) // events是一个 Seq[Future[Event]] Future.sequence(events) // 把 Seq[Future[Event]]转为Future[Seq[Event]]。当异步调用callArtistCalendarService都完成后，就可以对futrue取值了 &#125; 与sequence方法相似的还有一个traverse方法。下面是例子：12345def getPlannedEventsWithTraverse(event:Event, artists:Seq[Artist]) = &#123; Future.traverse(artists) &#123; artist=&gt; // traverse方法接受一个方法块返回一个futrue对象。 callArtistCalendarService(artist, event.location) &#125;&#125; 使用sequence的话，我们必须事先创建一个Seq[Future[Event]]，才能转化出Future[Seq[Event]]。traverse就不用什么中间步骤了。 终于到了TicketInfoService数据流的最后一步了。包含Weather的TicketInfo要和包含TravelAdvice的TicketInfo对象整合起来。我们使用fold方法来自完成：12345678val ticketInfos = Seq(infoWithTravelAdvice, infoWithWeather)val infoWithTravelAndWeather = Future.fold(ticketInfos)(info) &#123; (acc, elem) =&gt; val (travelAdvice, weather) = (elem.travelAdvice, elem.weather) acc.copy(travelAdvice = travelAdvice.orElse(acc.travelAdvice), weather = weather.orElse(acc.weather))&#125; fold方法需要一个初始值、一个方法块。集合中的每个元素都要经过这个方法块对初始值产生影响。 整体代码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546 def getTicketInfo(ticketNr:String, location:Location):Future[TicketInfo] = &#123; val emptyTicketInfo = TicketInfo(ticketNr, location) val eventInfo = getEvent(ticketNr, location).recover(withPrevious(emptyTicketInfo)) eventInfo.flatMap &#123; info =&gt; val infoWithWeather = getWeather(info) val infoWithTravelAdvice = info.event.map &#123; event =&gt; getTravelAdvice(info, event) &#125;.getOrElse(eventInfo) val suggestedEvents = info.event.map &#123; event =&gt; getSuggestions(event) &#125;.getOrElse(Future.successful(Seq())) val ticketInfos = Seq(infoWithTravelAdvice, infoWithWeather) val infoWithTravelAndWeather = Future.fold(ticketInfos)(info) &#123; (acc, elem) =&gt; val (travelAdvice, weather) = (elem.travelAdvice, elem.weather) acc.copy(travelAdvice = travelAdvice.orElse(acc.travelAdvice), weather = weather.orElse(acc.weather)) &#125; for(info &lt;- infoWithTravelAndWeather; suggestions &lt;- suggestedEvents ) yield info.copy(suggestions = suggestions) &#125; &#125;// 在TicketInfoService处理流中的错误恢复方法 type Recovery[T] = PartialFunction[Throwable,T] // recover with None def withNone[T]:Recovery[Option[T]] = &#123; case NonFatal(e) =&gt; None &#125; // recover with empty sequence def withEmptySeq[T]:Recovery[Seq[T]] = &#123; case NonFatal(e) =&gt; Seq() &#125; // recover with the ticketInfo that was built in the previous step def withPrevious(previous:TicketInfo):Recovery[TicketInfo] = &#123; case NonFatal(e) =&gt; previous &#125; 这段代码概括了TicketInfoService使用Futrue的逻辑。没有使用任何阻塞调用。combinator方法使这些异步掉用的结果很方便的整合起来。 5.5 Futrue和Actor 在前面的Up and Running 章节，我们使用Spray作为REST服务，它是使用Actor来处理HTTP 请求的，ask方法的返回值也是一个Futrue对象。1234567891011121314151617181920212223242526// 引入ask模型，它在ActorRef上添加了ask方法import akka.pattern.ask// context包含了actor的dispatcher定义。import context._// 为ask方法必须定义一个超时implicit val timeout = Timeout(5 seconds)// 抓获上下文中的senderval capturedSender = sender// 向TicketSeller请求GetEvents的本地方法def askEvent(ticketSeller:ActorRef): Future[Event] = &#123; // ask方法返回一个Futrue.我们使用mapTo方法将返回的Futrue[Any]转为Future[Int]。 val futureInt = ticketSeller.ask(GetEvents).mapTo[Int] // 查一下所有的子元素，对于指定event都还剩下多少票 futureInt.map &#123; nrOfTickets =&gt; Event(ticketSeller.actorRef.path.name, nrOfTickets) &#125;&#125;val futures = context.children.map &#123; ticketSeller =&gt; askEvent(ticketSeller)&#125;Future.sequence(futures).map &#123; // 给sender回送消息。这个sender就是发送原始GetEvents请求的sender。 events =&gt; capturedSender ! Events(events.toList)&#125; 这个例子比在第二章中的代码要清楚多了。每个消息的sender都不一样，这里结合闭包使用变量从context中获取当前message的sender，回送消息。 注意以下当在actor中使用futrue的时候，ActorContext会提供一个Actor的当前view。又由于actor是有状态的，所以闭包中的变量对于外部线程应该是不可变的。最容易达成这种效果的方式是使用不可变对象，然后在将这个值close over进futrue之前就获取这个不可变数据结构的引用，也就是上面例子中的capturedSender。 另一种模式是pipeTo：12345678910111213141516171819202122232425//引入pipe模型import akka.pattern.pipepath("events") &#123; get &#123; requestContext =&gt; // 创建一个actor，为所有的request使用一个response完成HTTP请求 val responder = createResponder(requestContext) // ask方法的Futrue返回值被pipe到responder actor boxOffice.ask(GetEvents).pipeTo(responder) &#125;&#125;class Responder(requestContext:RequestContext, ticketMaster:ActorRef) extends Actor with ActorLogging &#123; def receive = &#123; case Events(events) =&gt; requestContext.complete(StatusCodes.OK, events) self ! PoisonPill // other messages omitted.. &#125;&#125; 当我们想用一个actor来处理futrue对象，或者某些actor的ask结果还需要进一步处理的时候，就可以使用pipeTo方法了。 5.6 总结这一章介绍了一个futrue。使用futrue可以创建一个异步处理流，省去不必要的阻塞和线程等待，最大化资源使用率，最小化无作为延迟。 Futrue是一个function返回值的placeholder。 combinator方法可以方便地把futrue的值整合在一起。function可以并行执行，然后通过combinator变成一个有意义的结果。 futrue包含的对象应该是不可变的。 Futrue可以与Actor结合使用，但是要注意使用时close over的状态变量。sender的引用应该在进入闭包钱就获取。Futrue是ask方法的返回值类型，也是pipeTo方法的返回值类型。]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（4）]]></title>
    <url>%2F2016%2F08%2F09%2Fakka-in-action-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[内容提要： 自救系统 Let it crash Actor生命周期 Supervision 错误恢复策略 4.1 容错的来源我们最理想的世界其实是希望系统一直可用，而且能够保证每个action都能被成功执行。只有两种方法能让这个梦想照进现实：一是使用永远不会失败的组件，二是为每次失败都提供一个完美的补救措施，这个补救措施需要能保证最终能成功完成此次action。在多数架构里，是catch住预期的异常进行处理，但是无论怎样努力都不能阻止预期之外的异常导致失败。 能够容忍错误的系统理论上不错，但是要同时兼顾到高可用、分布式的系统基本是不可能的。主要原因是任何此类系统的大部分我们都不能控制，这部分可能会导致失败。网络是最好的例子，它随时都有可能消失，你的系统再强大也么有用。我们显然不能让一个server从它的灰烬中恢复元气，而且自动修复前面遇到的问题。这就是let it crash 出现的原因，不去想办法回复它，而是提供足够多的可用server。 既然我们不能保证不发生任何失败，那么我们不得不准备一些策略： 程序总会有失败。系统必须能够容错，才能继续运行其他服务。可恢复的错误不应该触发或导致灾难性的失败。 在某些条件允许的情况下，虽然系统的某一部分发生了问题，但是并不影响其他大部分的服务，那么我们应该切断这部分坏程序以免它产生一些不正确的结果，影响系统的其他部分正常运行。 一些重要的组件我们需要有备份（可以考虑放在不同的server或者使用不同的resources），当正在运行的失败的时候，能够进行热切换，使系统可用性快速恢复。 系统的一部分出问题不应该影响整个系统垮掉，所以我们需要一个措施来隔离某些特殊的失败，且需要在这些失败发生之后有处理措施。 当然，akka并没有容错的银弹。我们还是要处理失败，但是我们可以使用一种cleaner,more application-specifc的方式来处理。下面是Akka关于容错的一些特性： 名称 解释 Fault containment or isolation 一个fault应该是系统的一部分，不应导致整体crash Structure 隔离某个失败的组件意味着需要某个结构来实施隔离，系统需要一个定义好的Structure，把可以被隔离的部分放进去 冗余 当一个组件失败之后，应该有备份的替代它 替代 如果一个失败的组件可以被隔离，我们也可以在Structure中替换掉它。 重启 如果一个组件出现了不正确的状态，我们应该能够把它重新初始化。不正确的那个state或许就是导致错误的原因 组件生命周期 一个失败的组件必须被隔离，如果它不能恢复的话，就必须被终结和移除，或者重新处理化它的state。有些必要的周期：start, restart, terminate 暂停 当一个组件失败了，我们应该在它被修复之前把所有对它的调用都弄成suspended。这些调用应该保留下来，用来分析当前组件失败的原因 Separation of Concerns 分散关注 如果容错代码能够与普通执行代码分割开就最好了。在普通的流程当中，容错算是横向切入的了。把普通流程与容错恢复流程分离开会简化我们的工作 有人或许会问，用普通的异常捕捉机制就不能搞这套东西么？普通的异常是用来回退一系列操作来保证数据一致性的。但是既然提到了，我们还是用下面的部分来讨论一下。 4.1.1 普通异常方式假设有这样一个应用： 多线程，收集log，从文件中分析有兴趣的信息，然后转存为行对象，然后把这些行写入某个数据库。有一些file watcher进程持续盯着增长的文件，然后通知多个线程来处理新的文件。下图是一个概览： 如果数据库连接断了的话，我们期望切换到另一个数据库继续写数据，而不是回滚。如果连接开始出现问题，我们或许希望马上停掉它，以免app的其他部分使用它。有时我们只要重启这个连接，摆脱这个连接内某些错误的状态就可以了。可以用伪代码看一下潜在的问题区域。先看一下使用标准的异常处理方法直接在同一个数据库上重启一个新的连接。 首先，多线程里都已经准备好了要入库的数据。下图是创建writer： writer的依赖是通过构造方法传入的，数据库工厂的配置，都通过创建爱你writer的thread传入。下一步，我们准备log processor。每个processor都有一个writer的引用，用来存储行数据。如下图： 方法的调用栈如下图： 上图流程是被多线程同时调用的。下图战士了当一个DbBrokenConnectionException被跑出的时候的调用栈，这个异常暗示我们需要切换到另一个连接。 除了直接向上跑出异常之外，我们期望从DbBrokenConnectionException恢复，替换掉不能使用的连接。我们面临的第一个问题是很难在不打破既有设计的前提下添加代码去恢复连接。另外，我们没有足够的信息来重建连接：我们不知道我们已经成功处理到文件的哪一行了，在处理哪一行时出现的异常。 如果让我们执行的行数和连接信息对所有对象都可见的话，就打破了我们的简单性原则，而且违背了一些基本的实践：封装、控制反转、单一职则。我们只是需要把失败的组件替换掉而已。直接在异常处理中添加恢复的代码会混淆执行log文件的功能性。即便我们找到替换连接的方式，我们还不得不着重注意替换连接的时候，其他的线程没有在使用它，不然可能导致某些行数据丢失。 还有，让多线程之间针对某个异常进行沟通本来也不是什么标准特性，你必须要自己实现它。回顾容错的必备特性，想一下这种处理方式有么有可能站住脚： 错误隔离。许多线程都可以同时抛出异常，所以隔离相当困难。我们必须得加些锁才行。 Structure。必须自己创建 冗余。 异常不断被抛往上一层。或许会丢失掉一些重要的信息。 替换。 没有默认的策略支持在调用栈中替换一个对象，你必须得自己想个办法。有些依赖注入的框架支持这个特性，但是只是在特别简单的直接引用到旧的实例的时候。还有就是如果你要替换某个对象的时候，最好考虑好多线程问题。 重启。 组件生命周期。 暂停 关注分离。异常处理代码和业务代码混淆在一起，么有独立性。 简述以下主要的问题： 在first-class中，重新创建对象以及他们的依赖，在app中替换掉他们是不可能的 直接相互沟通的对象很难隔离 容错代码和功能代码混淆在一起 回头看看Akka。 Actor可以有Props对象重建，是actor system的一部分，actor之间通过actorRef进行沟通。下面我们看一下actor怎样把功能代码和容错代码分离，actor的生命周期怎样保证actor的暂停和重启。 4.1.2 Let it crash前面介绍了使用普通对象和异常处理机制来构建一个容错系统有多难，下面我们展示以下Actor的风采。 akka的容错恢复代码与功能代码是分开的。功能流包含处理普通message的actor，恢复流包含监控功能流中actor的actor。这种监控其他actor的actor叫做supervisor，如下图： actor中并不捕获异常，就直接让actor挂掉就性了。actor代码中只包含功能处理，而且没有异常处理逻辑或者恢复逻辑，不参与恢复过程，这样就清晰多了。crashed actor的mailbox在恢复流里的supervisor处理完exception之前都是暂停的。那么一个actor怎样才能变成一个supervisor呢？akka有一个父监管的机制，就是创建actor的actor自动成为被创建actor的supervisor。一个supervisor并不捕获异常，也不修复actor或者actor的状态，只负责判断怎样恢复，然后触发相应的策略。supervisor有4种策略： restart。使用失败actor的Props重建一个actor。这个新的actor重启之后，就继续处理消息。因为app的其他部分都是使用ActorRef来跟actor沟通，新的actor实例可以自动获取到下一条message。 resume。给当前actor新生，假装啥都没发生 stop。终结actor。 escalate。supervisor不知道怎样处理这个问题，抛给它的supervisor。 下图是针对log处理系统的一个示例： 上图中给出的容错方案是，当DbBrokenConnectionException发生的时候，使用restart策略，替换一个重建的dbWriter actor。 多数情况下，我们不希望重复执行那条导致失败的信息。比如logprocessor遇到一个损坏的文件，这导致mailbox走火入魔，任何message都不会被处理了，因为这个损坏的文件导致我们一次又一次的失败。鉴于此，如果是akka默认对于restart策略处理的actor，不再把处理失败的message放回mailbox了，如果不想这样，后面我们会说一下怎样做。 下图展示了当supervisor选择restart策略后，一个挂掉的dbWriter actor实例怎样被一个新的实例替换掉： 结合容错性，说一下let it crash的好处： 错误隔离。supervisor可以决定是否终结actor。 Structure。 actor system的actorRef层级使得在不影响其他actor的前提下替换actor实例 冗余。一个actor可以被另一个替换掉。在断掉的数据库连接例子中，新的actor实例可以连接到不同的数据库。supervisor也可以决定停掉错误的actor创建另一种类型的actor。另外还可以把message路由给一个负载均衡的actor集群中，后面第八章我们会讲到。 替换。actor总是可以被它的Props重建。supervisor不用了解重建actor的任何细节。 重启。通过restart实现 组件生命周期。actor是一个active component。可以被启动，停止，重启。下一节我们会讨论actor的生命周期。 暂停。当actor挂掉的时候，它的mailbox就暂停了，直到supervisor处理完毕。 关注隔离。负责功能的actor与负责容错的actor已经隔离开了。 4.2 Actor生命周期actor在创建之后就自动启动了。在被停止之前，actor一直保持在Started state。停止之后的state是Terminated，这时actor就不能处理message了，而且会被GC回收。当state是Started的时候，它既可以被重启也可以设置它的state为其他值。 一个actor的生命周期中有三种类型的event发生： 被创建和启动，Start 被重启，Restart 被停止，Stop 在Actor trait中有几个hook对应这生命周期的改变。我们可以在这些hook里添加自己的代码，用来为新建的actor指定一些自定义的state，也可以在restart之前处理前面处理失败的message，还可以清空某些资源占用。下面我们看看这三种event，并了解一下hook的重写。 4.2.1 Start事件一个普通的actor是通过actorOf方法创建并启动的。顶层actor是被ActorSystem的actorOf方法床架难点。父actor通过自身的ActorContext的actorOf方法创建子actor。 实例创建好之后，就会被Akka启动。preStart hook是在actor启动之前被调用的。123override def preStart() &#123; println("preStart")&#125; 一般这个hook可以用来设置actor的初始状态。 4.2.2 Stop事件actor通常通过调用ActorSystem和ActorContext对象的stop方法停止，或者发送一个PosionPill message给它。 postStop hook与preStart对称，在actor被终结之前，state被修改为Terminated之后调用。 123override def postStop()&#123; println("postStop")&#125; 一般用来释放资源，或者保存当前actor处理的一些信息到actor外界，以便新的actor实例可以使用。stopped actor和它的ActorRef会断开连接。ActorRef会被重定向到actor system的deadLetters ActorRef，这个ActorRef专门用来接收发送给挂了的actor的message。 4.2.3 Restart事件 restart发生后，触发挂掉的actor的preRestart方法。在这个hook中挂掉的actor实例可以储存它当前的一些状态信息。 12345override def preRestart(reason: Throwable, // 被当前actor跑出的异常 message: Option[Any]) &#123; //导致错误的message println("preRestart") super.preRestart(reason, message) //警告：这是调用super的实现 &#125; 重写这个hook的时候需要额外注意。默认的preRestart实现是先停止所有的子actor，然后再调用postStop hook。如果忘记调用super.preRestart方法，刚才提到的默认逻辑就不会执行了。记住actor是由Props创建的。Props对象其实是调用的actor的构造方法。actor可以在它的构造方法中创建子actor。如果挂掉的actor的子actor没有被停掉，那么后面随着父actor的多次restart，会有很多的子actor，就泛滥了。 注意restart和stop停掉actor 的方式是不一样的。在restart中挂掉的actor实例不会产生Terminated message。新的actor实例在restar期间就连接到了原来的ActorRef上。而stoped事件中stopped actor要将message重定向。两者相同的是在挂掉的actor被切离actor system之后都会调用postStop。 保存state的方式：可以给自己的mailbox发一条message，也可以存在类似DB的介质中。 调用完preStart hook之后，一个actor实例就被创建了。在postRestart hook被调用后。。。12345override def postRestart(reason: Throwable) &#123; //当前actor抛出的异常 println("postRestart") super.postRestart(reason) //警告：调用super实现&#125; 这里也有个警告，就是调用父级实现，它会触发preStart方法的执行。如果你确认不需要调用preStart方法，就不用调用这个父级实现了。 4.2.4 把生命周期整合起来把上面所有的event结合起来就是actor的整体生命周期了，下图只展示了一个restart： 对应整体生命周期的hook代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142package aia.faulttoleranceimport akka.actor._class LifeCycleHooks extends Actor with ActorLogging &#123; println("Constructor") //&lt;start id="ch3-life-start"/&gt; override def preStart() &#123; println("preStart") //&lt;co id="ch3-life-start-1" /&gt; &#125; //&lt;end id="ch3-life-start"/&gt; //&lt;start id="ch3-life-stop"/&gt; override def postStop() &#123; println("postStop") //&lt;co id="ch3-life-stop-1" /&gt; &#125; //&lt;end id="ch3-life-stop"/&gt; //&lt;start id="ch3-life-pre-restart"/&gt; override def preRestart(reason: Throwable, //&lt;co id="ch3-life-pre-restart-1a" /&gt; message: Option[Any]) &#123; //&lt;co id="ch3-life-pre-restart-1b" /&gt; println("preRestart") super.preRestart(reason, message) //&lt;co id="ch3-life-pre-restart-2" /&gt; &#125; //&lt;end id="ch3-life-pre-restart"/&gt; //&lt;start id="ch3-life-post-restart"/&gt; override def postRestart(reason: Throwable) &#123; //&lt;co id="ch3-life-post-restart-1" /&gt; println("postRestart") super.postRestart(reason) //&lt;co id="ch3-life-post-restart-2" /&gt; &#125; //&lt;end id="ch3-life-post-restart"/&gt; def receive = &#123; case "restart" =&gt; throw new IllegalStateException("force restart") case msg: AnyRef =&gt; println("Receive") sender() ! msg &#125;&#125; 下面测试一下：123456789101112131415161718192021222324252627package aia.faulttoleranceimport org.scalatest.&#123;WordSpecLike, BeforeAndAfterAll&#125;import akka.testkit.TestKitimport akka.actor._class LifeCycleHooksTest extends TestKit(ActorSystem("LifCycleTest")) with WordSpecLike with BeforeAndAfterAll &#123; override def afterAll() &#123; system.terminate() &#125; "The Child" must &#123; "log lifecycle hooks" in &#123; //&lt;start id="ch3-life-test"/&gt; val testActorRef = system.actorOf( //&lt;co id="ch3-life-test-start" /&gt; Props[LifeCycleHooks], "LifeCycleHooks") testActorRef ! "restart" //&lt;co id="ch3-life-test-restart" /&gt; testActorRef.tell("msg", testActor) expectMsg("msg") system.stop(testActorRef) //&lt;co id="ch3-life-test-stop" /&gt; Thread.sleep(1000) //&lt;end id="ch3-life-test"/&gt; &#125; &#125;&#125; 4.2.5 生命周期监控ActorContext的watch方法负责监控actor的死亡，unwatch方法反注册某个actor。一旦一个actor调用了某个ActorRef的watch方法，他就被这个ActorRef监控了。当被监控的actor被终结后，监控的actor会收到一个Terminated message，这个message只包含挂掉的actor的ActorRef。 123456789101112131415package aia.faulttoleranceimport akka.actor._import akka.actor.Terminatedobject DbStrategy2 &#123; class DbWatcher(dbWriter: ActorRef) extends Actor with ActorLogging &#123; context.watch(dbWriter) //watch dbWriter的生命周期 def receive = &#123; case Terminated(actorRef) =&gt; //被终结的actor的actorRef被放在Terminated message中传递 log.warning("Actor &#123;&#125; terminated", actorRef) //监控者打印出dbWriter被终结的消息 &#125; &#125; //&lt;end id="ch03-termination"/&gt;&#125; 与supervisor只能是父actor对子actor不同，actor之间的监控是没有任何规则的。只要某个actor可以访问到被监控actor的ActorRef，那么它就可以调用context.watch(actorRef)进行监控。监控(monitor)与监管(supervisor)联合起来使用会有特别强大的效果，下一节我们会介绍。 4.3 监管这一节我们继续以log处理app为例，深入看一下监管的细节处理。这一节我们关注一下用户空间也就是actor path为/user下的监管层级。这个actor path是所有app的actor的家。/user之外的path我们会在actor system关闭过程中涉及到。首先我们先看一下为某个app定义supervisor的层级的几种方式，对比一下优缺点。然后看一下怎样在supervisor中自定义监管策略。 4.3.1 监管层级actor在被创建的时候就注定了由它的创建者actor监管。父actor只有终结它的子actor才能接触这种监管关系。所在在app启动的时候就需要慎重考虑监管策略，尤其是不想整批整批的替换子actor的时候。 actor所处的层级越高，危险性越大。极端情况，顶层actor出现错误的话，整个系统的所有actor都需要重启。 下图展示了supervisor和actor之间的沟通，以及一个新的文件产生到存储入数据库的信息流： 图中可以看到fileWatcherSupervisor同时创建了fileWatcher和logProcessorsSupervisor。fileWatcher需要一个logProcessor的关联对象，而这个logProcessor是由logProcessorsSupervisor创建的，所以我们不能直接给fileWatcher一个ActorRef。我们不得不添加一些信息，发送给logProcessorsSupervisor，向它请求logProcessor的ActorRef，然后把ActorRef转给fileWatcher。 这种方法的好处是actor之间可以之间沟通，supervisor只负责监管与创建actor实例。缺点是我们之恩那个使用restart策略，否则message就会被发送给deadLetters，丢失掉。父监管模式使得监管从信息流中解耦有些难。logProcessor只能被logProcessorsSupervisor创建，这样就很难传递它的ActorRef给fileWatcher。 下一幅图是另一种实现方式。supervisor不只是创建和监管actor，它们还负责传递消息流给它的子actor。对于被监管的actor来说没有改变什么，因为真正的发送者是透明的。比如logProcessor会认为他的message是从fileWatcher获取的，但是实际上logProcessorSupervisor参与了这个message的转发。 这种方式的好处是间接提供了层级。supervisor可以在其他actor不知情的前提下终结或者扩张它的子actor。相比前面的方式，这种方式不会出现信息流断裂的情况。这种方案更适合父监管模式，因为supervisor可以直接使用Props对象创建它的子actor。比如fileWatcherSupervisor可以直接使用它创建的logProcessingSupervisor的ActorRef去创建fileWatcher。以下是实现代码： Props对象在actor之间来回传递，它包含了创建子actor所需要的依赖信息，这样每个actor在创建子actor的时候就不用关心细节了。最顶层的actor是由system.actorFor方法创建爱你的，因为我们想让系统中其他由supervisor创建的actor都沿着它这条发源地走下去。下一节我们会了解一下创建actor的具体过程：主要就是使用Props对象，调用context.actorOf(props)方法创建一个子actor，然后把当前actor作为每个新生actor的supervisor。 4.3.2 预订义监管策略创建app的顶层actor的是/user路径，它被user guardian监管。user guardian的默认监管策略是一旦发生异常，就直接重启它的子actor。actor的默认监管策略可以通过实现supervisorStrategy方法进行重写。在supervisorStrategy中有两个预订义的监管策略：defaultStrategy和stoppingStrategy。顾名思义，前者是所有的actor的默认监管策略，如果不重写的话，就一直使用这个策略： Decider对异常模式匹配选择一个Directive Directive包含：Stop, Start, Resume, Escalate这几种 defaultDecider返回OneForOneStrategy 有时我们只需要停掉出错的actor，有时一旦有一个子actor出错我们就需要停掉所有的子actor。OneForOneStrategy代表这子actor不会有相同的命运，只有挂掉的子actor会被Decider裁决。还有AllForOneStrategy，他就是只要有一个子actor出问题，所有的子actor就都需要遭受裁决。 下图是stoppingStrategy的代码： 发生任何异常stoppingStrategy都会停掉所有的子actor。如果不是发生Exception，而是Error呢？任何监管策略处理不了的Throwable都会向上抛给当前actor的父supervisor，抛到最顶层，也就是user guardian那里还处理不了的话就会导致整个actor system挂掉。 4.3.3 自定义策略每个app为了容错都需要定义一些自己的策略。前面我们提到一个supervisor对待挂掉的actor有四种处理措施，下面我们还是以log处理app来举例。 Resume。 忽略错误，使用现在的actor继续执行message Restart。 移除挂掉的actor实例，使用新的actor实例替换它 Stop。 永久终结这个actor Escalate。 把失败报给自己的supervisor，让自己的父actor去处理。 首先看一下在log处理app中可能出现的异常。1234567891011@SerialVersionUID(1L) class DiskError(msg: String) extends Error(msg) with Serializable //当硬盘挂掉导致资源不可用时的一个不可恢复的Error @SerialVersionUID(1L) class CorruptedFileException(msg: String, val file: File) extends Exception(msg) with Serializable //日志文件损坏造成的Exception @SerialVersionUID(1L) class DbBrokenConnectionException(msg: String) extends Exception(msg) with Serializable //数据库连接断开的Exception actor之间发送的message可以放在一个protocol对象中统一管理。 12345678 object LogProcessingProtocol &#123; // represents a new log file case class LogFile(file: File) //从fileWatcher接收到一个log文件。log processor要处理这个message // A line in the log file parsed by the LogProcessor Actor case class Line(time: Long, message: String, messageType: String) //LogFile里的一行数据。数据库的writer需要将这个数据写到数据库连接里 &#125;&#125; 首先从最底层看一下可能出现DbBrokenConnectionException的dbWriter。 1234567891011class DbWriter(databaseUrl: String) extends Actor &#123; val connection = new DbCon(databaseUrl) import LogProcessingProtocol._ def receive = &#123; case Line(time, message, messageType) =&gt; connection.write(Map('time -&gt; time, 'message -&gt; message, 'messageType -&gt; messageType)) //向数据库写数据就可能引发这个异常 &#125; &#125; DbWriter是由DbSupervisor监管的，前面说到DbSupervisor会把所有的message转发给dbWriter。123456789class DbSupervisor(writerProps: Props) extends Actor &#123; override def supervisorStrategy = OneForOneStrategy() &#123; case _: DbBrokenConnectionException =&gt; Restart //发生DbBrokenConnectionException的时候执行Restart策略 &#125; val writer = context.actorOf(writerProps) //supervisor使用一个Props对象创建dbWriter def receive = &#123; case m =&gt; writer forward (m) //supervisor把收到的所有的message转发给dbWriter ActorRef &#125; &#125; 再向上一些，就是logProcessor。123456789class LogProcessor(dbSupervisor: ActorRef) extends Actor with LogParsing &#123; import LogProcessingProtocol._ def receive = &#123; case LogFile(file) =&gt; val lines = parse(file) //读取文件就可能引发异常，导致当前actor挂掉 lines.foreach(dbSupervisor ! _) //把文件的每一行都发送给dbSupervisor &#125;&#125; 当出现文件损坏问题的时候，它的supervisor采取措施如下：12345678910111213class LogProcSupervisor(dbSupervisorProps: Props) extends Actor &#123; override def supervisorStrategy = OneForOneStrategy() &#123; case _: CorruptedFileException =&gt; Resume //出现CorruptedFileException.的时候，跳过损坏的文件，继续执行 &#125; val dbSupervisor = context.actorOf(dbSupervisorProps) //创建database的supervisor，而且被当前supervisor监管 val logProcProps = Props(new LogProcessor(dbSupervisor)) val logProcessor = context.actorOf(logProcProps) //通过Props创建logProcessor def receive = &#123; case m =&gt; logProcessor forward (m) //把所有的message都转发给logProcessor ActorRef &#125;&#125; 再向上一个层级，FileWatcher:1234567891011121314class FileWatcher(sourceUri: String, logProcSupervisor: ActorRef) extends Actor with FileWatchingAbilities &#123; register(sourceUri) //使用file的watching API注册监听文件变化 import FileWatcherProtocol._ import LogProcessingProtocol._ def receive = &#123; case NewFile(file, _) =&gt; //利用file watching API，当发现新文件的时候发送出去 logProcSupervisor ! LogFile(file) //发送给logProcSupervisor case SourceAbandoned(uri) if uri == sourceUri =&gt; self ! PoisonPill //如果出现硬盘损坏问题，fileWatcher就自杀 &#125; &#125; fileWatcher这个actor自杀之后，由于DiskError没有被定义处理策略，所以就会自动向上传递。这是一个不可恢复的error，所以FileWatchingSupervisor决定停止所有的actor，使用了AllForOneStrategy：12345678910111213141516171819202122class FileWatcherSupervisor(sources: Vector[String], logProcSuperProps: Props) extends Actor &#123; var fileWatchers: Vector[ActorRef] = sources.map &#123; source =&gt; val logProcSupervisor = context.actorOf(logProcSuperProps) val fileWatcher = context.actorOf(Props( new FileWatcher(source, logProcSupervisor))) context.watch(fileWatcher) //对filewatcher实施监控 fileWatcher &#125; override def supervisorStrategy = AllForOneStrategy() &#123; case _: DiskError =&gt; Stop //如果出现DiskError，就停掉所有的fileWatcher &#125; def receive = &#123; case Terminated(fileWatcher) =&gt; //确认这是fileWatcher的Terminated message fileWatchers = fileWatchers.filterNot(w =&gt; w == fileWatcher) if (fileWatchers.isEmpty) self ! PoisonPill //所有的fileWatcher都停掉之后，服毒自尽 &#125; &#125; OneForOneStrategy和AllForOneStrategy都提供了maxNrOfRetries和withinTimeRange作为构造参数，分别代表重试几次停止和停止超时时间，按需设置。设置之后，如果重试几次或者超时么有停止，就自动向上传递。下面是一个数据库supervisor的例子：1234567891011class DbImpatientSupervisor(writerProps: Props) extends Actor &#123; override def supervisorStrategy = OneForOneStrategy( maxNrOfRetries = 5, withinTimeRange = 60 seconds) &#123; //如果5次restart或者60s内没有解决这个问题，就自动上传这个问题给父级supervisor case _: DbBrokenConnectionException =&gt; Restart &#125; val writer = context.actorOf(writerProps) def receive = &#123; case m =&gt; writer forward (m) &#125; &#125; 4.4 总结容错是Akka相当给力的一部分，也是这个工具对于并发处理的重要组成。’Let it Crash’的哲学并不是主张把所有可能发生的问题都忽略，它也不能处理掉所有的问题。相反，编程人员是希望有恢复措施的，但是怎样使这措施顺利执行是空前的。这一届中我们使用log处理app讲解了容错： 监管意味着我们的恢复代码与功能代码的隔离 基于message的Actor模型意味着即便某个actor挂掉，我们还是能继续工作 我们可以忽略、丢弃、重启：自由选择 我们甚至可以将自己处理不了的错误直接抛给父级supervisor 有了akka，容错变得很简单。下一章我们要构建几个不同类型的基于actor的app，看一下怎样提供configuration, logging于发布之类的功能。]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（3）]]></title>
    <url>%2F2016%2F08%2F01%2Fakka-in-action-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这一篇我们一起看一下:使用akka测试驱动开发。 当TDD刚出现的时候，大家都感觉它浪费很长时间。尤其感觉让测试一些组件的协作更花费时间。Actor提供了一个又去的解决方案： actor可以更直接的测试，因为他们本身就代表具体行为。【几乎所有的TDD都有至少一个BDD（Behavior-Driven-Development）】 普通的单元测试只能测试接口，而且只能分别测试 Actor是基于消息的。也就是只要发送message就可以很轻松的模拟各种行为了。 3.1 测试Actor 我们使用ScalaTest单元测试框架来测试发送与接收message。 看代码以前，我们要了解，测试Actor相比测试不同的对象还是要稍微复杂一点儿的，原因如下： 时间。 发送message是一步的，所以不知道多长时间才能去assert返回值 异步。 Actor是在多个线程中并行运行的。多线程测试相比单线程测试要复杂以下，需要处理一些并发原是变量来同步多个actor的结果，比如lock、latch、barrier等。这些正是我们要远离的。仅仅是一个barrier的错用都会导致一个单元测试block住，从而影响整个test suite。 无状态。actor的内部状态是隐藏的，只能通过ActorRef来访问。单元测试的时候要注意这点。 协作/整合。如果要测试几个actor之间的整合，必须在这些actor之间窃听它们之间传递的message是不是期望值。 虽然有以上困难，好在Akka也提供了akka-testkit module。这个module包含了几个可以很方便的进行actor测试的工具。这个module覆盖了下面几种不同的测试类型： 单线程测试。 一个actor实例通常并不能直接使用。testkit提供了TestActorRef，它允许访问指定的actor实例。这样就能直接调用actor的方法，来测试我们定义的actor，向测试普通对象一样了。 多线程单元测试。TestKit和TestProbe两个类，让我们方便的接收回复，检查message，为特定message设置到达时限。TestKit有专门的方法来assert期望值。Actor在多线程环境中使用一个普通的dispatcher来运行。 多jvm测试。测试远程actor system的时候会用到。 TestActorRef继承了LocalActorRef，设置dispatcher为CallingThreadDispatcher, CallingThreadDispatcher只用来测试。（它直接使用调用者的线程，而不是另起一个线程）。 明眼人看的出来，多线程环境基本贴近于生产环境，所以我们接触最多。关于ScalaTest 3.1.1 准备测试因为有一些通用的逻辑，我们先定义一个Trait，使测试能够在单元测试结束后自动停止测试系统。12345678910111213package aia.testdriven//&lt;start id="ch02-stopsystem"/&gt;import org.scalatest.&#123; Suite, BeforeAndAfterAll &#125;import akka.testkit.TestKittrait StopSystemAfterAll extends BeforeAndAfterAll &#123; //继承ScalaTest的基础trait this: TestKit with Suite =&gt; //这个trait只能与使用TestKit的测试使用 override protected def afterAll() &#123; super.afterAll() system.terminate() //测试完成后关闭测试系统 &#125;&#125; 我们写测试的时候就可以mixin这个trait了。TestKit暴露了一个system变量，可以用来访问测试中创建的actor以及其他信息。 3.2 单向消息记住，我们已经不再使用那种触发某个动作，等待response的方式了，所以我们的例子都是使用tell来发送单向消息也就顺理成章了。拜这种fire and forget方式所赐，我们不知道message什么时候到达actor，或者即便到了，我们怎么样进行测试呢？我们期望是发送一个message给一个actor，然后这个actor能够正确的处理这条message。如果actor的处理动作对于外界完全不可见，我们只能保证它处理之后没有出现任何错误，我们应该使用TestActorRef来访问actor的状态，确保它完好。下面是我们需要注意的三种actor： SilentActor。这种actor的行为对于外界不可见，但是中间它会创建一些内部状态。只要处理中没有出现任何错误或异常我们就认为测试通过。检测处理中内部状态的变化就可以了。 SendingActor。这种actor处理完收到的message之后会发送message到其他actor。我们把这种actor当做一个黑盒，查看输入与输出的message就行了。 SideEffiectingActor。这种actor接收message，与一个普通对象进行一些交互。发送message给这种actor之后，需要确认以下普通对象是否被按照预期修改了。 下面我们为以上列举的每种actor都写个test。 3.2.1 SilentActor例子12345678910111213141516171819202122232425262728293031323334package aia.testdrivenimport org.scalatest.&#123;WordSpecLike, MustMatchers&#125;import akka.testkit.TestKitimport akka.actor._//This test is ignored in the BookBuild, it's added to the defaultExcludedNames//&lt;start id="ch02-silentactor-test01"/&gt;class SilentActor01Test extends TestKit(ActorSystem("testsystem")) with WordSpecLike //WordSpecLike提供了一种易读的DSL来测试BDD形式的测试 with MustMatchers //MustMatchers提供了易读的assertion with StopSystemAfterAll &#123; //我们刚刚定义的，完成测试后关闭system "A Silent Actor" must &#123; // 写上文本描述 "change state when it receives a message, single threaded" in &#123; // 每个in都描述一个指定的测试 //Write the test, first fail fail("not implemented yet") &#125; "change state when it receives a message, multi-threaded" in &#123; //Write the test, first fail fail("not implemented yet") &#125; &#125;&#125;//&lt;end id="ch02-silentactor-test01"/&gt;//定义一个Actorclass SilentActor extends Actor &#123; def receive = &#123; case msg =&gt; // 所有接收到的message都被吞如黑洞 &#125;&#125;//&lt;end id="ch02-silentactor-test01-imp"/&gt; 以上是一个WordSpec形式的测试，可以写多个文本描述。下面我们定义来一个Actor。 我们首先写测试发送message给这个actor，然后观察它内部状态的变化。 SilentActor和SilentActorProtocol都是为了辅助测试的，后者包含了所有前者支持的message类型，可以方便地对message进行分组管理。 12345678910111213141516171819202122232425262728293031323334353637package silentactor02 &#123;class SilentActorTest extends TestKit(ActorSystem("testsystem")) with WordSpecLike with MustMatchers with StopSystemAfterAll &#123; "A Silent Actor" must &#123; //&lt;start id="ch02-silentactor-test02"/&gt; "change internal state when it receives a message, single" in &#123; import SilentActor._ //引入Protocol定义的message类型 val silentActor = TestActorRef[SilentActor] //为单线程测试创建TestActorRef silentActor ! SilentMessage("whisper") silentActor.underlyingActor.state must (contain("whisper")) //测试刚刚放进的message在目标actor中 &#125; &#125; &#125; // 对应提到的Protocol object SilentActor &#123; case class SilentMessage(data: String) //SilentActor可以处理的一种message类型 case class GetState(receiver: ActorRef) &#125; class SilentActor extends Actor &#123; import SilentActor._ var internalState = Vector[String]() def receive = &#123; case SilentMessage(data) =&gt; internalState = internalState :+ data //state这里是被存储在一个vector中，每个message都会被加入这个vector &#125; def state = internalState //放回当前的所有内部状态 &#125;&#125; 由于返回的状态列表是不可变的，测试并不会影响我们验证期望数据。 下面我们看一下多线程版本的。1234567891011121314151617181920212223242526272829303132333435class SilentActorTest extends TestKit(ActorSystem("testsystem")) with WordSpecLike with MustMatchers with StopSystemAfterAll &#123; "A Silent Actor" must &#123; "change internal state when it receives a message, multi" in &#123; import SilentActor._ val silentActor = system.actorOf(Props[SilentActor], "s3") //创建actor的方式不同 silentActor ! SilentMessage("whisper1") silentActor ! SilentMessage("whisper2") silentActor ! GetState(testActor) // 添加了一种消息类型，来返回当前的状态 expectMsg(Vector("whisper1", "whisper2")) //测试断言 &#125; &#125; &#125; object SilentActor &#123; case class SilentMessage(data: String) case class GetState(receiver: ActorRef) //添加的消息类型 &#125; class SilentActor extends Actor &#123; import SilentActor._ var internalState = Vector[String]() def receive = &#123; case SilentMessage(data) =&gt; internalState = internalState :+ data case GetState(receiver) =&gt; receiver ! internalState //返回参数中内部状态给receiver，也就是GetState参数的ActorRef &#125; &#125; 多线程使用TestKit内置的testsystem这个ActorSystem来创建SilentActor。这样我们就不能使用多线程的actor去直接访问actor实例了。这里使用GetState消息封装了一个ActorRef作为参数。TestKit有一个testActor让你用来接收你期望的message。我们在receive里添加的GetState的方法返回了指定actorRef的内部状态。expectMsg方法是期望每个发送到testActor的message都被assert。 这里的internalState依然是不可变的，所以仍然是完全安全的。 3.2 SendingActor例子回想以下第一章中的买票的例子，我们需要测试的是党我们买了一个Ticket，那么可以卖的Ticket总数就要减少一个。既然TicketingAgent负责减票并将event继续传递给下一个TicketingAgent，我们只需要创建一个SendingActor，把它作为下一个接收者放到chain里去，然后观察票数变化情况就行了。 123456789101112131415161718192021222324252627282930313233343536373839package aia.testdrivenimport akka.testkit.TestKitimport akka.actor.&#123; Props, ActorRef, Actor, ActorSystem &#125;import org.scalatest.&#123;WordSpecLike, MustMatchers&#125;class SendingActor01Test extends TestKit(ActorSystem("testsystem")) with WordSpecLike with MustMatchers with StopSystemAfterAll &#123; "A Sending Actor" must &#123; "send a message to an actor when it has finished" in &#123; import Kiosk01Protocol._ val props = Props(new Kiosk01(testActor)) // 下一个TicketingAgent通过构造函数传递，这里我们传的是testActor val sendingActor = system.actorOf(props, "kiosk1") val tickets = Vector(Ticket(1), Ticket(2), Ticket(3)) val game = Game("Lakers vs Bulls", tickets) // 湖人对公牛的门票库 sendingActor ! game expectMsgPF() &#123; case Game(_, tickets) =&gt; //testAcotr应该接收到了一个event tickets.size must be(game.tickets.size - 1) // testActor应该接收到了一个ticket被减少了 &#125; &#125; &#125;&#125;object Kiosk01Protocol &#123; case class Ticket(seat: Int) case class Game(name: String, tickets: Seq[Ticket]) //包含message的event&#125;class Kiosk01(nextKiosk: ActorRef) extends Actor &#123; import Kiosk01Protocol._ def receive = &#123; case game @ Game(_, tickets) =&gt; nextKiosk ! game.copy(tickets = tickets.tail) //一个不可变的去掉第一个ticket的副本被发往下一个TicketingAgent &#125;&#125; 下面是一些SendingActor的变种： Actor 描述 MutatingCopyActor 这种actor创建一个变种的copy，然后发送副本到下一个actor，上面用到的就属于这种 ForwardingActor 这种actor不修改收到的message，只是转发出去 TransformingActor 这种actor会根据收到的message创建一个不同类型的message SequencingActor 这种actor基于收到的message创建很多的message，然后按序发送出去 前三种使用相同的方式进行测试。 FilteringActor不同于其他的是它会定位我们传递的message是为什么没有通过测试？SequencingActor跟它类似。那么我们怎样确认我们收到了正确数量的message呢？先给FilteringActor写个测试吧。我们要弄一个可以自动过滤重复message的FilteringActor。它有一个已经收到的所有的message的list，每次来新的message的时候都检查是不是已经有了相同的message。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475package aia.testdrivenimport akka.testkit.TestKitimport akka.actor.&#123; Actor, Props, ActorRef, ActorSystem &#125;import org.scalatest.&#123;MustMatchers, WordSpecLike &#125;class FilteringActorTest extends TestKit(ActorSystem("testsystem")) with WordSpecLike with MustMatchers with StopSystemAfterAll &#123; "A Filtering Actor" must &#123; "filter out particular messages" in &#123; import FilteringActor._ val props = FilteringActor.props(testActor, 5) val filter = system.actorOf(props, "filter-1") filter ! Event(1) // 发送一串message filter ! Event(2) filter ! Event(1) filter ! Event(3) filter ! Event(1) filter ! Event(4) filter ! Event(5) filter ! Event(5) filter ! Event(6) val eventIds = receiveWhile() &#123; //收集testActor收到的所有满足case语句的message case Event(id) if id &lt;= 5 =&gt; id &#125; eventIds must be(List(1, 2, 3, 4, 5)) //测试没有收入重复的message expectMsg(Event(6)) &#125; "filter out particular messages using expectNoMsg" in &#123; import FilteringActor._ val props = FilteringActor.props(testActor, 5) val filter = system.actorOf(props, "filter-2") filter ! Event(1) filter ! Event(2) expectMsg(Event(1)) expectMsg(Event(2)) filter ! Event(1) expectNoMsg filter ! Event(3) expectMsg(Event(3)) filter ! Event(1) expectNoMsg filter ! Event(4) filter ! Event(5) filter ! Event(5) expectMsg(Event(4)) expectMsg(Event(5)) expectNoMsg() &#125; &#125;&#125;object FilteringActor &#123; def props(nextActor: ActorRef, bufferSize: Int) = Props(new FilteringActor(nextActor, bufferSize)) case class Event(id: Long)&#125;class FilteringActor(nextActor: ActorRef, bufferSize: Int) extends Actor &#123;// 定义buffer大小 import FilteringActor._ var lastMessages = Vector[Event]() //上一批message的vector def receive = &#123; case msg: Event =&gt; if (!lastMessages.contains(msg)) &#123; lastMessages = lastMessages :+ msg nextActor ! msg // 如果buffer中没有event就发送给下个actor if (lastMessages.size &gt; bufferSize) &#123; // 当到了最大buffer，最老的message就被忽略了 lastMessages = lastMessages.tail //&lt;co id="ch02-filteringactor-discard"/&gt; &#125; &#125; &#125;&#125; receiveWhile方法同样也可以用来测试]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[django的POST或者GET参数映射到对象]]></title>
    <url>%2F2016%2F07%2F21%2Fdjango%E7%9A%84POST%E6%88%96%E8%80%85GET%E5%8F%82%E6%95%B0%E6%98%A0%E5%B0%84%E5%88%B0%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[原理及其简单，就算利用python的自省，有点儿类似java的反射调用。123456etl = ETL() dic = dict(request.POST) for k,v in dic.iteritems(): if hasattr(etl, k): setattr(etl, k, v) print etl 就这几行代码就可以搞定了。为了公用方便，可以抽出来作为一个module使用。 简单封装了以下1234567891011121314151617181920212223242526# -*- coding: utf-8 -*import logging'''http的一些常用的工具方法'''def post2obj(obj, post, *excludes): ''' 将post里的参数对应copy到obj里，主要是方便http接口直接使用obj进行操作 :param obj: :param post: :return: ''' dic = dict(post) for k, v in dic.iteritems(): if k not in excludes: if hasattr(obj, k): v = ''.join(v) setattr(obj, k, v) logging.info("post params has been copied to obj -&gt; %s" % obj)......get 同理 注意：上面我们使用了’’.join(v)。因为从dict里遍历出来的所有value都是list类型的。而request.POST[&#39;xxx&#39;]取出来的就是正常的unicode类型。list会在后面的处理中出现很多问题，所以这里转成str再使用。 再想想，其实可以通过python的装饰器【对应java里的注释】实现，但是目前不太确定python是否支持java的泛型类的东西，可以用来自动生成操作对象的实例。]]></content>
  </entry>
  <entry>
    <title><![CDATA[django的奇葩form]]></title>
    <url>%2F2016%2F07%2F21%2Fdjango%E7%9A%84%E5%A5%87%E8%91%A9form%2F</url>
    <content type="text"><![CDATA[记录一下遇到的最奇葩的组件：django的form插件。官方文档是：https://docs.djangoproject.com/ja/1.9/ref/forms/widgets/。 弄这个东西的初衷场景：python里暂时没有找到类似java里类似下面的功能。 123456@RequestMapping(value = "save",method = RequestMethod.POST, produces = MediaType.APPLICATION_JSON) public @ResponseBody Object addETL(ETL etl) throws Exception&#123; etl.setAuthor("will"); etlService.addETL(etl); return "&#123;\"message\" :\"success\"&#125;"; &#125; 调用这个接口的时候，只需要传递ETL的字段就可以了，不管是POST还是GET方法，都会自己序列化进入ETL对象，然后就可以操作client端传送上来的ETL对象了 。但是在python或者django里本人暂时没有找到这种方便的方式，只能通过request.POST[‘name’]之类的方法，挨个从POST对象里面取出来，再自己用这些属性去初始化ETL对象，之后再去执行save或者等等之类的操作….身为一个优秀的程序员，必须将“懒”的特性发回出来。所以就去django官网找能够实现类似功能的例子，所以就找到了django的form组件。 额外发现是，当我们需要编辑一个已经存在的ETL的时候，可以自动把所有字段渲染到前端。 那么满怀期待地开始了恶心的form探索之旅。 前端12345&lt;form action="etls/add" method="POST"&gt; &#123;% csrf_token %&#125; &#123;&#123; form.as_p &#125;&#125; &lt;input type="submit"/&gt; &lt;/form&gt; 简直是简单的不要不要的。 后端 views文件里定义一个Form。自定义一个form1234567class ETLForm(forms.ModelForm): preSql = forms.CharField(widget=forms.Textarea(attrs=&#123;'class': "form-control", "size": 10&#125;)) class Meta: model = ETL exclude = ['id', 'ctime'] 上面这段，定义了preSql这个字段的显示类型为form的textarea，然后定义来这个textarea的属性，包括class样式。Meta里定义了model的数据模型，还有要去除不显示的字段。也可以指定fields，就是要显示的字段。 怎么方便怎么来就是了。 其实恶心的东西就是这里写的样式了。好，就算能够接受后端把逻辑和样式一起写了。可是对于不是form的样式怎么办呢，有很多页面就不是form啊，比如列表～会有很多样式必须需要在前端写。那样就是前端有样式，后端也有样式，麻蛋，想想头就大。 其实是有种更方便的做法1Form = modelform_factory(Author, form=AuthorForm, localized_fields=("birth_date",)) 鉴于本人对上面这种后端写样式就已经感觉不能接受了，就没有再继续搞这个东西。 1234567891011121314@transaction.atomicdef edit(request, pk): if request.method == 'POST': form = ETLForm(request.POST) if form.is_valid(): new_etl = form.save() logger.info('ETL has been created successfully : ' + new_etl) return HttpResponseRedirect(reverse('metamap:index')) else: etl = ETL.objects.get(pk=pk) form = ETLForm(instance=etl) return render(request, 'etl/edit.html', &#123;'form': form&#125;) 路由配置是1url('etls/(?P&lt;pk&gt;[0-9]+)/$', etls.edit), 记录以下这个奇葩组件，然后使用传统form进行逻辑编写，就算是要到POST里逐个取值，也认了！！！！ widget相关：https://docs.djangoproject.com/ja/1.9/ref/forms/widgets/]]></content>
  </entry>
  <entry>
    <title><![CDATA[JPARepository杂记]]></title>
    <url>%2F2016%2F07%2F16%2FJPARepository%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[join时报错报错1QuerySyntaxException: expecting IDENT, found &apos;*&apos; near line 1 原始sql1hql:select a.* from Archive a,Employee e,Department d where a.contractEndDate = :date and a.status = 'A' and a.empId = e.id and e.status = 'A' and e.type = 'A' 改成1select a from Archive a,Employee e,Department d where a.contractEndDate = :date and a.status = 'A' and a.empId = e.id and e.status = 'A' and e.type = 'A' select 后面为对象 a .而非 a.* 参考：http://blog.sina.com.cn/s/blog_4bf2e5550100n2gh.html SQL面向Entity指的是在JPARepository的子类中自定义的函数123456789101112131415161718192021222324252627public interface TblBloodRepository extends JpaRepository&lt;TblBlood, Integer&gt;&#123; TblBlood findByTblName(String tblName); @Query("select a from " + "(select blood from TblBlood blood where valid = 1) a" + " left outer join " + "(select distinct parentTbl from TblBlood where valid = 1) b" + " on a.tblName = b.parentTbl" + " where b.parentTbl is null") List&lt;TblBlood&gt; selectAllLeaf(); @Modifying @Query("update TblBlood set valid = 0 where tblName=:tblName") void makePreviousInvalid(@Param("tblName")String tblName); @Query("select b from" + " TblBlood a join TblBlood b" + " on a.parentTbl = b.tblName and b.valid = 1" + " where a.valid = 1 and a.tblName = :tblName") List&lt;TblBlood&gt; selectParentByTblName(@Param("tblName")String tblName); @Query("from TblBlood where valid = 1 and parentTbl=:tblName") public List&lt;TblBlood&gt; selectByParentTblName(@Param("tblName")String tblName); @Query("from TblBlood where valid = 1 and tblName=:tblName") List&lt;TblBlood&gt; selectByTblName(@Param("tblName")String tblName);&#125; TblBlood就是对象的类名，而不是真正的表名。 自join12345@Query("select b from" + " TblBlood a join TblBlood b" + " on a.parentTbl = b.tblName and b.valid = 1" + " where a.valid = 1 and a.tblName = ?1") List&lt;TblBlood&gt; selectParentByTblName(String tblName); 报错123456789101112131415161718 Path expected for join! at org.hibernate.hql.internal.ast.HqlSqlWalker.createFromJoinElement(HqlSqlWalker.java:378) at org.hibernate.hql.internal.antlr.HqlSqlBaseWalker.joinElement(HqlSqlBaseWalker.java:3858) at org.hibernate.hql.internal.antlr.HqlSqlBaseWalker.fromElement(HqlSqlBaseWalker.java:3644) at org.hibernate.hql.internal.antlr.HqlSqlBaseWalker.fromElementList(HqlSqlBaseWalker.java:3522) org.springframework.beans.factory.BeanCreationException: Error creating bean with name &apos;tblBloodRepository&apos;: Invocation of init method failed; nested exception is java.lang.IllegalArgumentException: Validation failed for query for method public abstract java.util.List com.will.hivesolver.repositories.TblBloodRepository.selectParentByTblName(java.lang.String)! at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1574) ...Caused by: java.lang.IllegalArgumentException: Validation failed for query for method public abstract java.util.List com.will.hivesolver.repositories.TblBloodRepository.selectParentByTblName(java.lang.String)! at org.springframework.data.jpa.repository.query.SimpleJpaQuery.validateQuery(SimpleJpaQuery.java:84) .....Caused by: java.lang.IllegalStateException: No data type for node: org.hibernate.hql.internal.ast.tree.IdentNode \-[IDENT] IdentNode: &apos;b&apos; &#123;originalText=b&#125; at org.hibernate.hql.internal.ast.tree.SelectClause.initializeExplicitSelectClause(SelectClause.java:174) at org.hibernate.hql.internal.ast.HqlSqlWalker.useSelectClause(HqlSqlWalker.java:923) ... 不能更新12严重: Servlet.service() for servlet [api] in context with path [/metamap] threw exception [Request processing failed; nested exception is org.springframework.dao.InvalidDataAccessApiUsageException: Executing an update/delete query; nested exception is javax.persistence.TransactionRequiredException: Executing an update/delete query] with root causejavax.persistence.TransactionRequiredException: Executing an update/delete query Spring Data JPA，事务导致的异常 需要在springMVC的xml里添加,这里只扫描Controller所在的位置里的Controller1234&lt;context:component-scan base-package="com.will.hivesolver.controller" use-default-filters="false"&gt; &lt;context:include-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;context:include-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice"/&gt; &lt;/context:component-scan&gt; 在applicationContext.xml中添加，这里必须不能扫描上面扫描过的Controller：1234&lt;context:component-scan base-package="com.will.hivesolver"&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.stereotype.Controller"/&gt; &lt;context:exclude-filter type="annotation" expression="org.springframework.web.bind.annotation.ControllerAdvice"/&gt; &lt;/context:component-scan&gt; 参考：https://github.com/springside/springside4/wiki/Spring-MVC]]></content>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[express杂记]]></title>
    <url>%2F2016%2F07%2F16%2Fexpress%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[app.set/get用来查看或者设置一些配置参数 express里变量的换行出不来12345678router.get('/get_log', function (req, res) &#123; worker.exec("cat " + req.query.location, function (error, stdout, stderr) &#123; console.log('stderr : ' + stderr); console.log('stdout : ' + stdout); console.log('error : ' + error); res.render('etls/log', &#123;stdout : stdout, log : req.query.location&#125;); &#125;); &#125;); 后面控制台的输出12345678stderr : stdout : # job for mymeta@my_table# author : will# create time : 19700118070840# pre settings delete from default.tblselect * from battingerror : null 模板文件1#&#123;stdout&#125; 前端输出1# job for mymeta@my_table # author : will # create time : 19700118070840 # pre settings delete from default.tbl select * from batting 控制台的stdout变量是换行的，但是前端就不换行。试了一下res.send()，结果也是这样。 模板修改为1!&#123;stdout.replace(/\n/g, &apos;&lt;br/&gt;&apos;)&#125; 如果使用res.send()的话，就res.send(stdout.replace(/\n/g, ‘‘))就行了。 这个换行支持起来那么难么？比较low的感觉。 body-parser安装了body parser之后就可以方便的在req中使用 req.body来获取form内容，使用req.query来获取querystring里的内容了。 但是一定要注意express的app.use是有前后次序的。踩过坑]]></content>
      <tags>
        <tag>nodejs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ember散记]]></title>
    <url>%2F2016%2F06%2F26%2Fember%E6%95%A3%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[主要是弄ambari的时候，ambari view使用了emberjs框架，自己按照官网教程走了一遍，此文章只是记录一些散记，不成系统。 命令 命令 解释 扩展 ember g route xxx/contact 生成route handler url里访问的路径xxx/contact会被定位到routes/xxx/contact.js下。生成template文件，route文件，对应的单元测试文件 ember g component list 生成一个component 位于templates/components文件夹下，可以被其他template以模块的形式引用。对应的js文件在根目录的components下 ember install ember-cli-tutorial-style 安装插件到当前项目 示例中的这个插件，会自动copy一些css文件到指定目录 ember g model rental 生成一个ember data model 在models文件夹下生成对应的model文件还有单元测试文件 ember g helper rental-property-type 生成一个helper 在helpers文件夹下生成对应的js文件，还有单元测试文件 ember g controller index 生成一个controller 在controllers文件夹下生成对应js文件，单元测试文件 路由route handler中添加model方法，可以在template里直接使用。 12345export default Ember.Route.extend(&#123; model() &#123; return rentals; &#125;&#125;); 在template中的使用： 123456789101112131415161718[[#each model as |rental|]] &lt;article class="listing"&gt; &lt;h3&gt;[[rental.title]]&lt;/h3&gt; &lt;div class="detail owner"&gt; &lt;span&gt;Owner:&lt;/span&gt; [[rental.owner]] &lt;/div&gt; &lt;div class="detail type"&gt; &lt;span&gt;Type:&lt;/span&gt; [[rental.type]] &lt;/div&gt; &lt;div class="detail location"&gt; &lt;span&gt;Location:&lt;/span&gt; [[rental.city]] &lt;/div&gt; &lt;div class="detail bedrooms"&gt; &lt;span&gt;Number of bedrooms:&lt;/span&gt; [[rental.bedrooms]] &lt;/div&gt; &lt;/article&gt;[[/each]]~ 模板随着项目的创建，会在templates下自动生成一个application.js文件：123&lt;h2&gt;title or slogin&lt;/h2&gt;[[outlet]] 其中h2标题部分会在所有的模板中出现，[[outlet]]作为url中指定route handler的占位符。 插件ember有自己的插件机制，可以自由发布与引用,个人感觉有点儿类似python module的感觉。 https://emberobserver.com/ Ember data使用ember g model rental生成模板文件models/rental.js1234567891011import Model from 'ember-data/model';import attr from 'ember-data/attr';export default Model.extend(&#123; title: attr(), owner: attr(), city: attr(), type: attr(), image: attr(), bedrooms: attr()&#125;); 上面就定义好了这个model的各个属性。下面是使用model了，在某个route handler中定义model函数，下面这个示例会使用GET方法调用/rentals1234567import Ember from 'ember';export default Ember.Route.extend(&#123; model() &#123; return this.store.findAll('rental'); &#125;&#125;); 详情参考：https://guides.emberjs.com/v2.6.0/models/ 模块就是component，可以被其他的template直接引用。它包含两部分：template和js。template负责布局，js负责定义一些属性和action。 布局文件:templates/components/rental-listing.hbs12345678910111213141516171819&lt;article class="listing"&gt; &lt;a [[action 'toggleImageSize']] class="image [[if isWide "wide"]]"&gt; &lt;img src="[[rental.image]]" alt=""&gt; &lt;small&gt;View Larger&lt;/small&gt; &lt;/a&gt; &lt;h3&gt;[[rental.title]]&lt;/h3&gt; &lt;div class="detail owner"&gt; &lt;span&gt;Owner:&lt;/span&gt; [[rental.owner]] &lt;/div&gt; &lt;div class="detail type"&gt; &lt;span&gt;Type:&lt;/span&gt; [[rental.type]] &lt;/div&gt; &lt;div class="detail location"&gt; &lt;span&gt;Location:&lt;/span&gt; [[rental.city]] &lt;/div&gt; &lt;div class="detail bedrooms"&gt; &lt;span&gt;Number of bedrooms:&lt;/span&gt; [[rental.bedrooms]] &lt;/div&gt;&lt;/article&gt; 交互文件：components/rental-listing.js12345678910import Ember from 'ember';export default Ember.Component.extend(&#123; isWide: false, actions: &#123; toggleImageSize() &#123; this.toggleProperty('isWide'); &#125; &#125;&#125;); 可以看到isWide和actions属性与模板文件中的[[action ‘toggleImageSize’]]以及isWide变量对应。 Handlebars Helper有的时候会在render到template之前对一些属性进行公共的处理啥的，就用到这个handlebar，为属性起装饰作用。helpers/xxx.js文件：1234567891011121314151617import Ember from 'ember';const communityPropertyTypes = [ 'Condo', 'Townhouse', 'Apartment'];export function rentalPropertyType([type]/*, hash*/) &#123; if (communityPropertyTypes.contains(type)) &#123; return 'Community'; &#125; return 'Standalone';&#125;export default Ember.Helper.helper(rentalPropertyType); 在template文件中使用上面定义的rentalPropertyType方法。1&lt;span&gt;Type:&lt;/span&gt; [[rental-property-type rental.type]] - [[rental.type]] 这样在render模板的时候，就会把rental.type传入rentalPropertyType函数，处理后，返回相应的处理值。]]></content>
      <tags>
        <tag>ember</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python debug]]></title>
    <url>%2F2016%2F06%2F23%2Fpython-debug%2F</url>
    <content type="text"><![CDATA[今儿想着快些让hive view的java代码跟远程程序能够一起调试了，想要远程调试就得在远程ambari server启动的地方添加jdwp设置才行。 当我们安装好ambari-server之后，它默认是在/usr/sbin/ambari-server的一个shell文件，start命令导流到/usr/sbin/ambari-server.py脚本中。跟tomcat、jetty啥的不一样，并不提供静态的类似JAVA_OPTS之类的变量。ambari server是作为python脚本的子进程启动的，所以启动的参数都是通过python程序生成后执行的。自己沿着入口找了半天，越找越蒙圈，回头想了一下，还是debug来的更真实。 查下python的debug方式，最快能实施的就是使用pdb了。到ambari server的宿主机上开始进行调试。 pdb基础设置断点在要调试的python文件import pdb，然后在断点代码的上方添加pdb.set_trace()12345678910111213141516import pdb.... options.exit_message = "Ambari Server '%s' completed successfully." % action options.exit_code = None pdb.set_trace() try: action_obj.execute() if action_obj.need_restart: pstatus, pid = is_server_runing() if pstatus: print 'NOTE: Restart Ambari Server to apply changes' + \ ' ("ambari-server restart|stop+start")' 到命令行执行此python文件，到pdb.set_trace()的位置就会停下，跳出(pdb) pdb命令 命令 解释 help 显示所有命令 n 继续执行 step 进入当前方法的方法块 c 忽略调试，继续执行 p p str 打印某个对象 我就用了以上命令，完成了对于ambari的调试。如果有其他的要求，可以逐个试一下。 ambari调试过程设置断点 在ambari-server.py 引入pdb，在main方法上面添加pdb.set_trace() 1234567891011121314151617181920212223242526...def mainBody(): parser = optparse.OptionParser(usage="usage: %prog [options] action [stack_id os]",) init_parser_options(parser) (options, args) = parser.parse_args() pdb.set_trace() # check if only silent key set default_options = parser.get_default_values() silent_options = default_options silent_options.silent = True if options == silent_options: options.only_silent = True else: options.only_silent = False # set verbose set_verbose(options.verbose) if options.verbose: main(options, args, parser)... 执行命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382[root@data-test01 ~]# ambari-server startUsing python /usr/bin/python2Starting ambari-server&gt; /usr/sbin/ambari-server.py(584)main()-&gt; try:(Pdb) n&gt; /usr/sbin/ambari-server.py(585)main()-&gt; action_obj.execute()(Pdb) step--Call--&gt; /usr/sbin/ambari-server.py(59)execute()-&gt; def execute(self):(Pdb) n&gt; /usr/sbin/ambari-server.py(60)execute()-&gt; self.fn(*self.args, **self.kwargs)(Pdb) p *self.args*** SyntaxError: SyntaxError('invalid syntax', ('&lt;string&gt;', 1, 1, '*self.args'))(Pdb) p self.args(&lt;Values at 0xfbee60: &#123;'only_silent': False, 'jdbc_driver': None, 'verbose': False, 'init_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-CREATE.sql', 'dbms': None, 'silent': False, 'warnings': [], 'exit_code': None, 'cluster_name': None, 'database_username': None, 'drop_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-DROP.sql', 'upgrade_script_file': '/var/lib/ambari-server/resources/upgrade/ddl/Ambari-DDL-Postgres-UPGRADE-1.3.0.sql', 'java_home': None, 'ldap_sync_existing': False, 'force_repo_version': False, 'debug': False, 'ldap_sync_groups': None, 'must_set_database_options': True, 'sqla_server_name': None, 'ldap_sync_users': None, 'database_name': None, 'jdbc_db': None, 'ldap_sync_all': False, 'upgrade_stack_script_file': '/var/lib/ambari-server/resources/upgrade/dml/Ambari-DML-Postgres-UPGRADE_STACK.sql', 'desired_repo_version': None, 'suspend_start': False, 'database_port': None, 'sid_or_sname': 'sname', 'database_password': None, 'exit_message': "Ambari Server 'start' completed successfully.", 'database_host': None, 'postgres_schema': None&#125;&gt;,)(Pdb) p self.kwargs&#123;&#125;(Pdb) step--Call--&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(82)thunk()-&gt; def thunk(*args, **kwargs):(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(83)thunk()-&gt; fn_id_base = func.__module__ + "." + func.__name__(Pdb) jn*** NameError: name 'jn' is not defined(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(84)thunk()-&gt; fn_id = fn_id_base + "." + OSCheck.get_os_family()(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(85)thunk()-&gt; if fn_id not in self._func_impls:(Pdb) p fn_id'__main__.start.redhat'(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(86)thunk()-&gt; fn_id = fn_id_base + "." + OsFamilyImpl.DEFAULT(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(88)thunk()-&gt; fn = self._func_impls[fn_id](Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(89)thunk()-&gt; return fn(*args, **kwargs)(Pdb) p args(&lt;Values at 0xfbee60: &#123;'only_silent': False, 'jdbc_driver': None, 'verbose': False, 'init_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-CREATE.sql', 'dbms': None, 'silent': False, 'warnings': [], 'exit_code': None, 'cluster_name': None, 'database_username': None, 'drop_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-DROP.sql', 'upgrade_script_file': '/var/lib/ambari-server/resources/upgrade/ddl/Ambari-DDL-Postgres-UPGRADE-1.3.0.sql', 'java_home': None, 'ldap_sync_existing': False, 'force_repo_version': False, 'debug': False, 'ldap_sync_groups': None, 'must_set_database_options': True, 'sqla_server_name': None, 'ldap_sync_users': None, 'database_name': None, 'jdbc_db': None, 'ldap_sync_all': False, 'upgrade_stack_script_file': '/var/lib/ambari-server/resources/upgrade/dml/Ambari-DML-Postgres-UPGRADE_STACK.sql', 'desired_repo_version': None, 'suspend_start': False, 'database_port': None, 'sid_or_sname': 'sname', 'database_password': None, 'exit_message': "Ambari Server 'start' completed successfully.", 'database_host': None, 'postgres_schema': None&#125;&gt;,)(Pdb) p kwargs&#123;&#125;(Pdb) step--Call--&gt; /usr/sbin/ambari-server.py(103)start()-&gt; @OsFamilyFuncImpl(OsFamilyImpl.DEFAULT)(Pdb) n&gt; /usr/sbin/ambari-server.py(105)start()-&gt; status, pid = is_server_runing()(Pdb) n&gt; /usr/sbin/ambari-server.py(106)start()-&gt; if status:(Pdb) p statusFalse(Pdb) n&gt; /usr/sbin/ambari-server.py(110)start()-&gt; server_process_main(args)(Pdb) p args&lt;Values at 0xfbee60: &#123;'only_silent': False, 'jdbc_driver': None, 'verbose': False, 'init_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-CREATE.sql', 'dbms': None, 'silent': False, 'warnings': [], 'exit_code': None, 'cluster_name': None, 'database_username': None, 'drop_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-DROP.sql', 'upgrade_script_file': '/var/lib/ambari-server/resources/upgrade/ddl/Ambari-DDL-Postgres-UPGRADE-1.3.0.sql', 'java_home': None, 'ldap_sync_existing': False, 'force_repo_version': False, 'debug': False, 'ldap_sync_groups': None, 'must_set_database_options': True, 'sqla_server_name': None, 'ldap_sync_users': None, 'database_name': None, 'jdbc_db': None, 'ldap_sync_all': False, 'upgrade_stack_script_file': '/var/lib/ambari-server/resources/upgrade/dml/Ambari-DML-Postgres-UPGRADE_STACK.sql', 'desired_repo_version': None, 'suspend_start': False, 'database_port': None, 'sid_or_sname': 'sname', 'database_password': None, 'exit_message': "Ambari Server 'start' completed successfully.", 'database_host': None, 'postgres_schema': None&#125;&gt;(Pdb) step--Call--&gt; /usr/sbin/ambari_server_main.py(204)server_process_main()-&gt; def server_process_main(options, scmStatus=None):(Pdb) n&gt; /usr/sbin/ambari_server_main.py(206)server_process_main()-&gt; try:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(207)server_process_main()-&gt; set_debug_mode_from_options(options)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(211)server_process_main()-&gt; if not check_reverse_lookup():(Pdb) n&gt; /usr/sbin/ambari_server_main.py(216)server_process_main()-&gt; check_database_name_property()(Pdb) n&gt; /usr/sbin/ambari_server_main.py(217)server_process_main()-&gt; parse_properties_file(options)(Pdb) p options&lt;Values at 0xfbee60: &#123;'only_silent': False, 'jdbc_driver': None, 'verbose': False, 'init_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-CREATE.sql', 'dbms': None, 'silent': False, 'warnings': [], 'exit_code': None, 'cluster_name': None, 'database_username': None, 'drop_script_file': '/var/lib/ambari-server/resources/Ambari-DDL-Postgres-EMBEDDED-DROP.sql', 'upgrade_script_file': '/var/lib/ambari-server/resources/upgrade/ddl/Ambari-DDL-Postgres-UPGRADE-1.3.0.sql', 'java_home': None, 'ldap_sync_existing': False, 'force_repo_version': False, 'debug': False, 'ldap_sync_groups': None, 'must_set_database_options': True, 'sqla_server_name': None, 'ldap_sync_users': None, 'database_name': None, 'jdbc_db': None, 'ldap_sync_all': False, 'upgrade_stack_script_file': '/var/lib/ambari-server/resources/upgrade/dml/Ambari-DML-Postgres-UPGRADE_STACK.sql', 'desired_repo_version': None, 'suspend_start': False, 'database_port': None, 'sid_or_sname': 'sname', 'database_password': None, 'exit_message': "Ambari Server 'start' completed successfully.", 'database_host': None, 'postgres_schema': None&#125;&gt;(Pdb) n&gt; /usr/sbin/ambari_server_main.py(219)server_process_main()-&gt; ambari_user = read_ambari_user()(Pdb) step--Call--&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(501)read_ambari_user()-&gt; def read_ambari_user():(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(505)read_ambari_user()-&gt; properties = get_ambari_properties()(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(506)read_ambari_user()-&gt; if properties != -1:(Pdb) p properties&lt;ambari_server.properties.Properties object at 0xfe4910&gt;(Pdb) dir(properties)['_Properties__parse', '__class__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattr__', '__getattribute__', '__getitem__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_keymap', '_origprops', '_props', 'bspacere', 'fileName', 'getPropertyDict', 'get_property', 'load', 'othercharre', 'othercharre2', 'process_pair', 'propertyNames', 'removeOldProp', 'removeProp', 'sort_origprops', 'sort_props', 'store', 'store_ordered', 'unescape'](Pdb) type(properties)&lt;class 'ambari_server.properties.Properties'&gt;(Pdb) p properties fileName*** SyntaxError: SyntaxError('unexpected EOF while parsing', ('&lt;string&gt;', 1, 19, 'properties fileName'))(Pdb) p properties.fileName()*** TypeError: TypeError("'str' object is not callable",)(Pdb) p properties.fileName'/etc/ambari-server/conf/ambari.properties'(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(507)read_ambari_user()-&gt; user = properties[NR_USER_PROPERTY](Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(508)read_ambari_user()-&gt; if user:(Pdb) p user'root'(Pdb) p*** SyntaxError: SyntaxError('unexpected EOF while parsing', ('&lt;string&gt;', 0, 0, ''))(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(509)read_ambari_user()-&gt; return user(Pdb) n--Return--&gt; /usr/lib/python2.6/site-packages/ambari_server/serverConfiguration.py(509)read_ambari_user()-&gt;'root'-&gt; return user(Pdb) n&gt; /usr/sbin/ambari_server_main.py(220)server_process_main()-&gt; current_user = ensure_can_start_under_current_user(ambari_user)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(222)server_process_main()-&gt; print_info_msg("Ambari Server is not running...")(Pdb) p current_user'root'(Pdb) n&gt; /usr/sbin/ambari_server_main.py(224)server_process_main()-&gt; jdk_path = find_jdk()(Pdb) n&gt; /usr/sbin/ambari_server_main.py(225)server_process_main()-&gt; if jdk_path is None:(Pdb) p jdk_path'/server/java/jdk1.8.0_60'(Pdb) n&gt; /usr/sbin/ambari_server_main.py(231)server_process_main()-&gt; properties = get_ambari_properties()(Pdb) n&gt; /usr/sbin/ambari_server_main.py(234)server_process_main()-&gt; if is_root():(Pdb) n&gt; /usr/sbin/ambari_server_main.py(235)server_process_main()-&gt; print configDefaults.MESSAGE_SERVER_RUNNING_AS_ROOT(Pdb) print configDefaults.MESSAGE_SERVER_RUNNING_AS_ROOTAmbari Server running with administrator privileges.(Pdb) nAmbari Server running with administrator privileges.&gt; /usr/sbin/ambari_server_main.py(237)server_process_main()-&gt; ensure_jdbc_driver_is_installed(options, properties)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(239)server_process_main()-&gt; ensure_dbms_is_running(options, properties, scmStatus)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(241)server_process_main()-&gt; if scmStatus is not None:(Pdb) p scmStatusNone(Pdb) n&gt; /usr/sbin/ambari_server_main.py(244)server_process_main()-&gt; refresh_stack_hash(properties)(Pdb) p properties&lt;ambari_server.properties.Properties object at 0xfe4e50&gt;(Pdb) step--Call--&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(85)refresh_stack_hash()-&gt; def refresh_stack_hash(properties):(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(86)refresh_stack_hash()-&gt; resources_location = get_resources_location(properties)(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(87)refresh_stack_hash()-&gt; stacks_location = get_stack_location(properties)(Pdb) p resources_location'/var/lib/ambari-server/resources'(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(88)refresh_stack_hash()-&gt; resource_files_keeper = ResourceFilesKeeper(resources_location, stacks_location)(Pdb) p stacks_location'/var/lib/ambari-server/resources/stacks'(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(90)refresh_stack_hash()-&gt; try:(Pdb) p resource_files_keeper&lt;ambari_server.resourceFilesKeeper.ResourceFilesKeeper instance at 0x103c050&gt;(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(91)refresh_stack_hash()-&gt; print "Organizing resource files at &#123;0&#125;...".format(resources_location,(Pdb) n&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(92)refresh_stack_hash()-&gt; verbose=get_verbose())(Pdb) nOrganizing resource files at /var/lib/ambari-server/resources...&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(93)refresh_stack_hash()-&gt; resource_files_keeper.perform_housekeeping()(Pdb) p verbose*** NameError: NameError("name 'verbose' is not defined",)(Pdb) n--Return--&gt; /usr/lib/python2.6/site-packages/ambari_server/serverUtils.py(93)refresh_stack_hash()-&gt;None-&gt; resource_files_keeper.perform_housekeeping()(Pdb) n&gt; /usr/sbin/ambari_server_main.py(246)server_process_main()-&gt; if scmStatus is not None:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(249)server_process_main()-&gt; ensure_server_security_is_configured()(Pdb) n&gt; /usr/sbin/ambari_server_main.py(251)server_process_main()-&gt; if scmStatus is not None:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(254)server_process_main()-&gt; java_exe = get_java_exe_path()(Pdb) n&gt; /usr/sbin/ambari_server_main.py(256)server_process_main()-&gt; serverClassPath = ServerClassPath(properties, options)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(258)server_process_main()-&gt; debug_mode = get_debug_mode()(Pdb) p serverClassPath&lt;ambari_server.serverClassPath.ServerClassPath instance at 0x103cfc8&gt;(Pdb) n&gt; /usr/sbin/ambari_server_main.py(259)server_process_main()-&gt; debug_start = (debug_mode &amp; 1) or SERVER_START_DEBUG(Pdb) n&gt; /usr/sbin/ambari_server_main.py(260)server_process_main()-&gt; suspend_start = (debug_mode &amp; 2) or SUSPEND_START_MODE(Pdb) n&gt; /usr/sbin/ambari_server_main.py(261)server_process_main()-&gt; suspend_mode = 'y' if suspend_start else 'n'(Pdb) p debug_startFalse(Pdb) n&gt; /usr/sbin/ambari_server_main.py(263)server_process_main()-&gt; param_list = generate_child_process_param_list(ambari_user, java_exe,(Pdb) n&gt; /usr/sbin/ambari_server_main.py(264)server_process_main()-&gt; serverClassPath.get_full_ambari_classpath_escaped_for_shell(), debug_start,(Pdb) p param_list*** NameError: NameError("name 'param_list' is not defined",)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(265)server_process_main()-&gt; suspend_mode)(Pdb) p param_list*** NameError: NameError("name 'param_list' is not defined",)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(266)server_process_main()-&gt; environ = generate_env(options, ambari_user, current_user)(Pdb) p param_list['/bin/sh', '-c', "ulimit -n 10000 ; /server/java/jdk1.8.0_60/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Dsun.zip.disableMemoryMapping=true -Xms512m -Xmx2048m -Djava.security.auth.login.config=/etc/ambari-server/conf/krb5JAASLogin.conf -Djava.security.krb5.conf=/etc/krb5.conf -Djavax.security.auth.useSubjectCredsOnly=false -cp '/etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/share/java/mysql-connector-java.jar' org.apache.ambari.server.controller.AmbariServer &gt; /var/log/ambari-server/ambari-server.out 2&gt;&amp;1 || echo $? &gt; /var/run/ambari-server/ambari-server.exitcode &amp;"](Pdb) n&gt; /usr/sbin/ambari_server_main.py(268)server_process_main()-&gt; if not os.path.exists(configDefaults.PID_DIR):(Pdb) n&gt; /usr/sbin/ambari_server_main.py(271)server_process_main()-&gt; print_info_msg("Running server: " + str(param_list))(Pdb) n&gt; /usr/sbin/ambari_server_main.py(272)server_process_main()-&gt; procJava = subprocess.Popen(param_list, env=environ)(Pdb) p str(param_list)'[\'/bin/sh\', \'-c\', "ulimit -n 10000 ; /server/java/jdk1.8.0_60/bin/java -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Dsun.zip.disableMemoryMapping=true -Xms512m -Xmx2048m -Djava.security.auth.login.config=/etc/ambari-server/conf/krb5JAASLogin.conf -Djava.security.krb5.conf=/etc/krb5.conf -Djavax.security.auth.useSubjectCredsOnly=false -cp \'/etc/ambari-server/conf:/usr/lib/ambari-server/*:/usr/share/java/mysql-connector-java.jar\' org.apache.ambari.server.controller.AmbariServer &gt; /var/log/ambari-server/ambari-server.out 2&gt;&amp;1 || echo $? &gt; /var/run/ambari-server/ambari-server.exitcode &amp;"]'(Pdb) n&gt; /usr/sbin/ambari_server_main.py(274)server_process_main()-&gt; pidJava = procJava.pid(Pdb) p procJava.pid2216(Pdb) n&gt; /usr/sbin/ambari_server_main.py(275)server_process_main()-&gt; if pidJava &lt;= 0:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(288)server_process_main()-&gt; try:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(289)server_process_main()-&gt; os.setpgid(pidJava, 0)(Pdb) nOSError: (13, 'Permission denied')&gt; /usr/sbin/ambari_server_main.py(289)server_process_main()-&gt; os.setpgid(pidJava, 0)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(290)server_process_main()-&gt; except OSError, e:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(291)server_process_main()-&gt; print_warning_msg('setpgid(&#123;0&#125;, 0) failed - &#123;1&#125;'.format(pidJava, str(e)))(Pdb) nWARNING: setpgid(2216, 0) failed - [Errno 13] Permission denied&gt; /usr/sbin/ambari_server_main.py(292)server_process_main()-&gt; pass(Pdb) n&gt; /usr/sbin/ambari_server_main.py(293)server_process_main()-&gt; pidfile = os.path.join(configDefaults.PID_DIR, PID_NAME)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(294)server_process_main()-&gt; save_pid(pidJava, pidfile)(Pdb) n&gt; /usr/sbin/ambari_server_main.py(295)server_process_main()-&gt; print "Server PID at: "+pidfile(Pdb) nServer PID at: /var/run/ambari-server/ambari-server.pid&gt; /usr/sbin/ambari_server_main.py(296)server_process_main()-&gt; print "Server out at: "+configDefaults.SERVER_OUT_FILE(Pdb) nServer out at: /var/log/ambari-server/ambari-server.out&gt; /usr/sbin/ambari_server_main.py(297)server_process_main()-&gt; print "Server log at: "+configDefaults.SERVER_LOG_FILE(Pdb) nServer log at: /var/log/ambari-server/ambari-server.log&gt; /usr/sbin/ambari_server_main.py(299)server_process_main()-&gt; wait_for_server_start(pidfile, scmStatus)(Pdb) nWaiting for server start....................&gt; /usr/sbin/ambari_server_main.py(301)server_process_main()-&gt; if scmStatus is not None:(Pdb) n&gt; /usr/sbin/ambari_server_main.py(304)server_process_main()-&gt; return procJava(Pdb) p procJava&lt;subprocess.Popen object at 0x1038690&gt;(Pdb) dir(procJava)['__class__', '__del__', '__delattr__', '__dict__', '__doc__', '__format__', '__getattribute__', '__hash__', '__init__', '__module__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_check_timeout', '_child_created', '_close_fds', '_communicate', '_communicate_with_poll', '_communicate_with_select', '_communication_started', '_execute_child', '_get_handles', '_handle_exitstatus', '_input', '_internal_poll', '_remaining_time', '_set_cloexec_flag', '_translate_newlines', 'communicate', 'kill', 'pid', 'poll', 'returncode', 'send_signal', 'stderr', 'stdin', 'stdout', 'terminate', 'universal_newlines', 'wait'](Pdb) n--Return--&gt; /usr/sbin/ambari_server_main.py(304)server_process_main()-&gt;&lt;subproc...x1038690&gt;-&gt; return procJava(Pdb) n--Return--&gt; /usr/sbin/ambari-server.py(110)start()-&gt;None-&gt; server_process_main(args)(Pdb) n--Return--&gt; /usr/lib/python2.6/site-packages/ambari_commons/os_family_impl.py(89)thunk()-&gt;None-&gt; return fn(*args, **kwargs)(Pdb) n--Return--&gt; /usr/sbin/ambari-server.py(60)execute()-&gt;None-&gt; self.fn(*self.args, **self.kwargs)(Pdb) n&gt; /usr/sbin/ambari-server.py(587)main()-&gt; if action_obj.need_restart:(Pdb) n&gt; /usr/sbin/ambari-server.py(593)main()-&gt; if options.warnings:(Pdb) n&gt; /usr/sbin/ambari-server.py(608)main()-&gt; if options.exit_message is not None:(Pdb) n&gt; /usr/sbin/ambari-server.py(609)main()-&gt; print options.exit_message(Pdb) nAmbari Server 'start' completed successfully.&gt; /usr/sbin/ambari-server.py(611)main()-&gt; if options.exit_code is not None: # not all actions may return a system exit code(Pdb) n--Return--&gt; /usr/sbin/ambari-server.py(611)main()-&gt;None-&gt; if options.exit_code is not None: # not all actions may return a system exit code(Pdb) n--Return--&gt; /usr/sbin/ambari-server.py(635)mainBody()-&gt;None-&gt; main(options, args, parser)(Pdb) n--Return--&gt; /usr/sbin/ambari-server.py(644)&lt;module&gt;()-&gt;None-&gt; mainBody()(Pdb) n[root@data-test01 ~]# which ambari-server/usr/sbin/ambari-server[root@data-test01 ~]# vim /usr/sbin/ambari-server.py [root@data-test01 ~]# 从以上debug堆栈中我们可以看到ambari-server启动过程中python脚本的所有准备工作。其中java进程的参数主要在ambari_server_main.py的server_process_main()的param_list中。 那么param_list来源在哪儿呢？ 是在server_process_main的子方法generate_child_process_param_list()中的command_base变量，SERVER_START_CMD_DEBUG或者SERVER_START_CMD的形式如下。 {0} -server -XX:NewRatio=3 -XX:+UseConcMarkSweepGC -XX:-UseGCOverheadLimit -XX:CMSInitiatingOccupancyFraction=60 -Dsun.zip.disableMemoryMapping=true {1} {2} -cp {3} org.apache.ambari.server.controller.AmbariServer &gt; {4} 2&gt;&amp;1 || echo $? &gt; {5} &amp; 可以看到留下了很多占位符。还是看一下generate_child_process_param_list的code吧123456789101112131415161718192021222324252627282930313233343536@FamilyFuncImpl(OsFamilyImpl.DEFAULT) def generate_child_process_param_list(ambari_user, java_exe, class_path, debug_start, suspend_mode): from ambari_commons.os_linux import ULIMIT_CMD properties = get_ambari_properties() command_base = SERVER_START_CMD_DEBUG if debug_start else SERVER_START_CMD ulimit_cmd = "%s %s" % (ULIMIT_CMD, str(get_ulimit_open_files(properties))) command = command_base.format(java_exe, ambari_provider_module_option, jvm_args, class_path, configDefaults.SERVER_OUT_FILE, os.path.join(configDefaults.PID_DIR, EXITCODE_NAME), suspend_mode) # required to start properly server instance os.chdir(configDefaults.ROOT_FS_PATH) #For properly daemonization server should be started using shell as parent param_list = [locate_file('sh', '/bin'), "-c"] if is_root() and ambari_user != "root": # To inherit exported environment variables (especially AMBARI_PASSPHRASE), # from subprocess, we have to skip --login option of su command. That's why # we change dir to / (otherwise subprocess can face with 'permission denied' # errors while trying to list current directory cmd = "&#123;ulimit_cmd&#125; ; &#123;su&#125; &#123;ambari_user&#125; -s &#123;sh_shell&#125; -c '&#123;command&#125;'".format(ulimit_cmd=ulimit_cmd, su=locate_file('su', '/bin'), ambari_user=ambari_user, sh_shell=locate_file('sh', '/bin'), command=command) else: cmd = "&#123;ulimit_cmd&#125; ; &#123;command&#125;".format(ulimit_cmd=ulimit_cmd, command=command) param_list.append(cmd) return param_list 这个方法把java进程的相关的东西全部拼进cmd，之后放进param_list，其中放了一个jvm_args的参数就是jvm参数了。在当前python文件中追溯此数据的来源：jvm_args = os.getenv(‘AMBARI_JVM_ARGS’, ‘-Xms512m -Xmx2048m’) ，所以它是一个环境变量。依赖eclipse的索引，搜一下哪个脚本里设置了这个环境变量：ambari-env.sh。 参考http://www.ibm.com/developerworks/cn/linux/l-cn-pythondebugger/]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari-web解读（一）-请求流程]]></title>
    <url>%2F2016%2F06%2F20%2Fambari-web%E8%A7%A3%E8%AF%BB%EF%BC%88%E4%B8%80%EF%BC%89-%E8%AF%B7%E6%B1%82%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[最近为公司选型了大数据维护工具ambari，官网:ambari.apache.org。主要是为了运维方便一些。另外对于数据分析人员开放了一个hive view页面，提供提数支持。但是对于原始的hive view，我们有些不满意的地方： table的comment没有展示完全 后台一直在每15秒钟请求一次后台hiveserver。虽然这并不会对和iveserver造成很大压力，但是感觉不友好，产生很多垃圾log。 经过差不多一周左右断断续续地探查与学习，基本弄清楚了ambari-web的请求处理脉路。可能对nodejs大神来说这不算啥，但是个人感觉稍微有点儿吃力。学习了一下nodejs的新的工具ember.js。 1.参考 链接 说明 https://github.com/emberjs/starter-kit/ 学习ember.js的基础项目 http://www.html-js.com/article/1685 Ember.js Application Development How-to的中文翻译 http://www.ruanyifeng.com/blog/2011/08/a_detailed_explanation_of_jquery_deferred_object.html 对于jquery回调工具deferred的解释]]></content>
      <tags>
        <tag>ambari</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ambari-views中文翻译]]></title>
    <url>%2F2016%2F06%2F20%2Fambari-view%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"><![CDATA[简述ambari views可以在ambari-web中提供插件定制化的可视化、管理、监控等特性。 当用户添加了一个第三方的插件到ambari的时候，我们可以添加一个view来支持这个插件的UI，也就是说可以发布一个应用到ambari container里来。 每个view实例使用name和version来在ambari container里唯一标识自己。 Views in Ambari 是顶级的resource，并不直接跟cluster实例产生直接关联。 例子官网的例子在ambari-views/examples. 基础View Instances一个view可以有多个实例，每个实例可能有各自不同的属性。 View Contextview context让view componenets访问ambari container提供的运行时context。这个context会一直伴随这view实例，从出生到死亡。 一个view context可能会被container注入到多个不同的view componenet里去，比如resources, resource provider, servlets等。 context接口提供给view instance以下信息:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156/** * Get the current user name. * * @return the current user name */public String getUsername(); /** * Determine whether or not the access specified by the given permission name * is permitted for the given user. * * @param userName the user name * @param permissionName the permission name * * @throws SecurityException if the access specified by the given permission name * is not permitted */public void hasPermission(String userName, String permissionName) throws SecurityException; /** * Get the view name. * * @return the view name */public String getViewName(); /** * Get the view definition associated with this context. * * @return the view definition */public ViewDefinition getViewDefinition(); /** * Get the view instance name. * * @return the view instance name; null if no instance is associated */public String getInstanceName(); /** * Get the view instance definition associated with this context. * * @return the view instance definition; null if no instance is associated */public ViewInstanceDefinition getViewInstanceDefinition(); /** * Get the property values specified to create the view instance. * * @return the view instance property values; null if no instance is associated */public Map&lt;String, String&gt; getProperties(); /** * Save an instance data value for the given key. * * @param key the key * @param value the value * * @throws IllegalStateException if no instance is associated */public void putInstanceData(String key, String value); /** * Get the instance data value for the given key. * * @param key the key * * @return the instance data value; null if no instance is associated */public String getInstanceData(String key); /** * Get the instance data values. * * @return the view instance property values; null if no instance is associated */public Map&lt;String, String&gt; getInstanceData(); /** * Remove the instance data value for the given key. * * @param key the key * * @throws IllegalStateException if no instance is associated */public void removeInstanceData(String key); /** * Get a property for the given key from the ambari configuration. * * @param key the property key * * @return the property value; null indicates that the configuration contains no mapping for the key */public String getAmbariProperty(String key); /** * Get the view resource provider for the given resource type. * * @param type the resource type * * @return the resource provider; null if no instance is associated */public ResourceProvider&lt;?&gt; getResourceProvider(String type); /** * Get a URL stream provider. * * @return a stream provider */public URLStreamProvider getURLStreamProvider(); /** * Get a data store for view persistence entities. * * @return a data store; null if no instance is associated */public DataStore getDataStore(); /** * Get all of the available view definitions. * * @return the view definitions */public Collection&lt;ViewDefinition&gt; getViewDefinitions(); /** * Get all of the available view instance definitions. * * @return the view instance definitions */public Collection&lt;ViewInstanceDefinition&gt; getViewInstanceDefinitions(); /** * Get a view controller associated with this context. * * @return the view controller */public ViewController getController(); /** * Get the HTTP Impersonator. * * @return the HTTP Impersonator, which internally uses the App Cookie Manager */public HttpImpersonator getHttpImpersonator(); /** * Get the default settings for the Impersonator. * * @return the Impersonator settings. */public ImpersonatorSetting getImpersonatorSetting(); UI Componentsview包里可能包含一些web应用的组件，类似html或者javascript啥的。 WEB-INF/web.xmlweb.xml 是把view发布为一个web app的主要文件。 例如： 123456789&lt;servlet&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;servlet-class&gt;org.apache.ambari.view.hello.HelloServlet&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;HelloServlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/ui&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; ServletsServlets contained in a view archive will be deployed as part of the view and mapped as described in the web.xml. 在servlet的init方法中，可以view context当做一个servlet context使用。 例如:12345678910private ViewContext viewContext;@Overridepublic void init(ServletConfig config) throws ServletException &#123; super.init(config); ServletContext context = config.getServletContext(); viewContext = (ViewContext) context.getAttribute(ViewContext.CONTEXT_ATTRIBUTE);&#125; servlet可以使用view conetxt获取ambari container提供的一个信息。例如，在doGet()方法中，我们就可以使用view context instance访问当前的用户和view instance的属性。 1234567891011121314protected void doGet(HttpServletRequest request, HttpServletResponse response) throws IOException &#123; response.setContentType("text/html"); response.setStatus(HttpServletResponse.SC_OK); PrintWriter writer = response.getWriter(); Map&lt;String, String&gt; properties = viewContext.getProperties(); String name = properties.get("name"); if (name == null) &#123; name = viewContext.getUsername(); &#125; writer.println("&lt;h1&gt;Hello " + name + "!&lt;/h1&gt;");&#125; ResourcesResources一般不会暴露出来。resource都定义在view.xml里面。每个resource都需要指定名称和service class. 每个resource endpoint应该也可以被ambari api访问. API. Service Classservice类使用JAX-RS注解来定义resource的endpoint。注意需要注入ViewContext. 1234567891011121314151617181920@InjectViewContext context;/** * @see org.apache.ambari.view.filebrowser.DownloadService * @return service */@Path("/download")public DownloadService download() &#123; return new DownloadService(context);&#125; /** * @see org.apache.ambari.view.filebrowser.FileOperationService * @return service */@Path("/fileops")public FileOperationService fileOps() &#123; return new FileOperationService(context);&#125; Managed Resourcesmanaged resource是由ambari api框架管理的resource。除了name和service class之外，managed resource定义还需要plural name，resource class和provider class。 See view.xml. 使用managed resource的好处是任何通过rest api访问的查询都可以使用ambari api框架的一些既有的特性，包括partial response, query predicates, pagination, sub-resource queries等。See API. Service Classservice使用JAX-RS注解定义获取resource的action。 下面的例子展示了service class怎样将request代理给api框架处理。注意要注入ViewResourceHandler，我们使用它在api框架中传递control。1234567891011@InjectViewResourceHandler resourceHandler;…@GET@Path("&#123;scriptId&#125;")@Produces(MediaType.APPLICATION_JSON)public Response getScript(@Context HttpHeaders headers, @Context UriInfo ui, @PathParam("scriptId") String scriptId) &#123; return resourceHandler.handleRequest(headers, ui, scriptId);&#125; Resource Classresource class是一个包含属性和resource的JavaBean。 12345678910111213141516171819public class PigScript &#123; private String id; private String title; private String pigScript; private String pythonScript; private String templetonArguments; private Date dateCreated; private String owner; … public String getId() &#123; return id; &#125; public void setId(String id) &#123; this.id = id; &#125; … &#125; Resource Provider ClassResource provider class实现ResourceProvider接口. 给ambari api框架提供访问resource的方式。 123456789101112131415161718192021public class ScriptResourceProvider implements ResourceProvider&lt;PigScript&gt; &#123; @Inject ViewContext viewContext; @Override public FileResource getResource(String resourceId, Set&lt;String&gt; propertyIds) throws SystemException, NoSuchResourceException, UnsupportedPropertyException &#123; Map&lt;String, String&gt; properties = viewContext.getProperties(); String userScriptsPath = properties.get("dataworker.userScriptsPath"); try &#123; return getResource(resourceId, userScriptsPath, propertyIds); &#125; catch (IOException e) &#123; throw new SystemException("Can't get resource " + resourceId + ".", e); &#125; &#125; ... &#125; 权限administrator可以在任何view实例上设置VIEW.USE权限。 See API. 定制权限编辑 view.xml1234567891011121314151617181920212223 &lt;view&gt; &lt;name&gt;RESTRICTED&lt;/name&gt; &lt;label&gt;The Restricted View&lt;/label&gt; &lt;version&gt;1.0.0&lt;/version&gt; &lt;resource&gt; &lt;name&gt;restricted&lt;/name&gt; &lt;service-class&gt;org.apache.ambari.view.restricted.RestrictedResource&lt;/service-class&gt; &lt;/resource&gt; &lt;resource&gt; &lt;name&gt;unrestricted&lt;/name&gt; &lt;service-class&gt;org.apache.ambari.view.restricted.UnrestrictedResource&lt;/service-class&gt; &lt;/resource&gt; &lt;permission&gt; &lt;name&gt;RESTRICTED&lt;/name&gt; &lt;description&gt; Access permission for a restricted view resource. &lt;/description&gt; &lt;/permission&gt; &lt;instance&gt; &lt;name&gt;INSTANCE_1&lt;/name&gt; &lt;/instance&gt;&lt;/view&gt; 这个配置文件定义了一个叫做RESTRICTED的view，它有一个实例. 还定义了一个叫做RESTRICTED的权限，两个分别叫做 ‘unrestricted’和 ‘restricted’的resource.administrator可以将RESTRICTED权限分配给任意的RESTRICTED view的实例。 view的实现代码使用了view context来验证我们定义的权限。 1234567891011121314151617@InjectViewContext context;@GET@Produces(&#123;"text/html"&#125;)public Response getRestricted() throws IOException&#123; String userName = context.getUsername(); try &#123; context.hasPermission(userName, "RESTRICTED"); &#125; catch (org.apache.ambari.view.SecurityException e) &#123; return Response.status(401).build(); &#125; return Response.ok("&lt;b&gt;You have accessed a restricted resource.&lt;/b&gt;").type("text/html").build();&#125; ImpersonationViews can utilize the viewContext to facilitate calls that require impersonating a user. For example, a service may expose an endpoint that accepts parameters like “doAs=johndoe” to perform some action on behalf of that user.The HttpImpersonator Interface provides a contract for how to perform an HTTP GET request on a URL that supports some type of “doAs” parameter, and the username.123HttpImpersonator impersonator = viewContext.getHttpImpersonator();ImpersonatorSetting impersonatorSetting = viewContext.getImpersonatorSetting();String result = impersonator.requestURL(urlToRead, "GET", impersonatorSetting); The ImpersonatorSetting class contains the variables that are added to the URL params. Its default constructor sets “doAs” as the default query parameter name, and the currently logged on user as its value; both of these can be changed with the overloaded constructors. 持久化应用的数据并不像view实例的属性一样，他们或许包含任意字符串，view需要把这些数据持久化，而且还提供更新以及查询。 view context包含从应用数据地图里put或者set数据的方法。1234567891011121314151617181920212223242526272829/** * Save an instance data value for the given key. * * @param key the key * @param value the value */public void putInstanceData(String key, String value);/** * Get the instance data value for the given key. * * @param key the key * * @return the instance data value */public String getInstanceData(String key);/** * Get the instance data values. * * @return the view instance property values */public Map&lt;String, String&gt; getInstanceData();/** * Remove the instance data value for the given key. * * @param key the key */ DataStoredata store 允许view instance存储javabean到ambari数据库里。 view组件先通过view context获取一个datastore的实例。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556 /** * Get a data store for view persistence entities. * * @return a data store */ public DataStore getDataStore();The DataStore object exposes a simple interface for persisting view entities. The basic CRUD operations are supported … /** * Save the given entity to persistent storage. The entity must be declared as an * &lt;entity&gt; in the &lt;persistence&gt; element of the view.xml. * * @param entity the entity to be persisted. * * @throws PersistenceException thrown if the given entity can not be persisted */ public void store(Object entity) throws PersistenceException; /** * Remove the given entity from persistent storage. * * @param entity the entity to be removed. * * @throws PersistenceException thrown if the given entity can not be removed */ public void remove(Object entity) throws PersistenceException; /** * Find the entity of the given class type that is uniquely identified by the * given primary key. * * @param clazz the entity class * @param primaryKey the primary key * @param &lt;T&gt; the entity type * * @return the entity; null if the entity can't be found * * @throws PersistenceException thrown if an error occurs trying to find the entity */ public &lt;T&gt; T find(Class&lt;T&gt; clazz, Object primaryKey) throws PersistenceException; /** * Find all the entities for the given where clause. Specifying null for the where * clause should return all entities of the given class type. * * @param clazz the entity class * @param whereClause the where clause; may be null * @param &lt;T&gt; the entity type * * @return all of the entities for the given where clause; empty collection if no * entities can be found * * @throws PersistenceException */ public &lt;T&gt; Collection&lt;T&gt; findAll(Class&lt;T&gt; clazz, String whereClause) throws PersistenceException; 每个entity被持久化之前，都应该已经在view.xml里定义过了。 See view.xml. EventsView生命周期Events实现View接口之后，每个view都可以监控自己的生命周期。12345678910public interface View &#123; ... public void onDeploy(ViewDefinition definition); public void onCreate(ViewInstanceDefinition definition); public void onDestroy(ViewInstanceDefinition definition);&#125; View实现类应该在view.xml文件里声明注册1234567&lt;view&gt; &lt;name&gt;FILES&lt;/name&gt; &lt;label&gt;Files&lt;/label&gt; &lt;version&gt;0.1.0&lt;/version&gt; ... &lt;view-class&gt;org.apache.ambari.view.filebrowser.ViewImpl&lt;/view-class&gt;&lt;/view&gt; View的实现类在注册过之后，它的对应方法会在任何生命周期event发生的时候被触发。 Event 描述 参数 Deploy The view has been successfully deployed. The deployed view definition. Create An instance of the view has been created. The new view instance definition. Destroy An instance of the view has been destroyed. The destroyed view instance definition. 自定义event饿哦们也可以定义一些自定义事件。 Listenersview需要写一个listener接口的实现，然后注册这个listener到view框架。123456public interface Listener &#123; ... public void notify(Event event);&#125; listener的notify方法会在view发射一个event的时候被触发。123456789public class JobsListener implements Listener &#123; @Override public void notify(Event event) &#123; if (event.getId().equals("NEW_JOB")) &#123; ViewInstanceDefinition instance = event.getViewInstanceSubject(); ... &#125; &#125;&#125; ViewControllerViewController用来注册listener，还有就是发射event。12345678910public interface ViewController &#123; ... public void fireEvent(String eventId, Map&lt;String, String&gt; eventProperties); public void registerListener(Listener listener, String viewName); public void unregisterListener(Listener listener, String viewName); &#125; view组件可以访问通过注入的view context来时用view controller。12345678910111213141516171819 @Inject ViewContext context; ... ViewController controller = context.getController();``` view component可以注册监听其他view发射的event。```java @Inject ViewContext context; ... Listener listener = new JobsListener(); ... ViewController controller = context.getController(); controller.registerListener(listener, "JOBS"); view component也可以发射自定义的view通知event. 123456789101112@InjectViewContext context;...Map&lt;String, String&gt; jobEventProperties = new HashMap&lt;String, String&gt;();jobEventProperties.put("JobId", jobId);jobEventProperties.put("JobOwner", jobOwner);...ViewController controller = context.getController();controller.fireEvent("NEW_JOB", jobEventProperties); 打包所有的view都应该打包，包含配置文件和不同的组件，比如resource和resource provider啥的，还有一些 html, JavaSript and servlets. 依赖The view may include dependency classes and jars directly into the archive. Classes should be included under WEB-INF/classes. Jars should be included under WEB-INF/lib. view.xml 元素 说明 name The unique view name (required). label The user facing name. version The view version (required). description The description of the view. icon64 The 64x64 icon to display for this view. If this property is not set, the 32x32 sized icon will be used. icon The 32x32 icon to display for this view. Suggested size is 32x32 and will be displayed as 8x8 and 16x16 as necessary. If this property is not set, a default view framework icon is used. system Indicates whether or not this is a system view. Default is false. view-class The View class to receive framework events. masker-class The Masker class for masking view parameters. parameter Defines a configuration parameter that is used to when creating a view instance. resource Describe a resources exposed by the view. permission Defines a custom permission for the view. persistence Describe a view entities for persistence. instance Define an instances of a view. 参数 元素 描述 name The parameter name. description The parameter description. required Determines whether or not the parameter is required. masked Indicated this parameter value is to be “masked” in the Ambari Web UI (i.e. not shown in the clear). Omitting this element default to not-masked. Otherwise, if true, the parameter value will be “masked” in the Web UI. resource 元素 描述 name The resource name (i.e file). plural-name The parameter description (i.e. files). * id-property The id field of the managed resource class. * resource-class The class of the JavaBean that contains the attributes of a managed resource. * provider-class The class of the managed resource provider. * service-class The class of the JAX-RS annotated service resource. sub-resource-name The sub-resource name. 12345678910111213141516 &lt;resource&gt; &lt;name&gt;files&lt;/name&gt; &lt;service-class&gt;org.apache.ambari.view.filebrowser.FileBrowserService&lt;/service-class&gt; &lt;/resource&gt;\* only required for a managed resource. For example ... &lt;resource&gt; &lt;name&gt;script&lt;/name&gt; &lt;plural-name&gt;scripts&lt;/plural-name&gt; &lt;id-property&gt;id&lt;/id-property&gt; &lt;resource-class&gt;org.apache.ambari.view.pig.resources.scripts.models.PigScript&lt;/resource-class&gt; &lt;provider-class&gt;org.apache.ambari.view.pig.resources.scripts.ScriptResourceProvider&lt;/provider-class&gt; &lt;service-class&gt;org.apache.ambari.view.pig.resources.scripts.ScriptService&lt;/service-class&gt; &lt;/resource&gt; 权限 Element Description name The permission name. description The permission description. 持久化 Element Description entity An entity that may be persisted through the DataStore. entity Element Description class The class ot the JavaBean that contains the attributes of an entity. id-property The id field of the entity. 例子123456789101112131415&lt;persistence&gt; &lt;entity&gt; &lt;class&gt;org.apache.ambari.view.employee.EmployeeEntity&lt;/class&gt; &lt;id-property&gt;id&lt;/id-property&gt; &lt;/entity&gt; &lt;entity&gt; &lt;class&gt;org.apache.ambari.view.employee.AddressEntity&lt;/class&gt; &lt;id-property&gt;id&lt;/id-property&gt; &lt;/entity&gt; &lt;entity&gt; &lt;class&gt;org.apache.ambari.view.employee.ProjectEntity&lt;/class&gt; &lt;id-property&gt;id&lt;/id-property&gt; &lt;/entity&gt;&lt;/persistence&gt; instance Element Description name The unique instance name (required). label The display label of the view instance. If not set, the view definition label is used. description The description of the view instance. If not set, the view definition description is used. icon64 Overrides the view icon64 for this specific view instance. icon Overrides the view icon for this specific view instance. visible If true, for the view instance to show up in the users view instance list. The default value is true. property Specifies configuration parameters values for the view instance. property Element Description key The property key (must match view parameter name). value The property value. 完整的 view.xml例子123456789101112131415161718192021222324252627282930&lt;view&gt; &lt;name&gt;FILES&lt;/name&gt; &lt;label&gt;Files&lt;/label&gt; &lt;version&gt;0.1.0&lt;/version&gt; &lt;parameter&gt; &lt;name&gt;dataworker.defaultFs&lt;/name&gt; &lt;description&gt;FileSystem URI&lt;/description&gt; &lt;required&gt;true&lt;/required&gt; &lt;/parameter&gt; &lt;parameter&gt; &lt;name&gt;dataworker.username&lt;/name&gt; &lt;description&gt;The username (defaults to ViewContext username)&lt;/description&gt; &lt;required&gt;false&lt;/required&gt; &lt;/parameter&gt; &lt;resource&gt; &lt;name&gt;files&lt;/name&gt; &lt;service-class&gt;org.apache.ambari.view.filebrowser.FileBrowserService&lt;/service-class&gt; &lt;/resource&gt; &lt;instance&gt; &lt;name&gt;FILES_1&lt;/name&gt; &lt;property&gt; &lt;key&gt;dataworker.defaultFs&lt;/key&gt; &lt;value&gt;hdfs://&lt;server&gt;:8020&lt;/value&gt; &lt;/property&gt; &lt;/instance&gt;&lt;/view&gt; 这个例子中配置了一个叫FILES的view，也是有一个实例FILES_1。可以看到这个view有一个required的参数‘dataworker.defaultFs’以及一个optional参数‘dataworker.username’. 还有一个访问资源的入口‘files’. 发布要发布一个view，只需要将view的包放到ambari-server的views文件夹下，默认是在 /var/lib/ambari-server/resources/views 解压包view被发布之后，会自动在views文件夹下解压包。默认位置是: /var/lib/ambari-server/resources/views/work/:viewName{:viewVersion} 例如，上面例子中的files view解压后目录就是： /var/lib/ambari-server/resources/views/work/FILES{0.1.0} 用户可以直接修改解压后的文件，在ambari下次启动的时候就可以生效了。 使用APIGet Views获取所有已经发布的view。注意view是顶级resource，不属于任何其他resource。 GET http://&lt;server&gt;:8080/api/v1/views/ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/&quot;, &quot;items&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES&quot;, &quot;ViewInfo&quot; : { &quot;view_name&quot; : &quot;FILES&quot; } }, { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/HELLO_WORLD&quot;, &quot;ViewInfo&quot; : { &quot;view_name&quot; : &quot;HELLO_WORLD&quot; } } ] } Get View根据名称获取指定view，返回所有版本. GET http://&lt;server&gt;:8080/api/v1/views/FILES/ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/&quot;, &quot;ViewInfo&quot; : { &quot;view_name&quot; : &quot;FILES&quot; }, &quot;versions&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0&quot;, &quot;ViewVersionInfo&quot; : { &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot; } } ] } Get Versions获取一个view 的所有版本. GET http://&lt;server&gt;:8080/api/v1/views/FILES/versions/ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/&quot;, &quot;items&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0&quot;, &quot;ViewVersionInfo&quot; : { &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot; } } ] } Get Version获取某view的指定版本以及此版本下的所有实例. GET http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/&quot;, &quot;ViewVersionInfo&quot; : { &quot;archive&quot; : &quot;/var/lib/ambari-server/resources/views/work/FILES{0.1.0}&quot;, &quot;label&quot; : &quot;Files&quot;, &quot;parameters&quot; : [ { &quot;name&quot; : &quot;dataworker.defaultFs&quot;, &quot;description&quot; : &quot;FileSystem URI&quot;, &quot;required&quot; : true }, { &quot;name&quot; : &quot;dataworker.username&quot;, &quot;description&quot; : &quot;The username (defaults to ViewContext username)&quot;, &quot;required&quot; : false } ], &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot; }, &quot;instances&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_1&quot;, &quot;ViewInstanceInfo&quot; : { &quot;instance_name&quot; : &quot;FILES_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot; } } ] } Get Instances获取一个view 的所有实例. GET http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/&quot;, &quot;items&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_1&quot;, &quot;ViewInstanceInfo&quot; : { &quot;instance_name&quot; : &quot;FILES_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot; } } ] } 获取实例根据实例名称获取实例，此实例的所有resource也会被返回。 GET http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_1 { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_1&quot;, &quot;ViewInstanceInfo&quot; : { &quot;context_path&quot; : &quot;/views/FILES/0.1.0/FILES_1&quot;, &quot;instance_name&quot; : &quot;FILES_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot;, &quot;instance_data&quot; : { }, &quot;properties&quot; : { &quot;dataworker.defaultFs&quot; : &quot;hdfs://&lt;server&gt;:8020&quot; } }, &quot;resources&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_1/resources/files&quot;, &quot;instance_name&quot; : &quot;FILES_1&quot;, &quot;name&quot; : &quot;files&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;FILES&quot; } ] } 创建实例POST http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_2 更新实例PUT http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_2 [{ &quot;ViewInstanceInfo&quot; : { &quot;properties&quot; : { &quot;dataworker.defaultFs&quot; : &quot;hdfs://MyServer:8020&quot; } } }] 删除实例DELETE http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_2 ResourcesA view resource may be accessed through the REST API. The href for each view resource can be found in a request for the parent view instance. The resource endpoints and behavior depends on the implementation of the JAX-RS annotated service class specified for the resource element in the view.xml. GET http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/FILES_1/resources/files/fileops/listdir?path=%2F [{&quot;path&quot;:&quot;/app-logs&quot;,&quot;replication&quot;:0,&quot;isDirectory&quot;:true,&quot;len&quot;:0,&quot;owner&quot;:&quot;yarn&quot;,&quot;group&quot;:&quot;hadoop&quot;,&quot;permission&quot;:&quot;-rwxrwxrwx&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1400006792122,&quot;blockSize&quot;:0},{&quot;path&quot;:&quot;/mapred&quot;,&quot;replication&quot;:0,&quot;isDirectory&quot;:true,&quot;len&quot;:0,&quot;owner&quot;:&quot;mapred&quot;,&quot;group&quot;:&quot;hdfs&quot;,&quot;permission&quot;:&quot;-rwxr-xr-x&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1400006653817,&quot;blockSize&quot;:0},{&quot;path&quot;:&quot;/mr-history&quot;,&quot;replication&quot;:0,&quot;isDirectory&quot;:true,&quot;len&quot;:0,&quot;owner&quot;:&quot;hdfs&quot;,&quot;group&quot;:&quot;hdfs&quot;,&quot;permission&quot;:&quot;-rwxr-xr-x&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1400006653822,&quot;blockSize&quot;:0},{&quot;path&quot;:&quot;/tmp&quot;,&quot;replication&quot;:0,&quot;isDirectory&quot;:true,&quot;len&quot;:0,&quot;owner&quot;:&quot;hdfs&quot;,&quot;group&quot;:&quot;hdfs&quot;,&quot;permission&quot;:&quot;-rwxrwxrwx&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1400006720415,&quot;blockSize&quot;:0},{&quot;path&quot;:&quot;/user&quot;,&quot;replication&quot;:0,&quot;isDirectory&quot;:true,&quot;len&quot;:0,&quot;owner&quot;:&quot;hdfs&quot;,&quot;group&quot;:&quot;hdfs&quot;,&quot;permission&quot;:&quot;-rwxr-xr-x&quot;,&quot;accessTime&quot;:0,&quot;modificationTime&quot;:1400006610050,&quot;blockSize&quot;:0}] Managed ResourcesAny managed resources for a view may also be accessed through the REST API. Note that instances of a managed resource appear as sub-resources of an instance under the plural name specified for the resource in the view.xml. In this example, a list of managed resources named ‘scripts’ is included in the response for the view instance … GET http://&lt;server&gt;:8080/api/v1/views/PIG/versions/0.1.0/instances/INSTANCE_1 { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/PIG/versions/0.1.0/instances/INSTANCE_1&quot;, &quot;ViewInstanceInfo&quot; : { &quot;context_path&quot; : &quot;/views/PIG/0.1.0/INSTANCE_1&quot;, &quot;instance_name&quot; : &quot;INSTANCE_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;PIG&quot;, &quot;instance_data&quot; : { }, &quot;properties&quot; : { … } }, &quot;resources&quot; : [ ], &quot;scripts&quot; : [ { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/PIG/versions/0.1.0/instances/INSTANCE_1/scripts/script1&quot;, &quot;id&quot; : &quot;script1&quot;, &quot;instance_name&quot; : &quot;INSTANCE_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;PIG&quot; }, { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/FILES/versions/0.1.0/instances/INSTANCE_1/scripts/script2&quot;, &quot;id&quot; : &quot;script2&quot;, &quot;instance_name&quot; : &quot;INSTANCE_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;PIG&quot; }, … ] } 获取单个managed resource … GET http://&lt;server&gt;:8080/api/v1/views/PIG/versions/0.1.0/instances/INSTANCE_1/scripts/script1 { &quot;href&quot; : &quot;http://&lt;server&gt;:8080/api/v1/views/PIG/versions/0.1.0/instances/INSTANCE_1/scripts/script1&quot;, &quot;id&quot; : &quot;script1&quot;, &quot;pigScript&quot; : &quot;… &quot;, &quot;pythonScript&quot; : &quot;… &quot;, &quot;templetonArguments&quot; : &quot;… &quot;, &quot;dateCreated&quot; : … , &quot;owner&quot; : &quot;… &quot;, &quot;instance_name&quot; : &quot;INSTANCE_1&quot;, &quot;version&quot; : &quot;0.1.0&quot;, &quot;view_name&quot; : &quot;PIG&quot; } 由于resource是被ambari api框架管理的，也可以使用partial response和query predicate。 GET http://&lt;server&gt;:8080/api/v1/views/PIG/versions/0.1.0/instances/INSTANCE_1/scripts?fields=pythonScript&amp;owner=jsmith 设置view权限To grant privileges to access the restricted resource we can create a privilege sub-resource for the view instance. The following API will grant RESTICTED permission to the user ‘bob’ for the view instance ‘INSTANCE_1’ of the ‘RESTRICTED’ view. POST http://&lt;server&gt;/api/v1/views/RESTRICTED/versions/1.0.0/instances/INSTANCE_1 [ { &quot;PrivilegeInfo&quot; : { &quot;permission_name&quot; : &quot;RESTRICTED&quot;, &quot;principal_name&quot; : &quot;bob&quot;, &quot;principal_type&quot; : &quot;USER&quot; } } ] 查询view权限We can see a privilege sub resource for a view instance. GET http://&lt;server&gt;/api/v1/views/RESTRICTED/versions/1.0.0/instances/INSTANCE_1/privileges/5 { &quot;href&quot; : &quot;http://&lt;server&gt;/api/v1/views/RESTRICTED/versions/1.0.0/instances/INSTANCE_1/privileges/5&quot;, &quot;PrivilegeInfo&quot; : { &quot;instance_name&quot; : &quot;INSTANCE_1&quot;, &quot;permission_name&quot; : &quot;RESTRICTED&quot;, &quot;principal_name&quot; : &quot;bob&quot;, &quot;principal_type&quot; : &quot;USER&quot;, &quot;privilege_id&quot; : 5, &quot;version&quot; : &quot;1.0.0&quot;, &quot;view_name&quot; : &quot;RESTRICTED&quot; } } View UIThe context root of a view instance is … /views/:viewName/:viewVersion/:viewInstance/ For example, the context root can be found in the response for a view instance … So, to access the UI of the above Files view the user would browse to … http://&lt;server&gt;:8080/views/FILES/0.1.0/FILES_1/]]></content>
      <tags>
        <tag>ambari</tag>
        <tag>源码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步搞定ETL依赖调度平台（三）-调度优先级思考]]></title>
    <url>%2F2016%2F06%2F17%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%90%9E%E5%AE%9AETL%E4%BE%9D%E8%B5%96%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%EF%BC%88%E4%B8%89%EF%BC%89-%E8%B0%83%E5%BA%A6%E4%BC%98%E5%85%88%E7%BA%A7%E6%80%9D%E8%80%83%2F</url>
    <content type="text"><![CDATA[从《azkaban源码追踪（三）-上传zip的坑》可以看到azkaban本身的调度基本是广度优先执行的。 1.猎聘 优先级生成策略是从叶子节点向上遍历，每个节点的优先级=所有子节点优先级+edge数(也就是子节点数)。其实思想就是谁下面的小弟多，谁的优先级就越大。 但这通常并不完全适用于对于指定的单个叶子节点最优先的需求，极端点儿想的话，可能出现老的ETL有100多个相关成一个flow，可是新增的优先级特别高的需求super_etl在另一个只有1个节点的flow，或者在同一个flow，但是它没有小弟（这种其实蛮常见的）。那么super_etl的默认priority是1，根本没有地位！！！ LP的人说了：我们可以给它人工指定优先级啊！额。。。。是的，但是其他的tree是在一直生长的，按照你们的策略来看，这个super_etl的优先级要一直改变。LP的人又说了：我们可以动态指定！额。。。。是的，但是后面出现了super_etl01,super_etl02的时候，可能就会特别特别麻烦了。 再看一下，计划顺序与实际执行顺序的对比。本来C的优先级是比B高的，可猎聘给出的实际执行却是先执行B后执行c，音频中苏总说是并发问题，个人看来稍稍有些含糊其辞了。推测猎聘的实际调度的时候并不完全以priority为指导，而仍然是azkaban广度遍历优先，同一个level之内才是priority优先执行。 参考:http://mp.weixin.qq.com/s?src=3&amp;timestamp=1466131251&amp;ver=1&amp;signature=YfS7PgVR0SjjLt4KwIQ297t4SDiLrrJu5C7uuMXpkCvuhvjXk-DU*eJGfIBSfjDm-WlVuwaIbGtbfda-k-5rRgUewAlvjSUQBBNY4jnjuvQD1puqdaVzUqIq9ds*x1r-6dUnU6OiSERzBt5UWfjdUkekpmc7hNqTm1pklClkSkE= 2.美团 调度策略就不该是递归调度，而应该完全按照优先级别去调度。想来想去，这才是真正的优先级调度。东家靠谱儿！ 参考：http://mp.weixin.qq.com/s?src=3&amp;timestamp=1466131231&amp;ver=1&amp;signature=YfS7PgVR0SjjLt4KwIQ297t4SDiLrrJu5C7uuMXpkCtKEZcZ58gspxJhkgDlgXAkqQvXpI3IbmE4HzkI33W*4PJCOUoKyzSna56Q8uSQgUNnSl7vsxqLcREVfDBU0nPguiAiwnosiXyE9wB7T-edr3QrRn8rKMhB3oxjQDrS1RQ= 综上：后面本人要开发的调度系统优先级策略会借用美团的思想。]]></content>
      <tags>
        <tag>ETL</tag>
        <tag>调度平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban源码追踪（三）-上传zip的坑]]></title>
    <url>%2F2016%2F06%2F16%2Fazkaban%E6%BA%90%E7%A0%81%E8%BF%BD%E8%B8%AA%EF%BC%88%E4%B8%89%EF%BC%89-%E4%B8%8A%E4%BC%A0zip%E7%9A%84%E5%9D%91%2F</url>
    <content type="text"><![CDATA[前面其实已经解析了上传zip文件到azkaban的流程，但是自己都感觉写的实在是太操蛋了。 1. 时序图 2. 坑的表现ETL的依赖关系肯定的多父多子的，所以就上传了一个典型到azkaban上。结果被分成了两个flow 可以看到对于mymeta@my_table_c之上的etl，对于下面的node应该是公用的，像现在这样两个flow都有的话，就会导致这些node被多次执行。 3.源码分析根据上面的时序图，我们可以看到问题应该是出在分析依赖或者创建flow的过程中。所以过去剖析对应源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// 就是把每个node的dependency map放进HashMap&lt;String, Map&lt;String, Edge&gt;&gt; nodeDependencies里面，一个对应多个dependenciesprivate void resolveDependencies() &#123; // Add all the in edges and out edges. Catch bad dependencies and self // referrals. Also collect list of nodes who are parents. for (Node node : nodeMap.values()) &#123; Props props = jobPropsMap.get(node.getId()); List&lt;String&gt; dependencyList = props.getStringList(CommonJobProperties.DEPENDENCIES, (List&lt;String&gt;) null); if (dependencyList != null) &#123; Map&lt;String, Edge&gt; dependencies = nodeDependencies.get(node.getId()); if (dependencies == null) &#123; dependencies = new HashMap&lt;String, Edge&gt;(); for (String dependencyName : dependencyList) &#123; dependencyName = dependencyName == null ? null : dependencyName.trim(); if (dependencyName == null || dependencyName.isEmpty()) &#123; continue; &#125; Edge edge = new Edge(dependencyName, node.getId()); Node dependencyNode = nodeMap.get(dependencyName); if (dependencyNode == null) &#123; if (duplicateJobs.contains(dependencyName)) &#123; edge.setError("Ambiguous Dependency. Duplicates found."); dependencies.put(dependencyName, edge); errors.add(node.getId() + " has ambiguous dependency " + dependencyName); &#125; else &#123; edge.setError("Dependency not found."); dependencies.put(dependencyName, edge); errors.add(node.getId() + " cannot find dependency " + dependencyName); &#125; &#125; else if (dependencyNode == node) &#123; // We have a self cycle edge.setError("Self cycle found."); dependencies.put(dependencyName, edge); errors.add(node.getId() + " has a self cycle"); &#125; else &#123; dependencies.put(dependencyName, edge); &#125; &#125; if (!dependencies.isEmpty()) &#123; nodeDependencies.put(node.getId(), dependencies); &#125; &#125; &#125; &#125; &#125; 跟踪源码的时候，没有发现上面代码有任何问题。再看下根据依赖构建flow的过程123456789101112131415161718192021222324252627282930313233private void buildFlowsFromDependencies() &#123; // Find all root nodes by finding ones without dependents. /********************注意这里 ***********************/ HashSet&lt;String&gt; nonRootNodes = new HashSet&lt;String&gt;(); for (Map&lt;String, Edge&gt; edges : nodeDependencies.values()) &#123; for (String sourceId : edges.keySet()) &#123; nonRootNodes.add(sourceId); &#125; &#125; // Now create flows. Bad flows are marked invalid Set&lt;String&gt; visitedNodes = new HashSet&lt;String&gt;(); for (Node base : nodeMap.values()) &#123; // Root nodes can be discovered when parsing jobs /********************注意这里 ***********************/ if (rootNodes.contains(base.getId()) || !nonRootNodes.contains(base.getId())) &#123; rootNodes.add(base.getId()); Flow flow = new Flow(base.getId()); Props jobProp = jobPropsMap.get(base.getId()); // 遍历properties里的各种通知邮件啥的 flow.addFailureEmails(failureEmail); flow.addSuccessEmails(successEmail); flow.addAllFlowProperties(flowPropsList); constructFlow(flow, base, visitedNodes); flow.initialize(); flowMap.put(base.getId(), flow); &#125; &#125;&#125; 上面第一个地方是找到所有非root的node，看代码意思就是所有有dependency的node。第二个注意的地方是找rootNodes里有或者不存在与非rootNode集合的node。这不是叶子节点么，下面紧接着就add进去rootNodes了。也就是说azkaban中这里的node的root角色其实是指的叶子节点。第三个是flow是按照叶子节点的id命名的，也就是有多少叶子节点就会有多少flow被生成。解释通了，因为我们的project的zip文件中是有两个叶子节点的job，所以就被分成了两个flow。 4. 解决方案生成最终的zip文件之前，生成一个虚拟end job，把所有的叶子节点都配置成它的dependency，这样就只有一个flow了。 可以看到，并列在同一个level的node是并发执行的，因为他们两个之间相互没有任何影响。这样使用并发充分利用资源，但是azkaban的并发性能和稳定性需要进一步探查。 5.TODO每个任务的Runner稳定性测试与定制开发(希望不需要)]]></content>
      <tags>
        <tag>源码</tag>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步搞定ETL依赖调度平台（二）-zakaban整合实施]]></title>
    <url>%2F2016%2F06%2F16%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%90%9E%E5%AE%9AETL%E4%BE%9D%E8%B5%96%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%EF%BC%88%E4%BA%8C%EF%BC%89-%E4%B8%8Eazkaban%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[再列以下前面提到的步骤： 通过azkaban api创建一个project: etl_20160615; 调用java程序根据tbl_blood和etl内容生成azkaban的项目文件20160615.zip; 通过azkaban api上传此文件到etl_20160615的project上 通过azkaban api遍历etl_20160615项目下的所有flow，进行调度。 其实（一）里面是第二步的实现。今儿调研了以下azkaban的ajax api，并没有想象中的那么简单，所以就再单独说一下怎样完成其他步骤——重要的难点在于session-id的获取。 1. 方案与问题 方案 问题 解决方案 使用azkaban api 需要session-id进行server端验证 1. 使用phantomjs等模拟实现浏览器，从cookie中获取session-id——费事2. 想办法使一个session-id永不过期(这个实施起来最简单，但是稳定程度有待测试)3.azkaban web server启动了rest.li里的userManagerResource，login是返回session-id的，但是需要研习以下rest.li找了半天，解决了: curl -k -X POST –data “username=azkaban&amp;password=azkaban&amp;action=login” http://10.1.5.66:8001 直接操作数据 需要对上传zip文件后生成的mysql表足够了解，包括存储的每个字段的内容与规范 将现有的项目中的内容弄出来验证规范后，开始实施 抽取操作中用到的azkaban的主要类 准备实例化这些类的成员变量 这些成员变量或许比直接插入mysql内容差不多难搞 2. 实施2.1 获取session-id鉴于时间因素，获取session-id的方案： curl -k -X POST –data “username=azkaban&amp;password=azkaban&amp;action=login” http://10.1.5.66:8001返回值：1234&#123; "status" : "success", "session.id" : "4e3da52b-fa1f-478d-836c-42122ced2341"&#125; 解析session-id的脚本123curl -k -X POST --data "username=azkaban&amp;password=azkaban&amp;action=login" http://10.1.5.66:8001 &gt; etl_tmpsession_id=`cat etl_tmp | grep session | awk -F\" '!/[&#123;&#125;]/&#123;print $(NF-1)&#125;'`echo "we got session id : $session_id" 2.2 创建project这里其实先安装了一个shell的json解析工具JSON.sh：https://github.com/dominictarr/JSON.sh1234567curl -k -X POST --data "session.id=$&#123;session_id&#125;&amp;name=$&#123;project_name&#125;&amp;description=$&#123;project_desc&#125;" http://$&#123;host&#125;/manager?action=create &gt; $&#123;etl_tmp&#125;status=`cat $&#123;etl_tmp&#125; | JSON.sh -b| grep status | awk '&#123;print($2)&#125;'`if [ $status != '"success"' ]; then echo "error happends when creting project $&#123;project_name&#125;. status is $&#123;status&#125;"fiecho "project $&#123;project_name&#125; has been created successfully." 2.3 上传zip文件1234567curl -k -i -H "Content-Type: multipart/mixed" -X POST --form "session.id=$&#123;session_id&#125;" --form 'ajax=upload' --form "file=@$&#123;project_zip_file&#125;" --form "project=$&#123;project_name&#125;" http://$&#123;host&#125;/manager?ajax=upload &gt; $&#123;etl_tmp&#125;status=`cat $&#123;etl_tmp&#125; | awk '&#123;if(index($0,"error") &gt; 0) print("error")&#125;'`if [ $status == "error" ]; then echo "error happends when uploading project $&#123;project_name&#125;. status is $&#123;status&#125;" exit 1fiecho "uploading project $&#123;project_name&#125; successfully" ps: 没有继续使用JSON.sh的原因是解析报错…看来还是不健全 2.4 执行flow12345678curl -k --get --data "session.id=$&#123;session_id&#125;&amp;ajax=fetchprojectflows&amp;project=$&#123;project_name&#125;" http://$&#123;host&#125;/manager &gt; $&#123;etl_tmp&#125;for flow in `cat $&#123;etl_tmp&#125; | JSON.sh -b | grep flowId | awk '&#123;gsub("\"","",$2);print($2)&#125;'`do echo "flow is $flow, ready to execute" curl -k --get --data "session.id=$&#123;session_id&#125;" --data 'ajax=executeFlow' --data "project=$&#123;project_name&#125;" --data "flow=$&#123;flow&#125;" http://$&#123;host&#125;/executor &gt;$&#123;etl_tmp&#125;$&#123;flow&#125; echo "$flow execution done"doneecho "all flows for project $&#123;project_name&#125; has been executed" 3.shell全文123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384#!/bin/bashetl_tmp=/tmp/etl_tmp_loghost=10.0.1.62:8081metamap_host=10.0.1.62:8088project_desc=daily_schedule# 调用生成job的任务，返回任务名称或者失败信息curl -X GET http://$&#123;metamap_host&#125;/metamap/etls/generate_job_dag/ &gt; $&#123;etl_tmp&#125;filename=`cat $&#123;etl_tmp&#125;`if [ $filename == "error" -o $&#123;#filename&#125; -ne 14 ]; then echo "error happends when generate Job Scripts. ori_filename is $&#123;filename&#125;" echo "length is $&#123;#filename&#125;" exit 1fiproject_name=etl_$&#123;filename&#125;project_zip_file=/tmp/$&#123;filename&#125;.zipecho "project_name is $&#123;project_name&#125;"echo "project_zip_file is $&#123;project_zip_file&#125;"echo "azkaban host is $&#123;host&#125;"# 获取session idcurl -k -X POST --data "username=azkaban&amp;password=azkaban&amp;action=login" http://$&#123;host&#125; &gt; $&#123;etl_tmp&#125;echo result is `cat $&#123;etl_tmp&#125;`session_id=`cat $&#123;etl_tmp&#125; | grep session | awk -F\" '!/[&#123;&#125;]/&#123;print $(NF-1)&#125;'`echo "we got session id : $session_id"### 创建projectecho "project name is $&#123;project_name&#125;"curl -k -X POST --data "session.id=$&#123;session_id&#125;&amp;name=$&#123;project_name&#125;&amp;description=$&#123;project_desc&#125;" http://$&#123;host&#125;/manager?action=create &gt; $&#123;etl_tmp&#125;status=`cat $&#123;etl_tmp&#125; | JSON.sh -b| grep status | awk '&#123;print($2)&#125;'` if [ $status != '"success"' ]; then echo "error happends when creting project $&#123;project_name&#125;. status is $&#123;status&#125;" exit 1fiecho "project $&#123;project_name&#125; has been created successfully."# 上传zip文件到指定projectcurl -k -i -H "Content-Type: multipart/mixed" -X POST --form "session.id=$&#123;session_id&#125;" --form 'ajax=upload' --form "file=@$&#123;project_zip_file&#125;" --form "project=$&#123;project_name&#125;" http://$&#123;host&#125;/manager?ajax=upload &gt; $&#123;etl_tmp&#125;status=`cat $&#123;etl_tmp&#125; | awk '&#123;if(index($0,"error") &gt; 0) print("error")&#125;'`if [[ $status = "error" ]]; then echo "error happends when uploading project $&#123;project_name&#125;. status is $&#123;status&#125;" exit 1fiecho "uploading project $&#123;project_name&#125; successfully"# # 阻塞检察某个execution的进度#function check_exec_status()&#123; execid=$1 sleep 1h #查看前execution的执行状态，完成后退出 status=RUNNING until [ $status == '"SUCCEEDED"' ] do curl -k --get --data "session.id=$&#123;session_id&#125;&amp;ajax=fetchexecflowupdate&amp;execid=$&#123;execid&#125;&amp;lastUpdateTime=-1" http://$&#123;host&#125;/executor &gt; $&#123;etl_tmp&#125; status=`cat $&#123;etl_tmp&#125; | JSON.sh -b| grep status | grep -v node | awk '&#123;print($2)&#125;'` if [ $status == '"KILLED"' ]; then echo "$&#123;execid&#125; has been killed." break elif [ $status == '"FAILED"' ]; then echo "$&#123;execid&#125; has been failed." break fi echo "$&#123;execid&#125; not yet..." `cat $&#123;etl_tmp&#125;` sleep 10m done&#125;# 遍历执行project下的flowcurl -k --get --data "session.id=$&#123;session_id&#125;&amp;ajax=fetchprojectflows&amp;project=$&#123;project_name&#125;" http://$&#123;host&#125;/manager &gt; $&#123;etl_tmp&#125;for flow in `cat $&#123;etl_tmp&#125; | JSON.sh -b | grep flowId | awk '&#123;gsub("\"","",$2);print($2)&#125;'`do echo "flow is $flow, ready to execute" curl -k --get --data "session.id=$&#123;session_id&#125;" --data 'ajax=executeFlow' --data "project=$&#123;project_name&#125;" --data "flow=$&#123;flow&#125;" --data "failureAction=finishPossible" http://$&#123;host&#125;/executor &gt;$&#123;etl_tmp&#125;$&#123;flow&#125; execid=`cat $&#123;etl_tmp&#125;$&#123;flow&#125; | JSON.sh -b| grep execid | awk '&#123;print($2)&#125;'` echo "got execid : $&#123;execid&#125;" check_exec_status $&#123;execid&#125;echo "$flow execution done"doneecho "all flows for project $&#123;project_name&#125; has been executed"]]></content>
      <tags>
        <tag>ETL</tag>
        <tag>调度平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步步搞定ETL依赖调度平台（一）]]></title>
    <url>%2F2016%2F06%2F15%2F%E4%B8%80%E6%AD%A5%E6%AD%A5%E6%90%9E%E5%AE%9AETL%E4%BE%9D%E8%B5%96%E8%B0%83%E5%BA%A6%E5%B9%B3%E5%8F%B0%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[1. 依赖解析 sql AST解析。相关工具antlr,这是一个定义与解析特定语言的工具。 hive denpendency： explain dependency select * from a. 其他sql解析工具，功能类似hive dependency 鉴于我们的数据仓库使用hive搭建，使用hive dependency的兼容性更有保证。注意两点：1.如果a表不存在与hive中，就会报错找不到table。也就是说ETL开发的时候，它的依赖必须已经存在与hive库里；2. 由于依赖解析功能在ETL开发平台上，所以必须通过hiveserver2进行访问，效率可能不高，但是ETL平台对此要求也不会太高。 需要额外说一下antlr的好处：一是可以深入剖析sql，在这一层做字段级别的权限控制；二是可以做一些sql的分析，反馈给ETL开发者一些优化建议； 2. 存库1234567891011121314151617181920212223242526272829303132333435-- ------------------------------ Table structure for etl-- ----------------------------DROP TABLE IF EXISTS `etl`;CREATE TABLE `etl` ( `id` int(11) NOT NULL AUTO_INCREMENT, `query` varchar(2000) DEFAULT NULL COMMENT 'sql语句', `meta` varchar(20) DEFAULT NULL COMMENT '数据库', `tbl_name` varchar(30) NOT NULL COMMENT '表名', `author` varchar(30) DEFAULT NULL COMMENT '创建人', `pre_sql` varchar(2000) DEFAULT NULL COMMENT '先执行的sql', `priority` int(5) NOT NULL DEFAULT '5', `on_schedule` tinyint(4) DEFAULT NULL COMMENT '0 不调度 1 调度中', `valid` tinyint(4) DEFAULT NULL, `ctime` bigint(20) DEFAULT NULL, `utime` bigint(20) DEFAULT NULL, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=19 DEFAULT CHARSET=utf8;-- ------------------------------ Table structure for tbl_blood-- ----------------------------DROP TABLE IF EXISTS `tbl_blood`;CREATE TABLE `tbl_blood` ( `id` int(11) NOT NULL AUTO_INCREMENT, `tbl_name` varchar(20) NOT NULL, `parent_tbl` varchar(20) NOT NULL, `related_etl_id` int(11) NOT NULL, `valid` tinyint(4) NOT NULL DEFAULT '0', `ctime` bigint(20) DEFAULT '0', `utime` bigint(20) DEFAULT '0', PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=21 DEFAULT CHARSET=utf8; 每当新建一个ETL的时候都会分析它的依赖，如果分析失败，就不会插入ETL。 另外补充一个字段和表关系的，从hive meta库查询sql1234567891011121314SELECT db.DB_ID AS db_id, db.`NAME` AS db_name, a.TBL_ID AS tbl_id, a.TBL_NAME AS tbl_name, a.TBL_TYPE AS tbl_type, d.TYPE_NAME AS col_type_name, d.`COMMENT` AS col_comment, d.COLUMN_NAME AS column_nameFROM TBLS aLEFT JOIN SDS b ON a.SD_ID = b.SD_IDLEFT JOIN COLUMNS_V2 d ON b.CD_ID = d.CD_IDLEFT JOIN DBS db ON a.DB_ID = db.DB_ID 3.每日调度有了以上两步之后剩下的就是每日调度问题。分以下几个步骤： 通过azkaban api创建一个project: etl_20160615; 调用java程序根据tbl_blood和etl内容生成azkaban的项目文件20160615.zip; 通过azkaban api上传此文件到etl_20160615的project上 通过azkaban api遍历etl_20160615项目下的所有flow，进行调度。 生成azkaban文件的逻辑： 找出没有被任何其他ETL以来的叶子节点 遍历所有叶子节点，找到这个叶子节点的所有有效依赖，放入dependencies里，如果尚未生成该ETL的job文件 向上递归执行，直到没有dependency的节点，也就是最上一层节点就停止。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public String generateAzkabanDAG() throws MetaException, ArchiveException &#123; try &#123; String serFolder = DateUtil.getTodayDateTime(); List&lt;TblBlood&gt; leafBlood = tblBloodDao.selectAllLeaf(); Set&lt;String&gt; doneBlood = new HashSet&lt;String&gt;(); String serFolderLocation = AZKABAN_BASE_LOCATION + serFolder; if (leafBlood.size() &gt; 0) &#123; loadLeafBloods(leafBlood, doneBlood, serFolderLocation); &#125; ZipUtils.addFilesToZip(new File(serFolderLocation), new File(serFolderLocation + ".zip")); &#125; catch (IOException e) &#123; log.error("error happends when generateAzkabanDAG"); throw new MetaException("error happends when generateAzkabanDAG"); &#125; return null; &#125; private void loadLeafBloods(List&lt;TblBlood&gt; leafBlood, Set&lt;String&gt; doneBlood, String serFolder) throws IOException &#123; for (TblBlood leaf : leafBlood) &#123; List&lt;TblBlood&gt; parentNode = tblBloodDao.selectParentByTblName(leaf.getTblName()); if (!doneBlood.contains(leaf.getTblName())) &#123; generateJobFile(leaf, parentNode, serFolder); doneBlood.add(leaf.getTblName()); &#125; loadLeafBloods(parentNode, doneBlood, serFolder); &#125; &#125; private void generateJobFile(TblBlood currentBlood, List&lt;TblBlood&gt; parentNode, String serFolder) throws IOException &#123; String jobName = currentBlood.getTblName(); String jobType = "command"; String command = "echo 'I am command for ' " + jobName; String content; Set&lt;String&gt; dependencies = new HashSet&lt;String&gt;(); for (TblBlood blood : parentNode) &#123; dependencies.add(blood.getTblName()); &#125; String jobDependencies = joiner.join(dependencies); content = "# " + jobName + "\n" + "type=" + jobType +"\n" + "command=" + command + "\n"; if (StringUtils.isNotBlank(jobDependencies)) &#123; content += "dependencies=" + jobDependencies +"\n"; &#125; File file = new File(serFolder + "/" + jobName.replace("@", "-") + ".job"); FileUtils.write(file, content, "utf8", false); &#125; 4.参考http://azkaban.github.io/azkaban/docs/latest/#ajax-api 5. TODO flow和job目前都是不支持优先级的，需要探查下azkaban中同一个flow里job执行次序是怎样的。 目前字段解析只能通过部分从hive meta抽，获取到字段与当前表的关系，不能逐个找到字段与字段之间的血统关系。如果使用antlr的话可以分析到每一个as之前是什么，理论上是可以实现字段的血统关系的。 对于每月、每周、每小时的个别ETL的支持策略 支持ETL变量传入和临时调试]]></content>
      <tags>
        <tag>ETL</tag>
        <tag>调度平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban源码追踪（二）-单次调度flow]]></title>
    <url>%2F2016%2F06%2F15%2Fazkaban%E6%BA%90%E7%A0%81%E8%BF%BD%E8%B8%AA-%E5%8D%95%E6%AC%A1%E8%B0%83%E5%BA%A6flow%2F</url>
    <content type="text"><![CDATA[对应的操作是进入到某个project中，然后单次执行下面的某个flow。会触发web-server这边与exec-server的远程调用去执行flow。 相关类 Web-server ExecutorServlet.doGet ExecutorManager exec-server ExecutorServlet.doGet FlowRunnerManager FlowRunner ExecutableFlowBase ExecutableNode JobRunner JobTypeManager Job AzkabanProcessBuilder ####]]></content>
      <tags>
        <tag>源码</tag>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[azkaban源码追踪（一）-uploadProject流程]]></title>
    <url>%2F2016%2F06%2F15%2Fazkaban%E6%BA%90%E7%A0%81%E8%BF%BD%E8%B8%AA%EF%BC%88%E4%B8%80%EF%BC%89-uploadProject%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1. 相关类 ProjectMangerServlet ProjectManager ValidatorManager DirectoryFlowLoader JdbcProjectLoader 2. 对应的流程log12345678910111213141516171819202122232016/06/14 15:39:18.537 +0800 INFO [ProjectManagerServlet] [Azkaban] Uploading file willjob.zip2016/06/14 15:39:23.861 +0800 INFO [ProjectManager] [Azkaban] Uploading files to mytestProject2016/06/14 15:40:16.454 +0800 WARN [XmlValidatorManager] [Azkaban] Validator directory validators does not exist or is not a directory2016/06/14 15:40:16.455 +0800 WARN [XmlValidatorManager] [Azkaban] Azkaban properties file does not contain the key project.validators2016/06/14 15:40:35.752 +0800 INFO [ProjectManager] [Azkaban] Validating project willjob.zip using the registered validators [Director2016/06/14 16:11:45.952 +0800 INFO [XmlValidatorManager] [Azkaban] Validation status of validator Directory Flow is PASS2016/06/14 16:56:51.338 +0800 INFO [ProjectManager] [Azkaban] Uploading file to db willjob.zip2016/06/14 16:56:51.338 +0800 INFO [JdbcProjectLoader] [Azkaban] Uploading to mytestProject version:1 file:willjob.zip2016/06/14 16:56:51.341 +0800 INFO [JdbcProjectLoader] [Azkaban] Creating message digest for upload willjob.zip2016/06/14 16:56:51.343 +0800 INFO [JdbcProjectLoader] [Azkaban] Md5 hash created2016/06/14 16:56:51.359 +0800 INFO [JdbcProjectLoader] [Azkaban] Read bytes for willjob.zip size:5602016/06/14 16:56:51.359 +0800 INFO [JdbcProjectLoader] [Azkaban] Running update for willjob.zip chunk 02016/06/14 16:56:51.360 +0800 INFO [JdbcProjectLoader] [Azkaban] Finished update for willjob.zip chunk 02016/06/14 16:56:51.362 +0800 INFO [JdbcProjectLoader] [Azkaban] Commiting upload willjob.zip2016/06/14 16:56:51.363 +0800 INFO [ProjectManager] [Azkaban] Uploading flow to db willjob.zip2016/06/14 16:56:51.363 +0800 INFO [JdbcProjectLoader] [Azkaban] Uploading flows2016/06/14 16:56:51.418 +0800 INFO [JdbcProjectLoader] [Azkaban] Flow upload will3 is byte size 2142016/06/14 16:56:51.432 +0800 INFO [JdbcProjectLoader] [Azkaban] Flow upload will2 is byte size 2382016/06/14 16:56:51.434 +0800 INFO [ProjectManager] [Azkaban] Changing project versions willjob.zip2016/06/14 16:56:51.436 +0800 INFO [ProjectManager] [Azkaban] Uploading Job properties2016/06/14 16:56:51.820 +0800 INFO [ProjectManager] [Azkaban] Uploading Props properties2016/06/14 16:56:51.822 +0800 INFO [ProjectManager] [Azkaban] Uploaded project files. Cleaning up temp files.2016/06/14 16:56:51.927 +0800 INFO [ProjectManager] [Azkaban] Cleaning up old install files older than -2 3. ProjectManager白话一下ProjectManager处理上传zip文件的过程： 将文件存到/tmp目录并解压 使用DirectoryFlowLoader解析文件，放置到这个loader的状态变量中。这些变量会一直保存在内存里，以供后面使用 遍历loader里的flowMap，把每个flow都加上最新的project_id和version 上传project信息 4.1 把zip文件弄成字节流上传到mysql的project_files表中 4.2 把最新版本的project信息上传到project_versions表中 上传flow: 把flow逐个序列化成json，再弄成byte字节流，再使用gzip压缩，上传至project_flows表中 更新projects里的相关记录，并修改内存中的project信息【修改时间等、最新版本的flows】 上传job的详细信息到project_properties中，job的详细信息也是json-&gt;byte[]-&gt;gzip的过程 上传props的详细信息到project_properties中，流程同job完全一样【这个debug的时候数据是空的】 将此次操作插入到project_event中，之后删除本地/tmp下的文件 使用project_id和当前version判断依次清除先前版本的project_flows的flow、project_properties、project_files、project_versions中的记录【会有最近版本保留数量设置project.version.retention，默认是3】 完成4. 文件解析DirectoryFlowLoader加载文件目录，解析所有的文件，进行解析，将信息存放在自己的状态变量中：123456789101112131415private Props props; // 环境、配置信息private HashSet&lt;String&gt; rootNodes; // 根节点，也就是实际执行的时候的起始节点private HashMap&lt;String, Flow&gt; flowMap; // 所有的flow，key是flowIdprivate HashMap&lt;String, Node&gt; nodeMap; // 所有的node，key是JobId或者embed的flowId// node之间的dependency，key是job名称，value是一系列的相关依赖job，一般是children，然后Edge指定从谁到谁private HashMap&lt;String, Map&lt;String, Edge&gt;&gt; nodeDependencies; private HashMap&lt;String, Props&gt; jobPropsMap; // 包含job的详细信息// flow之间的依赖关系【embedeed flow】private HashMap&lt;String, Set&lt;String&gt;&gt; flowDependencies;private ArrayList&lt;FlowProps&gt; flowPropsList;private ArrayList&lt;Props&gt; propsList;private Set&lt;String&gt; errors;private Set&lt;String&gt; duplicateJobs; 1234567891011121314151617181920212223242526272829// 临时解压目录 baseDirectory : temp/46769225// 项目信息 project: testPropublic void loadProjectFlow(Project project, File baseDirectory) &#123; propsList = new ArrayList&lt;Props&gt;(); flowPropsList = new ArrayList&lt;FlowProps&gt;(); jobPropsMap = new HashMap&lt;String, Props&gt;(); nodeMap = new HashMap&lt;String, Node&gt;(); flowMap = new HashMap&lt;String, Flow&gt;(); errors = new HashSet&lt;String&gt;(); duplicateJobs = new HashSet&lt;String&gt;(); nodeDependencies = new HashMap&lt;String, Map&lt;String, Edge&gt;&gt;(); rootNodes = new HashSet&lt;String&gt;(); flowDependencies = new HashMap&lt;String, Set&lt;String&gt;&gt;(); // 加载所有的properties、job文件，创建node对象 loadProjectFromDir(baseDirectory.getPath(), baseDirectory, null); jobPropertiesCheck(project); // Create edges and find missing dependencies resolveDependencies(); // 创建flows. 在此之后flowMap里就已经构建好了flow以及job之间的依赖了，node之间通过level来界定先后关系 buildFlowsFromDependencies(); // 处理embedded flows，就是嵌入的flow resolveEmbeddedFlows(); &#125; loadProjectFromDir方法详细123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657private void loadProjectFromDir(String base, File dir, Props parent) &#123; // 找出所有的properties File[] propertyFiles = dir.listFiles(new SuffixFilter(PROPERTY_SUFFIX)); Arrays.sort(propertyFiles); for (File file : propertyFiles) &#123; String relative = getRelativeFilePath(base, file.getPath()); parent = new Props(parent, file); parent.setSource(relative); FlowProps flowProps = new FlowProps(parent); flowPropsList.add(flowProps); logger.info("Adding " + relative); propsList.add(parent); &#125; // 加载job文件 File[] jobFiles = dir.listFiles(new SuffixFilter(JOB_SUFFIX)); for (File file : jobFiles) &#123; String jobName = getNameWithoutExtension(file); if (!duplicateJobs.contains(jobName)) &#123; if (jobPropsMap.containsKey(jobName)) &#123; // 是否有重复 ... &#125; else &#123; Props prop = new Props(parent, file); String relative = getRelativeFilePath(base, file.getPath()); prop.setSource(relative); Node node = new Node(jobName); String type = prop.getString("type", null); if (type == null) &#123; errors.add("Job doesn't have type set '" + jobName + "'."); &#125; node.setType(type); // job类型 node.setJobSource(relative); // job相对路径 if (parent != null) &#123; node.setPropsSource(parent.getSource()); &#125; // Force root node if (prop.getBoolean(CommonJobProperties.ROOT_NODE, false)) &#123; rootNodes.add(jobName); &#125; // 把properties和node放进loader中的队列中 jobPropsMap.put(jobName, prop); nodeMap.put(jobName, node); &#125; &#125; &#125; // 检查是否有子目录 File[] subDirs = dir.listFiles(DIR_FILTER); for (File file : subDirs) &#123; loadProjectFromDir(base, file, parent); &#125;&#125; 常用的blob处理代码12345String propertyJSON = PropsUtils.toJSONString(props, true); byte[] data = propertyJSON.getBytes("UTF-8"); if (defaultEncodingType == EncodingType.GZIP) &#123; data = GZIPUtils.gzipBytes(data); &#125; 5.其他 level表示当前node所在的层次。]]></content>
      <tags>
        <tag>源码</tag>
        <tag>azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring + quartz集群配置全部资料]]></title>
    <url>%2F2015%2F12%2F10%2Fspring-quartz%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE%E5%85%A8%E9%83%A8%E8%B5%84%E6%96%99%2F</url>
    <content type="text"><![CDATA[今天玩儿quartz正玩儿的起劲儿，下午就被否掉了，另一个同事已经被分配了相关的工作，那么我这边的工作就重复了。暂时记下目前了解到的一些配置，方便以后使用。 quartz.xml配置文件123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd"&gt; &lt;bean id = "willtest" class="com.will.quartz.task.WillTask"/&gt; &lt;bean id="abnormalDitail" class="org.springframework.scheduling.quartz.JobDetailBean"&gt; &lt;property name="jobClass" value="com.will.quartz.task.WillSpringJob" /&gt; &lt;property name="jobDataAsMap"&gt; &lt;map&gt; &lt;entry key="timeout" value="5" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="anotherJob" class="org.springframework.scheduling.quartz.JobDetailBean"&gt; &lt;property name="jobClass" value="com.will.quartz.task.AnotherSJob" /&gt; &lt;property name="jobDataAsMap"&gt; &lt;map&gt; &lt;entry key="timeout" value="5" /&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 调度触发器 --&gt; &lt;bean id="abnormalTigger" class="org.springframework.scheduling.quartz.CronTriggerBean"&gt; &lt;property name="jobDetail" ref="abnormalDitail"/&gt; &lt;property name="cronExpression"&gt; &lt;value&gt;*/5 * * * * ?&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;!-- 调度触发器 --&gt; &lt;bean id="anotherJobTigger" class="org.springframework.scheduling.quartz.CronTriggerBean"&gt; &lt;property name="jobDetail" ref="anotherJob"/&gt; &lt;property name="cronExpression"&gt; &lt;value&gt;*/8 * * * * ?&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"&gt; &lt;property name="driverClass" value="$&#123;database.driverClassName&#125;" /&gt; &lt;property name="jdbcUrl" value="$&#123;database.quartz.url&#125;" /&gt; &lt;property name="user" value="$&#123;database.quartz.username&#125;" /&gt; &lt;property name="password" value="$&#123;database.quartz.password&#125;" /&gt; &lt;property name="initialPoolSize" value="$&#123;pool.initialPoolSize&#125;" /&gt; &lt;property name="minPoolSize" value="$&#123;pool.minPoolSize&#125;" /&gt; &lt;property name="maxPoolSize" value="$&#123;pool.maxPoolSize&#125;" /&gt; &lt;property name="maxIdleTime" value="$&#123;pool.maxIdleTime&#125;" /&gt; &lt;property name="checkoutTimeout" value="12000" /&gt; &lt;property name="acquireIncrement" value="$&#123;pool.acquireIncrement&#125;" /&gt; &lt;/bean&gt; &lt;bean id="quartzExceptionSchedulerListener" class="com.will.quartz.task.QuartzExceptionSchedulerListener"/&gt;&lt;!-- 调度工厂 --&gt;&lt;bean id="quartzScheduler" class="org.springframework.scheduling.quartz.SchedulerFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource" /&gt; &lt;property name="quartzProperties"&gt; &lt;props&gt; &lt;prop key="org.quartz.scheduler.instanceName"&gt;scheduler&lt;/prop&gt; &lt;prop key="org.quartz.scheduler.driverDelegateClass"&gt;org.quartz.impl.jdbcjobstore.StdJDBCDelegate&lt;/prop&gt; &lt;prop key="org.quartz.scheduler.instanceId"&gt;AUTO&lt;/prop&gt; &lt;!-- 线程池配置 --&gt; &lt;prop key="org.quartz.threadPool.class"&gt;org.quartz.simpl.SimpleThreadPool&lt;/prop&gt; &lt;prop key="org.quartz.threadPool.threadCount"&gt;20&lt;/prop&gt; &lt;prop key="org.quartz.threadPool.threadPriority"&gt;5&lt;/prop&gt; &lt;!-- JobStore 配置 --&gt; &lt;prop key="org.quartz.jobStore.class"&gt;org.quartz.impl.jdbcjobstore.JobStoreTX&lt;/prop&gt; &lt;!-- 集群配置 --&gt; &lt;prop key="org.quartz.jobStore.isClustered"&gt;true&lt;/prop&gt; &lt;prop key="org.quartz.jobStore.clusterCheckinInterval"&gt;15000&lt;/prop&gt; &lt;prop key="org.quartz.jobStore.maxMisfiresToHandleAtATime"&gt;1&lt;/prop&gt; &lt;prop key="org.quartz.jobStore.misfireThreshold"&gt;120000&lt;/prop&gt; &lt;prop key="org.quartz.jobStore.tablePrefix"&gt;QRTZ_&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt; &lt;property name="schedulerName" value="CRMscheduler" /&gt; &lt;!--必须的，QuartzScheduler 延时启动，应用启动完后 QuartzScheduler 再启动 --&gt; &lt;property name="startupDelay" value="30" /&gt; &lt;property name="applicationContextSchedulerContextKey" value="applicationContextKey" /&gt; &lt;!--可选，QuartzScheduler 启动时更新己存在的Job，这样就不用每次修改targetObject后删除qrtz_job_details表对应记录了 --&gt; &lt;property name="overwriteExistingJobs" value="true" /&gt; &lt;!-- 设置自动启动 --&gt; &lt;property name="autoStartup" value="true" /&gt; &lt;!-- 注册触发器 --&gt; &lt;property name="triggers"&gt; &lt;list&gt; &lt;ref bean="abnormalTigger" /&gt; &lt;ref bean="anotherJobTigger" /&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 注册jobDetail --&gt; &lt;property name="jobDetails"&gt; &lt;list&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name="schedulerListeners"&gt; &lt;list&gt; &lt;ref bean="quartzExceptionSchedulerListener" /&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;/beans&gt; 数据库表准备就是准备好集群需要的那些表，昨天找了半天的那个，参见：spring-quartz集群环境搭建遇到的问题。 config.properties就是xml里使用的那些变量1234567891011121314base.driverClassName=com.mysql.jdbc.Driverdatabase.quartz.url=jdbc:mysql://192.168.22.235:3306/testdb?useUnicode=true&amp;characterEncoding=utf8database.quartz.username=rootdatabase.quartz.password=123pool.initialPoolSize=5pool.minPoolSize=5pool.maxPoolSize=100pool.maxIdleTime=3600pool.acquireIncrement=1pool.idleConnectionTestPeriod=60pool.preferredTestQuery=SELECT 1pool.checkoutTimeout=60000 JobDetail也就是我们需要定时执行的任务。1234567public class WillSpringJob extends QuartzJobBean &#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println("sssssssssssssssss"); &#125;&#125; 123456public class AnotherSJob extends QuartzJobBean &#123; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println("fun.. I am a jerk."); &#125;&#125; SchedulerListener12345678public class QuartzExceptionSchedulerListener extends SchedulerListenerSupport &#123; private Logger logger = LoggerFactory.getLogger(QuartzExceptionSchedulerListener.class); @Override public void schedulerError(String message, SchedulerException e) &#123; super.schedulerError(message, e); logger.error(message, e.getUnderlyingException()); &#125;&#125; 其他其实主要的配置都在SchedulerFactoryBean这儿，我们可以看一下这个类里到底有哪些element，也就是xml里在它下面配置的property。其实主要是他的父类SchedulerAccessor，展示一下源码截图： 几个小问题 quartz虽然启动的时候会delay，但是delay的时候会囤积task，到时间会一股脑地将前面delay那段时间的Task全给执行咯；结果是：是的。不可避免 目前一个trigger里只能写一个job，看了一下cronTrigger里就只有一个JobDetail属性，而不是一个list。对于同一个调度频率的任务一定需要弄多个trigger吗？结果是：很少有这种需求，可以写成一个Job SchedulerFactoryBean里的jobDetails如果不配置，只设置trigger的话,JobDetail其实也是会被写入mysql对应的表里的。trigger里的JobDetail和SchedulerFactoryBean里的有什么区别和联系。结果是： ScheduelrAccessor里的注释是Register a list of JobDetail objects with the Scheduler that this FactoryBean creates, to be referenced by Triggers.This is not necessary when a Trigger determines the JobDetail itself: In this case, the JobDetail will be implicitly registered in combination with the Trigger. Done!! 参考 http://tech.meituan.com/mt-crm-quartz.html https://www.ibm.com/developerworks/cn/opensource/os-cn-quartz/ http://www.quartz-scheduler.org/generated/2.2.2/html/qtz-all/ https://github.com/xishuixixia/quartz-monitor 【quartz monitor】 http://hot66hot.iteye.com/blog/1726143]]></content>
      <tags>
        <tag>spring</tag>
        <tag>quartz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring + quartz集群环境搭建遇到的问题]]></title>
    <url>%2F2015%2F12%2F09%2Fspring-quartz%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[问题1总是数据库连接失败解决：dataSource的checkoutTimeout由120修改为12000 问题2 No row exists in table QRTZ_LOCKS for lock named: TRIGGER_ACCESS 临时解决方案： 12345INSERT INTO QRTZ_LOCKS values('my sched_name', 'TRIGGER_ACCESS'); INSERT INTO QRTZ_LOCKS values('my sched_name','JOB_ACCESS'); INSERT INTO QRTZ_LOCKS values('my sched_name','CALENDAR_ACCESS'); INSERT INTO QRTZ_LOCKS values('my sched_name','STATE_ACCESS'); INSERT INTO QRTZ_LOCKS values('my sched_name','MISFIRE_ACCESS'); 实际的表结构是123456CREATE TABLE `QRTZ_LOCKS` ( `SCHED_NAME` varchar(120) NOT NULL, `LOCK_NAME` varchar(40) NOT NULL, PRIMARY KEY (`SCHED_NAME`,`LOCK_NAME`)) ENGINE=InnoDB DEFAULT CHARSET=utf8 后面我们其实会发现，这个表是因为我copy的2.x版本的sql，所以导致表不兼容,也就是版本对应的话其实不会有这个问题的 因为SCHED_NAME其实我们目前并不知道，但是追到代码里有查询这个锁的代码：1StdRowLockSemaphore.executeSQL的注释：Execute the SQL select for update that will lock the proper database row. 问题3 Uable to serialize JobDataMap for insertion into database because the value of property ‘methodInvoker’ is not serializable: org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean 原因就是是用的org.springframework.scheduling.quartz.MethodInvokingJobDetailFactoryBean的个别属性不能序列化，所以不能把Job写到mysql的表里去。那就换个方式改用org.springframework.scheduling.quartz.JobDetailBean1234567891011public class WillSpringJob extends QuartzJobBean &#123; private Scheduler scheduler; @Override protected void executeInternal(JobExecutionContext context) throws JobExecutionException &#123; System.out.println("sssssssssssssssss"); &#125;&#125; 相关任务修改12345678&lt;bean id="abnormalDitail" class="org.springframework.scheduling.quartz.JobDetailBean"&gt; &lt;property name="jobClass" value="com.task.WillSpringJob" /&gt; &lt;property name="jobDataAsMap"&gt; &lt;map&gt; &lt;entry key="timeout" value="5" /&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt; 问题4 Error creating bean with name ‘quartzScheduler’ defined in file [/Users/will/work/dolphin/target/classes/task/TestTask.xml]: Invocation of init method failed; nested exception is org.quartz.JobPersistenceException: Couldn’t store job: Unknown column ‘IS_VOLATILE’ in ‘field list’ [See nested exception: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column ‘IS_VOLATILE’ in ‘field list’]:com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column ‘IS_VOLATILE’ in ‘field list’ 稍微想一下，验证过没有锁之后，启动spring之后，应该是读取job配置文件，然后就将这些job插入到mysql的表QRTZ_JOB_DETAILS中去，以便于集群共同承担任务调度。所以这个不在field list里的字段就是QRTZ_JOB_DETAILS表里的。看来我找的sql版本与目前用到的quartz版本不一致。。。 为了让自己当前稍微爽一下，先在mysql里直接修改表，添加这个字段。后续又出现了IS_STATEFUL。然后又报错：Field ‘SCHED_NAME’ doesn’t have a default value。fun，找版本，不玩儿了。最终定的版本是1.8.6。 建表语句用quartz管理任务计划很方便，但是当使用数据库作为存储介质的时候，必须要先创建表，不然就会报错。1.x和2.x的建表语句不同，以下是两个版本使用MySQL时的建表语句。1.x建表语句为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166## In your Quartz properties file, you'll need to set # org.quartz.jobStore.driverDelegateClass = org.quartz.impl.jdbcjobstore.StdJDBCDelegate### By: Ron Cordell - roncordell# I didn't see this anywhere, so I thought I'd post it here. This is the script from Quartz to create the tables in a MySQL database, modified to use INNODB instead of MYISAM.DROP TABLE IF EXISTS QRTZ_JOB_LISTENERS;DROP TABLE IF EXISTS QRTZ_TRIGGER_LISTENERS;DROP TABLE IF EXISTS QRTZ_FIRED_TRIGGERS;DROP TABLE IF EXISTS QRTZ_PAUSED_TRIGGER_GRPS;DROP TABLE IF EXISTS QRTZ_SCHEDULER_STATE;DROP TABLE IF EXISTS QRTZ_LOCKS;DROP TABLE IF EXISTS QRTZ_SIMPLE_TRIGGERS;DROP TABLE IF EXISTS QRTZ_CRON_TRIGGERS;DROP TABLE IF EXISTS QRTZ_BLOB_TRIGGERS;DROP TABLE IF EXISTS QRTZ_TRIGGERS;DROP TABLE IF EXISTS QRTZ_JOB_DETAILS;DROP TABLE IF EXISTS QRTZ_CALENDARS;CREATE TABLE QRTZ_JOB_DETAILS(JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,DESCRIPTION VARCHAR(250) NULL,JOB_CLASS_NAME VARCHAR(250) NOT NULL,IS_DURABLE VARCHAR(1) NOT NULL,IS_VOLATILE VARCHAR(1) NOT NULL,IS_STATEFUL VARCHAR(1) NOT NULL,REQUESTS_RECOVERY VARCHAR(1) NOT NULL,JOB_DATA BLOB NULL,PRIMARY KEY (JOB_NAME,JOB_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_JOB_LISTENERS (JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,JOB_LISTENER VARCHAR(200) NOT NULL,PRIMARY KEY (JOB_NAME,JOB_GROUP,JOB_LISTENER),INDEX (JOB_NAME, JOB_GROUP),FOREIGN KEY (JOB_NAME,JOB_GROUP)REFERENCES QRTZ_JOB_DETAILS(JOB_NAME,JOB_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_TRIGGERS (TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,IS_VOLATILE VARCHAR(1) NOT NULL,DESCRIPTION VARCHAR(250) NULL,NEXT_FIRE_TIME BIGINT(13) NULL,PREV_FIRE_TIME BIGINT(13) NULL,PRIORITY INTEGER NULL,TRIGGER_STATE VARCHAR(16) NOT NULL,TRIGGER_TYPE VARCHAR(8) NOT NULL,START_TIME BIGINT(13) NOT NULL,END_TIME BIGINT(13) NULL,CALENDAR_NAME VARCHAR(200) NULL,MISFIRE_INSTR SMALLINT(2) NULL,JOB_DATA BLOB NULL,PRIMARY KEY (TRIGGER_NAME,TRIGGER_GROUP),INDEX (JOB_NAME, JOB_GROUP),FOREIGN KEY (JOB_NAME,JOB_GROUP)REFERENCES QRTZ_JOB_DETAILS(JOB_NAME,JOB_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_SIMPLE_TRIGGERS (TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,REPEAT_COUNT BIGINT(7) NOT NULL,REPEAT_INTERVAL BIGINT(12) NOT NULL,TIMES_TRIGGERED BIGINT(10) NOT NULL,PRIMARY KEY (TRIGGER_NAME,TRIGGER_GROUP),INDEX (TRIGGER_NAME, TRIGGER_GROUP),FOREIGN KEY (TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_CRON_TRIGGERS (TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,CRON_EXPRESSION VARCHAR(120) NOT NULL,TIME_ZONE_ID VARCHAR(80),PRIMARY KEY (TRIGGER_NAME,TRIGGER_GROUP),INDEX (TRIGGER_NAME, TRIGGER_GROUP),FOREIGN KEY (TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_BLOB_TRIGGERS (TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,BLOB_DATA BLOB NULL,PRIMARY KEY (TRIGGER_NAME,TRIGGER_GROUP),INDEX (TRIGGER_NAME, TRIGGER_GROUP),FOREIGN KEY (TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_TRIGGER_LISTENERS (TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,TRIGGER_LISTENER VARCHAR(200) NOT NULL,PRIMARY KEY (TRIGGER_NAME,TRIGGER_GROUP,TRIGGER_LISTENER),INDEX (TRIGGER_NAME, TRIGGER_GROUP),FOREIGN KEY (TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_CALENDARS (CALENDAR_NAME VARCHAR(200) NOT NULL,CALENDAR BLOB NOT NULL,PRIMARY KEY (CALENDAR_NAME))ENGINE=InnoDB;CREATE TABLE QRTZ_PAUSED_TRIGGER_GRPS (TRIGGER_GROUP VARCHAR(200) NOT NULL,PRIMARY KEY (TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_FIRED_TRIGGERS (ENTRY_ID VARCHAR(95) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,IS_VOLATILE VARCHAR(1) NOT NULL,INSTANCE_NAME VARCHAR(200) NOT NULL,FIRED_TIME BIGINT(13) NOT NULL,PRIORITY INTEGER NOT NULL,STATE VARCHAR(16) NOT NULL,JOB_NAME VARCHAR(200) NULL,JOB_GROUP VARCHAR(200) NULL,IS_STATEFUL VARCHAR(1) NULL,REQUESTS_RECOVERY VARCHAR(1) NULL,PRIMARY KEY (ENTRY_ID))ENGINE=InnoDB;CREATE TABLE QRTZ_SCHEDULER_STATE (INSTANCE_NAME VARCHAR(200) NOT NULL,LAST_CHECKIN_TIME BIGINT(13) NOT NULL,CHECKIN_INTERVAL BIGINT(13) NOT NULL,PRIMARY KEY (INSTANCE_NAME))ENGINE=InnoDB;CREATE TABLE QRTZ_LOCKS (LOCK_NAME VARCHAR(40) NOT NULL,PRIMARY KEY (LOCK_NAME))ENGINE=InnoDB;INSERT INTO QRTZ_LOCKS values('TRIGGER_ACCESS');INSERT INTO QRTZ_LOCKS values('JOB_ACCESS');INSERT INTO QRTZ_LOCKS values('CALENDAR_ACCESS');INSERT INTO QRTZ_LOCKS values('STATE_ACCESS');INSERT INTO QRTZ_LOCKS values('MISFIRE_ACCESS');commit; 2.x版本的建表语句为：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188## In your Quartz properties file, you'll need to set # org.quartz.jobStore.driverDelegateClass = org.quartz.impl.jdbcjobstore.StdJDBCDelegate### By: Ron Cordell - roncordell# I didn't see this anywhere, so I thought I'd post it here. This is the script from Quartz to create the tables in a MySQL database, modified to use INNODB instead of MYISAM.DROP TABLE IF EXISTS QRTZ_FIRED_TRIGGERS;DROP TABLE IF EXISTS QRTZ_PAUSED_TRIGGER_GRPS;DROP TABLE IF EXISTS QRTZ_SCHEDULER_STATE;DROP TABLE IF EXISTS QRTZ_LOCKS;DROP TABLE IF EXISTS QRTZ_SIMPLE_TRIGGERS;DROP TABLE IF EXISTS QRTZ_SIMPROP_TRIGGERS;DROP TABLE IF EXISTS QRTZ_CRON_TRIGGERS;DROP TABLE IF EXISTS QRTZ_BLOB_TRIGGERS;DROP TABLE IF EXISTS QRTZ_TRIGGERS;DROP TABLE IF EXISTS QRTZ_JOB_DETAILS;DROP TABLE IF EXISTS QRTZ_CALENDARS;CREATE TABLE QRTZ_JOB_DETAILS(SCHED_NAME VARCHAR(120) NOT NULL,JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,DESCRIPTION VARCHAR(250) NULL,JOB_CLASS_NAME VARCHAR(250) NOT NULL,IS_DURABLE VARCHAR(1) NOT NULL,IS_NONCONCURRENT VARCHAR(1) NOT NULL,IS_UPDATE_DATA VARCHAR(1) NOT NULL,REQUESTS_RECOVERY VARCHAR(1) NOT NULL,JOB_DATA BLOB NULL,PRIMARY KEY (SCHED_NAME,JOB_NAME,JOB_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,JOB_NAME VARCHAR(200) NOT NULL,JOB_GROUP VARCHAR(200) NOT NULL,DESCRIPTION VARCHAR(250) NULL,NEXT_FIRE_TIME BIGINT(13) NULL,PREV_FIRE_TIME BIGINT(13) NULL,PRIORITY INTEGER NULL,TRIGGER_STATE VARCHAR(16) NOT NULL,TRIGGER_TYPE VARCHAR(8) NOT NULL,START_TIME BIGINT(13) NOT NULL,END_TIME BIGINT(13) NULL,CALENDAR_NAME VARCHAR(200) NULL,MISFIRE_INSTR SMALLINT(2) NULL,JOB_DATA BLOB NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,JOB_NAME,JOB_GROUP)REFERENCES QRTZ_JOB_DETAILS(SCHED_NAME,JOB_NAME,JOB_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_SIMPLE_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,REPEAT_COUNT BIGINT(7) NOT NULL,REPEAT_INTERVAL BIGINT(12) NOT NULL,TIMES_TRIGGERED BIGINT(10) NOT NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_CRON_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,CRON_EXPRESSION VARCHAR(120) NOT NULL,TIME_ZONE_ID VARCHAR(80),PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_SIMPROP_TRIGGERS ( SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,STR_PROP_1 VARCHAR(512) NULL,STR_PROP_2 VARCHAR(512) NULL,STR_PROP_3 VARCHAR(512) NULL,INT_PROP_1 INT NULL,INT_PROP_2 INT NULL,LONG_PROP_1 BIGINT NULL,LONG_PROP_2 BIGINT NULL,DEC_PROP_1 NUMERIC(13,4) NULL,EC_PROP_2 NUMERIC(13,4) NULL,BOOL_PROP_1 VARCHAR(1) NULL,OOL_PROP_2 VARCHAR(1) NULL,RIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),OREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP) REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_BLOB_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,BLOB_DATA BLOB NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP),INDEX (SCHED_NAME,TRIGGER_NAME, TRIGGER_GROUP),FOREIGN KEY (SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP)REFERENCES QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_CALENDARS (SCHED_NAME VARCHAR(120) NOT NULL,CALENDAR_NAME VARCHAR(200) NOT NULL,CALENDAR BLOB NOT NULL,PRIMARY KEY (SCHED_NAME,CALENDAR_NAME))ENGINE=InnoDB;CREATE TABLE QRTZ_PAUSED_TRIGGER_GRPS (SCHED_NAME VARCHAR(120) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,PRIMARY KEY (SCHED_NAME,TRIGGER_GROUP))ENGINE=InnoDB;CREATE TABLE QRTZ_FIRED_TRIGGERS (SCHED_NAME VARCHAR(120) NOT NULL,ENTRY_ID VARCHAR(95) NOT NULL,TRIGGER_NAME VARCHAR(200) NOT NULL,TRIGGER_GROUP VARCHAR(200) NOT NULL,INSTANCE_NAME VARCHAR(200) NOT NULL,FIRED_TIME BIGINT(13) NOT NULL,SCHED_TIME BIGINT(13) NOT NULL,PRIORITY INTEGER NOT NULL,STATE VARCHAR(16) NOT NULL,JOB_NAME VARCHAR(200) NULL,JOB_GROUP VARCHAR(200) NULL,IS_NONCONCURRENT VARCHAR(1) NULL,REQUESTS_RECOVERY VARCHAR(1) NULL,PRIMARY KEY (SCHED_NAME,ENTRY_ID))ENGINE=InnoDB;CREATE TABLE QRTZ_SCHEDULER_STATE (SCHED_NAME VARCHAR(120) NOT NULL,INSTANCE_NAME VARCHAR(200) NOT NULL,LAST_CHECKIN_TIME BIGINT(13) NOT NULL,CHECKIN_INTERVAL BIGINT(13) NOT NULL,PRIMARY KEY (SCHED_NAME,INSTANCE_NAME))ENGINE=InnoDB;CREATE TABLE QRTZ_LOCKS (SCHED_NAME VARCHAR(120) NOT NULL,LOCK_NAME VARCHAR(40) NOT NULL,PRIMARY KEY (SCHED_NAME,LOCK_NAME))ENGINE=InnoDB;CREATE INDEX IDX_QRTZ_J_REQ_RECOVERY ON QRTZ_JOB_DETAILS(SCHED_NAME,REQUESTS_RECOVERY);CREATE INDEX IDX_QRTZ_J_GRP ON QRTZ_JOB_DETAILS(SCHED_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_T_J ON QRTZ_TRIGGERS(SCHED_NAME,JOB_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_T_JG ON QRTZ_TRIGGERS(SCHED_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_T_C ON QRTZ_TRIGGERS(SCHED_NAME,CALENDAR_NAME);CREATE INDEX IDX_QRTZ_T_G ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_GROUP);CREATE INDEX IDX_QRTZ_T_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_N_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_N_G_STATE ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_GROUP,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_NEXT_FIRE_TIME ON QRTZ_TRIGGERS(SCHED_NAME,NEXT_FIRE_TIME);CREATE INDEX IDX_QRTZ_T_NFT_ST ON QRTZ_TRIGGERS(SCHED_NAME,TRIGGER_STATE,NEXT_FIRE_TIME);CREATE INDEX IDX_QRTZ_T_NFT_MISFIRE ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME);CREATE INDEX IDX_QRTZ_T_NFT_ST_MISFIRE ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_T_NFT_ST_MISFIRE_GRP ON QRTZ_TRIGGERS(SCHED_NAME,MISFIRE_INSTR,NEXT_FIRE_TIME,TRIGGER_GROUP,TRIGGER_STATE);CREATE INDEX IDX_QRTZ_FT_TRIG_INST_NAME ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,INSTANCE_NAME);CREATE INDEX IDX_QRTZ_FT_INST_JOB_REQ_RCVRY ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,INSTANCE_NAME,REQUESTS_RECOVERY);CREATE INDEX IDX_QRTZ_FT_J_G ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,JOB_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_FT_JG ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,JOB_GROUP);CREATE INDEX IDX_QRTZ_FT_T_G ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,TRIGGER_NAME,TRIGGER_GROUP);CREATE INDEX IDX_QRTZ_FT_TG ON QRTZ_FIRED_TRIGGERS(SCHED_NAME,TRIGGER_GROUP);commit;]]></content>
      <tags>
        <tag>spring</tag>
        <tag>quartz</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[springboot坑记1]]></title>
    <url>%2F2015%2F12%2F09%2Fspringboot%E5%9D%91%E8%AE%B01%2F</url>
    <content type="text"><![CDATA[背景业务上需要做一些定时任务的调度，最好还能对这些调度的状态进行记录与监控，可以通过一个友好的界面看到调度的执行情况是最好的。考虑的范围：spring + quartz、dropwizard、spring boot，因为当前公司的项目是使用spring mvc写的，也就是说公司的人力资源方面偏向于spring序列。放弃quartz的原因： - 只是负责调度，没有看到有对调度进度或者状态有监控的地方 - 对于调度堆叠的处理不是很清楚 最终定位到spring boot： - 计划是使用spring的web来进行调度监控方面的工作 - ApplicationRunner来启动自己写的调度器。]]></content>
      <tags>
        <tag>spring</tag>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql新技能-update join]]></title>
    <url>%2F2015%2F12%2F08%2Fmysql%E6%96%B0%E6%8A%80%E8%83%BD-update-join%2F</url>
    <content type="text"><![CDATA[上面说到了insert overwrite或者insert update的思想。这里说另一种场景，就是我们需要使用a表的直接或者间接数据来更新b表里的数据内容。比如a是所有订单，而b是所有商家订单总金额统计，业务就自然是定时计算a里所有订单的金额汇总，然后udpate到b表里面去。这里其实也有insert update的需要，比如判断某商家是否存在，但是我们这里只关注update 12345678910111213141516171819202122232425update b join( select poi_id, sum(fee) total_act_cost, sum(fee - poi_charge_fee) act_cost from a where ctime between #&#123;start_time&#125; and #&#123;end_time&#125; and valid=1 and status=9 group by poi_id) temp on a.poi_id = b.poi_idset b.total_act_cost = temp.total_act_cost, b.act_cost = temp.act_cost, b.act_flag=1where b.order_time between #&#123;start_time&#125; and #&#123;end_time&#125; 计算a里的指标结果，然后update到b的对应记录里面。]]></content>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql新技能-ON DUPLICATE KEY UPDATE]]></title>
    <url>%2F2015%2F12%2F08%2Fmysql%E6%96%B0%E6%8A%80%E8%83%BD-ON-DUPLICATE-KEY-UPDATE%2F</url>
    <content type="text"><![CDATA[向数据库插入数据的时候，我们会遇到一种情形：如果数据库中没有此条记录，就直接插入；如果已经存在此条记录，就更新其部分的字段值。这里定位“此条记录”是指在数据库中是唯一的，确定某个标识可以在数据库中唯一定位某一条记录。玩儿hive的同学请注意意思，虽R是insert overwrite的大概思想，但是不会覆盖整个分区数据，而仅仅针对“此条记录”。 ON DUPLICATE KEY不会指定具体的唯一key是哪个,因为并没有必要，这个语句要求table里有unique key,或者primary key, 它会自动比较这个key是不是有重复，如果有重复就update，没有的话就直接insert。 1234INSERT INTO table (a,b,c) VALUES (1,2,3) ON DUPLICATE KEY UPDATE c=c+1;UPDATE table SET c=c+1 WHERE a=1; 如果a=1这条记录存在，而且a是unique key的话，那么这两个句子是等同的。这个句子还支持一下方式，就是可以同时更新多个字段的值，而且可以相互引用：12INSERT INTO table (a,b,c) VALUES (1,2,3),(4,5,6) ON DUPLICATE KEY UPDATE c=VALUES(a)+VALUES(b), b=2;]]></content>
      <tags>
        <tag>mysql</tag>
        <tag>sql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim-vundle]]></title>
    <url>%2F2015%2F12%2F07%2Fvim-vundle%2F</url>
    <content type="text"><![CDATA[传说中的vim，今儿哥们儿要强迫自己继续装个B，吼吼~~ 为哥的勇气鼓掌！！ 0.安装vundlevundle是一个vim的插件管理器，涉猎了一段时间，发现很多人在用，为了更好更快地入门，就先玩儿个最活跃的，预料体验也不会特别差。 git clone https://github.com/VundleVim/Vundle.vim.git ~/.vim/bundle/Vundle.vim 1.配置vundle就是下载自己需要的一些插件，其实现在我毛线都不知道，就是网上随便搜了几个比较综合了一下。建议参考：https://github.com/VundleVim/Vundle.vim，了解最新的配置说明。BTW, 牛人参考: https://github.com/VundleVim/Vundle.vim/wiki/Examples 下面是我的~/.vimrc文件内容1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586et nocompatible " be iMprovedfiletype off " required!" set the runtime path to include Vundle and initializeset rtp+=~/.vim/bundle/vundle/call vundle#rc()" let Vundle manage Vundle" 使用vundle来管理Bundle 'gmarik/vundle'"my Bundle here:"" original repos on github" 相较于Command-T等查找文件的插件，ctrlp.vim最大的好处在于没有依赖，干净利落Bundle 'kien/ctrlp.vim'" 在输入()，""等需要配对的符号时，自动帮你补全剩余半个Bundle 'AutoClose'" 在()、""、甚至HTML标签之间快速跳转；Bundle 'matchit.zip'" 显示行末的空格Bundle 'ShowTrailingWhitespace'" 用全新的方式在文档中高效的移动光标，革命性的突破Bundle 'EasyMotion'" 自动识别文件编码；Bundle 'FencView.vim'" 必不可少，在VIM的编辑窗口树状显示文件目录Bundle 'The-NERD-tree'" 解放生产力的神器，简单配置，就可以按照自己的风格快速输入大段代码。Bundle 'UltiSnips'Bundle 'sukima/xmledit'Bundle 'sjl/gundo.vim'Bundle 'jiangmiao/auto-pairs'Bundle 'klen/python-mode'Bundle 'Valloric/ListToggle'"Bundle 'SirVer/ultisnips'" 迄今位置最好的自动VIM自动补全插件了吧Bundle 'Valloric/YouCompleteMe'Bundle 'scrooloose/syntastic'Bundle 't9md/vim-quickhl'" Bundle 'Lokaltog/vim-powerline'Bundle 'scrooloose/nerdcommenter'Plugin 'godlygeek/tabular'Plugin 'plasticboy/vim-markdown'".................................." vim-scripts reposBundle 'YankRing.vim'Bundle 'vcscommand.vim'Bundle 'ShowPairs'Bundle 'SudoEdit.vim'Bundle 'EasyGrep'Bundle 'VOoM'Bundle 'VimIM'".................................." non github repos" Bundle 'git://git.wincent.com/command-t.git'"......................................filetype plugin indent on "这一行就是结束了" 粘贴错位问题nnoremap &lt;F3&gt; :set invpaste paste?&lt;CR&gt;imap &lt;F3&gt; &lt;C-O&gt;:set invpaste paste?&lt;CR&gt;set pastetoggle=&lt;F3&gt;" nerdTree设置" 在 vim 启动的时候默认开启 NERDTree（autocmd 可以缩写为 au）autocmd VimEnter * NERDTree" 按下 F2 调出/隐藏 NERDTree" map :silent! NERDTreeTogglesilent! map &lt;F2&gt; :NERDTreeToggle&lt;CR&gt;let g:NERDTreeMapActivateNode="&lt;F2&gt;"" 将 NERDTree 的窗口设置在 vim 窗口的右侧（默认为左侧）" let NERDTreeWinPos="right"" 当打开 NERDTree 窗口时，自动显示 Bookmarkslet NERDTreeShowBookmarks=1 2.简单使用本人的一贯原则，首先找官方的文档或者手册怎样查找: 在vim中使用:h vundle定位 命令 描述 :PluginUpdate 更新指定的plugin :PluginList 显示已经安装的plugin :PluginClean 清除配置里没有的plugin，针对bundle目录 :PluginSearch xxx 搜索是否包含某个plugin 3. do it yourself既然配置好了插件，也学习了vundle的最基础命令，最后就只剩下我们去实际体验了。上面的过程其实就是在安装所有.vimrc里声明给vundle要去安装的插件，安装后的目录结构如下图：友情提示：安装完这些组件后，发现vundle的命令是可以通过tab进行自动补全的。 4.安装markdown插件由于刚刚立志要强迫自己玩儿转vim，所以先安装个markdown插件试试咯。 vimrc里添加 Plugin ‘godlygeek/tabular’Plugin ‘plasticboy/vim-markdown’ 之后自己写这个*.md文件，发现vim已经识别了这个文件是markdown，表现是能够识别符号，比如-xxx之后换行会自动-等 遗憾是没有发现preview的方法，目前的方式是先启动hexo，然后一边编写另一边进行实际的view 5.资源 http://vim-scripts.org/vim/scripts.htmlhttps://github.com/vim-scripts]]></content>
      <tags>
        <tag>vim</tag>
        <tag>vundle</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mac使用杂记]]></title>
    <url>%2F2015%2F12%2F04%2Fmac%E4%BD%BF%E7%94%A8%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.查看磁盘空间开始 -&gt; 关于本机 -&gt; 存储 2.快捷键 键 描述 空格 可以查看各种媒体文件 command + option + esc 强杀进程 F11 一键显示桌面 3. 多jdk环境1234567891011121314151617181920212223➜ ~ cat ~/.bashrcexport NVM_DIR=~/.nvmsource $(brew --prefix nvm)/nvm.shexport JAVA_6_HOME=/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Homeexport JAVA_7_1_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_07.jdk/Contents/Homeexport JAVA_7_HOME=/Library/Java/JavaVirtualMachines/jdk1.7.0_80.jdk/Contents/Homeexport JAVA_HOME=$JAVA_7_HOME# 下面的命令用来切换当前jdk环境alias jdk6=&apos;export JAVA_HOME=$JAVA_6_HOME &amp;&amp; export PATH=$JAVA_HOME/bin:$PATH&apos;alias jdk71=&apos;export JAVA_HOME=$JAVA_7_1_HOME &amp;&amp; export PATH=$JAVA_HOME/bin:$PATH&apos;alias jdk72=&apos;export JAVA_HOME=$JAVA_7_2_HOME &amp;&amp; export PATH=$JAVA_HOME/bin:$PATH&apos;➜ ~ java -versionjava version &quot;1.7.0_80&quot;Java(TM) SE Runtime Environment (build 1.7.0_80-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)➜ ~ jdk6➜ ~ java -versionjava version &quot;1.6.0_65&quot;Java(TM) SE Runtime Environment (build 1.6.0_65-b14-468-11M4833)Java HotSpot(TM) 64-Bit Server VM (build 20.65-b04-468, mixed mode) 参考自：http://ningandjiao.iteye.com/blog/2045955]]></content>
      <tags>
        <tag>mac</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[强奸R语言 1]]></title>
    <url>%2F2015%2F12%2F03%2F%E5%BC%BA%E5%A5%B8R%E8%AF%AD%E8%A8%80-1%2F</url>
    <content type="text"><![CDATA[就是为了学习一些机器学习的知识，需要用到R语言，但是感觉专门再去研究一门语言有些太繁琐。所以索性直接上手，也就是抄写各种机器学习的R程序。 首先弄的是文本挖掘的小程序：http://mp.weixin.qq.com/s?__biz=MzA3MDg0MjgxNQ==&amp;mid=400900310&amp;idx=1&amp;sn=4002ae3fc938a46060558480f0583fb2&amp;3rd=MzA3MDU4NTYzMw==&amp;scene=6#rd 好，文章里面提示要安装package。搜一下R安装package的方法：http://www.r-bloggers.com/installing-r-packages/ 先尝试图形界面，简单。使用R的图形界面找不到Snowball这个分词程序 再使用命令行：123install.packages("Snowball")Warning message:package ‘Snowball’ is not available (for R version 3.2.2) 到Google搜索 r snowball，定位到 https://cran.r-project.org/src/contrib/Archive/Snowball/ wget https://cran.r-project.org/src/contrib/Archive/Snowball/Snowball_0.0-11.tar.gz 123456789101112&gt; getwd() // 获取当前工作目录[1] "/Users/will"&gt; setwd('/Users/will/softs/clean') //设置工作目录到刚才下载的包那里&gt; getwd()[1] "/Users/will/softs/clean"&gt; install.packages("Snowball_0.0-11.tar.gz") // 再次安装inferring 'repos = NULL' from 'pkgs'ERROR: dependencies ‘RWeka’, ‘rJava’ are not available for package ‘Snowball’* removing ‘/Library/Frameworks/R.framework/Versions/3.2/Resources/library/Snowball’Warning message:In install.packages("Snowball_0.0-11.tar.gz") : 安装程序包‘Snowball_0.0-11.tar.gz’时退出狀態的值不是0 可以看到上面报出两个package找不到:RWeka和rJava 尝试一下有么有像maven一样自动下载依赖的方法：1234567891011&gt; install.packages("Snowball_0.0-11.tar.gz", dependencies=TRUE)inferring 'repos = NULL' from 'pkgs'* installing *source* package ‘Snowball’ ...** 成功将‘Snowball’程序包解包并MD5和检查** R** inst** preparing package for lazy loadingWarning message:In install.packages("Snowball_0.0-11.tar.gz", dependencies = TRUE) : 安装程序包‘Snowball_0.0-11.tar.gz’时退出狀態的值不是0No Java runtime present, requesting install. 马达, mac提示让安装1.6版本的jdk才行，日了。123java version &quot;1.7.0_80&quot;Java(TM) SE Runtime Environment (build 1.7.0_80-b15)Java HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode) 自己手动来1234567891011121314151617&gt; install.packages('RWeka')also installing the dependencies ‘RWekajars’, ‘rJava’试开URL’https://dirichlet.mat.puc.cl/bin/macosx/mavericks/contrib/3.2/RWekajars_3.7.12-1.tgz'Content type 'application/x-gzip' length 6473509 bytes (6.2 MB)==================================================downloaded 6.2 MB试开URL’https://dirichlet.mat.puc.cl/bin/macosx/mavericks/contrib/3.2/rJava_0.9-7.tgz'Content type 'application/x-gzip' length 604271 bytes (590 KB)==================================================downloaded 590 KB试开URL’https://dirichlet.mat.puc.cl/bin/macosx/mavericks/contrib/3.2/RWeka_0.4-24.tgz'Content type 'application/x-gzip' length 536260 bytes (523 KB)==================================================downloaded 523 KB 同样的方式，安装其他包(安装的时候需要注意当前目录getwd一下，是不是源package所在的位置) wget http://download.r-forge.r-project.org/src/contrib/Rwordseg_0.2-1.tar.gz 12345678910111213141516171819202122232425262728293031323334&gt; install.packages("Rwordseg_0.2-1.tar.gz", dependencies=TRUE)inferring 'repos = NULL' from 'pkgs'* installing *source* package ‘Rwordseg’ ...** R** demo** inst** preparing package for lazy loading** help*** installing help indices** building package indices** testing if installed package can be loaded* DONE (Rwordseg)&gt; install.packages("tm")also installing the dependencies ‘NLP’, ‘slam’试开URL’https://dirichlet.mat.puc.cl/bin/macosx/mavericks/contrib/3.2/NLP_0.1-8.tgz'Content type 'application/x-gzip' length 275184 bytes (268 KB)==================================================downloaded 268 KB试开URL’https://dirichlet.mat.puc.cl/bin/macosx/mavericks/contrib/3.2/slam_0.1-32.tgz'Content type 'application/x-gzip' length 101732 bytes (99 KB)==================================================downloaded 99 KB试开URL’https://dirichlet.mat.puc.cl/bin/macosx/mavericks/contrib/3.2/tm_0.6-2.tgz'Content type 'application/x-gzip' length 660405 bytes (644 KB)==================================================downloaded 644 KB下载的二进制程序包在 /var/folders/j_/gk6hxy7x705419zqy5sm167w0000gn/T//RtmpXdkdjK/downloaded_packages里 杂记12345678910111213141516171819202122232425262728&gt; library("arulesViz")&gt; data("Groceries") #加载指定的数据集&gt; summary(Groceries) # 查看transactions as itemMatrix in sparse format with 9835 rows (elements/itemsets/transactions) and 169 columns (items) and a density of 0.02609146 most frequent items: whole milk other vegetables rolls/buns soda 2513 1903 1809 1715 yogurt (Other) 1372 34055 element (itemset/transaction) length distribution:sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 16 17 18 19 20 21 22 23 24 26 27 28 29 32 46 29 14 14 9 11 4 6 1 1 1 1 3 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information - examples: labels level2 level11 frankfurter sausage meet and sausage2 sausage sausage meet and sausage3 liver loaf sausage meet and sausage min: 最小值1st Qu: 一分位数Median: 中位数Mean: 平均值3rd Qu: 三分位数Max： 最大值most frequent items： 最频繁出现的item 1234567891011121314151617181920212223242526272829303132333435363738394041&gt; rules &lt;- apriori(Groceries, parameter = list(support = 0.001, confidence = 0.5))AprioriParameter specification: confidence minval smax arem aval originalSupport support minlen maxlen 0.5 0.1 1 none FALSE TRUE 0.001 1 10 target ext rules FALSEAlgorithmic control: filter tree heap memopt load sort verbose 0.1 TRUE TRUE FALSE TRUE 2 TRUEAbsolute minimum support count: 9 set item appearances ...[0 item(s)] done [0.00s].set transactions ...[169 item(s), 9835 transaction(s)] done [0.00s].sorting and recoding items ... [157 item(s)] done [0.00s].creating transaction tree ... done [0.00s].checking subsets of size 1 2 3 4 5 6 done [0.01s].writing ... [5668 rule(s)] done [0.00s].creating S4 object ... done [0.00s].&gt; rulesset of 5668 rules &gt; inspect(head(sort(rules, by = &quot;lift&quot;), 3)) lhs rhs support confidence53 &#123;Instant food products,soda&#125; =&gt; &#123;hamburger meat&#125; 0.001220132 0.6315789 37 &#123;soda,popcorn&#125; =&gt; &#123;salty snack&#125; 0.001220132 0.6315789 444 &#123;flour,baking powder&#125; =&gt; &#123;sugar&#125; 0.001016777 0.5555556 lift 53 18.9956537 16.69779444 16.40807&gt; inspect(head(sort(rules, by = &quot;lift&quot;), 3)) lhs rhs support confidence lift 53 &#123;Instant food products,soda&#125; =&gt; &#123;hamburger meat&#125; 0.001220132 0.6315789 18.9956537 &#123;soda,popcorn&#125; =&gt; &#123;salty snack&#125; 0.001220132 0.6315789 16.69779444 &#123;flour,baking powder&#125; =&gt; &#123;sugar&#125; 0.001016777 0.5555556 16.40807&gt; plot(rules)&gt; plot(rules)&gt; plot(rules, measure = c(&quot;support&quot;, &quot;lift&quot;), shading = &quot;confidence&quot;)]]></content>
      <tags>
        <tag>R</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ElasticSearch kibana Dashboard 定义与使用]]></title>
    <url>%2F2015%2F12%2F03%2FElasticSearch-kibana-Dashboard-%E5%AE%9A%E4%B9%89%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[以前在骡迹物流的时候只是玩儿了一下kibana简单的功能，并没有投入使用，当时也基本上玩儿遍了各种可视化，就是么有弄dashboard。在美团这边前两天被拉进elasticsearch的群里，然后收到一个kibana的地址，今儿下午尝试性地玩儿了一下，还蛮爽的，有些惊异。 dashboard是一个画布，然后这个画布上需要放置很多的visualize，也就是说在弄dashboard之前是需要先准备好visualize的，它由visualize和search组成。 1. 准备源材料创建并保存visualization和search 2. 新建dashboard 3. 为dashboard添加元素 4. 调整后保存 好了，这已经非常好看了，完全可以直接看了。以后再有运营提数据需求，我就直接创建search或者visualization后，生成dashboard。可是问题又来了，咋给他看呢？总不能让所有的运营人员都登陆kibana页面来瞅吧，那样的话还得加权限神马的，比如对个别页面有只读权限，其他页面或者操作没有任何权限才对。静静地思考，kibana和elasticsearch已然有了思路，是不可能倒在权限这个小问题上的，肯定有一种合理的方式去分享给需求端看。fun，找找看~ 5. dashboard组件化叫我脑残，功能就放在眼前 获取到iframe后，自然就会想到，这个东西是可以嵌入到我们自己的网页中的。如此一来的话，就可以直接将数据存储在elasticsearch中，之后在elasticsearch和kibana之上建立自己的search，visualization，dashboard。然后将最终的dashboard嵌入到应用页面。 还是有些问题，直接在浏览器访问这个dashboard的url的话会发现其实就是kibana那个页面，但是放进iframe之后就成了不可编辑的状态，而且只有组件，上面不会有kibana的head，这个状态比较理想。可是右击查看源代码的话，还是可以看到kibana的页面接口URL，如果直接访问还是可以直接进行编辑。 查了一下权限相关 https://github.com/floragunncom/search-guard http://drops.wooyun.org/tips/8129 http://3gods.com/2015/09/10/Shield%20Control.html]]></content>
      <tags>
        <tag>ElasticSearch</tag>
        <tag>kibana</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[guava整理 - Basic utilities]]></title>
    <url>%2F2015%2F11%2F19%2Fguava%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[总是看到guava的身影，今天来大概浏览一下这个Google项目。 1. 对于null的思考null值的特点： 不能清晰表达任何业务模型 就是说在很多情况下，我们看到null的时候通常要分析一下context才能知道这里是什么情况 容易被人忽视 很多时候coder会专注于自己正在思考的核心业务场景，编写代码逻辑。容易忘记处理null对象的情况，所以一些nullPointerException通常是由于不可避免的粗心遗漏造成的。 Optional 当获取某个值的时候，尽量使用Optional, T能够帮助我们理解当前的业务模型，而Optional.get()这一个不是很方便的调用则会时刻提醒我们充分考虑当前场景下，如果主对象为null应当采取的措施。 有异曲同工之妙的还有scala中的Option API概览： http://docs.guava-libraries.googlecode.com/git/javadoc/com/google/common/base/Optional.html 2. 前置条件就是一些常用的、通用的、简单的条件判断，API:http://docs.guava-libraries.googlecode.com/git-history/release/javadoc/com/google/common/base/Preconditions.html java7其实也添加了一些方法：http://docs.oracle.com/javase/7/docs/api/java/util/Objects.html#requireNonNull(java.lang.Object,java.lang.String) 如果不满足这些条件，就会抛出Exception 3. 通用对象方法对于Object默认的toString hashCode等方法提供简单实现1234Objects.equal(&quot;a&quot;, &quot;a&quot;); // returns trueObjects.equal(null, &quot;a&quot;); // returns falseObjects.equal(&quot;a&quot;, null); // returns falseObjects.equal(null, null); // returns true equals方法给处理了null值情况。 java7也是用了类似guava的http://google.github.io/guava/releases/snapshot/api/docs/com/google/common/base/Objects.html#hashCode(java.lang.Object...)的http://docs.oracle.com/javase/7/docs/api/java/util/Objects.html#hash(java.lang.Object…) 可读的toString123456789// Returns &quot;ClassName&#123;x=1&#125;&quot; MoreObjects.toStringHelper(this) .add(&quot;x&quot;, 1) .toString(); // Returns &quot;MyObject&#123;x=1&#125;&quot; MoreObjects.toStringHelper(&quot;MyObject&quot;) .add(&quot;x&quot;, 1) .toString(); http://docs.guava-libraries.googlecode.com/git-history/release/javadoc/com/google/common/base/Objects.html#equal(java.lang.Object, java.lang.Object) 4. ComparisonChain任何一个条件返回非0值，就比较结束。 1234567public int compareTo(Foo that) &#123; return ComparisonChain.start() .compare(this.aString, that.aString) .compare(this.anInt, that.anInt) .compare(this.anEnum, that.anEnum, Ordering.natural().nullsLast()) .result(); &#125; 5. 异常传播简化Throwable的处理有时候我们catch住一个Exception，但是并不想在这个catch块里处理，就会throw出去给下个块。最常见的例子是RuntimeException和Error，本来我们并不想catch它们，但是通常它们又会被catch住。 具体使用没看太明白 集合类使用 1. 不可变集合 安全 不存在并发问题 空间可预测 注：目前guava的不可变集合都不许有null元素，如果要使用带有null的不可变集合，那么选用JDK自带的Collections.unmodifiableXXX 使用不可变集合 copyOf：如ImmutableSet.copyOf(set); of：如ImmutableSet.of(“a”, “b”, “c”)或 ImmutableMap.of(“a”, 1, “b”, 2); builder：public static final ImmutableSet GOOGLE_COLORS =ImmutableSet.&lt;Color\&gt;builder() .addAll(WEBSAFE_COLORS) .add(new Color(0, 191, 255)) .build(); asList视图 所有不可变集合都有一个asList()方法提供ImmutableList视图，来帮助你用列表形式方便地读取集合元素。例如，你可以使用sortedSet.asList().get(k)从ImmutableSortedSet中读取第k个最小元素。 asList()返回的ImmutableList通常是——并不总是——开销稳定的视图实现，而不是简单地把元素拷贝进List。也就是说，asList返回的列表视图通常比一般的列表平均性能更好，比如，在底层集合支持的情况下，它总是使用高效的contains方法。 2. 新的集合类型JDK没有，但是又被非常广泛使用的集合类型。 MultiSet 12345678910111213141516171819202122232425262728293031private void outMuiltSet() &#123; System.out.println(&quot;-------------------&quot;); System.out.println(strs.size()); System.out.println(strs.count(new Object())); //count(Object element): 返回给定参数元素的个数 System.out.println(strs.count(&quot;sdfsd&quot;)); //count(Object element): 返回给定参数元素的个数 System.out.println(strs); &#125; public void test3() throws Throwable &#123; strs = TreeMultiset.create(); strs.add(&quot;sdfsd&quot;, 2); //add(E element,int occurrences): 向其中添加指定个数的元素 strs.add(&quot;sdfsd&quot;); // add(E element): 向其中添加单个元素 strs.add(&quot;dlkjlkj&quot;); System.out.println(strs.elementSet()); // elementSet(): 将不同的元素放入一个Set中 Set&lt;Multiset.Entry&lt;Comparable&gt;&gt; entries = strs.entrySet(); //类似与Map.entrySet 返回Set。包含的Entry支持使用getElement()和getCount() for (Multiset.Entry entry: entries) &#123; System.out.println(entry.getElement()); System.out.println(entry.getCount()); &#125; outMuiltSet(); strs.remove(&quot;sdfsd&quot;); // remove(E element): 移除一个元素，其count值 会响应减少 outMuiltSet(); strs.setCount(&quot;sdfsd&quot;, 10); // 设定某一个元素的重复次数 outMuiltSet(); strs.remove(&quot;sdfsd&quot;, 2); //remove(E element,int occurrences): 移除相应个数的元素 outMuiltSet(); strs.setCount(&quot;sdfsd&quot;, 8, 20); //setCount(E element,int oldCount,int newCount): 将符合原有重复个数的元素修改为新的重复次数 strs.setCount(&quot;sdfsd&quot;, 8, 20); strs.setCount(&quot;dlkjlkj&quot;, 8, 20); outMuiltSet(); &#125; SortedMultiset SortedMultiset是Multiset 接口的变种，它支持高效地获取指定范围的子集。比方说，你可以用 latencies.subMultiset(0,BoundType.CLOSED, 100, BoundType.OPEN).size()来统计你的站点中延迟在100毫秒以内的访问，然后把这个值和latencies.size()相比，以获取这个延迟水平在总体访问中的比例。 TreeMultiset实现SortedMultiset接口。 Multimap可以用两种方式思考Multimap的概念: “键-单个值映射”的集合: a-&gt;1, a-&gt;2, a-&gt;4, b-&gt;3, c-&gt;5“键-值集合映射”的映射: a-&gt;[1,2,4], b-&gt;3, c-&gt;5一般情况下都会使用ListMultimap或SetMultimap接口，它们分别把键映射到List或Set。Multimap.get(key)以集合形式返回键所对应的值视图, 即使没有任何对应的值，也会返回空集合。对值视图集合进行的修改最终都会反映到底层的Multimap。 12345678public void testMap() &#123; ImmutableListMultimap&lt;String, Integer&gt; of = ImmutableListMultimap.of(&quot;a&quot;, 1, &quot;a&quot;, 2, &quot;b&quot;, 1); System.out.println(of.asMap()); ListMultimap map = ArrayListMultimap.create(of); map.put(&quot;a&quot;, 234); map.putAll(&quot;c&quot;, ImmutableSet.of(123,3243)); System.out.println(map); &#125; BiMap实现键值对的双向映射,并保持它们间的同步。 1234567BiMap&lt;String, Integer&gt; map = HashBiMap.create(); map.put(&quot;foo&quot;, 1); map.put(&quot;bar&quot;, 2); map.put(&quot;quux&quot;, 3); map.inverse().forcePut(1, &quot;quux&quot;); // 会导致foo被替换成quux，而quux的value也成了1，就是&#123;bar=2, quux=1&#125; System.out.println(map); Table 1234567891011String v1 = &quot;a&quot;; String v2 = &quot;b&quot;; String v3 = &quot;c&quot;; Table&lt;String, String, Integer&gt; weightedGraph = HashBasedTable.create(); weightedGraph.put(v1, v2, 4); weightedGraph.put(v1, v3, 20); weightedGraph.put(v2, v3, 5); System.out.println(weightedGraph); System.out.println(weightedGraph.row(v1) + &quot;........sdfsd&quot;); // 按照rowKey查询 System.out.println(weightedGraph.column(v3)); // 按照Columnkey查询 ClassToInstanceMap 123ClassToInstanceMap&lt;Number&gt; numberDefaults=MutableClassToInstanceMap.create(); numberDefaults.putInstance(Integer.class, Integer.valueOf(0)); numberDefaults.putInstance(Integer.class, Float.valueOf(0)); // 编译报错 RangeSetRangeSet描述了一组不相连的、非空的区间。当把一个区间添加到可变的RangeSet时，所有相连的区间会被合并，空区间会被忽略。 123456789101112RangeSet&lt;Integer&gt; rangeSet = TreeRangeSet.create(); System.out.println(rangeSet); rangeSet.add(Range.closed(1, 10)); // &#123;[1,10]&#125; System.out.println(rangeSet); rangeSet.add(Range.closedOpen(11, 15));//不相连区间:&#123;[1,10], [11,15)&#125; System.out.println(rangeSet); rangeSet.add(Range.closedOpen(15, 20)); //相连区间; &#123;[1,10], [11,20)&#125; System.out.println(rangeSet); rangeSet.add(Range.openClosed(0, 0)); //空区间; &#123;[1,10], [11,20)&#125; System.out.println(rangeSet); rangeSet.remove(Range.open(5, 10)); //分割[1, 10]; &#123;[1,5], [10,10], [11,20)&#125; System.out.println(rangeSet); RangeMapRangeMap描述了”不相交的、非空的区间”到特定值的映射。和RangeSet不同，RangeMap不会合并相邻的映射，即便相邻的区间映射到相同的值。 123456789RangeMap&lt;Integer, String&gt; rangeMap = TreeRangeMap.create(); rangeMap.put(Range.closed(1, 10), &quot;foo&quot;); //&#123;[1,10] =&gt; &quot;foo&quot;&#125; System.out.println(rangeMap); rangeMap.put(Range.open(3, 6), &quot;bar&quot;); //&#123;[1,3] =&gt; &quot;foo&quot;, (3,6) =&gt; &quot;bar&quot;, [6,10] =&gt; &quot;foo&quot;&#125; System.out.println(rangeMap); rangeMap.put(Range.open(10, 20), &quot;foo&quot;); //&#123;[1,3] =&gt; &quot;foo&quot;, (3,6) =&gt; &quot;bar&quot;, [6,10] =&gt; &quot;foo&quot;, (10,20) =&gt; &quot;foo&quot;&#125; System.out.println(rangeMap); rangeMap.remove(Range.closed(5, 11)); //&#123;[1,3] =&gt; &quot;foo&quot;, (3,5) =&gt; &quot;bar&quot;, (11,20) =&gt; &quot;foo&quot;&#125; System.out.println(rangeMap);]]></content>
      <tags>
        <tag>guava</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[node杂记]]></title>
    <url>%2F2015%2F10%2F15%2Fnode%E6%9D%82%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[因为新的项目这边前端是使用的nodejs，精通自己，熟知周边是我一直依赖的坚持。所以现在要对nodejs有个hello world的了解，便于以后自己调试自己的接口以及其他功能代码。 nodejs的依赖管理工具是npm, 有些类似Python的pip + java的maven，既负责安装全局性的依赖，也能处理当前项目的依赖。 npm install xxx 是安装依赖到当前目录的node_modules文件夹下，只有当前项目能够使用这些依赖； npm install xxx -g 安装依赖到npm根目录的node_modules文件夹下，本机所有nodejs项目都可以使用 npm主要针对package.json进行，其实npm有啥功能只要看一下package.json有啥配置项就可以了。但是为了完整性，还是列一下: 命令 描述 npm install 安装package.json里指定的所有依赖 npm install xxx 安装xxx,其实这个命令感觉跟当前项目根本没啥关系了 npm install xxx –save 安装xxx,同时将xxx的最新版本加入package.json的dependency里 npm set init-author-name ‘Your name’ 等于为npm init设置了默认值，以后执行npm init的时候，package.json的作者姓名、邮件、主页、许可证字段就会自动写入预设的值。这些信息会存放在用户主目录的~/.npmrc文件，使得用户不用每个项目都输入。如果某个项目有不同的设置，可以针对该项目运行npm config。 npm info xxx 查看模块xxx的具体信息 npm search xxx 查找xxx模块是否存在于npm仓库 npm list 树形结构列出当前项目安装的所有module npm info xxx 查看模块xxx的具体信息 注： nodejs自身还有版本管理工具，就是nvm了，可以在一个机器上安装多个版本的nodejs. git clone https://github.com/creationix/nvm.git ~/.nvm source ~/.nvm/nvm.sh# 安装最新版本 nvm install node 安装指定版本 nvm install 0.12.1 使用已安装的最新版本 nvm use node 使用指定版本的node nvm use 0.12 查看本地安装的所有版本 nvm ls 查看服务器上所有可供安装的版本。 nvm ls-remote 退出已经激活的nvm，使用deactivate命令。 nvm deactivate nodejs是一种事件驱动的思想：nodejs的event流有些像java web这边的servlet chain，不过他们的servlet叫做middleware，而且很多层次都可以有middleware。 app123456789101112131415// a middleware sub-stack which handles GET requests to /user/:idapp.get(&apos;/user/:id&apos;, function (req, res, next) &#123; // if user id is 0, skip to the next route if (req.params.id == 0) next(&apos;route&apos;); // else pass the control to the next middleware in this stack else next(); //&#125;, function (req, res, next) &#123; // render a regular page res.render(&apos;regular&apos;);&#125;);// handler for /user/:id which renders a special pageapp.get(&apos;/user/:id&apos;, function (req, res, next) &#123; res.render(&apos;special&apos;);&#125;); router12345678910111213141516171819202122232425262728293031323334353637var app = express();var router = express.Router();// a middleware with no mount path, gets executed for every request to the routerrouter.use(function (req, res, next) &#123; console.log(&apos;Time:&apos;, Date.now()); next();&#125;);// a middleware sub-stack shows request info for any type of HTTP request to /user/:idrouter.use(&apos;/user/:id&apos;, function(req, res, next) &#123; console.log(&apos;Request URL:&apos;, req.originalUrl); next();&#125;, function (req, res, next) &#123; console.log(&apos;Request Type:&apos;, req.method); next();&#125;);// a middleware sub-stack which handles GET requests to /user/:idrouter.get(&apos;/user/:id&apos;, function (req, res, next) &#123; // if user id is 0, skip to the next router if (req.params.id == 0) next(&apos;route&apos;); // else pass the control to the next middleware in this stack else next(); //&#125;, function (req, res, next) &#123; // render a regular page res.render(&apos;regular&apos;);&#125;);// handler for /user/:id which renders a special pagerouter.get(&apos;/user/:id&apos;, function (req, res, next) &#123; console.log(req.params.id); res.render(&apos;special&apos;);&#125;);// mount the router on the appapp.use(&apos;/&apos;, router); error handler四个参数，多一个err。如果完事儿不调用next的话，就需要指定状态码，不然就会向普通的middleware一样报错，并且执行失败。1234app.use(function(err, req, res, next) &#123; console.error(err.stack); res.status(500).send(&apos;Something broke!&apos;);&#125;); built-inexpresss.static(root, [options]) 一般指定应用的静态资源根目录12345678910111213var options = &#123; dotfiles: &apos;ignore&apos;, etag: false, extensions: [&apos;htm&apos;, &apos;html&apos;], index: false, maxAge: &apos;1d&apos;, redirect: false, setHeaders: function (res, path, stat) &#123; res.set(&apos;x-timestamp&apos;, Date.now()); &#125;&#125;app.use(express.static(&apos;public&apos;, options)); 也可以指定多个目录123app.use(express.static(&apos;public&apos;));app.use(express.static(&apos;uploads&apos;));app.use(express.static(&apos;files&apos;)); third-party123456var express = require(&apos;express&apos;);var app = express();var cookieParser = require(&apos;cookie-parser&apos;);// load the cookie parsing middlewareapp.use(cookieParser()); node启动express项目：node app.js，如果指定参数-p 8080,就会启动port为8080的server。process.env是指引用系统的环境变量 jade是一种node的模板引擎，将一些关键字映射为html里的元素，使简洁，而且生成的HTML是最小的、确保正确的。 使用的时候可以使好多公共的东西模块化，然后include进来 模板之间可以extends，一般extends layout, 然后给里面的一些变量赋值,一般都是处理block详见官网吧。 bootstrap + jade，you know… UI: bootstrap模板引擎：jadecss引擎：stylus前端：jquery + socket.io后端：express]]></content>
  </entry>
  <entry>
    <title><![CDATA[mt JIRA+wiki笔记]]></title>
    <url>%2F2015%2F10%2F10%2F%E5%86%85%E9%83%A8JIRA%2Bwiki%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.JIRA目前美团用的是JIRA-4，付费买的，然后进行了一些二次开发. 1.1 功能追踪bug、task 帮助自己：管理TODO，提高效率 帮助同伴：了解进展、及时通知和讨论 帮助leader：监控 帮助所有人：记录和通知进度、慧谷总结 RSS订阅 小工具： 图表神马的 过滤器：可以写JQL，也可以可视化操作；可以私人拥有，也可以共享给大家 关注其他人的ticket 预期时间 抄送 集成git、用户反馈邮件自动生成ticket 1.2 Ticket 状态：open -&gt; in progress -&gt; resolved -&gt; closed 类型：任务、改进、新功能、缺陷、电话问题 优先级：blocker、critical、major、trivial 使用原则： 自主推动与交付，随时增加注释、更新进度 使用Dashboard来管理自己的TODO 合理安排优先级 任务放进对应的版块 1.3 添加备注时机： 初始目标出现变动 开发和追查的问题有进展和结论 延期或者需要其他依赖条件 分配给其他人、reopen、close的时候 推荐使用google calendar来管理每日计划 2. Wiki专业wiki，注重企业知识管理与协调。公司内部是很注重知识的积累的，所以比较重视wiki这块。一般是一些tips，比如一些技巧、或者一些坑、一些心得什么的，可以分享出来供大家参考。 2.1 特点 出色的站点管理 简单而强大的编辑功能 团队成员间的交流 插件扩展 2.2 功能 知识管理 社交 分享、积累 文档归档 查找文档 监视文档的每次变化 关注文档 收藏文档 文档归类 2.3 查找搜索 目录结构 页面链接 收藏夹 添加书签【用于收藏外部资源】 2.4 作业创建个人空间，添加一个页面，分享或者总结一下。要求：有段落、标题。可选：图片、表格、代码。]]></content>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（2）]]></title>
    <url>%2F2015%2F08%2F10%2Fakka-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[clone git clone https://github.com/RayRoestenburg/akka-in-action.git 先切换到ch02分支上【这个项目的分支有点儿乱 】，找到目录里的chapter2目录，引入idea 引入过程中IDE应该会自动下载sbt依赖，如果没有的话，到项目根目录下执行： sbt assembly 此项目使用的依赖中除了Spray其他都是scala栈中的成员。当然后面可能会引入更多的其他依赖，比如根据不同的环境进行不同参数的调整。 使用下面命令测试是否构建成功： java -jar target/scala-2.10/goticks-server.jar Slf4jEventHandler started akka://com-goticks-Main/user/io-bridge started akka://com-goticks-Main/user/http-server started on /0.0.0.0:5000 sbtsbt的配置文件就是项目根目录下的build.sbt文件。 import AssemblyKeys._ import com.typesafe.sbt.SbtStartScript // 有两个jar需要用到上面两个import。一个类似maven里的shade，将项目打成一个大jar；另一个则是用来部署项目到Heroku上。 name := &quot;goticks&quot; version := &quot;0.1-SNAPSHOT&quot; organization := &quot;com.goticks&quot; scalaVersion := &quot;2.11.2&quot; resolvers ++= Seq(&quot;Typesafe Repository&quot; at &quot;http://repo.typesafe.com/typesafe/releases/&quot;, &quot;Spray Repository&quot; at &quot;http://repo.spray.io&quot;) // 指定repositories libraryDependencies ++= { val akkaVersion = &quot;2.3.9&quot; val sprayVersion = &quot;1.3.2&quot; Seq( &quot;com.typesafe.akka&quot; %% &quot;akka-actor&quot; % akkaVersion, &quot;io.spray&quot; %% &quot;spray-can&quot; % sprayVersion, &quot;io.spray&quot; %% &quot;spray-routing&quot; % sprayVersion, &quot;io.spray&quot; %% &quot;spray-json&quot; % &quot;1.3.1&quot;, &quot;com.typesafe.akka&quot; %% &quot;akka-slf4j&quot; % akkaVersion, &quot;ch.qos.logback&quot; % &quot;logback-classic&quot; % &quot;1.1.2&quot;, &quot;com.typesafe.akka&quot; %% &quot;akka-testkit&quot; % akkaVersion % &quot;test&quot;, &quot;org.scalatest&quot; %% &quot;scalatest&quot; % &quot;2.2.0&quot; % &quot;test&quot; ) } // Assembly settings mainClass in Global := Some(&quot;com.goticks.Main&quot;) jarName in assembly := &quot;goticks-server.jar&quot; assemblySettings // StartScript settings seq(SbtStartScript.startScriptForClassesSettings: _*) REST server到根目录下启动server sbt run 存取[为方便测试，请事先安装Linux工具httpie]: root@ubuntu:~/gitLearning/akka-in-action/chapter2# http PUT localhost:5000/events event=RHCP nrOfTickets:=10 HTTP/1.1 200 OK Content-Length: 2 Content-Type: text/plain; charset=UTF-8 Date: Mon, 10 Aug 2015 06:28:08 GMT Server: GoTicks.com REST API OK root@ubuntu:~/gitLearning/akka-in-action/chapter2# http PUT localhost:5000/events event=DjMadlib nrOfTickets:=15 HTTP/1.1 200 OK Content-Length: 2 Content-Type: text/plain; charset=UTF-8 Date: Mon, 10 Aug 2015 06:28:45 GMT Server: GoTicks.com REST API OK root@ubuntu:~/gitLearning/akka-in-action/chapter2# http GET localhost:5000/events HTTP/1.1 200 OK Content-Length: 92 Content-Type: application/json; charset=UTF-8 Date: Mon, 10 Aug 2015 06:28:52 GMT Server: GoTicks.com REST API [ { &quot;event&quot;: &quot;DjMadlib&quot;, &quot;nrOfTickets&quot;: 15 }, { &quot;event&quot;: &quot;RHCP&quot;, &quot;nrOfTickets&quot;: 10 } ] root@ubuntu:~/gitLearning/akka-in-action/chapter2# http GET localhost:5000/ticket/RHCP HTTP/1.1 200 OK Content-Length: 32 Content-Type: application/json; charset=UTF-8 Date: Mon, 10 Aug 2015 06:29:32 GMT Server: GoTicks.com REST API { &quot;event&quot;: &quot;RHCP&quot;, &quot;nr&quot;: 1 } root@ubuntu:~/gitLearning/akka-in-action/chapter2# http GET localhost:5000/events HTTP/1.1 200 OK Content-Length: 91 Content-Type: application/json; charset=UTF-8 Date: Mon, 10 Aug 2015 06:29:46 GMT Server: GoTicks.com REST API [ { &quot;event&quot;: &quot;DjMadlib&quot;, &quot;nrOfTickets&quot;: 15 }, { &quot;event&quot;: &quot;RHCP&quot;, &quot;nrOfTickets&quot;: 9 } ] 以上已经包含了CRUD操作，当然这并不全面。我们没有考虑如果一个Event被卖出去了，那么它的状态应该马上变为不可卖，后面我们会再详细谈论此类问题。 Actors你可以尝试跟着github上搞下来的源码自己一步步构建项目试试。现在我们已经知道了，actor可以有四种操作：create, send\receive, become, supervise。这里我们会接触到前两个。先看看actor之间怎样相互作用传递信息: 创建event、改票、完成event。 例子中的app只包含三个Actor类。首先，我们是创建这些Actor的容器ActorSystem, 之后Actor再去创建它的子Actor。下图是次序： RestInterface Actor负责处理HTTP request，其实就是一个HTTP请求的adapter。TicketSeller则负责一些tickets，所有关于票务买卖的event它都管。下图解释了一个request怎样通过actor system创建一个event： 注意：我看的版本里不是TicketMaster而是BoxOffice 以上就是这个小APP里主要的业务流了，主要就是让customer能够跟可用的票之间能够建立连接，进行交互。 下面让我们退回去看一下代码，首先是入口Main类 1234567891011121314151617181920212223242526272829303132333435363738package com.goticksimport scala.concurrent.duration._import akka.actor._import akka.io.IOimport akka.pattern.askimport akka.util.Timeoutimport spray.can.Httpimport spray.can.Http.Boundimport com.typesafe.config.ConfigFactoryobject Main extends App &#123; val config = ConfigFactory.load() // 加载配置文件 val host = config.getString("http.host") val port = config.getInt("http.port") implicit val system = ActorSystem("goticks") val api = system.actorOf(Props(new RestInterface()), "httpInterface") // 创建最高层的actor implicit val executionContext = system.dispatcher implicit val timeout = Timeout(10 seconds) // 启动Spray HTTP Server IO(Http).ask(Http.Bind(listener = api, interface = host, port = port)) .mapTo[Http.Event] .map &#123; case Http.Bound(address) =&gt; println(s"REST interface bound to $address") case Http.CommandFailed(cmd) =&gt; println("REST interface could not bind to " + s"$host:$port, $&#123;cmd.failureMessage&#125;") system.shutdown() &#125;&#125; 这个App内部沟通的消息类型都定义在TicketProtocol中：1234567891011121314151617181920212223242526272829303132333435363738object TicketProtocol &#123; import spray.json._ case class Event(event:String, nrOfTickets:Int) case object GetEvents case class Events(events:List[Event]) case object EventCreated case class TicketRequest(event:String) case object SoldOut case class Tickets(tickets:List[Ticket]) case object BuyTicket case class Ticket(event:String, nr:Int) //---------------------------------------------- // JSON //---------------------------------------------- object Event extends DefaultJsonProtocol &#123; implicit val format = jsonFormat2(Event.apply) &#125; object TicketRequest extends DefaultJsonProtocol &#123; implicit val format = jsonFormat1(TicketRequest.apply) &#125; object Ticket extends DefaultJsonProtocol &#123; implicit val format = jsonFormat2(Ticket.apply) &#125;&#125; 作为一个典型的APP，我们有一个接口来解决围绕Event主体和Ticket整个生命周期的所有问题。记住，Akka会使用immutable message将这些部分链接起来，所以actor应该知道它需要的所有的信息。 TicketSellerTicketSeller是由BoxOffice创建的, 主要负责处理订单的，自己维护了一些ticket。每次有买票的请求，它就从自己的ticket list里拿出第一张来：1234567891011121314151617181920212223class TicketSeller extends Actor &#123; import TicketProtocol._ var tickets = Vector[Ticket]() // 票 def receive = &#123; case GetEvents =&gt; sender ! tickets.size // 返回当前票数 case Tickets(newTickets) =&gt; tickets = tickets ++ newTickets //添加新票 case BuyTicket =&gt; // 有请求买票 if (tickets.isEmpty) &#123; sender ! SoldOut self ! PoisonPill // 如果卖光了就服毒自杀，这会导致当前actor被清理 &#125; tickets.headOption.foreach &#123; ticket =&gt; tickets = tickets.tail sender ! ticket &#125; &#125;&#125; BoxOfficeBoxOffice 需要为每个event都创建TicketSeller，并把买票请求代理给他们。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364class BoxOffice extends Actor with CreateTicketSellers with ActorLogging &#123; import TicketProtocol._ import context._ implicit val timeout = Timeout(5 seconds) def receive = &#123; case Event(name, nrOfTickets) =&gt; log.info(s"Creating new event $&#123;name&#125; with $&#123;nrOfTickets&#125; tickets.") if(context.child(name).isEmpty) &#123; // 如果这个name的TicketSeller还没有的话 val ticketSeller = createTicketSeller(name) // 塞进初始数量的票 val tickets = Tickets((1 to nrOfTickets).map(nr=&gt; Ticket(name, nr)).toList) ticketSeller ! tickets &#125; sender ! EventCreated case TicketRequest(name) =&gt; log.info(s"Getting a ticket for the $&#123;name&#125; event.") // 将BuyTicket message传递给TicketSeller context.child(name) match &#123; case Some(ticketSeller) =&gt; ticketSeller.forward(BuyTicket) case None =&gt; sender ! SoldOut // 如果找不到对应的ticketSeller，就返回SoldOut message给sender，也就是RestInterface &#125; // 向所有的ticketSeller收集当前票数，然后弄成一个list。ask是一个异步操作，因为我们不希望BoxOffice阻塞，而停止处理其他request。 case GetEvents =&gt; import akka.pattern.ask val capturedSender = sender // 向TicketSeller传递GetEvents message的本地方法 def askAndMapToEvent(ticketSeller:ActorRef) = &#123; // 不阻塞的获取每个name下的票数，futureInt会在执行完后获取到应得的value val futureInt = ticketSeller.ask(GetEvents).mapTo[Int] futureInt.map(nrOfTickets =&gt; // 将future的值转化为Event Event(ticketSeller.actorRef.path.name, nrOfTickets)) &#125; // 遍历所有的ticketSeller val futures = context.children.map(ticketSeller =&gt; askAndMapToEvent(ticketSeller)) // 所有future都得到值后，发送信息。 Future.sequence(futures).map &#123; events =&gt; capturedSender ! Events(events.toList) &#125; &#125;&#125;// 注意是使用context创建的actor：谁创建的，谁就是这个新actor的supervisortrait CreateTicketSellers &#123; self:Actor =&gt; def createTicketSeller(name:String) = context.actorOf(Props[TicketSeller], name)&#125; 上例中我们使用ask返回的是一个Future对象，这个东西是在未来某个时间点才会有值的。获取一个future引用，定义当这个值可用后要做的操作，而不是阻塞等待执行完毕，也不要直接使用这个future。第七章我们会深入探索Future的特性，弄清楚这些非阻塞的异步请求的运作原理。 RestInterfaceRestInterface使用了Spray routing DSL，后面第九章会详解：当service越来越大，复杂的路由也越来越多。我们这个例子里倒是很少，因为只是卖票而已。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091package com.goticksimport akka.actor._import spray.routing._import spray.http.StatusCodesimport spray.httpx.SprayJsonSupport._import spray.routing.RequestContextimport akka.util.Timeoutimport scala.concurrent.duration._import scala.language.postfixOpsclass RestInterface extends HttpServiceActor with RestApi &#123; def receive = runRoute(routes)&#125;trait RestApi extends HttpService with ActorLogging &#123; actor: Actor =&gt; import context.dispatcher import com.goticks.TicketProtocol._ implicit val timeout = Timeout(10 seconds) import akka.pattern.ask import akka.pattern.pipe // 初始化就创建一个BoxOffice val boxOffice = context.actorOf(Props[BoxOffice]) def routes: Route = path("events") &#123; put &#123; entity(as[Event]) &#123; event =&gt; requestContext =&gt; val responder = createResponder(requestContext) boxOffice.ask(event).pipeTo(responder) &#125; &#125; ~ get &#123; requestContext =&gt; val responder = createResponder(requestContext) boxOffice.ask(GetEvents).pipeTo(responder) &#125; &#125; ~ path("ticket") &#123; /** entity方法把request直接解析成一个TicketRequest对象，不用我们自己再去编写代码来映射到对象。 response被pipe到一个使用pipe模型的responder。 */ get &#123; entity(as[TicketRequest]) &#123; ticketRequest =&gt; requestContext =&gt; val responder = createResponder(requestContext) boxOffice.ask(ticketRequest).pipeTo(responder) &#125; &#125; &#125; ~ path("ticket" / Segment) &#123; eventName =&gt; requestContext =&gt; val req = TicketRequest(eventName) val responder = createResponder(requestContext) boxOffice.ask(req).pipeTo(responder) &#125; def createResponder(requestContext:RequestContext) = &#123; context.actorOf(Props(new Responder(requestContext))) &#125;&#125;// 围绕一个HTTP request生命周期的Responder，发送消息给BoxOffice，同时也处理来自TicketSeller和BoxOffice的response。class Responder(requestContext:RequestContext) extends Actor with ActorLogging &#123; import TicketProtocol._ import spray.httpx.SprayJsonSupport._ // responder接收到TicketSeller返回的以下四类消息后就算完成了HTTP request。之后自杀。 def receive = &#123; case ticket:Ticket =&gt; requestContext.complete(StatusCodes.OK, ticket) self ! PoisonPill case EventCreated =&gt; requestContext.complete(StatusCodes.OK) self ! PoisonPill case SoldOut =&gt; requestContext.complete(StatusCodes.NotFound) self ! PoisonPill case Events(events) =&gt; requestContext.complete(StatusCodes.OK, events) self ! PoisonPill &#125;&#125;]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr上手（四）]]></title>
    <url>%2F2015%2F08%2F06%2Fsolr%E4%B8%8A%E6%89%8B-4%2F</url>
    <content type="text"><![CDATA[走马观花过完了solr官方的入门文档，我们下一个目的就是搞清楚Collection的CURD, 以及弄清楚schema在哪里配置或者生成的。 找到官方wiki跟做. 创建Collection localhost:8983/admin/collections?action=CREATE&amp;name=willCollection&amp;numShards=2&amp;replicationFactor=1 其他操作其实就参考https://cwiki.apache.org/confluence/display/solr/Collections+API就可以了，没什么好抄袭的感觉。 schema相关一直纳闷儿solr里是不是像ES那样的数据结构： index -&gt; type -&gt; field？经过各种纠结的实验后，总结出：No！！配置有两种情况，当运行solr单例的时候，5.2.1版本配置文件是放置在server/configsets/basic_configs/conf/schema.xml的，这里面记录了所有field的schema，没错整个示例都用同一个schema！不分什么type，大家在一起！ 但是我怎么也找不到例子cloud里的那些字段所在的schema文件，到example和solr目录下都么有。。。功夫不负有心人，想到了zookeeper，连接上去查看一下，果然看到了预期的结果。 贼不走空，我更不闲看到的信息多，顺便就也看了一下其他的数据节点。 另外，其实solr的schema在restful API也是可查的，这个可参考：https://cwiki.apache.org/confluence/display/solr/Schema+API]]></content>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr上手（三）]]></title>
    <url>%2F2015%2F08%2F06%2Fsolr%E4%B8%8A%E6%89%8B-3%2F</url>
    <content type="text"><![CDATA[前面说了一些简单的查询，这一章我们介绍一些稍微复杂些的 —— faceting，可以理解是切片，它是solr最牛逼的特性之一。就是针对查到的数据再进行一些聚合处理。看例子，明白的快些。 facet[分组查询]field facets除了查询返回的信息，还会包括根据facet.field进行分组的count数. range facets http://localhost:8983/solr/gettingstarted/select?q=*:*&amp;wt=json&amp;indent=on&amp;rows=0&amp;facet=true%20&amp;facet.range=price&amp;f.price.facet.range.start=0&amp;f.price.facet.range.end=600%20&amp;f.price.facet.range.gap=50&amp;facet.range.other=after 看到，这个例子是针对某个字段进行一些区间划分，然后计算每个区间的count数。 Pivot facets不太好理解，可以里结果多维分组count。 http://localhost:8983/solr/gettingstarted/select?q=*:*&amp;rows=0&amp;wt=json&amp;indent=on&amp;facet=on&amp;facet.pivot=cat,inStock 此例是先对cat字段分组，之后基于此，再对inStock进行分组计数。 参考：http://www.cnblogs.com/ontheroad_lee/p/3526385.htmlhttp://hongweiyi.com/2013/03/apache-solr-facet-introduction/https://cwiki.apache.org/confluence/display/solr/Faceting spatial[空间查询]solr是支持复杂的空间查询的，例如某个位置点的某个距离的圆的范围，按照距离排序等。我们可以到example/exampledocs/*.xml看一些示例数据，用这些数据来展示solr牛逼的空间能力。 启动测试solr bin/solr start -e techproducts 把数据导入进本地solr里 bin/post -c techproducts example/exampledocs/*.xml 空间查询和其他类型的查询也是可以结合在一起进行的，例如查距离San Francisco十公里内的ipod。 http://localhost:8983/solr/techproducts/browse?q=ipod&amp;pt=37.7752,-122.4232&amp;d=10&amp;sfield=store&amp;fq={!bbox}&amp;queryOpts=spatial&amp;queryOpts=spatial 这里我们使用了/browse界面进行查询【奇怪的是cloud和techproducts的/browse界面是不一样的】 详见https://cwiki.apache.org/confluence/display/solr/Spatial+Search 总结目前位置我们玩儿了： 以SolrCloud方式启动solr，两个节点，每个core两个shard，两个备份 向solr里批量导入数据 使用Solr admin UI进行一些查询 使用/browse接口的一些功能 其实有个脚本。。。我擦擦擦擦！才他么说！！！！1234567891011date ;bin/solr start -e cloud -noprompt ; open http://localhost:8983/solr ; bin/post -c gettingstarted docs/ ; open http://localhost:8983/solr/gettingstarted/browse ; bin/post -c gettingstarted example/exampledocs/*.xml ; bin/post -c gettingstarted example/exampledocs/books.json ; bin/post -c gettingstarted example/exampledocs/books.csv ; bin/post -c gettingstarted -d &quot;&lt;delete&gt;&lt;id&gt;SP2514N&lt;/id&gt;&lt;/delete&gt;&quot; ; bin/solr healthcheck -c gettingstarted ;date ; 清理工作 bin/solr stop -all ; rm -Rf example/cloud/]]></content>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Morphlines: 集成Hadoop ETL app的利器]]></title>
    <url>%2F2015%2F08%2F05%2FMorphlines-%E9%9B%86%E6%88%90Hadoop-ETL-app%E7%9A%84%E5%88%A9%E5%99%A8%2F</url>
    <content type="text"><![CDATA[Morphlines replaces Java programming with simple configuration steps, reducing the cost and effort of doing custom ETL . 使用Morphlines我们可以不用进行实际的繁琐编程而进行ETL工作，不用了解MR技术。一个morphline是指的一个配置文件，通过这个配置文件，我们可以从任意的数据源消费数据，然后通过一个的transformation chain，最终将处理结果送进某个Hadoop组件中去。相比编程java而言，我们只需要配置一下就可以了 。 Morphlines是一个可嵌入的java工具包，一个morphline是transformation command的一个内置container。command是morphline的Plugin，也就是加载数据、解析数据、转化数据或者其他针对某record进行某些操作。这里说的record是指的内存中的name-values对，它也可以有一些blob附件或者pojo附件。这个框架已经以一种直接而简单的方式将现有的功能与一些第三方系统进行了集成，而且扩展性较好。 Morphlines是服务于cloudera search的。它致力于flume和MR的ETL，前者是实时流，后者是批处理。 这篇文章，我们会通过一个例子讲明如果使用Morphlines怎样收集syslog的输出，并使之能在solr中进行查询。但是在此之前，我们还是先认识一下Morphlines的数据模型和数据处理模型。 数据处理模型Morphlines可以说是Unix pipeline的进化，它将要处理的数据泛化。一个morphline负责的工作：消费record(可以是flume events, HDFS文件，RDBMS 表，或者Apache avro 对象等)，将消费的record转为流，将这些流以管道的形式通过一些列的配置好的transformation，最后进入目的地，例如本例中的solr。 如上图所示，Flume Source读取syslog后将event弄到Morphlines Sink里去，然后Morphlines Sink把flume的event转为一个record，交给后面的一连串command进行处理。第一个command readLine抽取log行，第二个command grok使用正则抽取log行里的一部分关键信息。第三个command loadSolr就把上个command输出的结构化的信息索引进入solr了。在这个过程中，原始数据或者半结构化数据根据app的模型需求通过一些列转化成为结构化数据被存储。 Morphlines已经有了一些常用的转化和IO操作功能，但是我们仍然可以通过Plugin system加入一些新的转化和IO操作类型，也可以集成其他的第三方框架。 使用Morphlines可以很简单地实现ETL应用间的ETL逻辑的重用。 SDK是在运行时编译的。所有的command都在同一个线程内执行，也就是说pipline是通过某个方法调用另一个方法实现的。不存在队列，不存在线程间对象传递，不存在context切换，不存在command之间的序列化问题。 数据模型Morphlines操作海量数据。一个command可能对一个record进行transformation之后会产生零个或多个record。数据模型如下：一个record包含一些列的name-value对，所有record包含的filed应该是一样的(可以理解成command的产出是有一定的schema的)，而一个name可以对应多个value，一个value可以是任何java对象。这个灵活的数据模型其实和Lucene/solr的数据模型很像。 一个morphline可以处理结构化数据和二进制数据。还有一些可选的附件属性，这些附件属性可以结合Email那些东西进行理解。 这种通用的数据模型可以支持很多应用。典型应用就是Apache flume Morphlines Solr Sink。它把flume event的body放进了morphline record的_attachment_body字段中，event的header信息全部写成record的name-value对。另一个例子就是MapReduceIndexerTool的Mapper，将当前正在处理的HDFS文件的InputStream写入morphline record的_attachment_body字段中，把HDFS文件的元数据信息写成了record的name-value对。 用例command可以操作record的所有字段，包括解析、添加、删除、重命名、分割，甚至还能删除整个record。command有一个返回值来指示处理结果成功或失败。 例如一个多行的输入可以被一个command把每行切分为一个record输出，也就是产生多个record输出，然后另一个command在针对每一行使用正则切分为单词或短语(根据业务需求)，产生更多的record输出。 command可以对record做的操作包括：抽取、清理、转化、join、integrate、enrich、以各种方式装饰record。例如，可以使record与外部数据(RDBMS, KVDB,本地文件等)join。 command还可以消费record，然后把它们传送给某个外部系统。 嵌入host系统一个Morphline没有任何的持久化、分布式计算、可靠性、节点失败恢复等概念——它只是一个当前线程中的transformation chain而已。morphline没必要管理多线程、多节点，因为这些已经有MR,Flume，Storm等去做了。但是morphline还是会给command子树传递一些通知消息：BEGIN_TRANSACTION, COMMIT_TRANSACTION, ROLLBACK_TRANSACTION, SHUTDOWN. 语法配置文件使用HOCON格式，参考http://github.com/typesafehub/config/blob/master/HOCON.md 目前可用command详见http://kitesdk.org/docs/current/kite-morphlines/morphlinesReferenceGuide.html syslog案例需求：抽取syslog的内容，并索引到solr里。 Feb 4 10:46:14 syslog sshd[607]: listening on 0.0.0.0 port 22. 以上为log示例. 我们需要抽取的record如下： 1234567priority : 164timestamp : Feb 4 10:46:14hostname : syslogprogram : sshdpid : 607msg : listening on 0.0.0.0 port 22.message : Feb 4 10:46:14 syslog sshd[607]: listening on 0.0.0.0 port 22. morphline配置 morphline.conf内容如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283morphlines : [ &#123; # Name used to identify a morphline. E.g. used if there are multiple # morphlines in a morphline config file id : morphline1 # Import all morphline commands in these java packages and their # subpackages. Other commands that may be present on the classpath are # not visible to this morphline. importCommands : [&quot;com.cloudera.**&quot;, &quot;org.apache.solr.**&quot;] commands : [ &#123; # Parse input attachment and emit a record for each input line readLine &#123; charset : UTF-8 &#125; &#125; &#123; grok &#123; # Consume the output record of the previous command and pipe another # record downstream. # # A grok-dictionary is a config file that contains prefabricated # regular expressions that can be referred to by name. grok patterns # specify such a regex name, plus an optional output field name. # The syntax is %&#123;REGEX_NAME:OUTPUT_FIELD_NAME&#125; # The input line is expected in the &quot;message&quot; input field. dictionaryFiles : [src/test/resources/grok-dictionaries] expressions : &#123; message : &quot;&quot;&quot;%&#123;SYSLOGTIMESTAMP:timestamp&#125; %&#123;SYSLOGHOST:hostname&#125; %&#123;DATA:program&#125;(?:\[%&#123;POSINT:pid&#125;\])?: %&#123;GREEDYDATA:msg&#125;&quot;&quot;&quot; &#125; &#125; &#125; # Consume the output record of the previous command, convert # the timestamp, and pipe another record downstream. # # convert timestamp field to native Solr timestamp format # e.g. 2012-09-06T07:14:34Z to 2012-09-06T07:14:34.000Z &#123; convertTimestamp &#123; field : timestamp inputFormats : [&quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSS&apos;Z&apos;&quot;, &quot;&quot;MMM d HH:mm:ss&quot;] inputTimezone : America/Los_Angeles outputFormat : &quot;yyyy-MM-dd&apos;T&apos;HH:mm:ss.SSS&apos;Z&apos;&quot; outputTimezone : UTC &#125; &#125; # Consume the output record of the previous command, transform it # and pipe the record downstream. # # This command deletes record fields that are unknown to Solr # schema.xml. Recall that Solr throws an exception on any attempt to # load a document that contains a field that isn&apos;t specified in # schema.xml. &#123; sanitizeUnknownSolrFields &#123; # Location from which to fetch Solr schema solrLocator : &#123; collection : collection1 # Name of solr collection zkHost : &quot;127.0.0.1:2181/solr&quot; # ZooKeeper ensemble &#125; &#125; &#125; # log the record at INFO level to SLF4J &#123; logInfo &#123; format : &quot;output record: &#123;&#125;&quot;, args : [&quot;@&#123;&#125;&quot;] &#125; &#125; # load the record into a Solr server or MapReduce Reducer &#123; loadSolr &#123; solrLocator : &#123; collection : collection1 # Name of solr collection zkHost : &quot;127.0.0.1:2181/solr&quot; # ZooKeeper ensemble &#125; &#125; &#125; ] &#125;] 想要看到每个command处理的record的内容的话，可以配置log4j.properties： log4j.logger.com.cloudera.cdk.morphline=TRACE 其他文档 http://www.cloudera.com/content/cloudera/en/documentation/cloudera-search/v1-latest/Cloudera-Search-User-Guide/csug_morphline_example.html http://www.slideshare.net/cloudera/using-morphlines-for-onthefly-etl https://flume.apache.org/FlumeUserGuide.html#morphlinesolrsink]]></content>
      <tags>
        <tag>Morphlines</tag>
        <tag>ETL</tag>
        <tag>Flume</tag>
        <tag>Solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr上手（二）]]></title>
    <url>%2F2015%2F08%2F04%2Fsolr%E4%B8%8A%E6%89%8B-2%2F</url>
    <content type="text"><![CDATA[前面是提到了solr是支持很多中其他交流方式的。这章我们使用solr的REST API进行查询工作。 简单查询首先在主界面选择我们要查询的core，然后点击query，默认查询的条件是:，也就是任何字段任何值都是可以接收的，也就是全量数据了。 单击请求的实际URL部分，可以跳入这个请求：http://localhost:8983/solr/willCollection_shard1_replica1/select?q=*%3A*&amp;wt=json&amp;indent=true 基本查询 curl “http://localhost:8983/solr/gettingstarted/select?wt=json&amp;indent=true&amp;q=foundation&amp;fl=id“ 把×换成了foundation，也就是包含这个foundation关键字的doc，然后后面的fl指定了我们要的字段，所以结果如下图： curl “http://localhost:8983/solr/gettingstarted/select?wt=json&amp;indent=true&amp;q=cat:book“ 上面这个查询q后面成了cat:book，其实q可以是key:value的方式来查询满足条件的doc。 短语查询如果要匹配多个单词组合起来的短语，就需要使用双引号：q=”multiple terms here”,而且在URL中需要把空格替换为+加号,如下所示： curl “http://localhost:8983/solr/gettingstarted/select?wt=json&amp;indent=true&amp;q=\&quot;CAS+latency\“” 组合查询有时我们需要在一个查询中匹配多个单词或短语，有时也需要除去一些不需要的，这时就用到了组合查询。当我们想要某个匹配项的时候就使用+，不想要就使用-]]></content>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr上手（一）]]></title>
    <url>%2F2015%2F08%2F03%2Fsolr%E4%B8%8A%E6%89%8B-1%2F</url>
    <content type="text"><![CDATA[首发 1. 先下载咯2. 解压，执行命令 solr start -e cloud -noprompt解释一下这个命令的意思，启动就是启动咯。-e 其实是example的简写，也就是指定启动solr官网的某个例子，然后cloud，你懂得。。。后面那个暂时还不太清楚，目测是不需要交互神马的 1234567891011121314151617181920212223242526272829root@ubuntu:/opt/softwares/solr-5.2.1# ./bin/solr start -e cloud -nopromptWelcome to the SolrCloud example!Starting up 2 Solr nodes for your example SolrCloud cluster.Creating Solr home directory /opt/softwares/solr-5.2.1/example/cloud/node1/solrCloning Solr home directory /opt/softwares/solr-5.2.1/example/cloud/node1 into /opt/softwares/solr-5.2.1/example/cloud/node2Starting up SolrCloud node1 on port 8983 using command:solr start -cloud -s example/cloud/node1/solr -p 8983 -m 512m Waiting to see Solr listening on port 8983 [-] Started Solr server on port 8983 (pid=7677). Happy searching! Starting node2 on port 7574 using command:solr start -cloud -s example/cloud/node2/solr -p 7574 -z localhost:9983 -m 512m Waiting to see Solr listening on port 7574 [|] Started Solr server on port 7574 (pid=7932). Happy searching!Connecting to ZooKeeper at localhost:9983Uploading /opt/softwares/solr-5.2.1/server/solr/configsets/data_driven_schema_configs/conf for config gettingstarted to ZooKeeper at localhost:9983 打开页面 http://localhost:8983/solr，看到solr管理的主界面，可以通过点击Cloud标签看到：包含两个shard,每个shard有一个备份。 3. 使用docs文件夹索引数据现在solr虽然已经运行起来了，但是没有任何数据。为了测试方便，官网给带了一个快速插入数据的脚本bin/post，运行一下 bin/post -c gettingstarted docs/ 注意目前只支持shell，如果是window的话，需要运行【java -jar example/exampledocs/post.jar -h】 命令行参数的解释： -c gettingstarted: 要把index创建到哪个collection下 docs/: 一个基于solr安装路径下docs目录的相对路径 4. 使用xml/json/csv创建索引5. 更新细心的你一定发现即便重复插入同样的数据，我们查询的时候还是不会出现多个，这是因为例子中的schema.xml中指定了主键uniqueKey为id.当我们向solr插入一个已经存在的uniqueKey的时候，会××自动替换××掉原来的记录。我们可以实验一下，一直插入同样一条文档，观察一下solr实例的状况，Num Docs一直不变。Max Docs除了包含所有的doc，还包含已经逻辑删除，但是还没有物理删除的doc。 6. 删除可以指定文档的uniqueKey来删除指定文档，也可以找到满足某个条件的所有文档进行删除，只要向solr的更新URL发送一个删除命令就可以了。 bin/post -c gettingstarted -d “SP2514N“ 7. 搜索我们可以通过REST client，curl，wget，Postman以及本地的solr客户端进行solr查询。这章使用的命令行在实际程序里使用还是很少的。下一章我们会介绍使用REST对solr进行查询操作。 异常Expected mime type application/octet-stream but got text/html.针对不存在的索引插入数据时引发。]]></content>
      <tags>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[matlib 安装与卸载]]></title>
    <url>%2F2015%2F08%2F03%2Fmatlib-%E5%AE%89%E8%A3%85%E4%B8%8E%E5%8D%B8%E8%BD%BD%2F</url>
    <content type="text"><![CDATA[2004版本的那个主要就是兼容性问题，在win7下需要调整程序的兼容性，启动时候调整到Vista，卸载时调整到win 2000. 以下是2015a版本的安装，这个没有发现与win7的兼容性问题。转自http://tieba.baidu.com/p/3660998181?pn=1 这个文件是分压缩卷，解压其中一个压缩包就行了，解压到你想解压到的地方，解压出来是一个镜像文件（即ISO格式的），然后用虚拟机加载，（可以下个“软碟通”进行加载），加载后，在我的电脑（计算机）中，硬盘分区下面的可移动储存设备中找到CD驱动器，双击进去.进去后进行下面的操作： 1.用管理员权限运行里面的setup.exe，然后等一会，他需要安装某些东西。 2.选择不联网安装，我有密钥，下一步，选择我有密钥（即第一个），然后把 58691-35070-25550-28046-23042 这串数字复制粘贴进去，点下一步。 3.选择你想安装的东西，在前面打钩（他都给你打好了），不想装的把勾去掉，然后下一步，让他安装，这个安装比较漫长，等吧，可以干其他的事情去. 4.安装完成 5.这是时我们找到刚才计算机里硬盘分区下面的可移动储存设备中找到CD驱动器，进去后，找到crack这个文件夹，进去后复制 bin,java,toolbox这三个文件夹，然后找到我们刚才安装这个软件的安装目录，把这三个文件夹粘贴进去，后面会弹出是否覆盖替换，选择覆盖替换，全部都覆盖替换。 6.由于这个软件没有创建桌面快捷方式，开始菜单里也没有，所以我们去这个软件的安装目录，找到你刚才安装这个软件的地方，这个软件的 安装目录下找到bin这个文件夹，双击进去把matlab.exe执行文件发送到桌面快捷方式（右键，发送，桌面快捷方式）。 6.在桌面上双击这个matlab.exe图标，因为这是第一次打开，等一会，会弹出一个激活框，选择“不连接Internet激活”，下一步，选择“输入许可证文件的完整路径”，点击“浏览”找到刚才计算机里硬盘分区下面的可移动储存设备中找到CD驱动器，双击进去找到crack这个文件夹，找到lic_standalone.dat，选择这个文件，然后点下一步，激活成功！ 7.完成！！！ 尽情享受吧！！！ 别忘了把刚才那个计算机里硬盘分区下面的可移动储存设备中找到CD驱动器弹出！（右键，弹出） 养成用最高管理员运行软件的好习惯！！！ 下载地址：http://pan.baidu.com/s/1sjI603b 密码：zxd0]]></content>
      <tags>
        <tag>matlib</tag>
        <tag>2015a</tag>
        <tag>2004</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[akka in action 读书笔记（1）]]></title>
    <url>%2F2015%2F07%2F16%2Fakka-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[什么是AKKAakka： 并行处理请求 和service与client进行并发交互 异步响应 事件驱动编程模型 akka是被设计为一个工具而不是一个框架。框架是一个栈的某个独立的元素【比如UI层、web层等】，而akka可以用于这个栈中的任何部分，也可以为这些部分之间提供连接。akka的每个module是由jar包组成的，已经尽量较少了各个jar之间的耦合，甚至akka-actor除了scala标准包没有任何其他依赖。另外akka还提供了一些集成其他组件(例如camel, zeromq等)的module。下图是akka技术栈的主要组件，也解释了client如果省去一些细节麻烦： akka自身提供运行时环境，有一个很小的叫做play mini的内核，可以做的事情简直让你叹为观止。akka应用主要是由actor组成，基于actor编程模型，借鉴自函数式编程语言ERLANG和Fanton。 并发问题：考虑一个卖票的程序，如果一百万人同时向100个售票处买票，但是只有一个地方打印票。 面对以上问题，好多TicketingAgents都互相竞争，抢用打印机资源。解决方案有两个。对于一个基于线程的程序来说，最简单的实现就是为每个request直接fork一个线程，这简直就是灾难性的。就算是线程池的话，也可能每个请求都花比较长的时间而造成timeout，客户端还是会失败。另外，如果几个线程互相争抢并修改公用的资源同样会造成不良后果。 方案1：共享状态看下图： TicketingAgent取走一张票，然后告诉printing office打印。如果不能打印的话，就到3c，也就是卖完了。 这样，我们不必费力地控制Agent之间对Ticket的竞争。You can see, we didn’t have to go to great lengths to keep the Agents from blocking each other’s access to Tickets, and the result is also a bit clearer than what we would probably end up with if someone came in later and rewrote the selling sequence to make it concurrent. 使用线程的话有三个原因可能导致灾难性失败： thread starvation 当所有的线程都很忙或者都死锁了，无暇顾及新request的时候，都可能导致server关闭 race conditions 当某个共享资源被多个线程同时修改，可能会产生混乱，产生意想不到的结果 deadlock 即便你避免了以上所有问题，就这些避免的措施也够让人头疼的。锁控制的边界不是太大就是太小。使用过多的锁，其实实际上的并发就会少了特别特别多。使用的锁不够多，就可能遇到各种各样的bug。掌握一个好的平衡点是很困难的事情。 TicketAgent们都得互相等待，这些TicketAgent的客户就也得等待。TicketAgent A的客户需要等着TicketAgent B的某个客户，听上去有点儿奇怪，有些等待的时间并没有那么必要。 方案2：消息驱动我们使用AKKA使得事情变简单的根源在于我们避免使用共享状态。我们必须改变以下三个方面： event和ticket在售票处和打印室中间以不可变的消息状态传递 售票处的打印请求被打印室异步放入队列，不用等待打印方法完成或者接受确认 售票处和打印室相互之间不是直接使用相互的引用，而是记录联系对方的地址，他们之间通过相互传递message来进行通讯，这些message暂时被放置于一个mailbox中，后面会被按照放入的次序被处理掉。 当然，有些新的问题也会出现：TicketAgents在卖光自己的票以后需要有一种方式获取更多的票供给。这很重要，我们使用传递不可变message的方式完成了这项工作，我们不必担心app是否能够承担住激增的请求。下图是修改后的顺序图： 那么售票处具体怎么拿到票呢？打印室会给每个售票处发送取票信息，每个Event都会包含N多票和这个Event的状态信息。有很多种方式发送message给所有的售票处，这里我们使用的是让售票处之间相互传递的方式。售票处们也都知道各自的位置，会传达给目标(我们会在akka中常常见到chain的形式，从简单的传递工作状态到分布式的基于actor的责任链设计模式)如果一个售票处取了event中的属于自己的票后还有剩余的，就继续传递下去，代表还有其他售票处没有拿到应得的票。如果所有的售票处都看过这个event了，一段时间后仍然没人认领，那就让这些票过期。 基于消息传递的模式有两个好处，一个是不用管任何锁，另外售票处也不用干等着response，也就是打印的票完成，再去处理下一个，现在打印室打印完了会通知售票处取票。即便打印室器材坏了，售票处仍然可以继续卖票，只要把新的打印室的地址标成原来打印室的地址即可。 这种方式其实就是典型的AKKA实现。TicketingAgent和打印室都可以通过AKKA Actor实现。Actor之间不共享状态，只能通过不可变的message进行通信，通过actorRef来进行交流，就像上面提到的地址。这种模型满足了我们上面提到的三处改变。那么它简单到了哪里呢？ 不用管理锁，不用为保护共享状态殚精竭虑 不用担心死锁、race condition、或者thread starvation。 那么akka中就永远不会面临锁了么？当然不是的，只是你不用直接面对它而已。归根到底所有东西都是运行在线程和低级并发原始变量上的。Akka使用java的concurrent库来协调处理message，尽少地使用锁。 容灾基于message的方式可以允许在系统部分失灵的情况下保证系统继续有效运行。原因之一就是独立性，每个actor并不直接进行相互之间的交互。message是被发送到mailbox里的，爱谁处理谁处理，爱咋处理咋处理，sender发完就完事儿了。同样的事儿对于调用对象方法就行不通了。 独立性还带来了另外的好处。如果某个actor挂了，他的地址被另一个actor所替代，会怎样呢？只要原来的actor被新的actor正确替换了，那么所有这个地址上的message都会被归到新actor名下。错误会被包含在处理不恰当的老的actor中，而新actor只要处理之后产生的message就可以了。（在这种方式下，出错的老actor在抛出异常后就不能继续处理message了，它要自杀然后魂飞魄散）。类似Sping的这些框架在container/bean级别做了这些处理，而akka是在运行时提供的可替代性(这就是actor的let it crash思想):我们事先就做好了准备，一旦某个handler会因为某些莫须有的理由处理不好他们的任务，这时就把它的任务交给别人来避免灾难性的失败。 需要注意的是：这是双向的。即便是上层失败了，那么下层可以继续工作直到新的领导人来接替工作。不会有断裂的chain，开发者不用关心任何chain可能会断裂的可能性，当然也不用针对每个可能性采取相应措施。一个失败的actor被替换的过程不会影响到任何其他的合作者，见下图 以上这种方案是akka中对于容错的一种方案Restart strategies,其他方案包括：Resume, Stop以及Escalate.Akka提供了选择容错方案的方法。akka控制所有的actors处理message的方式，它知道所有actor的地址，如果有异常发生，就检查一下针对这种异常应该采取怎样的容错策略并执行之。 容错并不意味着对所有的错误全部都完全恢复。容错系统只是保证系统在部分失灵的情况下能够依然运行。不同的错误有不同的处理策略，有的错误需要重启部分系统，有的错误或许在检查点不能处理需要上级帮忙….后面我们可以关注一下Akka怎样通过Supervision来处理这些案例。 拿个简单的异常处理来说，一般的程序遇到异常肯定要停下正在处理的工作，处理这个异常或者抛给上级。对于线程组之间肯定是不能共享某个异常的，除非你自己写一个这样的底层机制。异常不会被传 到产生它的线程的线程组之外，也就是说我们必须找到在不同线程组之间交流异常的其他方式。一般来说如果一个线程遇到异常基本都是忽视继续执行或者直接停止。或许你可以在log里找到一些证据证明某个thread挂掉或者停止了，但是让系统的其他部分知道这些却是比较难了。当这些线程再分不到不同的机器上的时候，事情变得更加棘手。Akka提供了一种模型来处理错误，不管actor分布在多少机器。Akka不仅仅可以很好的处理并发，对于容错同样也很在行! scale up and outscale up :在一个server上运行更多的TicketingAgentscale out : 在更多的server上运行TicketingAgent scale out我们前面都是在一个JVM上运行的agent的例子。我们这次来看看怎样在多台机器上运行。message的传递是通过地址来进行的，我们只需要改变地址链接actor的方式就可以了。 akka可以给一个位于remote server上的remote actor发送message，然后再通过网络把处理结果传递回来。TicketingAgent并不知道它在命令一个远程的打印室。其实在同一个jvm运行的内存实例也是基本一致的。唯一不一样的地方就是怎样找到remote actor reference，这只要通过配置文件就可以了。也就是说，不用改变任何代码我们就可以吧scale out切换为scale up。在akka中灵活改变actor地址是非常常见的。remote actor, 集群化甚至测试工具，都会用到。 同样的情况，对于共享可变状态的情形可就惨了，最常见的方案就是把JVM里的状态都push到一个db里去。最大的威胁是在不得不更多地联系控制的需求下，这个db会变得越来越牛逼，而中间层就鸡肋了：所有的class都是DTO或者无状态无实际操作的东西。最让人头痛的就是当需要更多复杂的处理的时候，db同样也要面临这些改变，这将是灾难性的。还有，系统的透明度也很重要。我们希望能够很容易地追踪flow的执行进度，而不是使用精心制作的控制机制处理一片片交织在一起的复杂行为。 好了，贬低完了共享可变状态的不好，来看看我们牛逼的akka，你可以使用精心制作的控制系统来将错综复杂的处理逻辑不定转化为操作流。在某些架构中，message queue和web service会组合起来防止这些，但其实就是一种类似akka的模型。Akka帮我们省去了很多共享可变资源的麻烦，基于message模式，而且还可以基于同一段代码同时实现scale up和scale out。 scale up如果我们只想在一个机器上提升性能并scale up呢？假设我为机器新增加了几个cpu核数。在共享可变资源的情况下就是添加线程数了。但是我们都知道，锁会导致资源争夺，就是说工作的线程数总是少于总数，因为有些线程不得不相互等待。尽量少共享就是说尽量少玩儿锁，这就是message机制的目的。使用message机制可以使用更少的线程，只要优化一下处理流程和message分发，性能还胜过使用锁。每个线程都有一个用来存储运行时数据的栈。不同的操作系统的栈的大小是不一样的，比如Linux 64位的一般是256K。栈的大小是影响同时运行在某个server上线程数量的因素之一。在Linux 64系统上大约4096个线程会用掉1G的内存。 actor运行在一个叫做dispatcher的抽象概念上，dispatcher关注使用哪个线程模型，还有处理mailbox里的message。跟线程池处理方式很像，只是线程池只管调度，而Akka的dispatcher/mailbox结合体处理并传递message。我们可以通过配置层指定dispatch策略，不用修改任何代码。 actor是轻量的，因为他们运行于dispatcher之上，actor的数量不一定要跟线程数量成正比。actor比thread占用的空间要小得多，1G内存大约可以有270万个actor。针对不用的需求，有多种类型的dispatcher可选。actor之间可以共享同一个dispatcher，也可以自己有自己的。在调整性能的时候，可以通过配置灵活进行。 既然我们已经了解了基于message实现的并发、容错、扩展。是时候看一下akka里的具体组件了，还要了解这些组件是如果协同工作的。 Akka Actor 和 AkkaSystem前面已经讨论了几个akka的主要概念： actor的地址是提供一个方向 mailbox用来临时保存message和actor自身 我们下面讨论一下这些组件是怎样有机结合起来，解释一下akka底层的线程机制，并且解释一下akka怎样运行在这样的一套机制之下。Actor只是ActorSystem的一部分，ActorSystem负责提供一些组件，而且能够让actor之间相互能够找到彼此。 akka系统中需要保证： 不存在可变共享资源 传递的message不可变 异步发送message 当我们使用某个工具包或者框架的时候，这个东西肯定为我们提供了方便的直接可用的东西。像下图1.10中展示的一样，一个actor就是我们可以向他发送message的对象，我们并不关心message被怎样传递，谁来接收这些message。我们的代码只负责在接收到message之后进行对应的处理即可。还可以通过某个协议与其他人进行协作。 现在我们接触了actor，mailbox，address。akka是使用scala原生api构建的actor。scala API提供了actor trait给人继承来构建自己的actor。售票处就可以继承akka的Actor了，因为akka的Actor已经继承了scala的Actor trait，并且还有一些状态信息，还有一个内部队列用来存放票。address就是我们前面提到的ActorRef，也就是Actor reference的简写。这个售票处的ActorRef是给打印室发送message给售票处用的。在akka中，ActorRef有多种形式，只是不会显露出来而已。对于用户而言，只需要使用ActorRef的API就可以了。 下面我们通过图1.10走一个小小的例子，讲述一些我们的代码要做什么，ActorSystem会为我们做什么。如你所见：我们向售票处actor要一个ActorRef，获取之后，就用它来发送请求，也就是买票(1至5步)。ActorSystem没有向我们暴露任何细节：怎样处理的这个request，mailbox在哪儿，message是否已经被传递了等等。 当然，前面我们提到的好处依然存在：我们并没有阻塞，干巴巴等待request被处理完毕；request中不包含任何共享的状态信息；实际的消息处理过程也没有对任何公共资源使用锁；我们不必搭理任何并发问题。 那么我们怎样从ActorSystem中获取一个ActorRef呢？ActorPath！在上面的例子中，’poffice1’可以被换成’TicketingAgent2’。然后通过’TicketingAgent2’的ActorRef使用’../TicketingAgent3’还可以拿到它的兄弟节点actor的ActorRef。但是守卫actor只能是’user’。 akka中的监督机制就来源于这种层级制度。每个actor都自动是它的子节点的supervisor。当一个child挂了，parent就需要操心使用什么样的策略进行弥补。错误也可以被逐级向上抛出，直到上面的某个supervisor对这类错误感兴趣，处理掉。 akka不推荐直接通过constructor来创建actor，因为这样会破坏actor的层级。直接通过constructor创建的actor可以被直接访问并调用其方法，这也破坏了message传递过程中的并发保护。ActorSystem可以创建ActorRef，提供通用的方式来获取某个ActorRef，提供root actor来创建actor层级，并且把运行时其他组件与actor整合起来。 Actor的操作： CREATE每个actor都可以创建自己的子actor，actor的拓扑结构是动态的。 SENDactor之间可以异步相互发送message到彼此的mailbox里。 BECOMEactor的行为是可以动态改变的。每收到一条message，actor都可以以不同的方式处理下一条message，也就是切换它的行为，后面章节我们会遇到。 SUPERVISEactor监控自己的子actor，并且为这些孩子擦屁股。在第三章中，我们会见识到处理message和处理error的明确不同。 总结我们接触了ActorSystem和Actor实现的理论与实践，见识到了akka强大的能力。下一章你会发现这么厉害的能力竟然只需消耗一点点儿能量：你可以很快的运行akka。消息管理及并发的复杂性交给akka来搞定。综上： 基于消息传递让并发更加简单 并发的同时，我们还可以很简单地scale up 和scale out 我们可以任意扩展应用里的request和消息处理的组件 基于消息传递同样解决了容错 监管机制提供了一个并发并且容错的模型 akka让我们的代码不用特别复杂，但是拥有强大的力量 下一章我们会启动运行我们的第一个akka应用。]]></content>
      <tags>
        <tag>akka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[heroku上部署自己的akka项目]]></title>
    <url>%2F2015%2F07%2F16%2Fheroku%E4%B8%8A%E9%83%A8%E7%BD%B2%E8%87%AA%E5%B7%B1%E7%9A%84akka%E9%A1%B9%E7%9B%AE%2F</url>
    <content type="text"><![CDATA[heroku这个东西貌似是可以在云端发布scala项目的，我是在akka-in-action这本书遇到。既然玩儿了，就顺便记录一下。 1. 注册到heroku.com使用邮箱注册一个免费用户【注意：163邮箱被屏蔽了，我用的阿里的】 2. 安装工具在本地Ubuntu上安装heroku toolbelt, 可参考https://toolbelt.heroku.com/debian wget -O- https://toolbelt.heroku.com/install-ubuntu.sh | sh 3. 本地登录heroku： heroku login 4. 在heroku上建立一个app1234root@ubuntu:~/gitLearning/akka-in-action/chapter2# heroku createCreating floating-woodland-3032... done, stack is cedar-14https://floating-woodland-3032.herokuapp.com/ | https://git.heroku.com/floating-woodland-3032.gitGit remote heroku added 上面执行的heroku create命令会同时在本地git配置里以heroku为名添加一个git remote。 5. 修改参数我们需要修改几个参数，这样heroku才知道怎样构建我们的项目。 project/build.properties 【经验证，这个可以不用改】 sbt.version=0.12.2 project/plugins.sbt【经验证，这个可以不用改】 resolvers += Classpaths.typesafeResolveraddSbtPlugin(“com.eed3si9n” % “sbt-assembly” % “0.8.7”)addSbtPlugin(“com.typesafe.startscript” % “xsbt-start-script-plugin” % “0.5.3”) 以上包含了我们目前使用的所有的Plugin【貌似上面提到两个文件只是为了说明我们需要这几个依赖，但是并不严格需要弄成上面的那种】。startscript是为我们在Heroku上运行项目创建一个启动脚本用的。我们还需要项目根目录下的Procfile文件，这个文件会告诉Heroku我们的app应该运行在一个Web Dyno上。可以看一下这个文件里的内容： web: target/start com.goticks.Main这是指定了Heroku应该在启动脚本上添加的主类参数。 6. 本地项目运行测试 sbt clean compile stage 123456789101112131415161718root@ubuntu:~/gitLearning/akka-in-action/chapter2# sbt clean compile stage[info] Loading project definition from /root/gitLearning/akka-in-action/chapter2/project[info] Set current project to goticks (in build file:/root/gitLearning/akka-in-action/chapter2/)[success] Total time: 1 s, completed Jul 15, 2015 1:15:05 AM[info] Updating &#123;file:/root/gitLearning/akka-in-action/chapter2/&#125;chapter2...[info] Resolving jline#jline;2.12 ...[info] Done updating.[warn] Scala version was updated by one of library dependencies:[warn] * org.scala-lang:scala-library:(2.11.2, 2.11.1, 2.11.0) -&gt; 2.11.5[warn] To force scalaVersion, add the following:[warn] ivyScala := ivyScala.value map &#123; _.copy(overrideScalaVersion = true) &#125;[warn] Run &apos;evicted&apos; to see detailed eviction warnings[info] Compiling 4 Scala sources to /root/gitLearning/akka-in-action/chapter2/target/scala-2.11/classes...[warn] there was one feature warning; re-run with -feature for details[warn] one warning found[success] Total time: 41 s, completed Jul 15, 2015 1:15:47 AM[info] Wrote start script for mainClass := Some(com.goticks.Main) to /root/gitLearning/akka-in-action/chapter2/target/start[success] Total time: 1 s, completed Jul 15, 2015 1:15:47 AM 这个命令是后面再heroku上会运行的，它会编译项目并创建target/start启动脚本。然后可以使用heroku的命令本地运行Procfile文件： 7. 本地启动并测试使用heroku的工具foreman启动一下项目 foreman start 1234root@ubuntu:~/gitLearning/akka-in-action/chapter2# foreman start01:20:02 web.1 | started with pid 703001:20:10 web.1 | REST interface bound to /0:0:0:0:0:0:0:0:500001:20:10 web.1 | [INFO] [07/15/2015 01:20:10.437] [goticks-akka.actor.default-dispatcher-3] [akka://goticks/user/IO-HTTP/listener-0] Bound to /0.0.0.0:5000 到另一个session里测试：12345678root@ubuntu:~# http GET localhost:5000/eventsHTTP/1.1 200 OKContent-Length: 2Content-Type: application/json; charset=UTF-8Date: Wed, 15 Jul 2015 08:20:20 GMTServer: GoTicks.com REST API[] 8. 部署到heroku git push keroku master 在提交了所有的本地变化后将本地项目push到git仓库的master分支，heroku会hook到这个git进程的执行，并且认出这是一个scala app。它会在云端下载所有依赖，编译代码，之后启动这个app。输出如下:123456789root@ubuntu:~/gitLearning/akka-in-action/chapter2# git push keroku masterfatal: &apos;keroku&apos; does not appear to be a git repositoryfatal: Could not read from remote repository.Please make sure you have the correct access rightsand the repository exists.root@ubuntu:~/gitLearning/akka-in-action/chapter2# git remoteherokuorigin …..报错了。查看remote，发现已经有了heroku这个remote了。参考https://devcenter.heroku.com/articles/git，执行下列命令：使用 git remote -v, 查看当前项目下的remote url，确定我们的项目名称12345root@ubuntu:~/gitLearning/akka-in-action/chapter2# git remote -vheroku https://git.heroku.com/floating-woodland-3032.git (fetch)heroku https://git.heroku.com/floating-woodland-3032.git (push)origin https://github.com/RayRoestenburg/akka-in-action.git (fetch)origin https://github.com/RayRoestenburg/akka-in-action.git (push) heroku git:remote -a floating-woodland-3032 123root@ubuntu:~/gitLearning/akka-in-action/chapter2# heroku git:remote -a floating-woodland-3032Installing plugin heroku-git... doneset git remote heroku to https://git.heroku.com/floating-woodland-3032.git 添加remote参考自: https://devcenter.heroku.com/articles/git【经后面证实，如果不**重命名**，可以不管这个，因为git remote -v 里已经有了。重命名使用-r other_remote_name】然后再次执行push: git push heroku master1234567891011121314151617181920212223242526272829303132333435363738Counting objects: 1776, done.Delta compression using up to 2 threads.Compressing objects: 100% (791/791), done.Writing objects: 100% (1776/1776), 298.54 KiB | 0 bytes/s, done.Total 1776 (delta 564), reused 1768 (delta 561)remote: Compressing source files... done.remote: Building source:remote: remote: -----&gt; Play! app detectedremote: -----&gt; Installing OpenJDK 1.6... doneremote: -----&gt; WARNING: Play! version not specified in dependencies.yml. Default version: 1.3.1 being used....remote: -----&gt; Installing Play! 1.3.1.....remote: -----&gt; doneremote: -----&gt; Building Play! application...remote: ~ _ _ remote: ~ _ __ | | __ _ _ _| |remote: ~ | &apos;_ \| |/ _&apos; | || |_|remote: ~ | __/|_|\____|\__ (_)remote: ~ |_| |__/ remote: ~remote: ~ play! 1.3.1, https://www.playframework.comremote: ~remote: 1.3.1remote: Building Play! application at directory ./remote: Resolving dependencies: .play/play dependencies ./ --forProd --forceCopy --silent -Duser.home=/tmp/build_4bb492e51a64b714c9d6757c67b0ccf2 2&gt;&amp;1remote: ~ !! /tmp/build_4bb492e51a64b714c9d6757c67b0ccf2/conf/dependencies.yml does not existremote: ! Failed to build Play! applicationremote: ! Cleared Play! framework from cacheremote: remote: ! Push rejected, failed to compile Play! appremote: remote: Verifying deploy....remote: remote: ! Push rejected to floating-woodland-3032.remote: To https://git.heroku.com/floating-woodland-3032.git ! [remote rejected] master -&gt; master (pre-receive hook declined)error: failed to push some refs to &apos;https://git.heroku.com/floating-woodland-3032.git&apos; 主要在于这个远程的hook拒绝了我的提交。我擦，简直不能忍！！！Google~~~因为没有成功build play，原因是没有找到play的依赖配置文件。看了几个解说的，貌似是因为当前目录是一个git repository的子目录。 下载安装一个git插件 git clone https://github.com/apenwarr/git-subtree.git 切换到最近的分支，执行 sh install.sh 到git repository目录，也就是本地的git主目录下，我的机器上就是~/gitLearning/akka-in-action，执行： git subtree push –prefix “chapter2” heroku master 这个prefix是我们要push的项目相对git repository的路径。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859git push using: heroku masterCounting objects: 201, done.Delta compression using up to 2 threads.Compressing objects: 100% (119/119), done.Writing objects: 100% (201/201), 22.30 KiB | 0 bytes/s, done.Total 201 (delta 54), reused 155 (delta 38)remote: Compressing source files... done.remote: Building source:remote: remote: -----&gt; Scala app detectedremote: -----&gt; Installing OpenJDK 1.8... doneremote: -----&gt; Running: sbt compile stageremote: Downloading sbt launcher for 0.13.8:remote: From http://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.8/sbt-launch.jarremote: To /tmp/scala_buildpack_build_dir/.sbt_home/launchers/0.13.8/sbt-launch.jarremote: Getting org.scala-sbt sbt 0.13.8 ...remote: downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.8/jars/sbt.jar ...remote: [SUCCESSFUL ] org.scala-sbt#sbt;0.13.8!sbt.jar (321ms)remote: downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar ...remote: [SUCCESSFUL ] org.scala-lang#scala-library;2.10.4!scala-library.jar (2957ms)remote: downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/main/0.13.8/jars/main.jar ................remote: [SUCCESSFUL ] org.scala-lang#jline;2.10.4!jline.jar (73ms)remote: downloading https://repo1.maven.org/maven2/org/fusesource/jansi/jansi/1.4/jansi-1.4.jar ...remote: [SUCCESSFUL ] org.fusesource.jansi#jansi;1.4!jansi.jar (49ms)remote: :: retrieving :: org.scala-sbt#boot-scalaremote: confs: [default]remote: 5 artifacts copied, 0 already retrieved (24459kB/42ms)remote: [info] Loading global plugins from /tmp/scala_buildpack_build_dir/.sbt_home/pluginsremote: [info] Updating &#123;file:/tmp/scala_buildpack_build_dir/.sbt_home/plugins/&#125;global-plugins...remote: [info] Resolving org.scala-lang#scala-library;2.10.4 .................remote: [info] Resolving org.fusesource.jansi#jansi;1.4 ...remote: [info] Done updating.remote: [info] Compiling 1 Scala source to /tmp/scala_buildpack_build_dir/.sbt_home/plugins/target/scala-2.10/sbt-0.13/classes...remote: [error] [/tmp/scala_buildpack_build_dir/project/plugins.sbt]:3: &apos;;&apos; expected but &apos;#&apos; found.remote: [error] [/tmp/scala_buildpack_build_dir/project/plugins.sbt]:11: &apos;;&apos; expected but &apos;#&apos; found.remote: Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? remote: ! ERROR: Failed to run sbt!remote: We&apos;re sorry this build is failing! If you can&apos;t find the issue in applicationremote: code, please submit a ticket so we can help: https://help.heroku.comremote: You can also try reverting to the previous version of the buildpack by running:remote: $ heroku buildpacks:set https://github.com/heroku/heroku-buildpack-scala#previous-versionremote: remote: Thanks,remote: Herokuremote: remote: remote: ! Push rejected, failed to compile Scala appremote: remote: Verifying deploy....remote: remote: ! Push rejected to floating-woodland-3032.remote: To https://git.heroku.com/floating-woodland-3032.git ! [remote rejected] 9bfe06799b9975c15c426ab17618967b7b8e65b0 -&gt; master (pre-receive hook declined)error: failed to push some refs to &apos;https://git.heroku.com/floating-woodland-3032.git&apos; 可以看到已经成功进行项目的build了，但还是报错了。。。。从输出来看原因是project/plugins.sbt里的#应该被替换成;，好吧，我是sbt新手。。。修改后重新push123456789101112131415161718192021222324252627282930313233343536373839404142434445464748root@ubuntu:~/gitLearning/akka-in-action# git subtree push --prefix &quot;chapter2&quot; heroku mastergit push using: heroku masterCounting objects: 205, done.Delta compression using up to 2 threads.Compressing objects: 100% (123/123), done.Writing objects: 100% (205/205), 22.61 KiB | 0 bytes/s, done.Total 205 (delta 56), reused 155 (delta 38)remote: Compressing source files... done.remote: Building source:remote: remote: -----&gt; Scala app detectedremote: -----&gt; Installing OpenJDK 1.8... doneremote: -----&gt; Running: sbt compile stageremote: Downloading sbt launcher for 0.13.8:remote: From http://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt-launch/0.13.8/sbt-launch.jarremote: To /tmp/scala_buildpack_build_dir/.sbt_home/launchers/0.13.8/sbt-launch.jarremote: Getting org.scala-sbt sbt 0.13.8 ...remote: downloading https://repo.typesafe.com/typesafe/ivy-releases/org.scala-sbt/sbt/0.13.8/jars/sbt.jar .............remote: [info] downloading https://repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.2/scala-parser-combinators_2.11-1.0.2.jar ...remote: [info] [SUCCESSFUL ] org.scala-lang.modules#scala-parser-combinators_2.11;1.0.2!scala-parser-combinators_2.11.jar(bundle) (185ms)remote: [info] downloading https://repo1.maven.org/maven2/jline/jline/2.12/jline-2.12.jar ...remote: [info] [SUCCESSFUL ] jline#jline;2.12!jline.jar (99ms)remote: [info] Done updating.remote: [warn] Scala version was updated by one of library dependencies:remote: [warn] * org.scala-lang:scala-library:(2.11.2, 2.11.1, 2.11.0) -&gt; 2.11.5remote: [warn] To force scalaVersion, add the following:remote: [warn] ivyScala := ivyScala.value map &#123; _.copy(overrideScalaVersion = true) &#125;remote: [warn] Run &apos;evicted&apos; to see detailed eviction warningsremote: [info] Compiling 4 Scala sources to /tmp/scala_buildpack_build_dir/target/scala-2.11/classes...remote: [info] &apos;compiler-interface&apos; not yet compiled for Scala 2.11.2. Compiling...remote: [info] Compilation completed in 33.95 sremote: [warn] there was one feature warning; re-run with -feature for detailsremote: [warn] one warning foundremote: [success] Total time: 87 s, completed Jul 15, 2015 10:04:22 AMremote: [info] Wrote start script for mainClass := Some(com.goticks.Main) to /tmp/scala_buildpack_build_dir/target/startremote: [success] Total time: 1 s, completed Jul 15, 2015 10:04:22 AMremote: -----&gt; Discovering process typesremote: Procfile declares types -&gt; webremote: remote: -----&gt; Compressing... done, 165.8MBremote: -----&gt; Launching... done, v5remote: https://floating-woodland-3032.herokuapp.com/ deployed to Herokuremote: remote: Verifying deploy.... done.To https://git.heroku.com/floating-woodland-3032.git * [new branch] 75151c92e9b25df8462fc0422b0345cf280f01ce -&gt; master 嘻嘻~~ 大功告成！！！现在我们的项目已经发布到https://floating-woodland-3032.herokuapp.com。 9. 测试我们可以使用httpie或者浏览器来测试一下1234567891011121314151617181920212223242526root@ubuntu:~# http PUT https://floating-woodland-3032.herokuapp.com/events event=RHCP nrOfTickets:=10HTTP/1.1 200 OKConnection: keep-aliveContent-Length: 2Content-Type: text/plain; charset=UTF-8Date: Wed, 15 Jul 2015 10:09:06 GMTServer: GoTicks.com REST APIVia: 1.1 vegurOKroot@ubuntu:~# http GET https://floating-woodland-3032.herokuapp.com/events event=RHCP nrOfTickets:=10HTTP/1.1 200 OKConnection: keep-aliveContent-Length: 44Content-Type: application/json; charset=UTF-8Date: Wed, 15 Jul 2015 10:09:21 GMTServer: GoTicks.com REST APIVia: 1.1 vegur[ &#123; &quot;event&quot;: &quot;RHCP&quot;, &quot;nrOfTickets&quot;: 10 &#125;] Plus : 二次提交测试因为还不是很熟悉heroku，自己很纳闷，如果我们远程的APP在运行状态，能否直接提交新的commit，它会自己重启吗？稍微修改一下项目后，重新push上去报错123456789root@ubuntu:~/gitLearning/akka-in-action# git subtree push --prefix &quot;chapter2&quot; heroku mastergit push using: heroku masterTo https://git.heroku.com/floating-woodland-3032.git ! [rejected] 855562ff41e936b6769cdf808d0f798f1dbf371b -&gt; master (non-fast-forward)error: failed to push some refs to &apos;https://git.heroku.com/floating-woodland-3032.git&apos;hint: Updates were rejected because a pushed branch tip is behind its remotehint: counterpart. Check out this branch and integrate the remote changeshint: (e.g. &apos;git pull ...&apos;) before pushing again.hint: See the &apos;Note about fast-forwards&apos; in &apos;git push --help&apos; for details. 原来是因为我刚才修改之前reset了一次，造成了commit滞后。那就先pull下来，解决冲突之后，commit再push。结果还是报同样的错误。。。。无奈，git reflog后再reset到线上的commit，重新修改后commit + push。123root@ubuntu:~/gitLearning/akka-in-action# git subtree push --prefix &quot;chapter2&quot; heroku mastergit push using: heroku masterEverything up-to-date heroku这个东西是完全免费的吗？怎样关闭这个东西？它引起了我的好奇心~~~简单看了一下help，有点儿类似docker的感觉。 PlusPlus: 项目关闭12345root@ubuntu:~/gitLearning/akka-in-action# heroku ps:stop web.1Stopping web.1 dyno... doneroot@ubuntu:~/gitLearning/akka-in-action# heroku ps=== web (Free): `target/start com.goticks.Main`web.1: idle 2015/07/15 03:46:31 (~ 8s ago) 关闭了自己又起来了~靠~！！！！退出登录，同样失败。12root@ubuntu:~/gitLearning/akka-in-action# heroku auth:logoutLocal credentials cleared. 解决方案：把web节点扩展到0个。。。。。竟然是这种方式，我真是醉了。。。这时再次访问就是503了。heroku ps没有任何进程。12root@ubuntu:~/gitLearning/akka-in-action# heroku ps:scale web=0Scaling dynos... done, now running web at 0:Free. 对于git子目录提交的解决，参考自： https://gitlab.com/gitlab-com/support-forum/issues/40 http://stackoverflow.com/questions/14286266/deploy-play-app-on-heroku-from-a-subdirectory-of-git-repo http://stackoverflow.com/questions/17148784/play-1-2-5-application-deployment-on-heroku-oops-conf-routes-or-conf-applicati]]></content>
      <tags>
        <tag>akka</tag>
        <tag>heroku</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume 源码阅读笔记（5）]]></title>
    <url>%2F2015%2F07%2F12%2Fflume-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%885%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前面分析了source的运行机制与过程，其实sink和source是很像的。 1. 入口123456789for (Entry&lt;String, SinkRunner&gt; entry : materializedConfiguration.getSinkRunners() .entrySet()) &#123; try&#123; supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); &#125; catch (Exception e) &#123; logger.error("Error while starting &#123;&#125;", entry.getValue(), e); &#125;&#125; 哈哈，经过上一节的流程，你是不是跟我一样信心满满啊~~ 嘻嘻—— SinkRunner 2. SinkRunner同SourceRunner的身份地位是一样滴，可以理解为驱动器加管理者的角色，当channel里有event的时候就尝试消费之。不同于source的是，sink都是类似于PollableSource的这种，也就是需要自己对channel里的event进行抽取消费，因为channel里的东西不会自己sink，也不知道要sink到何处去。 runner : PollingRunner //我还需要说什么吗 runnerThread : Thread lifecycleState : LifecycleState policy : SinkProcessor 入口方法start() 123456789101112131415161718public void start() &#123; SinkProcessor policy = getPolicy(); policy.start(); runner = new PollingRunner(); runner.policy = policy; runner.counterGroup = counterGroup; runner.shouldStop = new AtomicBoolean(); runnerThread = new Thread(runner); runnerThread.setName("SinkRunner-PollingRunner-" + policy.getClass().getSimpleName()); runnerThread.start(); lifecycleState = LifecycleState.START;&#125; 好熟悉，完全就是PollableSourceRunner的同样思路。虽然猜到了结果，但还是要求证一下。上这个内部类PollingRunner：1234567891011121314public void run() &#123; .... while (!shouldStop.get()) &#123; try &#123; if (policy.process().equals(Sink.Status.BACKOFF)) &#123; // .... &#125; else &#123; counterGroup.set("runner.backoffs.consecutive", 0L); &#125; &#125; catch (InterruptedException e) &#123; ..... &#125; &#125;&#125; 好吧，还是稍微有一点点区别，我列举一下这两个PollingRunner的成员。PollableSourceRunner： source : PollableSource //内置getProcessor()接口 shouldStop : AtomicBoolean counterGroup : CounterGroup SinkRunner: policy : SinkProcessor shouldStop : AtomicBoolean counterGroup : CounterGroup 差别就在policy和source上了，而这两者又有同样的接口API process方法来对channel里的event进行各自的处理。前面我们也分析过了source是通过channelProcessor进行实际的针对channel的操作的。 这一节营养实在太少。顺便说一声CounterGroup吧，内置一个HashMap，做各种计数与指标统计的工作，主要当然是针对event和各个组件。]]></content>
      <tags>
        <tag>源码</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sbt使用遇到的问题(一)——repository]]></title>
    <url>%2F2015%2F07%2F12%2Fsbt%E4%BD%BF%E7%94%A8%E9%81%87%E5%88%B0%E7%9A%84%E9%97%AE%E9%A2%98-%E4%B8%80-%E2%80%94%E2%80%94repository%2F</url>
    <content type="text"><![CDATA[你懂得，由于当前国内的网络环境，前面使用了oschina的镜像作为sbt的repository。 [repositories]localosc: http://maven.oschina.net/content/groups/public/oschina-ivy: http://maven.oschina.net/content/groups/public/, [organization]/[module]/(scala[scalaVersion]/)(sbt[sbtVersion]/)[revision]/[type]s/artifact.[ext]typesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala[scalaVersion]/)(sbt[sbtVersion]/)[revision]/[type]s/artifact.[ext], bootOnly#sonatype-oss-releases#maven-central#sonatype-oss-snapshots 配置了以上文件repositories后，把它放到~/.sbt/下就可以了。也可以自己制定配置文件的地址。具体可参考：http://www.scala-sbt.org/0.13/docs/Proxy-Repositories.html 但是最近一段时间貌似oschina不再提供maven镜像库的服务了，而最近自己又想学习akka，尝试了好多次都是下面的输出：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990compile[info] Updating &#123;file:/root/gitLearning/akka-in-action/chapter-channels/&#125;channels...[info] Resolving org.scala-lang#scala-library;2.11.6 ...[warn] module not found: org.scala-lang#scala-library;2.11.6[warn] ==== local: tried[warn] /root/.ivy2/local/org.scala-lang/scala-library/2.11.6/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-actor_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-actor_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-actor_2.11/2.3.10/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-slf4j_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-slf4j_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-slf4j_2.11/2.3.10/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-remote_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-remote_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-remote_2.11/2.3.10/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-multi-node-testkit_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-multi-node-testkit_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-multi-node-testkit_2.11/2.3.10/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-contrib_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-contrib_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-contrib_2.11/2.3.10/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-remote-tests_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-remote-tests_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-remote-tests_2.11/2.3.10/ivys/ivy.xml[info] Resolving com.typesafe.akka#akka-testkit_2.11;2.3.10 ...[warn] module not found: com.typesafe.akka#akka-testkit_2.11;2.3.10[warn] ==== local: tried[warn] /root/.ivy2/local/com.typesafe.akka/akka-testkit_2.11/2.3.10/ivys/ivy.xml[info] Resolving org.scalatest#scalatest_2.11;2.2.0 ...[warn] module not found: org.scalatest#scalatest_2.11;2.2.0[warn] ==== local: tried[warn] /root/.ivy2/local/org.scalatest/scalatest_2.11/2.2.0/ivys/ivy.xml[info] Resolving org.scala-lang#scala-compiler;2.11.6 ...[warn] module not found: org.scala-lang#scala-compiler;2.11.6[warn] ==== local: tried[warn] /root/.ivy2/local/org.scala-lang/scala-compiler/2.11.6/ivys/ivy.xml[warn] ::::::::::::::::::::::::::::::::::::::::::::::[warn] :: UNRESOLVED DEPENDENCIES ::[warn] ::::::::::::::::::::::::::::::::::::::::::::::[warn] :: org.scala-lang#scala-library;2.11.6: not found[warn] :: com.typesafe.akka#akka-actor_2.11;2.3.10: not found[warn] :: com.typesafe.akka#akka-slf4j_2.11;2.3.10: not found[warn] :: com.typesafe.akka#akka-remote_2.11;2.3.10: not found[warn] :: com.typesafe.akka#akka-multi-node-testkit_2.11;2.3.10: not found[warn] :: com.typesafe.akka#akka-contrib_2.11;2.3.10: not found[warn] :: com.typesafe.akka#akka-remote-tests_2.11;2.3.10: not found[warn] :: com.typesafe.akka#akka-testkit_2.11;2.3.10: not found[warn] :: org.scalatest#scalatest_2.11;2.2.0: not found[warn] :: org.scala-lang#scala-compiler;2.11.6: not found[warn] ::::::::::::::::::::::::::::::::::::::::::::::[warn] [warn] Note: Unresolved dependencies path:[warn] com.typesafe.akka:akka-contrib_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] com.typesafe.akka:akka-remote-tests_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] com.typesafe.akka:akka-remote_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] com.typesafe.akka:akka-actor_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] com.typesafe.akka:akka-testkit_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] org.scala-lang:scala-library:2.11.6 ((sbt.Classpaths) Defaults.scala#L1203)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] com.typesafe.akka:akka-slf4j_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] com.typesafe.akka:akka-multi-node-testkit_2.11:2.3.10 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] org.scalatest:scalatest_2.11:2.2.0 (/root/gitLearning/akka-in-action/chapter-channels/build.sbt#L14)[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[warn] org.scala-lang:scala-compiler:2.11.6[warn] +- manning:akka-sample-multi-node-scala_2.11:0.1-SNAPSHOT[trace] Stack trace suppressed: run last *:update for the full output.[error] (*:update) sbt.ResolveException: unresolved dependency: org.scala-lang#scala-library;2.11.6: not found[error] unresolved dependency: com.typesafe.akka#akka-actor_2.11;2.3.10: not found[error] unresolved dependency: com.typesafe.akka#akka-slf4j_2.11;2.3.10: not found[error] unresolved dependency: com.typesafe.akka#akka-remote_2.11;2.3.10: not found[error] unresolved dependency: com.typesafe.akka#akka-multi-node-testkit_2.11;2.3.10: not found[error] unresolved dependency: com.typesafe.akka#akka-contrib_2.11;2.3.10: not found[error] unresolved dependency: com.typesafe.akka#akka-remote-tests_2.11;2.3.10: not found[error] unresolved dependency: com.typesafe.akka#akka-testkit_2.11;2.3.10: not found[error] unresolved dependency: org.scalatest#scalatest_2.11;2.2.0: not found[error] unresolved dependency: org.scala-lang#scala-compiler;2.11.6: not found[error] Total time: 0 s, completed Jul 11, 2015 5:13:57 PM 今儿灵机一动，想到了会不会是oschina镜像库的问题(尼玛，都忘记了以前自己跟sbt配置过oschina的镜像了)。打开配置文件~~~ 果然如此。。。这里感觉sbt稍微有点儿不好了，看日志也没看到它去哪个网址下载库的，maven库路径是没有输出的，如果没有经验的人看这些东西压根儿看不出啥来。 解决方式： 把原来配置的repositories文件重命名bak一下，重新到项目路径下启动sbt，再次compile：12345678910111213141516compile[info] Updating &#123;file:/root/gitLearning/akka-in-action/chapter-channels/&#125;channels...[info] Resolving org.sonatype.oss#oss-parent;7 ...[info] downloading https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.6/scala-library-2.11.6.jar ...[info] [SUCCESSFUL ] org.scala-lang#scala-library;2.11.6!scala-library.jar (142453ms)[info] downloading https://repo1.maven.org/maven2/com/typesafe/akka/akka-actor_2.11/2.3.10/akka-actor_2.11-2.3.10.jar ...[info] [SUCCESSFUL ] com.typesafe.akka#akka-actor_2.11;2.3.10!akka-actor_2.11.jar (67998ms)[info] downloading https://repo1.maven.org/maven2/com/typesafe/akka/akka-slf4j_2.11/2.3.10/akka-slf4j_2.11-2.3.10.jar ...[info] [SUCCESSFUL ] com.typesafe.akka#akka-slf4j_2.11;2.3.10!akka-slf4j_2.11.jar (1201ms)[info] downloading https://repo1.maven.org/maven2/com/typesafe/akka/akka-remote_2.11/2.3.10/akka-remote_2.11-2.3.10.jar .......[info] Done updating.[info] Compiling 3 Scala sources to /root/gitLearning/akka-in-action/chapter-channels/target/scala-2.11/classes...[info] &apos;compiler-interface&apos; not yet compiled for Scala 2.11.6. Compiling...[info] Compilation completed in 20.627 s[success] Total time: 1518 s, completed Jul 11, 2015 5:41:41 PM 额，好吧。成功的时候才会显示地址~成功了再看这个有毛线作用啊~]]></content>
      <tags>
        <tag>scala</tag>
        <tag>sbt</tag>
        <tag>repository</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume-源码阅读笔记（4）]]></title>
    <url>%2F2015%2F07%2F10%2Fflume-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%884%EF%BC%89%2F</url>
    <content type="text"><![CDATA[这一章详细分析一下source的运行方式 1. 入口前面我们提到了source的启动入口为：1234567891011for (Entry&lt;String, SourceRunner&gt; entry : materializedConfiguration .getSourceRunners().entrySet()) &#123; try&#123; logger.info("Starting Source " + entry.getKey()); // 下面这行就会以每3秒一次的频率调用SourceRunner supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); &#125; catch (Exception e) &#123; logger.error("Error while starting &#123;&#125;", entry.getValue(), e); &#125; &#125; 注意，上面的value不是Source，而是SourceRunner，那么我们的中心就围绕它展开了。 2. SourceRunner简而言之就是source的驱动器和控制器，对source进行了一层包裹。根据source的不同分为PollableSourceRunner和EventDrivenSourceRunner。 先看一下SourceRunner的元素： source : Source forSource(Source) 静态工厂方法，根据传入的source的类型实例化SourceRunner的实例，也就是上面提到的两种 PollableSourceRunner管理的是PollableSource，这种source是需要外界来抽取event的，例如KafkaSource，需要去消费。 EventDrivenSourceRunner管理的是EventDrivenSource，这种source自己就满足消息机制，可以向channel产生event，不需要外界来抽取。 很容易看出来，这种是稍微省心点儿的~~~ SourceRunner里的元素比较少，还是分析它的两个子类吧。为方便对比，先看稍微简单一些的 ### EventDrivenSourceRunner元素只有一个lifecycleState : LifecycleState启动代码：1234567public void start() &#123; Source source = getSource(); ChannelProcessor cp = source.getChannelProcessor(); cp.initialize(); // 初始化interceptorChain source.start(); // 启动source lifecycleState = LifecycleState.START; &#125; 上面可见，source是在这里启动的。我们来参照一个具体实现ExecSource：1234567891011121314151617181920public void start() &#123; executor = Executors.newSingleThreadExecutor(); // 好，我们发现了一个Runnable runner = new ExecRunnable(shell, command, getChannelProcessor(), sourceCounter, restart, restartThrottle, logStderr, bufferCount, batchTimeout, charset); // FIXME: Use a callback-like executor / future to signal us upon failure. runnerFuture = executor.submit(runner); /* * NB: This comes at the end rather than the beginning of the method because * it sets our state to running. We want to make sure the executor is alive * and well first. */ sourceCounter.start(); super.start(); logger.debug("Exec source started"); &#125; 看一下上面代码里的runnable： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public void run() &#123; do &#123; ....... try &#123; // fork进程，执行shell if(shell != null) &#123; String[] commandArgs = formulateShellCommand(shell, command); process = Runtime.getRuntime().exec(commandArgs); &#125; else &#123; String[] commandArgs = command.split("\\s+"); process = new ProcessBuilder(commandArgs).start(); &#125; .... // 首先如果当前eventList不为空，先处理 future = timedFlushService.scheduleWithFixedDelay(new Runnable() &#123; @Override public void run() &#123; try &#123; synchronized (eventList) &#123; if(!eventList.isEmpty() &amp;&amp; timeout()) &#123; flushEventBatch(eventList); // 批处理咯 &#125; &#125; &#125; catch (Exception e) &#123; ... &#125; &#125; &#125;, batchTimeout, batchTimeout, TimeUnit.MILLISECONDS); // 读取shell进程输出 while ((line = reader.readLine()) != null) &#123; synchronized (eventList) &#123; sourceCounter.incrementEventReceivedCount(); eventList.add(EventBuilder.withBody(line.getBytes(charset))); if(eventList.size() &gt;= bufferCount || timeout()) &#123; flushEventBatch(eventList); // 批处理咯 &#125; &#125; &#125; ..... &#125; catch (Exception e) &#123; .... &#125; finally &#123; .... &#125; ...... &#125; while(restart); &#125; 前面总是看到ChannelProcessor，还被迷惑地以为这个东西是给channel用的，后来自己才想明白，channel只是个容器。主动方应该是Source和Sink才对。。。果然，ChannelProcessor是给Source用的： 123456private void flushEventBatch(List&lt;Event&gt; eventList)&#123; channelProcessor.processEventBatch(eventList); sourceCounter.addToEventAcceptedCount(eventList.size()); eventList.clear(); lastPushToChannel = systemClock.currentTimeMillis(); &#125; ChannelProcessor提供的元素如下： interceptorChain : InterceptorChain selector : ChannelSelector processEventBatch(List) processEvent(Event) PS: 猛然发现，前面其实我们已经通过channelProcessor的initialize方法初始化了interceptorChain，没错，interceptor就是交给它管理的。因为ChannelProcessor是处理event的实际作用点，所以对于event的过滤与处理也应该是在这里。 event处理流程： 通过interceptor 分类到各个channel -&gt; 通过channelSelector找到对应关系 遍历channel，调用对应的transaction及其方法，进行put操作，关闭事务 展示一下事务这部分，以便联想起前面总结的事务部分内容，也可以串起来：12345678910111213141516171819202122232425public void processEvent(Event event) &#123; ... // Process required channels List&lt;Channel&gt; requiredChannels = selector.getRequiredChannels(event); for (Channel reqChannel : requiredChannels) &#123; Transaction tx = reqChannel.getTransaction(); Preconditions.checkNotNull(tx, "Transaction object must not be null"); try &#123; tx.begin(); // 开始 // 这里就是调用前面提到BasicTransactionSemantics的API了 reqChannel.put(event); tx.commit(); //提交 &#125; catch (Throwable t) &#123; tx.rollback(); // 回滚 .... &#125; finally &#123; if (tx != null) &#123; tx.close(); // 关闭 &#125; &#125; &#125; ...&#125; PollableSourceRunner元素： runner : PollingRunner // 关键就在这上面了 runnerThread : Thread lifecycleState : LifecycleState shouldStop : AtomicBoolean 看一下它实现的start方法12345678910111213141516171819PollableSource source = (PollableSource) getSource(); ChannelProcessor cp = source.getChannelProcessor(); cp.initialize(); source.start(); // 至此前面与EventDrivenSourceRunner的处理是一模一样的 runner = new PollingRunner(); runner.source = source; runner.counterGroup = counterGroup; runner.shouldStop = shouldStop; runnerThread = new Thread(runner); runnerThread.setName(getClass().getSimpleName() + "-" + source.getClass().getSimpleName() + "-" + source.getName()); runnerThread.start(); lifecycleState = LifecycleState.START; 可以看到，后面起了一个线程，runnable就是PollingRunner。 123456789101112131415public void run() &#123; .... while (!shouldStop.get()) &#123; ...... try &#123; if (source.process().equals(PollableSource.Status.BACKOFF)) &#123; //只有一行关键 ... &#125; else &#123; counterGroup.set("runner.backoffs.consecutive", 0L); &#125; &#125; catch (InterruptedException e) &#123; ...... &#125; &#125; &#125; 明白了哈？就是执行了一下PollableSource里的process方法。前面提到了，这个接口的子类并非自身就是消息驱动的那种，需要别人来抽取event。所以前面的东西只是做准备，而PollingRunner来负责event的抽取以及将event放入channel的工作。 再次找个实现类，KafkaSource12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public Status process() throws EventDeliveryException &#123; .... try &#123; while (eventList.size() &lt; batchUpperLimit &amp;&amp; System.currentTimeMillis() &lt; batchEndTime) &#123; iterStatus = hasNext(); if (iterStatus) &#123; // get next message MessageAndMetadata&lt;byte[], byte[]&gt; messageAndMetadata = it.next(); kafkaMessage = messageAndMetadata.message(); kafkaKey = messageAndMetadata.key(); // Add headers to event (topic, timestamp, and key) headers = new HashMap&lt;String, String&gt;(); headers.put(KafkaSourceConstants.TIMESTAMP, String.valueOf(System.currentTimeMillis())); headers.put(KafkaSourceConstants.TOPIC, topic); if (kafkaKey != null) &#123; headers.put(KafkaSourceConstants.KEY, new String(kafkaKey)); &#125; if (log.isDebugEnabled()) &#123; log.debug("Message: &#123;&#125;", new String(kafkaMessage)); &#125; event = EventBuilder.withBody(kafkaMessage, headers); eventList.add(event); &#125; &#125; ..... if (eventList.size() &gt; 0) &#123; getChannelProcessor().processEventBatch(eventList); counter.addToEventAcceptedCount(eventList.size()); eventList.clear(); if (log.isDebugEnabled()) &#123; log.debug("Wrote &#123;&#125; events to channel", eventList.size()); &#125; if (!kafkaAutoCommitEnabled) &#123; // commit the read transactions to Kafka to avoid duplicates long commitStartTime = System.nanoTime(); consumer.commitOffsets(); long commitEndTime = System.nanoTime(); counter.addToKafkaCommitTimer((commitEndTime-commitStartTime)/(1000*1000)); &#125; &#125; ...... return Status.READY; &#125; catch (Exception e) &#123; ...... &#125; &#125; 看了这段代码后，可确定kafka的event就是由PollingRunner搞的。另外发现一个事儿，就是ChannelProcessor专门负责处理event后，放入channel。]]></content>
      <tags>
        <tag>源码</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume源码阅读笔记 （3）]]></title>
    <url>%2F2015%2F07%2F10%2Fflume-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%883%EF%BC%89%2F</url>
    <content type="text"><![CDATA[前面介绍了几个关键组件以及相互的关系，这次我们来看看flume里对于事务的封装与处理。 TransactionBasicTransactionSemantics state : State //NEW, OPEN, COMPLETED, CLOSED initialThreadId : long //记录创建它的ThreadId begin() commit() rollback() close() put(Event) take()An implementation of basic Transaction semantics designed to work in concert with BasicChannelSemantics to simplify creation of robust Channel implementations. This class ensures that each transaction implementation method is called only while the transaction is in the correct state for that method, and only by the thread that created the transaction. Nested calls to begin() and close() are supported as long as they are balanced. 是基本Transaction的实现，与BasicChannelSemantics配合。这个类保证transaction实现方法只在当前transaction适当的状态下触发，而且只能被创建当前transaction的线程触发。 Subclasses need only implement doPut, doTake, doCommit, and doRollback, and the developer can rest assured that those methods are called only after transaction state preconditions have been properly met. doBegin and doClose may also be implemented if there is work to be done at those points. 子类只需要实现doPut, doTake, doCommit以及doRollback方法，同时开发人员还需要保证只有在transaction的state被正确的修改后，才能触发这些方法。如果有必要的话，也可以实现doBegin和doClose方法。 All InterruptedException exceptions thrown from the implementations of the doXXX methods are automatically wrapped to become ChannelExceptions, but only after restoring the interrupted status of the thread so that any subsequent blocking method calls will themselves throw InterruptedException rather than blocking. The exception to this rule is doTake, which simply returns null instead of wrapping and propagating the InterruptedException, though it still first restores the interrupted status of the thread.当doXXX方法发生异常的时候，首先把当前线程的状态修改为interrupted，然后把所有的InterruptedException转化为ChannelException抛出，这样后面的方法就不会阻塞住。doTake就遵循这样的规则，首先把线程状态改为interrupted，然后不包装以及传递InterruptedException，而是返回null。 ChannelBasicChannelSemantics currentTransaction : ThreadLocal&lt;BasicTransactionSemantics&gt; put(Event) take() getTransaction() 实现类MemoryTransaction在MemoryChannel里的使用MemoryTransaction takeList : LinkedBlockingDeque // 消费者的队列 putList : LinkedBlockingDeque // 生产者的队列 putByteCounter : int takeByteCounter : int channelCounter : ChannelCounter event放入channel123456789101112@Override protected void doPut(Event event) throws InterruptedException &#123; channelCounter.incrementEventPutAttemptCount(); int eventByteSize = (int)Math.ceil(estimateEventSize(event)/byteCapacitySlotSize); if (!putList.offer(event)) &#123; // 放入生产者队列 throw new ChannelException( "Put queue for MemoryTransaction of capacity " + putList.size() + " full, consider committing more frequently, " + "increasing capacity or increasing thread count"); &#125; putByteCounter += eventByteSize; &#125; 消费channel里的event1234567891011121314151617181920212223protected Event doTake() throws InterruptedException &#123; channelCounter.incrementEventTakeAttemptCount(); if(takeList.remainingCapacity() == 0) &#123; throw new ChannelException("Take list for MemoryTransaction, capacity " + takeList.size() + " full, consider committing more frequently, " + "increasing capacity, or increasing thread count"); &#125; if(!queueStored.tryAcquire(keepAlive, TimeUnit.SECONDS)) &#123; return null; &#125; Event event; synchronized(queueLock) &#123; event = queue.poll(); // 从channel中消费一条数据 &#125; Preconditions.checkNotNull(event, "Queue.poll returned NULL despite semaphore " + "signalling existence of entry"); takeList.put(event); // 放入Transaction的消费者队列 int eventByteSize = (int)Math.ceil(estimateEventSize(event)/byteCapacitySlotSize); takeByteCounter += eventByteSize; return event; &#125; 提交事务：12345678910111213141516171819202122232425262728293031323334353637383940@Override protected void doCommit() throws InterruptedException &#123; ... if(!queueRemaining.tryAcquire(-remainingChange, keepAlive, TimeUnit.SECONDS)) &#123; bytesRemaining.release(putByteCounter); throw new ChannelFullException("Space for commit to queue couldn't be acquired." + " Sinks are likely not keeping up with sources, or the buffer size is too tight"); &#125; &#125; int puts = putList.size(); int takes = takeList.size(); synchronized(queueLock) &#123; if(puts &gt; 0 ) &#123; while(!putList.isEmpty()) &#123; //这里的queue是MemoryChannel的Event队列，也就是将这段时间put的event放进channel的意思了 if(!queue.offer(putList.removeFirst())) &#123; throw new RuntimeException("Queue add failed, this shouldn't be able to happen"); &#125; &#125; &#125; putList.clear(); takeList.clear(); &#125; bytesRemaining.release(takeByteCounter); takeByteCounter = 0; putByteCounter = 0; queueStored.release(puts); if(remainingChange &gt; 0) &#123; queueRemaining.release(remainingChange); &#125; if (puts &gt; 0) &#123; channelCounter.addToEventPutSuccessCount(puts); &#125; if (takes &gt; 0) &#123; channelCounter.addToEventTakeSuccessCount(takes); &#125; channelCounter.setChannelSize(queue.size()); &#125; 回滚代码, 可想而知，肯定是回复channel之前的状态，将刚刚消费出来的东西塞回去，塞进去的东西拿出来：123456789101112131415161718protected void doRollback() &#123; int takes = takeList.size(); synchronized(queueLock) &#123; // 检查还能不能塞回去了 Preconditions.checkState(queue.remainingCapacity() &gt;= takeList.size(), "Not enough space in memory channel " + "queue to rollback takes. This should never happen, please report"); while(!takeList.isEmpty()) &#123; queue.addFirst(takeList.removeLast()); //消费的event塞回去 &#125; putList.clear();//这是要塞进去的event清空 &#125; bytesRemaining.release(putByteCounter); putByteCounter = 0; takeByteCounter = 0; queueStored.release(takes); channelCounter.setChannelSize(queue.size()); &#125; 这里比较容易看到的是，如果塞了一半报错，会出现一些rollback差错，因为已经放进去的，并没有拿出来。如果需要实现的话，其实应该在放的时候起一个计数器，然后rollback的时候按照数量先往外拿，然后再把前面拿出来的放回去~~~~嘿嘿,是不是可以提交到社区 ^ _ ^]]></content>
      <tags>
        <tag>源码</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搭建Kafka源码调试环境]]></title>
    <url>%2F2015%2F07%2F09%2F%E6%90%AD%E5%BB%BAKafka%E6%BA%90%E7%A0%81%E8%B0%83%E8%AF%95%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[简单流程 git clone https://github.com/apache/kafka.git git tag -l 瞅准0.8.2.1 git checkout 0.8.2.1 -b will0821 http://gradle.org/下载安装gradle，也为自己的IDE安装gradle插件【对gradle不是特别了解，所以多用插件】。我下载的是gradle2.5，解压后需要在IDE里指定GRADLE_HOME变量。Eclipse是通过gradle EnIDE的配置窗口指定的。其他请自行解决。 build，尝试运行 错误处理 我用的是eclipse，报错“找不到或无法加载主类”，按照这个思路去Google，没有找到实际能解决的办法。此时注意到eclipse的下方Maker里有一条错误提示“scalatest_2.10-1.9.1.jar of core build path is cross-compiled with an incompatible version of Scala (2.10.0)”，找到官方JIRA提供的解决方案 https://issues.apache.org/jira/browse/KAFKA-1873。将kafka下的**gradle.properties**里scalaVersion从2.10.4修改到2.11.5即可。猜测这个文件应该是类似于maven里的build配置项。重新build一把，再次执行入口类Kafka即可。]]></content>
  </entry>
  <entry>
    <title><![CDATA[flume 源码阅读笔记（2）]]></title>
    <url>%2F2015%2F07%2F08%2Fflume-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%882%EF%BC%89%2F</url>
    <content type="text"><![CDATA[通过寻找LifecycleAware的子类就可以发现，所有flume的component都继承了这个接口。下面是几个主要的 Channel连接source和sink，可以看做是MQ或者buffer，保证自身线程安全。实在是懒得翻译，直接copy文档A channel connects a Source to a Sink. The source acts as producer while the sink acts as a consumer of events. The channel itself is the buffer between the two.A channel exposes a Transaction interface that can be used by its clients to ensure atomic put and take semantics. This is necessary to guarantee single hop reliability between agents. For instance, a source will successfully produce an event if and only if that event can be committed to the source’s associated channel. Similarly, a sink will consume an event if and only if its respective endpoint can accept the event. The extent of transaction support varies for different channel implementations ranging from strong to best-effort semantics.Channels are associated with unique names that can be used for separating configuration and working namespaces.Channels must be thread safe, protecting any internal invariants as no guarantees are given as to when and by how many sources/sinks they may be simultaneously accessed by. * Transaction getTransaction() * put(Event event) * Event take() Sink连接channel，然后消费channel里的内容，根据sink类型的不同，把这些message发往不同的目的地。可以根据不同的行为使用SinkGroup和SinkProcessor来为Sink分组。SinkRunner通过processor定期轮询他们。 * Status process() 在某个transaction范围内消费channel里的message，如果transaction成功，就提交。若没成功，需要**回滚**这个transaction。 要确保这个方法只有一个线程调用 * Channel getChannel() * setChannel(Channel channel) SourceA source generates {@plainlink Event events} and calls methods on the configured ChannelProcessor to persist those events into the configured channels.Sources are associated with unique names that can be used for separating configuration and working namespaces.不用搭理线程安全。 * ChannelProcessor getChannelProcessor() * setChannelProcessor(ChannelProcessor channelProcessor) SinkProcessorInterface for a device that allows abstraction of the behavior of multiple sinks, always assigned to a SinkRunner 。A sink processors SinkProcessor.process() method will only be accessed by a single runner thread. However configuration methods such as Configurable.configure may be concurrently accessed. * Status process() Handle a request to **poll the owned sinks**.The processor is expected to call **Sink.process()** on whatever sink(s) appropriate, handling failures as appropriate and throwing EventDeliveryException when there is a failure to deliver any events according to the delivery policy defined by the sink processor implementation. See specific implementations of this interface for delivery behavior and policies. * setSinks(List&lt;Sink&gt; sinks) 看起来这个processor有些像wrapper + manager的意思。 SinkSelectorAn interface that allows the LoadBalancingSinkProcessor to use a load-balancing strategy such as round-robin, random distribution etc. Implementations of this class can be plugged into the system via processor configuration and are used to select a sink on every invocation.An instance of the configured sink selector is create during the processor configuration, its setSinks(List) method is invoked following which it is configured via a subcontext. Once configured, the lifecycle of this selector is tied to the lifecycle of the sink processor.At runtime, the processor invokes the createSinkIterator() method for every process call to create an iteration order over the available sinks. The processor then loops through this iteration order until one of the sinks succeeds in processing the event. If the iterator is exhausted and none of the sinks succeed, the processor will raise an EventDeliveryException. * setSinks(List&lt;Sink&gt; sinks) * Iterator&lt;Sink&gt; createSinkIterator() * void informSinkFailed(Sink failedSink) SourceRunnerA source runner controls how a source is driven. This is an abstract class used for instantiating derived classes. * Source source * SourceRunner forSource(Source source) 根据source类型实例化SourceRunner实现类的静态工厂方法 SinkRunnerA driver for sinks that polls them, attempting to process events if any are available in the Channel.Note that, unlike sources, all sinks are polled. * Thread runnerThread * LifecycleState lifecycleState * **SinkProcessor** policy * CounterGroup counterGroup * PollingRunner runner 轮询调用sink的驱动器，如果channel里有任何可用的event，都会尝试执行。与source不同，所有的sink都会被涉及。Runner像是Processor的Wrapper。 1234567891011121314@Override public void start() &#123; SinkProcessor policy = getPolicy(); policy.start(); runner = new PollingRunner(); //封装shouldStop决定是否运行 runner.policy = policy; runner.counterGroup = counterGroup; runner.shouldStop = new AtomicBoolean(); runnerThread = new Thread(runner); runnerThread.setName("SinkRunner-PollingRunner-" + policy.getClass().getSimpleName()); runnerThread.start(); lifecycleState = LifecycleState.START; &#125;]]></content>
      <tags>
        <tag>源码</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[flume 源码阅读笔记（1）]]></title>
    <url>%2F2015%2F07%2F08%2Fflume-%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%881%EF%BC%89%2F</url>
    <content type="text"><![CDATA[#1 . 启动脚本最开始肯定是从启动脚本flume-ng下手了1234FLUME_AGENT_CLASS=&quot;org.apache.flume.node.Application&quot; # 这个是主要入口FLUME_AVRO_CLIENT_CLASS=&quot;org.apache.flume.client.avro.AvroCLIClient&quot;FLUME_VERSION_CLASS=&quot;org.apache.flume.tools.VersionInfo&quot;FLUME_TOOLS_CLASS=&quot;org.apache.flume.tools.FlumeToolsMain&quot; #2. 主程序入口Application12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576Options options = new Options();..... //一大堆命令行参数 CommandLineParser parser = new GnuParser(); CommandLine commandLine = parser.parse(options, args); // 以上两行，对命令行参数解析完毕 // 打印帮助信息 if (commandLine.hasOption('h')) &#123; new HelpFormatter().printHelp("flume-ng agent", options, true); return; &#125; String agentName = commandLine.getOptionValue('n'); boolean reload = !commandLine.hasOption("no-reload-conf"); // 是否有zookeeper的相关配置 if (commandLine.hasOption('z') || commandLine.hasOption("zkConnString")) &#123; isZkConfigured = true; &#125; // 主角登场 Application application = null; if (isZkConfigured) &#123; // get options String zkConnectionStr = commandLine.getOptionValue('z'); String baseZkPath = commandLine.getOptionValue('p'); if (reload) &#123; EventBus eventBus = new EventBus(agentName + "-event-bus"); List&lt;LifecycleAware&gt; components = Lists.newArrayList(); PollingZooKeeperConfigurationProvider zookeeperConfigurationProvider = new PollingZooKeeperConfigurationProvider( agentName, zkConnectionStr, baseZkPath, eventBus); components.add(zookeeperConfigurationProvider); application = new Application(components); eventBus.register(application); &#125; else &#123; StaticZooKeeperConfigurationProvider zookeeperConfigurationProvider = new StaticZooKeeperConfigurationProvider( agentName, zkConnectionStr, baseZkPath); application = new Application(); application.handleConfigurationEvent(zookeeperConfigurationProvider .getConfiguration()); &#125; &#125; else &#123; File configurationFile = new File(commandLine.getOptionValue('f')); /* * 如果配置文件不存在，就直接报错 */ if (!configurationFile.exists()) &#123; ... &#125; List&lt;LifecycleAware&gt; components = Lists.newArrayList(); if (reload) &#123; EventBus eventBus = new EventBus(agentName + "-event-bus"); PollingPropertiesFileConfigurationProvider configurationProvider = new PollingPropertiesFileConfigurationProvider( agentName, configurationFile, eventBus, 30); components.add(configurationProvider); application = new Application(components); eventBus.register(application); &#125; else &#123; PropertiesFileConfigurationProvider configurationProvider = new PropertiesFileConfigurationProvider( agentName, configurationFile); // 下面为典型启动过程 application = new Application(); application.handleConfigurationEvent(configurationProvider .getConfiguration()); &#125; &#125; application.start(); // 程序启动 // 后面是shutdown hook，作清理工作 综上来看，有营养的代码很少，就是读个配置文件，然后直接就启动程序了。那么application的初始化和start方法里都有些什么操作呢？ 123456789101112131415161718192021222324 // 1. 构造方法里只是注册一个LifecycleSupervisor，也就是管理生命周期的 public Application() &#123; this(new ArrayList&lt;LifecycleAware&gt;(0)); &#125; public Application(List&lt;LifecycleAware&gt; components) &#123; this.components = components; supervisor = new LifecycleSupervisor(); &#125;// 2. 根据conf重启所有components @Subscribe public synchronized void handleConfigurationEvent(MaterializedConfiguration conf) &#123; stopAllComponents(); startAllComponents(conf); // 这个conf来自于配置文件，你懂得，这些component肯定就是source channel sink 之类的了 &#125; // 3. 使用1中初始化的supervisor对所有component进行监管 public synchronized void start() &#123; for(LifecycleAware component : components) &#123; supervisor.supervise(component, new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); &#125; &#125; 好，启动component的入口也明晰了1234567891011121314151617181920212223242526272829303132private void startAllComponents(MaterializedConfiguration materializedConfiguration) &#123; for (Entry&lt;String, Channel&gt; entry : materializedConfiguration.getChannels().entrySet()) &#123; try&#123; logger.info("Starting Channel " + entry.getKey()); // 1. 将channel交给supervisor来进行生命周期管理，这里是启动 supervisor.supervise(entry.getValue(), new SupervisorPolicy.AlwaysRestartPolicy(), LifecycleState.START); &#125; catch (Exception e)&#123; logger.error("Error while starting &#123;&#125;", entry.getValue(), e); &#125; &#125; // 2. 启动sink 和 souce之前，确保所有的channel无误地启动完毕 for(Channel ch: materializedConfiguration.getChannels().values())&#123; while(ch.getLifecycleState() != LifecycleState.START &amp;&amp; !supervisor.isComponentInErrorState(ch))&#123; logger.info("Waiting for channel: " + ch.getName() + " to start. Sleeping for 500 ms"); Thread.sleep(500); &#125; &#125; // 3. 同上方式，启动sink // 4. 同上方式，启动source // 监控： 目前支持ganlia和http jetty两种，需要在命令行指定type // $ bin/flume-ng agent --conf-file example.conf --name a1 -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=com.example:1234,com.example2:5455 this.loadMonitoring(); &#125; 至此启动流程就完毕了。太简单了也~~~]]></content>
      <tags>
        <tag>源码</tag>
        <tag>flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[在pycharm中开发pySpark程序]]></title>
    <url>%2F2015%2F05%2F08%2F%E5%9C%A8pycharm%E4%B8%AD%E5%BC%80%E5%8F%91pySpark%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[Apache spark是一个用来分析和处理大数据的有力工具。虽然知道怎样运行pySpark shell，但是开发的时候我们总会喜欢常用的IDE。开始我尝试使用pycharm 的preference setting，添加pySpark module到external library(Figure 1).。但是编辑器里还是不能找到引用的对象(Figure 2)。最后，只能在python的运行脚本里添加相应引用了。 通过下面的代码，添加适当的SPARK HOME目录和PYSPARK目录来引入Apache spark module12345678910111213import osimport sys# Path for spark source folderos.environ['SPARK_HOME']="your_spark_home_folder"# Append pyspark to Python Pathsys.path.append("your_pyspark_folder ")try: from pyspark import SparkContext from pyspark import SparkConf print("Successfully imported Spark Modules")except ImportError as e: print("Can not import Spark Modules",e) sys.exit(1) 成功引入后会打印：“Successfully imported Spark Modules” (Figure 3).再来写个小程序验证PySpark是否能够正常工作. 引入pySpark module后，添加以下代码 (Figure 4).1234# Initialize SparkContextsc=SparkContext('local')words=sc.parallelize(["scala","java","hadoop","spark","akka"])printwords.count() 转自：http://renien.github.io/blog/accessing-pyspark-pycharm]]></content>
  </entry>
  <entry>
    <title><![CDATA[Flume 测试笔记（8）]]></title>
    <url>%2F2015%2F05%2F06%2FFlume-%E6%B5%8B%E8%AF%95%E7%AC%94%E8%AE%B0%EF%BC%888%EF%BC%89%2F</url>
    <content type="text"><![CDATA[已经打通了经脉，还需要考虑负载均衡以及容灾问题。鉴于现在log的数据压力不是很大，而且投入的机器也不是很多，暂时只考虑容灾，暂时不考虑负载均衡。 1.flume代码逻辑flume里原生一个Failover Sink Processor，维护failedSinks和liveSinks两个东西来进行工作。首先会测试配置的所有sink，&gt;工作时只取liveSink里的一个作为activeSink，进行实质性的工作。 FailoverSinkProcessor中的逻辑如下：12345678910111213141516171819202122232425262728293031323334353637383940@Override public Status process() throws EventDeliveryException &#123; // Retry any failed sinks that have gone through their &quot;cooldown&quot; period Long now = System.currentTimeMillis(); while(!failedSinks.isEmpty() &amp;&amp; failedSinks.peek().getRefresh() &lt; now) &#123; FailedSink cur = failedSinks.poll(); Status s; try &#123; s = cur.getSink().process(); if (s == Status.READY) &#123; liveSinks.put(cur.getPriority(), cur.getSink()); activeSink = liveSinks.get(liveSinks.lastKey()); logger.debug(&quot;Sink &#123;&#125; was recovered from the fail list&quot;, cur.getSink().getName()); &#125; else &#123; // if it&apos;s a backoff it needn&apos;t be penalized. failedSinks.add(cur); &#125; return s; &#125; catch (Exception e) &#123; cur.incFails(); failedSinks.add(cur); &#125; &#125; Status ret = null; while(activeSink != null) &#123; try &#123; ret = activeSink.process(); return ret; &#125; catch (Exception e) &#123; logger.warn(&quot;Sink &#123;&#125; failed and has been sent to failover list&quot;, activeSink.getName(), e); activeSink = moveActiveToDeadAndGetNext(); &#125; &#125; throw new EventDeliveryException(&quot;All sinks failed to process, &quot; + &quot;nothing left to failover to&quot;); &#125; 分析一下执行逻辑，在初始化的时候，failedSink是空的【可以参见configure方法】。在执行第二个while循环的时候，当有activeSink处理失败，就会把这个sink加到failedSinks里去，然后获取liveSinks里的下一个sink作为activeSink。但是这个process方法并没有返回执行的逻辑，那么就应该被重复调用，才可能执行到上面的第一个while循环。SinkRunner中的逻辑：12345678910111213141516171819202122232425262728293031323334@Override public void run() &#123; logger.debug("Polling sink runner starting"); while (!shouldStop.get()) &#123; //只要shouldStop为false便会一次次的执行 try &#123; if (policy.process().equals(Sink.Status.BACKOFF)) &#123; //这里的policy就是SinkProcessor counterGroup.incrementAndGet("runner.backoffs"); Thread.sleep(Math.min( counterGroup.incrementAndGet("runner.backoffs.consecutive") * backoffSleepIncrement, maxBackoffSleep)); &#125; else &#123; counterGroup.set("runner.backoffs.consecutive", 0L); &#125; &#125; catch (InterruptedException e) &#123; logger.debug("Interrupted while processing an event. Exiting."); counterGroup.incrementAndGet("runner.interruptions"); &#125; catch (Exception e) &#123; logger.error("Unable to deliver event. Exception follows.", e); if (e instanceof EventDeliveryException) &#123; counterGroup.incrementAndGet("runner.deliveryErrors"); &#125; else &#123; counterGroup.incrementAndGet("runner.errors"); &#125; try &#123; Thread.sleep(maxBackoffSleep); &#125; catch (InterruptedException ex) &#123; Thread.currentThread().interrupt(); &#125; &#125; &#125; logger.debug("Polling runner exiting. Metrics:&#123;&#125;", counterGroup); &#125; 这样看来，其实flume的FailoverSinkProcessor也是会自动发现恢复了的Sink的。 2. flume官网文档 Failover Sink Processor maintains a prioritized list of sinks, guaranteeing that so long as one is available events will be processed (delivered).The failover mechanism works by relegating failed sinks to a pool where they are assigned a cool down period, increasing with sequential failures before they are retried. Once a sink successfully sends an event, it is restored to the live pool. To configure, set a sink groups processor to failover and set priorities for all individual sinks. All specified priorities must be unique. Furthermore, upper limit to failover time can be set (in milliseconds) using maxpenalty property. Example for agent named a1:a1.sinkgroups = g1a1.sinkgroups.g1.sinks = k1 k2a1.sinkgroups.g1.processor.type = failovera1.sinkgroups.g1.processor.priority.k1 = 5a1.sinkgroups.g1.processor.priority.k2 = 10a1.sinkgroups.g1.processor.maxpenalty = 10000 我们看到，这里又涉及到了sink group这个东西，其实主要是为了做负载均衡和容灾时更加方便一些，也很好理解，就是把sink分个组而已，然后负载均衡和容灾可以指定一个组就可以了，也就针对这个组里的sink进行对应的策略实施。找了一个比较好的例子123456789101112131415161718192021222324252627282930# channelsagent.channels = mem_channelagent.channels.mem_channel.type = memory# sourcesagent.sources = event_sourceagent.sources.event_source.type = avroagent.sources.event_source.bind = 127.0.0.1agent.sources.event_source.port = 10000agent.sources.event_source.channels = mem_channel# sinksagent.sinks = main_sink backup_sinkagent.sinks.main_sink.type = avroagent.sinks.main_sink.hostname = 127.0.0.1agent.sinks.main_sink.port = 10001agent.sinks.main_sink.channel = mem_channelagent.sinks.backup_sink.type = avroagent.sinks.backup_sink.hostname = 127.0.0.1agent.sinks.backup_sink.port = 10002agent.sinks.backup_sink.channel = mem_channel# sink groups agent.sinkgroups = failover_groupagent.sinkgroups.failover_group.sinks = main_sink backup_sinkagent.sinkgroups.failover_group.processor.type = failoveragent.sinkgroups.failover_group.processor.priority.main_sink = 10agent.sinkgroups.failover_group.processor.priority.backup_sink = 5]]></content>
  </entry>
</search>
