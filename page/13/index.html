
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5b99dfd487346155d274c0c49c3fb869";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>

  
    <title>Will&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Will Chen">
    

    
    <meta name="description" content="左右水色 右手天光">
<meta property="og:type" content="website">
<meta property="og:title" content="Will's Blog">
<meta property="og:url" content="http://willcup.com/page/13/index.html">
<meta property="og:site_name" content="Will's Blog">
<meta property="og:description" content="左右水色 右手天光">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Will's Blog">
<meta name="twitter:description" content="左右水色 右手天光">

    
    <link rel="alternative" href="/atom.xml" title="Will&#39;s Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Will&#39;s Blog" title="Will&#39;s Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Will&#39;s Blog">Will&#39;s Blog</a></h1>
				<h2 class="blog-motto">简易 变易 不易</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/about">关于</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:willcup.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/26/hiveserver2负载均衡/" title="hiveserver2负载均衡" itemprop="url">hiveserver2负载均衡</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-26T08:47:44.000Z" itemprop="datePublished"> 发表于 2017-04-26</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>hiveserver2是通过在zookeeper注册一个namespace，然后管理所有的hiveserver实例，实现动态服务发现的。</p>
<p>znode样式：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">/&lt;hiveserver2_namespace&gt;/serverUri=&lt;host:port&gt;;version=&lt;versionInfo&gt;; sequence=&lt;sequence_number&gt;,</div></pre></td></tr></table></figure></p>
<p>hiveserver实例会在znode上设置一个watch。当znode被修改的时候，watch会发送给hiveserver相关信息。这个通知能够让所有的hiveserver实例知道它是不是对于client端可用。</p>
<p>当有hiveserver实例退出时，会从zk的对应node里移除，但是只对新的客户端连接生效。(已经连接的session不能生效了)。只有已经连接到这个hiveserver的最后一个client的session结束后，才会自动把这个hiveserver完全关闭，下面这个命令就是做这个工作的。</p>
<p>使用下面命令移除一个hiveserver<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hive --service hiveserver2 --deregister &lt;package ID&gt;</div></pre></td></tr></table></figure></p>
<h4 id="没有zk时的查询"><a href="#没有zk时的查询" class="headerlink" title="没有zk时的查询"></a>没有zk时的查询</h4><p>下面是一个传统的查询流程：</p>
<ul>
<li>通过JDBC/ODBC driver连接到HS2实例，建立一个session</li>
<li>然后每次查询的时候client都发送语句给HS2，转化成Hadoop上的执行任务</li>
<li>每个查询的结果都写道一个临时文件中</li>
<li>客户端driver从HS2抽取临时文件中的数据记录</li>
</ul>
<p><img src="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_hadoop-ha/content/figures/2/figures/Query_Ex_Path_No_ZK.png" alt="典型查询流程"></p>
<h4 id="带有zk的查询"><a href="#带有zk的查询" class="headerlink" title="带有zk的查询"></a>带有zk的查询</h4><p>因为可以使用动态服务发现，所以客户端driver必须知道怎样使用这个特性。对于HDP2.2或者JDBC driver2.0.0版本之后才能支持。</p>
<p>动态服务发现实现如下：</p>
<ul>
<li>多个HS2实例使用zk注册自己</li>
<li>客户端driver连接zk<blockquote>
<p> jdbc:hive2://<zookeeper_ensemble>;serviceDiscoveryMode=zooKeeper; zooKeeperNamespace=&lt;hiveserver2_namespace</zookeeper_ensemble></p>
</blockquote>
</li>
<li>zk随机返回一个host:port给客户端</li>
<li>客户端执行单个服务传统查询过程</li>
</ul>
<p><img src="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_hadoop-ha/content/figures/2/figures/Query_Ex_Path_With_ZK.png" alt="带有zk的查询过程"></p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>hive.zookeeper.quorum zk列表<br>hive.zookeeper.session.timeout 超时就关闭session<br>hive.server2.support.dynamic.service.discovery  设置为true<br>hive.server2.zookeeper.namespace    指定一个就行了，默认是hiveserver2</p>
<ul>
<li><a href="https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_hadoop-ha/content/ha-hs2-requests.html" target="_blank" rel="external">https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.4.0/bk_hadoop-ha/content/ha-hs2-requests.html</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/26/hue对于多个hiveserver2的支持/" title="hue对于多个hiveserver2的支持" itemprop="url">hue对于多个hiveserver2的支持</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-26T08:26:28.000Z" itemprop="datePublished"> 发表于 2017-04-26</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>相关连接代码：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">hiveserver2_jdbc_url</span><span class="params">()</span>:</span></div><div class="line">  urlbase = <span class="string">'jdbc:hive2://%s:%s/default'</span> % (beeswax.conf.HIVE_SERVER_HOST.get(),</div><div class="line">                                            beeswax.conf.HIVE_SERVER_PORT.get())</div><div class="line">  <span class="keyword">if</span> get_conf().get(_CNF_HIVESERVER2_USE_SSL, <span class="string">'FALSE'</span>).upper() == <span class="string">'TRUE'</span>:</div><div class="line">    <span class="keyword">return</span> <span class="string">'%s;ssl=true;sslTrustStore=%s;trustStorePassword=%s'</span> % (urlbase,</div><div class="line">            get_conf().get(_CNF_HIVESERVER2_TRUSTSTORE_PATH),</div><div class="line">            get_conf().get(_CNF_HIVESERVER2_TRUSTSTORE_PASSWORD))</div><div class="line">  <span class="keyword">else</span>:</div><div class="line">     <span class="keyword">return</span> urlbase</div></pre></td></tr></table></figure></p>
<p>可以看到，除非启用SSL，否则咋样都拼不进去下面这种串：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">jdbc:hive2://zkNode1:<span class="number">2181</span>,zkNode2:<span class="number">2181</span>,zkNode3:<span class="number">2181</span>/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2_zk</div></pre></td></tr></table></figure></p>
<p>Tip:</p>
<p>可以先用tengine配置TCP代理，然后在这里实现负载均衡，hue端只需要配置代理的地址就可以了。 —— 希望hive的driver可以支持，有待测试。</p>
<p>参考：</p>
<ul>
<li><a href="http://lxw1234.com/archives/2016/05/675.htm" target="_blank" rel="external">http://lxw1234.com/archives/2016/05/675.htm</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/25/hive中使用向量化查询引擎Vectorized-Query-Execution/" title="hive中使用向量化查询引擎Vectorized-Query-Execution" itemprop="url">hive中使用向量化查询引擎Vectorized-Query-Execution</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-25T11:59:52.000Z" itemprop="datePublished"> 发表于 2017-04-25</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>向量化查询引擎可以在执行scan, filter, aggregate, join等操作的时候很大程度上降低CPU损耗。标准sql执行系统每次只处理一行数据，内部执行的时候会包括很长的代码路径和重要的metadata解析。向量化查询引擎简化了这个过程，一次读取1024行数据，在这个数据块中，每个字段都被存储成向量化的结构(一个原是数据类型的数组)。对于一些简单的算法可以通过快速迭代这个vector完成，只需要在循环过程中调用甚至不用调用几个函数。这些loop以一种流式方式编译，在固定时间内结束，为了提高效率，会使用processor pipline和内存缓存。<a href="https://issues.apache.org/jira/browse/HIVE-4160" target="_blank" rel="external">详细设计</a></p>
<p>个人理解：就是每行都执行很小的计算，但是要处理N多次。改成一次处理多行，同时针对多行做计算处理，节省CPU时间。</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h4 id="启用"><a href="#启用" class="headerlink" title="启用"></a>启用</h4><p>要使用向量化查询引擎，必须先把hive表存储成ORC格式的。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">set hive.vectorized.execution.enabled = true;</div></pre></td></tr></table></figure></p>
<p>默认是没有开启向量化查询引擎的，需要手动开启。</p>
<h4 id="支持的数据类型与操作"><a href="#支持的数据类型与操作" class="headerlink" title="支持的数据类型与操作"></a>支持的数据类型与操作</h4><ul>
<li>tinyint</li>
<li>smallint</li>
<li>int</li>
<li>bigint</li>
<li>boolean</li>
<li>float</li>
<li>double</li>
<li>decimal</li>
<li>date</li>
<li>timestamp (see Limitations below)</li>
<li>string<br>其他的数据类型还是会按照传统方式一行行处理。</li>
</ul>
<p>支持的表达式：</p>
<ul>
<li>简单算法: +, -, *, /, %</li>
<li>AND, OR, NOT</li>
<li>比较 &lt;, &gt;, &lt;=, &gt;=, =, !=, BETWEEN, IN ( list-of-constants ) as filters</li>
<li>布尔表达式 (non-filters) using AND, OR, NOT, &lt;, &gt;, &lt;=, &gt;=, =, !=</li>
<li>IS [NOT] NULL</li>
<li>所有的匹配函数 (SIN, LOG, etc.)</li>
<li>string相关函数 SUBSTR, CONCAT, TRIM, LTRIM, RTRIM, LOWER, UPPER, LENGTH</li>
<li>类型转换</li>
<li>UDF</li>
<li>日期函数 (YEAR, MONTH, DAY, HOUR, MINUTE, SECOND, UNIX_TIMESTAMP)</li>
<li>IF 条件表达式</li>
</ul>
<p>UDF通过向后兼容的方式支持，不过执行的时候就是执行向量化查询引擎，但是没有内置的快。向量化的filter操作是从左到右执行，所以最好把UDF放在带有and的where语句的最后：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">where column1 = 10 and myUDF(column2) = "x"</div></pre></td></tr></table></figure></p>
<h2 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h2><p>Timestamps 只能是 1677-09-20 到2262-04-11. </p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/21/集群service配置相关/" title="集群service配置相关" itemprop="url">集群service配置相关</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-21T08:41:02.000Z" itemprop="datePublished"> 发表于 2017-04-21</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>现有配置</th>
<th>TO </th>
</tr>
</thead>
<tbody>
<tr>
<td>resourcemanager</td>
<td>1G</td>
<td>4G</td>
</tr>
<tr>
<td>nodemanager</td>
<td>1G</td>
<td>2G</td>
</tr>
<tr>
<td>timeline server</td>
<td>1G</td>
<td>4G</td>
</tr>
</tbody>
</table>
<p>resource + timline server = 2G</p>
<h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS "></a>HDFS </h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>现有配置</th>
<th>TO</th>
</tr>
</thead>
<tbody>
<tr>
<td>datanode</td>
<td>1G</td>
<td>2G</td>
</tr>
<tr>
<td>namenode</td>
<td>3G </td>
</tr>
</tbody>
</table>
<p>namenode * 2 =  6G</p>
<h2 id="MAPRED"><a href="#MAPRED" class="headerlink" title="MAPRED"></a>MAPRED</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>现有配置</th>
<th>TO</th>
</tr>
</thead>
<tbody>
<tr>
<td>history server</td>
<td>1G</td>
<td>2G</td>
</tr>
</tbody>
</table>
<h2 id="HIVE"><a href="#HIVE" class="headerlink" title="HIVE"></a>HIVE</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>hiveserver2</td>
<td>769M</td>
</tr>
<tr>
<td>metastore</td>
<td>1G</td>
</tr>
<tr>
<td>hcatserver</td>
<td>1G</td>
</tr>
</tbody>
</table>
<p>3G</p>
<h2 id="HBASE"><a href="#HBASE" class="headerlink" title="HBASE"></a>HBASE</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>regionserver</td>
<td>2G</td>
</tr>
<tr>
<td>master</td>
<td>1G</td>
</tr>
</tbody>
</table>
<p>regionserver <em> 8 + master </em> 2 = 18G</p>
<h2 id="ZK"><a href="#ZK" class="headerlink" title="ZK"></a>ZK</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>zk server</td>
<td>1G</td>
</tr>
</tbody>
</table>
<p>zk * 3 = 3G</p>
<h2 id="KAFKA"><a href="#KAFKA" class="headerlink" title="KAFKA"></a>KAFKA</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>broker</td>
<td>1G</td>
</tr>
</tbody>
</table>
<p>broker * 3 =3G</p>
<h2 id="SPAKR"><a href="#SPAKR" class="headerlink" title="SPAKR"></a>SPAKR</h2><table>
<thead>
<tr>
<th>服务名称</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark history server</td>
<td>1G</td>
</tr>
</tbody>
</table>
<h2 id="需要copy的机器"><a href="#需要copy的机器" class="headerlink" title="需要copy的机器"></a>需要copy的机器</h2><p>以下机器copy完成后，全部将内存提升至32G、CPU提升到8核</p>
<h5 id="迁出机器"><a href="#迁出机器" class="headerlink" title="迁出机器"></a>迁出机器</h5><ul>
<li>datanode15.will.com     -&gt; NM，DN</li>
<li>1.103-prd-datanode14.will.com   -&gt;kafka, DN,NM</li>
<li>1.108-prd-datanode19.will.com -&gt; regionserver, DN,NM</li>
<li>1.110-prd-datanode21.will.com -&gt; NM, DN</li>
<li>1.99-prd-datanode10.will.com -&gt; kafka, DB, NM</li>
<li>1.113-fengkong.data.com 风控 -&gt; NONE</li>
<li>1.87-prd-datanode05.data.com -&gt; DN, NM, regionserver</li>
<li>1.97-prd-datanode08.data.com -&gt; DN, NM, regionserver</li>
<li>1.95-prd-datanode06.data.com -&gt; kafka, DN, NM, SB HbaseMaster</li>
<li>1.96-prd-datanode07.data.com -&gt; DN, NM, regionserver</li>
</ul>
<p>原有机器迁出会占新资源的内存32G <em> 10 + CPU</em>80</p>
<h5 id="原有机器资源升级"><a href="#原有机器资源升级" class="headerlink" title="原有机器资源升级"></a>原有机器资源升级</h5><p>在原有资源基础上，添加16G内存，CPU核数翻倍【把迁移出去的机器空闲出来的资源充分利用】：</p>
<ul>
<li>1.107-prd-datanode18.will.com</li>
<li>1.109-prd-datanode20.will.com</li>
<li>1.102-prd-datanode13.will.com</li>
<li>1.106-prd-datanode17.will.com</li>
<li>1.100-prd-datanode11.will.com</li>
<li>1.98-prd-datanode09.will.com</li>
<li>1.110-prd-datanode21.will.com</li>
<li>1.86-prd-datanode04.data.com</li>
<li>1.82-prd-datanode01.data.com</li>
<li>1.83-prd-datanode02.data.com</li>
<li>1.84-prd-datanode03.data.com</li>
<li>1.105-prd-datanode16.will.com</li>
<li>1.101-prd-datanode12.will.com</li>
</ul>
<p>理论上，对原有机器不会有额外资源要求，对新机器资源也不会有影响。</p>
<h2 id="需要新建的机器"><a href="#需要新建的机器" class="headerlink" title="需要新建的机器"></a>需要新建的机器</h2><h4 id="新服务节点"><a href="#新服务节点" class="headerlink" title="新服务节点"></a>新服务节点</h4><p>可按照service01.will.com开始弄主机名。</p>
<p>单机配置： 32G内存 + 8核 + 50G硬【整机空间即可】 + CentOS release 6.8 (Final)</p>
<p>机器数量： 8台<br>共占：内存32G <em> 8 + CPU8 </em> 8 = 内存256G + CPU 64</p>
<h4 id="新数据节点"><a href="#新数据节点" class="headerlink" title="新数据节点"></a>新数据节点</h4><p>目前最新数据节点的主机名编号到了21，新机器可以datanode22.will.com开始。</p>
<p>单机配置：32G内存 + 8核CPU + 640G硬盘【/server/目录挂载空间】 + CentOS release 6.8 (Final)</p>
<p>机器数量： 12【如果物理机资源不够，可以暂时弄10台，以后酌情扩容】</p>
<p>共占：内存32G <em> 12 + CPU8 </em> 12 = 内存384G + CPU 96</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/20/ambari中启用hive-ACID事务/" title="ambari中启用hive-ACID事务" itemprop="url">ambari中启用hive-ACID事务</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-20T12:21:45.000Z" itemprop="datePublished"> 发表于 2017-04-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>场景</p>
<ul>
<li>数据重新执行</li>
<li>数据流重新执行</li>
<li>逐渐改变的维度</li>
<li>维度历史变更</li>
</ul>
<p>标准sql通过insert、update、delete、事务还有最近出现的merge方式来提供acid操作。这些已经被证明足够使用的了。</p>
<h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><ul>
<li>事务表。hive支持单表事务，但是目标表必须需要声明是支持事务的。</li>
<li>分区表。hive支持表分区，把数据分开以提供快速查询。分区与ACID是相互独立的概念。通常大表都是有分区的。</li>
<li>ACID操作（insert/update/delete）。</li>
<li>主键</li>
<li>流式数据。数据可以通过storm、flume等流式进入hive的事务表中</li>
<li>优化并发。</li>
<li>压缩。数据必须时段性的被压缩一下，节省空间，优化数据访问。最好让系统自动处理这件事情。，不过也可以设置外部的调度器。</li>
</ul>
<h4 id="启用ACID事务"><a href="#启用ACID事务" class="headerlink" title="启用ACID事务"></a>启用ACID事务</h4><h4 id="hello"><a href="#hello" class="headerlink" title="hello"></a>hello</h4><p>创建一个事务表，并插入一些数据<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> hello_acid;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> hello_acid (<span class="keyword">key</span> <span class="built_in">int</span>, <span class="keyword">value</span> <span class="built_in">int</span>)</div><div class="line">partitioned <span class="keyword">by</span> (load_date <span class="built_in">date</span>)</div><div class="line">clustered <span class="keyword">by</span>(<span class="keyword">key</span>) <span class="keyword">into</span> <span class="number">3</span> buckets</div><div class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">'transactional'</span>=<span class="string">'true'</span>);</div></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hello_acid <span class="keyword">partition</span> (load_date=<span class="string">'2016-03-01'</span>) <span class="keyword">values</span> (<span class="number">1</span>, <span class="number">1</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hello_acid <span class="keyword">partition</span> (load_date=<span class="string">'2016-03-02'</span>) <span class="keyword">values</span> (<span class="number">2</span>, <span class="number">2</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> hello_acid <span class="keyword">partition</span> (load_date=<span class="string">'2016-03-03'</span>) <span class="keyword">values</span> (<span class="number">3</span>, <span class="number">3</span>);</div><div class="line"><span class="keyword">select</span> * <span class="keyword">from</span> hello_acid;</div></pre></td></tr></table></figure>
<p>删除数据：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> hello_acid <span class="keyword">where</span> <span class="keyword">key</span> = <span class="number">2</span>;</div><div class="line"><span class="keyword">select</span> * <span class="keyword">from</span> hello_acid;</div></pre></td></tr></table></figure></p>
<p>更新数据：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> hello_acid <span class="keyword">set</span> <span class="keyword">value</span> = <span class="number">10</span> <span class="keyword">where</span> <span class="keyword">key</span> = <span class="number">3</span>;</div><div class="line"><span class="keyword">select</span> * <span class="keyword">from</span> hello_acid;</div></pre></td></tr></table></figure></p>
<p>这些DML语句意在在大数据中执行少量修改，应该是记录级别的数据管理。如果你有小的多个批量修改，应该使用streaming data ingestion。</p>
<h4 id="Streaming-Data-Ingestion"><a href="#Streaming-Data-Ingestion" class="headerlink" title="Streaming Data Ingestion"></a>Streaming Data Ingestion</h4><p>许多时候我们需要处理连续的实时数据流，想要更方便的操作这些数据。hive可以自动把流式数据加入hive表中，也支持实时的数据抽取与查询。</p>
<p>现在hive提供两种：</p>
<ul>
<li>已经存在是<a href="https://github.com/apache/storm/tree/master/external/storm-hive" target="_blank" rel="external">storm hive bolt</a>方案，<a href="https://flume.apache.org/FlumeUserGuide.html#hive-sink" target="_blank" rel="external">flume hive sink</a>方案。这些工具对数据有较少的操作，重在转移数据</li>
<li>直接使用低级的<a href="https://cwiki.apache.org/confluence/display/Hive/Streaming+Data+Ingest" target="_blank" rel="external">Streaming Ingest API</a>.</li>
</ul>
<p>在使用streaming api之前，需要先创建好分区事务表。从查询角度来看，一切应该都是确定好的。</p>
<h4 id="4-实践"><a href="#4-实践" class="headerlink" title="4. 实践"></a>4. 实践</h4><p>插入几条数据对我们测试起来很简单，但是实际环境中，我们需要一次性处理几千或者几百万的数据。下面我们通过几个通用场景讨论一下怎样处理数据批次。</p>
<p>这些模型需要你建立一个主键。hive并不强制主键唯一，所以你必须在你app中控制一下。虽然hive2.1介绍了non-validating 外键，但是这东西目前还并没有被完整全面的验证过。</p>
<h5 id="4-1-searched-updates"><a href="#4-1-searched-updates" class="headerlink" title="4.1 searched updates"></a>4.1 searched updates</h5><p>hive  ACID支持searched updates，这是一种常见的更新方式。注意，基于hive ACID的架构，更新必须通过批处理的方式执行。一次更新一行这种事情在实际使用中是行不通的。如果想通过某种方式一次性更新大量数据，那么searched updates就可以帮你了。</p>
<p>假设有一个维度表，包含的一个flag，指示当前记录是不是最新的值。这样我们就可以沿时间追踪维度变更情况。当维度表发生更新的时候，我们就把已经存在记录的设置成old。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> mydim;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> mydim (<span class="keyword">key</span> <span class="built_in">int</span>, <span class="keyword">name</span> <span class="keyword">string</span>, zip <span class="keyword">string</span>, is_current <span class="built_in">boolean</span>)</div><div class="line">clustered <span class="keyword">by</span>(<span class="keyword">key</span>) <span class="keyword">into</span> <span class="number">3</span> buckets</div><div class="line"><span class="keyword">stored</span> <span class="keyword">as</span> orc tblproperties (<span class="string">'transactional'</span>=<span class="string">'true'</span>);</div></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> mydim <span class="keyword">values</span></div><div class="line">  (<span class="number">1</span>, <span class="string">'bob'</span>,   <span class="string">'95136'</span>, <span class="literal">true</span>),</div><div class="line">  (<span class="number">2</span>, <span class="string">'joe'</span>,   <span class="string">'70068'</span>, <span class="literal">true</span>),</div><div class="line">  (<span class="number">3</span>, <span class="string">'steve'</span>, <span class="string">'22150'</span>, <span class="literal">true</span>);</div></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">drop</span> <span class="keyword">table</span> <span class="keyword">if</span> <span class="keyword">exists</span> updates_staging_table;</div><div class="line"><span class="keyword">create</span> <span class="keyword">table</span> updates_staging_table (<span class="keyword">key</span> <span class="built_in">int</span>, newzip <span class="keyword">string</span>);</div><div class="line"><span class="keyword">insert</span> <span class="keyword">into</span> updates_staging_table <span class="keyword">values</span> (<span class="number">1</span>, <span class="number">87102</span>), (<span class="number">3</span>, <span class="number">45220</span>);</div></pre></td></tr></table></figure>
<p>执行更新,执行前后可以看一下维度表的数据变化，其实就是更新了flag而已。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">update</span> mydim <span class="keyword">set</span> is_current=<span class="literal">false</span></div><div class="line">  <span class="keyword">where</span> mydim.key <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">key</span> <span class="keyword">from</span> updates_staging_table);</div></pre></td></tr></table></figure></p>
<h5 id="4-2-searched-deletes"><a href="#4-2-searched-deletes" class="headerlink" title="4.2 searched deletes"></a>4.2 searched deletes</h5><p>批量删除也可以使用staging table轻松搞定。但是需要你在表之间放置一个公共key，有些类似RDBMS中的主键。<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">delete</span> <span class="keyword">from</span> mydim</div><div class="line"><span class="keyword">where</span> mydim.key <span class="keyword">in</span> (<span class="keyword">select</span> <span class="keyword">key</span> <span class="keyword">from</span> updates_staging_table);</div><div class="line"><span class="keyword">select</span> * <span class="keyword">from</span> mydim;</div></pre></td></tr></table></figure></p>
<h4 id="5-批量覆盖更新"><a href="#5-批量覆盖更新" class="headerlink" title="5. 批量覆盖更新"></a>5. 批量覆盖更新</h4><p>有的时候我们需要批量更新一些数据。例如第一种SCD更新或者数据重导。hive目前还不支持merge操作，在支持之前，我们只能考虑使用先删除后插入的方式，但这样可能会造成查询客户端脏读。或者也可以在重新清洗的时候临时停掉查询。</p>
<h4 id="6-ACID工具"><a href="#6-ACID工具" class="headerlink" title="6. ACID工具"></a>6. ACID工具</h4><p>ACID事务执行过程中会创建一系列的锁。事务和他们的事务锁可以通过hive的一些工具来查看。</p>
<h5 id="6-1-查看事务"><a href="#6-1-查看事务" class="headerlink" title="6.1 查看事务"></a>6.1 查看事务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">show transactions</div></pre></td></tr></table></figure>
<h5 id="6-1-查看锁"><a href="#6-1-查看锁" class="headerlink" title="6.1 查看锁"></a>6.1 查看锁</h5><p>有read, update，X lock。update锁与update操作互斥，但是与read兼容。X锁，与所有锁都互斥，属于独占。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">show locks</div></pre></td></tr></table></figure></p>
<h5 id="6-1-终止事务"><a href="#6-1-终止事务" class="headerlink" title="6.1 终止事务"></a>6.1 终止事务</h5><p>注意这并不会马上kill掉所有相关查询。ACID查询是周期性执行的，默认是2.5分钟，如果他们检测到自己的事务被kill，就会执行自动退出。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">abort transactions T1 T2 T3</div></pre></td></tr></table></figure></p>
<h4 id="7-性能考虑"><a href="#7-性能考虑" class="headerlink" title="7. 性能考虑"></a>7. 性能考虑</h4><ul>
<li>创建分区</li>
<li>insert快，update和delete都会比较慢一些，因为要扫描整个分区。</li>
<li>如果你们的工作里需要大量更新数据，那么请周期行执行compaction操作。不然数据大小会越来越大，查询也会越来越慢。</li>
</ul>
<h4 id="8-深入探究"><a href="#8-深入探究" class="headerlink" title="8. 深入探究"></a>8. 深入探究</h4><p>在使用之前一定要深入了解这套系统的工作原理，并且在你的可以容忍丢失的数据上执行测试。过程中也注意备份数据。</p>
<p>ACID表有一个隐藏字段<code>row__id</code>。这个系统内置的字段名称有可能会变。你应该构建一个基于这个字段的长远的方案。</p>
<p>这个字段记录的内容有：</p>
<ul>
<li>数据被insert或者update的active的事务id</li>
<li>数据所在bucket的bucketid</li>
<li>此次事务或者bucket中的rowid</li>
</ul>
<p>看一下<br><figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">hive&gt; select row__id from hello_acid;</div><div class="line">OK</div><div class="line">&#123;"transactionid":12,"bucketid":0,"rowid":0&#125;</div><div class="line">&#123;"transactionid":10,"bucketid":1,"rowid":0&#125;</div></pre></td></tr></table></figure></p>
<p>常用场景就是确认所有的数据都load进来了。假设上游数据provider认为hive里的持久化数据丢失了，那么你的provider(storm Bolt比如)就会告诉你插入这些数据的事务ID。然后我们可以使用这个事务id计算一下实际的记录数。查询的时候使用X替换掉你的事务ID<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">set</span> hive.optimize.ppd=<span class="literal">false</span>;</div><div class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> hello_acid <span class="keyword">where</span> row__id.transactionid = X;</div></pre></td></tr></table></figure></p>
<p>记牢，事务中插入的数据有可能会被后续的update或者delete语句影响到，所以如果count数并不符合，那就可能是这些因素造成的。</p>
<p>参考：</p>
<p><a href="https://hortonworks.com/hadoop-tutorial/using-hive-acid-transactions-insert-update-delete-data/" target="_blank" rel="external">https://hortonworks.com/hadoop-tutorial/using-hive-acid-transactions-insert-update-delete-data/</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/20/HIVE创建bucketed表/" title="HIVE创建bucketed表" itemprop="url">HIVE创建bucketed表</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-20T10:54:03.000Z" itemprop="datePublished"> 发表于 2017-04-20</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>建表语句<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE user_info_bucketed(user_id BIGINT, firstname STRING, lastname STRING)</div><div class="line">COMMENT &apos;A bucketed copy of user_info&apos;</div><div class="line">PARTITIONED BY(ds STRING)</div><div class="line">CLUSTERED BY(user_id) INTO 256 BUCKETS;</div></pre></td></tr></table></figure></p>
<p>基于字段user_id分桶的。</p>
<p>使用<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">set</span> hive.enforce.bucketing = <span class="literal">true</span>;  <span class="comment">-- (<span class="doctag">Note:</span> Not needed in Hive 2.x onward)</span></div><div class="line">FROM user_id</div><div class="line"><span class="keyword">INSERT</span> OVERWRITE <span class="keyword">TABLE</span> user_info_bucketed</div><div class="line"><span class="keyword">PARTITION</span> (ds=<span class="string">'2009-02-25'</span>)</div><div class="line"><span class="keyword">SELECT</span> userid, firstname, lastname <span class="keyword">WHERE</span> ds=<span class="string">'2009-02-25'</span>;</div></pre></td></tr></table></figure></p>
<p>hive怎样把row打散到不同的bucket中去的呢？一般是决定于hash函数，使用什么hash函数又取决于分桶字段的类型。比如整型字段就使用hash_int函数。</p>
<p>注意，如果分桶字段类型不同于insert进来的数据类型会出错的，或者手动使用不同类型的数据执行分桶操作也会出错。只要设置了<code>set hive.enforce.bucketing=true</code>，就能够正确的把数据发布到对应的地方。</p>
<p>再来一个例子<br>Bucketed Sorted Table<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> page_view(viewTime <span class="built_in">INT</span>, userid <span class="built_in">BIGINT</span>,</div><div class="line">     page_url <span class="keyword">STRING</span>, referrer_url <span class="keyword">STRING</span>,</div><div class="line">     ip <span class="keyword">STRING</span> <span class="keyword">COMMENT</span> <span class="string">'IP Address of the User'</span>)</div><div class="line"> <span class="keyword">COMMENT</span> <span class="string">'This is the page view table'</span></div><div class="line"> PARTITIONED <span class="keyword">BY</span>(dt <span class="keyword">STRING</span>, country <span class="keyword">STRING</span>)</div><div class="line"> CLUSTERED <span class="keyword">BY</span>(userid) SORTED <span class="keyword">BY</span>(viewTime) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS</div><div class="line"> <span class="keyword">ROW</span> <span class="keyword">FORMAT</span> <span class="keyword">DELIMITED</span></div><div class="line">   <span class="keyword">FIELDS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\001'</span></div><div class="line">   COLLECTION ITEMS <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\002'</span></div><div class="line">   <span class="keyword">MAP</span> <span class="keyword">KEYS</span> <span class="keyword">TERMINATED</span> <span class="keyword">BY</span> <span class="string">'\003'</span></div><div class="line"> <span class="keyword">STORED</span> <span class="keyword">AS</span> SEQUENCEFILE;</div></pre></td></tr></table></figure></p>
<p>这个表根据userid分桶，每个bucket中按照viewTime升序排列。这种组织结果允许用户更高效的抽取clustered的字段，这里就是userid字段了。排序属性让内部操作能够快速处理估算查询的任务。MAP KEYS和COLLECTION ITEMS关键词可以用来处理字段是list或者map的情况。</p>
<p>CLUSTERED BY 和 SORTED BY并不影响insert数据。用户需要注意reducer数必须要跟bucket的数量一致，在query语句中还要使用CLUSTER BY 和SORT BY语句。</p>
<p>还有一种倾斜表。适用于某些表中确认包含倾斜数据的情况。通过指定倾斜key，hive会把他们打散到不同的文件中。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> list_bucket_single (<span class="keyword">key</span> <span class="keyword">STRING</span>, <span class="keyword">value</span> <span class="keyword">STRING</span>)</div><div class="line">  SKEWED <span class="keyword">BY</span> (<span class="keyword">key</span>) <span class="keyword">ON</span> (<span class="number">1</span>,<span class="number">5</span>,<span class="number">6</span>) [<span class="keyword">STORED</span> <span class="keyword">AS</span> DIRECTORIES];</div></pre></td></tr></table></figure>
<p>下面这个是有两个倾斜key<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">CREATE TABLE list_bucket_multiple (col1 STRING, col2 int, col3 STRING)</div><div class="line">  SKEWED BY (col1, col2) ON ((&apos;s1&apos;,1), (&apos;s3&apos;,3), (&apos;s13&apos;,13), (&apos;s78&apos;,78)) [STORED AS DIRECTORIES];</div></pre></td></tr></table></figure></p>
<p>参考：</p>
<ul>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL+BucketedTables</a></li>
<li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-BucketedSortedTables" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-BucketedSortedTables</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/19/关于hive-update/" title="关于hive-update" itemprop="url">关于hive-update</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-19T13:49:54.000Z" itemprop="datePublished"> 发表于 2017-04-19</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h2 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h2><ul>
<li>A: atomicity。原子性，一个操作要么成功要么失败，不能存在中间状态。</li>
<li>C：consistency。一致性。一旦某个操作完成之后，后续所有的操作都能访问到上个操作完成之后的状态</li>
<li>I：isolation。隔离性。一个未完成的操作只对当前操作用户有影响，不能影响到其他用户</li>
<li>D：durability。持久性。一个操作完成之后，就会把成功的状态持久化，即便系统挂掉也要持久化。</li>
</ul>
<p>0.13版本之前，只在分区层次上支持原子性、一致性、持久性。隔离性通过调整可用的锁机制来达成(通过zk或者内存)。0.13之后完全支持了row级别的ACID，这样就可以在别人读取数据的时候添加行，也不会有影响。</p>
<h4 id="用例"><a href="#用例" class="headerlink" title="用例"></a>用例</h4><p>一般具有ACID特性的事务适用于以下场景</p>
<ul>
<li>流式数据接受。许多用户使用flume、storm、kafka等将数据放到hadoop集群中。这些工具可能会以每秒几百条甚至更多的速度写入。hive只能每十五分钟或者1个小时创建一个分区来接受他们。添加分区的策略会导致表里的分区暴增。也可以把流式数据放到既有的分区中，但是这会导致用户脏读（他们可能会看到一些他们开始查询时还没有的数据），还有就是会有很多的小文件产生，这对namenode也是一个很大的压力。有了ACID之后，就可以解决这个问题了</li>
<li>慢慢改变的维度表。在一个典型的星型数据仓库架构中，维度表可能会随着时间推移慢慢修改。例如，零售商可能会慢慢增开新店，那么新店就应该被添加到store表里，或者已经存在的店面可能会修改店面大小或者其他的因素。这些改变都会产生数据的insert或者update。从0.14开始，支持了</li>
<li>数据重导。有时手机来的数据发现不对，需要纠正。或者，开始有90%的数据，后面又有全量数据了。或者业务规则改变需要修改对应规则的一些数据。0.14之后用户可以insert、update、delete。</li>
<li>使用SQL MERGE语句批量更新。</li>
</ul>
<h4 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h4><ul>
<li>BEGIN,COMMIT,ROLLBACK目前还不支持。所有的语言都是auto-commit。后面版本会支持</li>
<li>只支持ORC的文件格式。</li>
<li>默认这种事务是关闭的。</li>
<li>数据表必须是bucketed的。外部表不支持ACID，因为它不受compactor的控制<a href="https://issues.apache.org/jira/browse/HIVE-13175" target="_blank" rel="external">HIVE-13175</a></li>
<li>在一个非ACID的session里执行ACID操作是不可以的。hive的事务manager必须设置成  <code>org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</code>才能使用ACID表。</li>
<li>目前只支持快照级别的隔离性。当一个查询开始后，会给它提供一个一致的数据快照。不支持dirty read, read committed, repeatable read, seializable。</li>
<li>已经存在的zk或者内存锁manager与此类事务并不兼容。这里不详说了，参考<a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions#HiveTransactions-BasicDesign" target="_blank" rel="external">这里</a></li>
<li><p>使用oracle作为元数据库可能会有些问题，设置<code>datanucleus.connectionPoolingType=DBCP</code></p>
<h4 id="语法改变"><a href="#语法改变" class="headerlink" title="语法改变"></a>语法改变</h4></li>
<li><p>添加了 <code>SHOW TRANSACTIONS</code>、<code>SHOW COMPACTIONS</code>、<code>ABORT TRANSACTIONS</code></p>
</li>
<li><code>SHOW LOCKS</code>命令改成了提供事务相关的锁的信息。如果在使用ak或者内存的lock manager的话，你会发现其实输出的东西没什么区别。</li>
<li>alter table的时候增加了一个是否执行表或者分区的压缩。通常用户不用管，系统会自动做这些事情。然而，如果compaction被关闭了，那么系统就不会自动处理了。alter table用来初始化压缩。具体参考<a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-AlterTable/PartitionCompact" target="_blank" rel="external">这里</a>。这会把压缩请求放进一个队列里。要看压缩进度的话，就使用<code>SHOW COMPACTIONS</code>语句。</li>
</ul>
<h4 id="基本设计"><a href="#基本设计" class="headerlink" title="基本设计"></a>基本设计</h4><p>HDFS是不支持直接修改文件的。当有人写数据到文件的时候也不提供读的一致性。为了提供这些特性，我们使用了其他的数据仓库工具。表和分区的数据存在一系列的base文件里。新的insert、update、delete存在delta文件中，读取的时候把这些更新都执行上。</p>
<h5 id="base和delta文件夹"><a href="#base和delta文件夹" class="headerlink" title="base和delta文件夹"></a>base和delta文件夹</h5><p>以前一个分区的所有文件都放在一个单独的目录下。ACID的writer会有一个base文件的目录，还有一堆delta文件的目录。下面是一个没有分区的数据表“t”<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hive&gt; dfs -ls -R /user/hive/warehouse/t;</div><div class="line">drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022</div><div class="line">-rw-r--r--   1 ekoifman staff        602 2016-06-09 17:03 /user/hive/warehouse/t/base_0000022/bucket_00000</div><div class="line">drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000</div><div class="line">-rw-r--r--   1 ekoifman staff        611 2016-06-09 17:06 /user/hive/warehouse/t/delta_0000023_0000023_0000/bucket_00000</div><div class="line">drwxr-xr-x   - ekoifman staff          0 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000</div><div class="line">-rw-r--r--   1 ekoifman staff        610 2016-06-09 17:07 /user/hive/warehouse/t/delta_0000024_0000024_0000/bucket_00000</div></pre></td></tr></table></figure></p>
<h5 id="compactor"><a href="#compactor" class="headerlink" title="compactor"></a>compactor</h5><p>compactor是在metastore中运行的一系列的后台进程，用来支持ACID系统。包含Initiator, Worker, Cleaner,AcidHouseKeeperService等。</p>
<h6 id="delta文件压缩"><a href="#delta文件压缩" class="headerlink" title="delta文件压缩"></a>delta文件压缩</h6><p>随着修改表数据的操作越来越多，delta文件也越来越多，需要压缩提高后面的处理效率。类似hbase，也是有minor compaction和major compaction</p>
<ul>
<li>Minor compaction把已经存在的同一个bucket的多个delta文件合并成一个</li>
<li>Majro compaction把bucket中的base文件和delta文件合并到一个新的base文件。这个会耗时多一些，不过也更高效。</li>
</ul>
<p>压缩工作是后台自动运行的，不会阻止并发的读写操作。执行压缩之后，系统会等用户读完所有的旧文件之后移除这些旧文件。</p>
<h6 id="Initiator"><a href="#Initiator" class="headerlink" title="Initiator"></a>Initiator</h6><p>这个模块是负责发现哪些table或者分区需要压缩。使用<code>hive.compactor.initiator.on</code>参数启用。每个compaction task负责处理一个分区(没分区的就是一个表)。如果连续压缩失败，超过<code>hive.compactor.initiator.failed.compacts.threshold</code>设置的阈值,就停止compaction了。</p>
<h6 id="Worker"><a href="#Worker" class="headerlink" title="Worker"></a>Worker</h6><p>一个Woker处理一个compaction task。通常是MR任务，名字格式是：主机名-compactor-数据库.表名.分区。使用<code>hive.compactor.worker.threads</code>设置每个metastore启动的compactor线程数。</p>
<h6 id="Cleaner"><a href="#Cleaner" class="headerlink" title="Cleaner"></a>Cleaner</h6><p>用来删除压缩之后，之前的一些旧文件</p>
<h6 id="AcidHouseKeeperService"><a href="#AcidHouseKeeperService" class="headerlink" title="AcidHouseKeeperService"></a>AcidHouseKeeperService</h6><p>找到心跳超时<code>hive.txn.timeout</code>的事务，终止它。释放资源。</p>
<h4 id="SHOW-COMPACTIONS"><a href="#SHOW-COMPACTIONS" class="headerlink" title="SHOW COMPACTIONS"></a>SHOW COMPACTIONS</h4><p>这个命令用来显示目前正在运行的compaction信息，还有最近的压缩历史。</p>
<h4 id="事务-锁-manager"><a href="#事务-锁-manager" class="headerlink" title="事务/锁 manager"></a>事务/锁 manager</h4><p>添加了新的逻辑实体<code>事务管理器</code>，包含原有的<code>database/table/partition lock manager</code>的概念在里面。原来默认的<code>hive.lock.manager</code>是<code>org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager</code>。新的事务管理器现在也负责管理事务锁了。默认的<code>DummyTxnManager</code>跟原有版本的hive一样：并没有事务，使用hive.lock.manager来为表、分区、数据库创建lock manager。新的<code>DbTxnManager</code>使用hive metastore中的<code>DbLockManager</code>管理所有的锁与事务（系统挂掉也不会碍事儿）。这就是说启用事务特性之后，并没有必要弄什么zookeeper来搞这些了。为了避免客户端挂掉、离开当前事务、或者锁悬停的情况，我们需要锁的拥有者和事务发起者以一定频率发送心跳给metastore。如果心跳超时几次，那么这个锁或者食物就会被终止。</p>
<p>在1.3.0版本中，DbLockManager会持续重试去获取锁。每次获取不到，都会double下一次获取锁的时间间隔，再重试，以支持短时间的查询，也降低metastore压力。假设初始等待时间是100ms，如果重试10次的话，总时间会是91m:42s:300ms。</p>
<p>注意DbTxnManager的lock manager会获取所有表的锁。默认情况下，对于非事务表的insert操作会获取一个排它锁，不允许其他的读写操作。虽然从技术角度来看是正确的，但是违背hive的传统工作方式的。为了向后兼容，提供了<code>hive.txn.strict.locking.mode</code>参数，可以让lock manager在对于非事务表insert操作的时候获取共享锁。这样就保证在读取的时候，不会出现数据被drop的情况。注意，对于事务表，insert操作是总会获取共享锁的，因为这些表在存储层支持MVCC架构，即便有并发写的时候也能够提供对于读的强一致性(快照隔离).</p>
<h6 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h6><p>客户端</p>
<ul>
<li>hive.support.concurrency - true</li>
<li>hive.enforce.bucketing - true(2.0版本后不需要)</li>
<li>hive.exec.dynamic.partition.mode - nonstrict</li>
<li>hive.txn.manager - org.apache.hadoop.hive.ql.lockmgr.DbTxnManager</li>
</ul>
<p>服务端(metastore)</p>
<ul>
<li>hive.compactor.initiator.on - true</li>
<li>hive.compactor.worker.threads - 正数就行，看情况</li>
</ul>
<h4 id="事务相关的新配置参数"><a href="#事务相关的新配置参数" class="headerlink" title="事务相关的新配置参数"></a>事务相关的新配置参数</h4><h4 id="表属性"><a href="#表属性" class="headerlink" title="表属性"></a>表属性</h4><p>如果一个表要使用ACID写操作，那么必须在表上设置<code>transactional=true</code>。从0.14.0开始，一旦一个表通过·TBLPROPERTIES (“transactional”=”true”)·定义为一个ACID表之后，就不能在改成非ACID表了，就是说不能执行TBLPROPERTIES (“transactional”=”false”) 这种修改。还有，hive.txn.manager必须设置为org.apache.hadoop.hive.ql.lockmgr.DbTxnManager, 可以再hive-site.xml里设置，也可以在session开始的时候设置。不然insert就会按照老的方式运行，delete操作会被禁止。</p>
<p>如果一个表的owner不想让系统自动决定什么时候compact，那就在表上设置<code>NO_AUTO_COMPACTION</code>。</p>
<p>在表创建之后表属性通过TBLPROPERTIES语句设置。</p>
<p>更多关于压缩的选项可以通过TBLPROPERTIES设置。可以在创建表时设置，也可以执行修改表的时候设置。例如，要覆盖一个压缩任务的MR参数，就可在创建表或者修改表的时候加上<code>compactor.&lt;mr property&gt;=xx</code>。</p>
<p>表级别设置<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> table_name (</div><div class="line">  <span class="keyword">id</span>                <span class="built_in">int</span>,</div><div class="line">  <span class="keyword">name</span>              <span class="keyword">string</span></div><div class="line">)</div><div class="line">CLUSTERED <span class="keyword">BY</span> (<span class="keyword">id</span>) <span class="keyword">INTO</span> <span class="number">2</span> BUCKETS <span class="keyword">STORED</span> <span class="keyword">AS</span> ORC</div><div class="line">TBLPROPERTIES (<span class="string">"transactional"</span>=<span class="string">"true"</span>,</div><div class="line">  <span class="string">"compactor.mapreduce.map.memory.mb"</span>=<span class="string">"2048"</span>,     <span class="comment">-- specify compaction map job properties</span></div><div class="line">  <span class="string">"compactorthreshold.hive.compactor.delta.num.threshold"</span>=<span class="string">"4"</span>,  <span class="comment">-- trigger minor compaction if there are more than 4 delta directories</span></div><div class="line">  <span class="string">"compactorthreshold.hive.compactor.delta.pct.threshold"</span>=<span class="string">"0.5"</span> <span class="comment">-- trigger major compaction if the ratio of size of delta files to</span></div><div class="line">                                                                   <span class="comment">-- size of base files is greater than 50%</span></div><div class="line">);</div></pre></td></tr></table></figure></p>
<p>单次请求时候设置<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">COMPACT</span> <span class="string">'minor'</span> </div><div class="line">   <span class="keyword">WITH</span> OVERWRITE TBLPROPERTIES (<span class="string">"compactor.mapreduce.map.memory.mb"</span>=<span class="string">"3072"</span>);  <span class="comment">-- specify compaction map job properties</span></div><div class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> table_name <span class="keyword">COMPACT</span> <span class="string">'major'</span></div><div class="line">   <span class="keyword">WITH</span> OVERWRITE TBLPROPERTIES (<span class="string">"tblprops.orc.compress.size"</span>=<span class="string">"8192"</span>);         <span class="comment">-- change any other Hive table properties</span></div></pre></td></tr></table></figure></p>
<p>参考：<br><a href="https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions" target="_blank" rel="external">https://cwiki.apache.org/confluence/display/Hive/Hive+Transactions</a></p>
<p><a href="https://hortonworks.com/hadoop-tutorial/using-hive-acid-transactions-insert-update-delete-data/" target="_blank" rel="external">https://hortonworks.com/hadoop-tutorial/using-hive-acid-transactions-insert-update-delete-data/</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/18/django中使用ipython-shell/" title="django中使用ipython-shell" itemprop="url">django中使用ipython-shell</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-18T10:29:15.000Z" itemprop="datePublished"> 发表于 2017-04-18</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>安装两个东西<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ipython</div><div class="line">django_extensions</div></pre></td></tr></table></figure></p>
<p>在django配置文件的<code>INSTALLED_APPS</code>中添加<code>jdango_extensions</code></p>
<p>命令行启动<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./manage.py shell_plus</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./manage.py shell_plus --notebook</div></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/17/gradle一次性配置所有的maven仓库/" title="gradle一次性配置所有的maven仓库" itemprop="url">gradle一次性配置所有的maven仓库</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-17T09:32:32.000Z" itemprop="datePublished"> 发表于 2017-04-17</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>在~/.gradle/init.gradle文件中添加<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">allprojects&#123;</div><div class="line">    repositories &#123;</div><div class="line">        def REPOSITORY_URL = &apos;http://maven.oschina.net/content/groups/public&apos;</div><div class="line">        all &#123; ArtifactRepository repo -&gt;</div><div class="line">            if(repo instanceof MavenArtifactRepository)&#123;</div><div class="line">                def url = repo.url.toString()</div><div class="line">                if (url.startsWith(&apos;https://repo1.maven.org/maven2&apos;) || url.startsWith(&apos;https://jcenter.bintray.com/&apos;)) &#123;</div><div class="line">                    project.logger.lifecycle &quot;Repository $&#123;repo.url&#125; replaced by $REPOSITORY_URL.&quot;</div><div class="line">                    remove repo</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        maven &#123;</div><div class="line">            url REPOSITORY_URL</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>参考：</p>
<ul>
<li><a href="https://docs.gradle.org/current/userguide/init_scripts.html" target="_blank" rel="external">https://docs.gradle.org/current/userguide/init_scripts.html</a></li>
<li><a href="https://yrom.net/blog/2015/02/07/change-gradle-maven-repo-url/" target="_blank" rel="external">https://yrom.net/blog/2015/02/07/change-gradle-maven-repo-url/</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/04/16/linkedin--gobblin/" title="linkedin--gobblin" itemprop="url">linkedin--gobblin</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-04-16T09:00:37.000Z" itemprop="datePublished"> 发表于 2017-04-16</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>这些年来，linkedin的数据设施团队为集成多种数据源到hadoop生态系统中提供了很多解决方案。目前，有15中整合pipeline正在运行，包含重要的数据质量、元数据管理、元数据开发、运维相关的调整等等。</p>
<p>这些挑战与经验促使我们构建了gobblin。gobblin是为从各种数据源【数据库、rest api、 ftp server， 文件等】抽取、转化、加载数据到hadoop上。gobblin包含了日常通用的数据ETL操作任务，如任务调度、任务分区、错误处理、状态管理、数据质量检测、数据发布等等。Gobblin把多种数据源集成进同样的执行框架中，然后在同一个地方管理不同的数据源的元数据。 还有自动扩容、容错、数据质量保证、扩展性高、处理数据模型演化等特性，gobblin很易用、而且可以自我服务，是一个高效的数据集成框架。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/page/12/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><span class="page-number current">13</span><a class="page-number" href="/page/14/">14</a><a class="page-number" href="/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/14/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  


  

  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/youdaonote/" title="youdaonote">youdaonote<sup>209</sup></a></li>
			
		
			
				<li><a href="/tags/源码/" title="源码">源码<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/akka/" title="akka">akka<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/flume/" title="flume">flume<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/ETL/" title="ETL">ETL<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/solr/" title="solr">solr<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/spring/" title="spring">spring<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/调度平台/" title="调度平台">调度平台<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/azkaban/" title="azkaban">azkaban<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/scala/" title="scala">scala<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ambari/" title="ambari">ambari<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/quartz/" title="quartz">quartz<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ember/" title="ember">ember<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/nodejs/" title="nodejs">nodejs<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/R/" title="R">R<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/guava/" title="guava">guava<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/heroku/" title="heroku">heroku<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hdfs/" title="hdfs">hdfs<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://github.com/willcup" target="_blank" title=" 我自己的github">github</a>
            
          </li>
        
          <li>
            
            	<a href="http://thisding.com" target="_blank" title="朋友的主页">Steven&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=null&verifier=&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Will Chen in MeiTuan. <br/>
			元 亨 利 贞.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
		<a href="mailto:chenxin15@meituan.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2017 
		
		<a href="/about" target="_blank" title="Will Chen">Will Chen</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fe6d1f421bbc9962127a50488f9ed37d1' type='text/javascript'%3E%3C/script%3E"));
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
