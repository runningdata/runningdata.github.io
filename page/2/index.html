
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5b99dfd487346155d274c0c49c3fb869";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>

  
    <title>Will&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Will Chen">
    

    
    <meta name="description" content="左右水色 右手天光">
<meta property="og:type" content="website">
<meta property="og:title" content="Will's Blog">
<meta property="og:url" content="http://willcup.com/page/2/index.html">
<meta property="og:site_name" content="Will's Blog">
<meta property="og:description" content="左右水色 右手天光">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Will's Blog">
<meta name="twitter:description" content="左右水色 右手天光">

    
    <link rel="alternative" href="/atom.xml" title="Will&#39;s Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Will&#39;s Blog" title="Will&#39;s Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Will&#39;s Blog">Will&#39;s Blog</a></h1>
				<h2 class="blog-motto">简易 变易 不易</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/about">关于</a></li>
					
					<li>
 					
					<form class="search" action="//google.com/search" method="get" accept-charset="utf-8">
						<label>Search</label>
						<input type="search" id="search" name="q" autocomplete="off" maxlength="20" placeholder="搜索" />
						<input type="hidden" name="q" value="site:willcup.com">
					</form>
					
					</li>
				</ul>
			</nav>			
</div>
    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/15/presto命令行/" title="presto命令行" itemprop="url">presto命令行</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-15T06:28:04.000Z" itemprop="datePublished"> 发表于 2017-12-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/15/presto之hive-connector/" title="presto之hive-connector" itemprop="url">presto之hive-connector</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-15T03:40:46.000Z" itemprop="datePublished"> 发表于 2017-12-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h4><p>hive是由三个部分组成的</p>
<ul>
<li>特定格式的HDFS文件</li>
<li>metadata数据库，通常是mysql</li>
<li>HQL及其执行引擎</li>
</ul>
<p>presto会用到前面两者。</p>
<h4 id="支持的文件类型"><a href="#支持的文件类型" class="headerlink" title="支持的文件类型"></a>支持的文件类型</h4><p>ORC、Parquet、Avro、RCFile、SequenceFile、JSON、Text</p>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><p>创建<code>etc/catalog/hive.properties</code>文件，挂载<code>hive-hadoop2</code> connector作为<code>hive</code> catalog，替换掉你的hive metastore的thrift host和port：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">connector.name=hive-hadoop2</div><div class="line">hive.metastore.uri=thrift://example.net:9083</div></pre></td></tr></table></figure>
<h4 id="多个hive集群"><a href="#多个hive集群" class="headerlink" title="多个hive集群"></a>多个hive集群</h4><p>随便你使用多少catalog，添加其他的hive 集群就是了，新增properties文件到<code>etc/catalog</code>就可以了。</p>
<h4 id="HDFS配置"><a href="#HDFS配置" class="headerlink" title="HDFS配置"></a>HDFS配置</h4><p>对于基本设置，presto自动配置了HDFS client，不需要额外的配置文件。但是对于hdfs联邦和NN HA的情况，需要额外指定一下参数才能正常访问HDFS cluster。这个要添加<code>hive.config.resources</code>引用到你的hdfs配置文件中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">hive.config.resources=/etc/hadoop/conf/core-site.xml,/etc/hadoop/conf/hdfs-site.xml</div></pre></td></tr></table></figure></p>
<p>官方建议尽量少添加配置项，多余的配置项可能更容易引起问题。</p>
<p>还有就是这些配置文件必须在每个presto节点上都是有效存在的。</p>
<h4 id="HDFS用户"><a href="#HDFS用户" class="headerlink" title="HDFS用户"></a>HDFS用户</h4><p>在没有整合Kerberos的HDFS中，presto会使用presto进程的运行用户访问HDFS。我们可以通过配置presto的JVM参数指定访问HDFS的用户。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">-DHADOOP_USER_NAME=hdfs_user</div></pre></td></tr></table></figure></p>
<h4 id="访问带有kerberos认证的HDFS"><a href="#访问带有kerberos认证的HDFS" class="headerlink" title="访问带有kerberos认证的HDFS"></a>访问带有kerberos认证的HDFS</h4><p>kerberos认证对于HDFS和hive metastore是都支持的。但是，通过ticket cache进行认证的还没有支持。</p>
<h4 id="hive配置项"><a href="#hive配置项" class="headerlink" title="hive配置项"></a>hive配置项</h4><table>
<thead>
<tr>
<th>Property</th>
<th>Name</th>
<th>Description    Default</th>
</tr>
</thead>
<tbody>
<tr>
<td>hive.metastore.uri</td>
<td>Hive metastore 的thrift URI. 如果多个的话，默认使用第一个，后面的作为备用，逗号隔开。Example: thrift://192.0.2.3:9083 or thrift://192.0.2.3:9083,thrift://192.0.2.4:9083</td>
<td></td>
</tr>
<tr>
<td>hive.config.resources</td>
<td>逗号分隔的HDFS配置文件, 每个presto server所在的节点都要有效存在.  Example: /etc/hdfs-site.xml</td>
<td></td>
</tr>
<tr>
<td>hive.storage-format</td>
<td>创建新表的默认文件格式</td>
<td>RCBINARY</td>
</tr>
<tr>
<td>hive.compression-codec</td>
<td>写文件的压缩格式</td>
<td>GZIP</td>
</tr>
<tr>
<td>hive.force-local-scheduling</td>
<td>Force splits to be scheduled on the same node as the Hadoop DataNode process serving the split data. This is useful for installations where Presto is collocated with every DataNode.</td>
<td>false</td>
</tr>
<tr>
<td>hive.respect-table-format</td>
<td>Should new partitions be written using the existing table format or the default Presto format?</td>
<td>true</td>
</tr>
<tr>
<td>hive.immutable-partitions</td>
<td>新数据能不能insert到已经存在的partitions?</td>
<td>false</td>
</tr>
<tr>
<td>hive.max-partitions-per-writers</td>
<td>每个writer的最大partition数.</td>
<td>100</td>
</tr>
<tr>
<td>hive.max-partitions-per-scan</td>
<td>单个table scan可扫描的最大partition数</td>
<td>100,000</td>
</tr>
<tr>
<td>hive.metastore.authentication.type</td>
<td>Hive metastore认证类型,可是是 NONE 或者 KERBEROS.</td>
<td>NONE</td>
</tr>
<tr>
<td>hive.metastore.service.principal</td>
<td>The Kerberos principal of the Hive metastore service.</td>
<td></td>
</tr>
<tr>
<td>hive.metastore.client.principal</td>
<td>The Kerberos principal that Presto will use when connecting to the Hive metastore service.</td>
<td></td>
</tr>
<tr>
<td>hive.metastore.client.keytab</td>
<td>Hive metastore client keytab 位置.</td>
<td></td>
</tr>
<tr>
<td>hive.hdfs.authentication.type</td>
<td>HDFS 认证类型，可以是 NONE 或者 KERBEROS.</td>
<td>NONE</td>
</tr>
<tr>
<td>hive.hdfs.impersonation.enabled</td>
<td>启用 HDFS终端user可以假装.</td>
<td>false</td>
</tr>
<tr>
<td>hive.hdfs.presto.principal</td>
<td>The Kerberos principal that Presto will use when connecting to HDFS.</td>
<td></td>
</tr>
<tr>
<td>hive.hdfs.presto.keytab</td>
<td>HDFS client keytab 位置.</td>
<td></td>
</tr>
<tr>
<td>hive.security</td>
<td>参考Hive Security Configuration.</td>
<td></td>
</tr>
<tr>
<td>security.config-file</td>
<td>Path of config file to use when hive.security=file. See File Based Authorization for details.</td>
<td></td>
</tr>
<tr>
<td>hive.non-managed-table-writes-enabled</td>
<td>Enable writes to non-managed (external) Hive tables.</td>
<td>false</td>
</tr>
</tbody>
</table>
<h4 id="schema演化"><a href="#schema演化" class="headerlink" title="schema演化"></a>schema演化</h4><p>hive允许同一个table的不同partition的数据有不同的schema。有的时候字段类型也有变化，Hive connector 也支持。</p>
<ul>
<li>tinyint, smallint, integer，bigint与varchar 之间的相互转化</li>
<li>real转为double</li>
<li>int类型的扩展转换，比如tinyint、smallint。</li>
</ul>
<p>和hive一样，转化失败就会返回null，比如把’foo’转为int</p>
<h4 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h4><p>hive connector支持查询与操作hive表和库。然而一些特殊的操作还是需要通过hive执行。</p>
<p>下面创建一个指定位置的hive库：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">SCHEMA</span> hive.web</div><div class="line"><span class="keyword">WITH</span> (location = <span class="string">'s3://my-bucket/'</span>)</div></pre></td></tr></table></figure></p>
<p>创建一个ORC格式存储的page_views 表，使用date和conuntry进行分区，根据user进行分桶，共计50个桶(注意分区字段必须是最后的字段)</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive.web.page_views (</div><div class="line">  view_time <span class="keyword">timestamp</span>,</div><div class="line">  user_id <span class="built_in">bigint</span>,</div><div class="line">  page_url <span class="built_in">varchar</span>,</div><div class="line">  ds <span class="built_in">date</span>,</div><div class="line">  country <span class="built_in">varchar</span></div><div class="line">)</div><div class="line"><span class="keyword">WITH</span> (</div><div class="line">  <span class="keyword">format</span> = <span class="string">'ORC'</span>,</div><div class="line">  partitioned_by = <span class="built_in">ARRAY</span>[<span class="string">'ds'</span>, <span class="string">'country'</span>],</div><div class="line">  bucketed_by = <span class="built_in">ARRAY</span>[<span class="string">'user_id'</span>],</div><div class="line">  bucket_count = <span class="number">50</span></div><div class="line">)</div></pre></td></tr></table></figure>
<p>删除分区<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">DELETE</span> <span class="keyword">FROM</span> hive.web.page_views</div><div class="line"><span class="keyword">WHERE</span> ds = <span class="built_in">DATE</span> <span class="string">'2016-08-09'</span></div><div class="line">  <span class="keyword">AND</span> country = <span class="string">'US'</span></div></pre></td></tr></table></figure></p>
<p>查询：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> hive.web.page_views</div></pre></td></tr></table></figure></p>
<p>创建外部表：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> hive.web.request_logs (</div><div class="line">  request_time <span class="keyword">timestamp</span>,</div><div class="line">  <span class="keyword">url</span> <span class="built_in">varchar</span>,</div><div class="line">  ip <span class="built_in">varchar</span>,</div><div class="line">  user_agent <span class="built_in">varchar</span></div><div class="line">)</div><div class="line"><span class="keyword">WITH</span> (</div><div class="line">  <span class="keyword">format</span> = <span class="string">'TEXTFILE'</span>,</div><div class="line">  external_location = <span class="string">'s3://my-bucket/data/logs/'</span></div><div class="line">)</div></pre></td></tr></table></figure></p>
<p>删除外部表，只会删除metadata，不会删除真实数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">DROP TABLE hive.web.request_logs</div></pre></td></tr></table></figure>
<h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><p>delete只支持where语句能满足整个分区的情况，也就是说只能一个分区为单位进行delete。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/15/yarn的节点label/" title="yarn的节点label" itemprop="url">yarn的节点label</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-15T02:59:13.000Z" itemprop="datePublished"> 发表于 2017-12-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>nodel label是用来把一些共同特征的节点进行归组的手段，然后任务可以指定label要求，运行到指定label的节点上。</p>
<p>现在只支持node partition：</p>
<p>一个node只有一个node partition，所以一个集群被分为多个子集，这些子集互不相交。默认情况下，node是属于DEFAULT partition的。用户可以配置每个partition对于queue中可用多少资源。具体需要看下一节。</p>
<p>有两种不同的node partition：</p>
<ul>
<li>exclusive： container会分配在node label完全匹配的node上。(例如指定了partition=”x”，那么就被分配到partition=“x”的node上，默认的就分配到默认的node上)</li>
<li>non-exclusive: 如果一个partition是non-exclusive的，它就可以把空闲资源分配给请求DEFAULT partition的。</li>
</ul>
<p>用户可以为每个queue指定可以访问的多个node label，app只能使用这个queue能使用的node label的node。</p>
<h4 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h4><ul>
<li>Partition cluster - 每个node都可以有一个label，这样cluster就会被划分为包含多个不相交的子集partition.</li>
<li>ACL of node-labels on queues - 用户可以指定每个queue可以访问的label的node。</li>
<li>指定一个queue可以访问某个label的比例 - 例如queue A可以使用label=hbase的30%的node resource。</li>
<li>在resource request中指定nodel label的要求, 那么就只会分配这个label属性的节点资源。如果什么都没有指定的话，就默认为DEFAULT partition。</li>
<li>运维相关<ul>
<li>nodel label和node label的映射关系可以通过RM restart进行恢复</li>
<li>更新node label - admin可在RM运行态更新node和queue的label</li>
</ul>
</li>
</ul>
<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><h5 id="设置ResourceManager启用Node-Labels"><a href="#设置ResourceManager启用Node-Labels" class="headerlink" title="设置ResourceManager启用Node Labels:"></a>设置ResourceManager启用Node Labels:</h5><p>yarn-site.xml</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>yarn.node-labels.fs-store.root-dir</td>
<td>hdfs://namenode:port/path/to/store/node-labels/</td>
</tr>
<tr>
<td>yarn.node-labels.enabled</td>
<td>true</td>
</tr>
</tbody>
</table>
<p>注意:</p>
<p>确保<code>yarn.node-labels.fs-store.root-dir</code>已经存在，而且RM有权限访问(一般是yarn用户访问)</p>
<p>如果用户想把node label存储在RM的本地文件系统，就是用<code>file:///home/yarn/node-label</code></p>
<h6 id="添加-修改-node-labels-list和node-to-labels-mapping-to-YARN"><a href="#添加-修改-node-labels-list和node-to-labels-mapping-to-YARN" class="headerlink" title="添加/修改 node labels list和node-to-labels mapping to YARN"></a>添加/修改 node labels list和node-to-labels mapping to YARN</h6><p>这是两个连续的步骤。</p>
<ul>
<li><p>添加Node labels池到cluster中，后面给node添加的label只能是这里面的label，并不是给集群所有节点默认label，这个要注意一下:</p>
<ul>
<li>执行<code>yarn rmadmin -addToClusterNodeLabels &quot;label_1(exclusive=true/false),label_2(exclusive=true/false)</code> 添加 node label.</li>
<li>如果用户没有指定 “(exclusive=…)”, execlusive默认为true.</li>
<li>运行<code>yarn cluster --list-node-labels</code>，检查新加的node label是否已生效</li>
</ul>
</li>
<li><p>给node添加label</p>
<ul>
<li>执行<code>yarn rmadmin -replaceLabelsOnNode “node1[:port]=label1 node2=label2”</code>. 给node1添加label1,给node2添加label2. 如果用户没有指定port, 就给这个node上运行的所有的NodeManagers都添加.</li>
</ul>
</li>
</ul>
<h5 id="给node-labels配置Schedulers"><a href="#给node-labels配置Schedulers" class="headerlink" title="给node labels配置Schedulers"></a>给node labels配置Schedulers</h5><h6 id="Capacity-Scheduler-配置"><a href="#Capacity-Scheduler-配置" class="headerlink" title="Capacity Scheduler 配置"></a>Capacity Scheduler 配置</h6><table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.capacity</queue-path></td>
<td>Set the percentage of the queue can access to nodes belong to DEFAULT partition. The sum of DEFAULT capacities for direct children under each parent, must be equal to 100.</td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.accessible-node-labels</queue-path></td>
<td>Admin need specify labels can be accessible by each queue, split by comma, like “hbase,storm” means queue can access label hbase and storm. All queues can access to nodes without label, user don’t have to specify that. If user don’t specify this field, it will inherit from its parent. If user want to explicitly specify a queue can only access nodes without labels, just put a space as the value.</td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.accessible-node-labels.<label>.capacity</label></queue-path></td>
<td>Set the percentage of the queue can access to nodes belong to <label> partition . The sum of <label> capacities for direct children under each parent, must be equal to 100. By default, it’s 0.</label></label></td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.accessible-node-labels.<label>.maximum-capacity</label></queue-path></td>
<td>Similar to yarn.scheduler.capacity.<queue-path>.maximum-capacity, it is for maximum-capacity for labels of each queue. By default, it’s 100.</queue-path></td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.default-node-label-expression</queue-path></td>
<td>Value like “hbase”, which means: if applications submitted to the queue without specifying node label in their resource requests, it will use “hbase” as default-node-label-expression. By default, this is empty, so application will get containers from nodes without label.</td>
</tr>
</tbody>
</table>
<p>node label配置示例:</p>
<p>假设我们的queue是这样的</p>
<pre><code>           root
       /     |    \
engineer    sales  marketing
</code></pre><p>We have 5 nodes (hostname=h1..h5) in the cluster, each of them has 24G memory, 24 vcores. 1 among the 5 nodes has GPU (assume it’s h5). So admin added GPU label to h5.</p>
<p>我们集群中有5个node，每个node有24G内存，24个vcore。其中一个node h5有GPU，那么admin想给很h5添加一个GPU的label。</p>
<p>用户就可以这样配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">yarn.scheduler.capacity.root.queues=engineering,marketing,sales</div><div class="line">yarn.scheduler.capacity.root.engineering.capacity=33</div><div class="line">yarn.scheduler.capacity.root.marketing.capacity=34</div><div class="line">yarn.scheduler.capacity.root.sales.capacity=33</div><div class="line"></div><div class="line">yarn.scheduler.capacity.root.engineering.accessible-node-labels=GPU</div><div class="line">yarn.scheduler.capacity.root.marketing.accessible-node-labels=GPU</div><div class="line"></div><div class="line">yarn.scheduler.capacity.root.engineering.accessible-node-labels.GPU.capacity=50</div><div class="line">yarn.scheduler.capacity.root.marketing.accessible-node-labels.GPU.capacity=50</div><div class="line"></div><div class="line">yarn.scheduler.capacity.root.engineering.default-node-label-expression=GPU</div></pre></td></tr></table></figure></p>
<p>我们设置了<code>root.engineering/marketing/sales.capacity=33</code>, 这样每个队列都保证有33%的资源,也就是 24 <em> 4 </em> (1/3) = (32G mem, 32 v-cores).</p>
<p>只有engineering/marketing queue 有权限使用GPU partition (因为 root.<queue-name>.accessible-node-labels).</queue-name></p>
<p>engineering/marketing queue 都可以使用partition=GPU的 1/2资源，也就是h5的1/2的资源，也就是24 * 0.5 = (12G mem, 12 v-cores).</p>
<p>注意:</p>
<p>配置完后，要执行<code>yarn rmadmin -refreshQueues</code>来使配置生效, 然后去RM的web UI检查一下。</p>
<h4 id="为APP指定node-label"><a href="#为APP指定node-label" class="headerlink" title="为APP指定node label"></a>为APP指定node label</h4><p>可以通过java API指定node label的要求。</p>
<ul>
<li>ApplicationSubmissionContext.setNodeLabelExpression(..) 设置node label表达式</li>
<li>ResourceRequest.setNodeLabelExpression(..) 设置单独resource request的node label表达式, 这个可以overwrite ApplicationSubmissionContext的表达式设置</li>
<li>在ApplicationSubmissionContext中的setAMContainerResourceRequest.setNodeLabelExpressio用来定位app master container要求的node label</li>
</ul>
<h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><h5 id="通过UI监控"><a href="#通过UI监控" class="headerlink" title="通过UI监控"></a>通过UI监控</h5><p>web UI提供的指标:</p>
<p>Nodes page: <a href="http://RM-Address:port/cluster/nodes" target="_blank" rel="external">http://RM-Address:port/cluster/nodes</a><br>Node labels page: <a href="http://RM-Address:port/cluster/nodelabels,可以获取到类型" target="_blank" rel="external">http://RM-Address:port/cluster/nodelabels,可以获取到类型</a> (exclusive/non-exclusive), active node managers的数量, 每个partition的总资源<br>Scheduler page: <a href="http://RM-Address:port/cluster/scheduler" target="_blank" rel="external">http://RM-Address:port/cluster/scheduler</a>, 每个queue关于label的配置</p>
<h5 id="通过命令行监控"><a href="#通过命令行监控" class="headerlink" title="通过命令行监控"></a>通过命令行监控</h5><ul>
<li>yarn cluster –list-node-labels</li>
<li>yarn node -status <nodeid> </nodeid></li>
</ul>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>YARN Capacity Scheduler, if you need more understanding about how to configure Capacity Scheduler<br>Write YARN application using node labels, you can see following two links as examples: YARN distributed shell, Hadoop MapReduce</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/14/slider源码走读1/" title="slider源码走读1" itemprop="url">slider源码走读1</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-14T07:12:59.000Z" itemprop="datePublished"> 发表于 2017-12-14</time>
    
  </p>
</header>
    <div class="article-content">
        
        
        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/12/hive-on-spark配置/" title="hive-on-spark配置" itemprop="url">hive-on-spark配置</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-12T10:15:55.000Z" itemprop="datePublished"> 发表于 2017-12-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h4><p>hive on spark默认也是支持spark on yarn的模式的。</p>
<h4 id="配置YARN"><a href="#配置YARN" class="headerlink" title="配置YARN"></a>配置YARN</h4><p>不能使用capacity scheduler，必须使用fair scheduler【….看到这里就想放弃了】</p>
<p>yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</p>
<h4 id="配置Hive"><a href="#配置Hive" class="headerlink" title="配置Hive"></a>配置Hive</h4><h5 id="添加spark依赖："><a href="#添加spark依赖：" class="headerlink" title="添加spark依赖："></a>添加spark依赖：</h5><ul>
<li>hive2.2.0之前，把spark-assembly jar链接到HIVE_HOME/lib中</li>
<li>2.2.0之后，hive on spark需要运行在spark 2.0.0以上的版本，没有assembly jar包 <ul>
<li>要运行YARN模式的话，添加下面的包到HIVE_HOME/lib<ul>
<li>scala-library</li>
<li>spark-core</li>
<li>spark-network-common</li>
</ul>
</li>
<li>运行local模式的话，添加下面包的链接到hive_home/lib中<ul>
<li>chill-java  chill  jackson-module-paranamer  jackson-module-scala  jersey-container-servlet-core</li>
<li>jersey-server  json4s-ast  kryo-shaded  minlog  scala-xml  spark-launcher</li>
<li>spark-network-shuffle  spark-unsafe  xbean-asm5-shaded</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="配置hive的执行引擎"><a href="#配置hive的执行引擎" class="headerlink" title="配置hive的执行引擎"></a>配置hive的执行引擎</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">set hive.execution.engine=spark;</div></pre></td></tr></table></figure>
<h5 id="配置spark相关的参数"><a href="#配置spark相关的参数" class="headerlink" title="配置spark相关的参数"></a>配置spark相关的参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">set spark.master=&lt;Spark Master URL&gt;</div><div class="line">set spark.eventLog.enabled=true;</div><div class="line">set spark.eventLog.dir=&lt;Spark event log folder (must exist)&gt;</div><div class="line">set spark.executor.memory=512m;             </div><div class="line">set spark.serializer=org.apache.spark.serializer.KryoSerializer;</div></pre></td></tr></table></figure>
<p>hive 2.2.0以前，要把spark-assembly jar到HDS中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;spark.yarn.jar&lt;/name&gt;</div><div class="line">  &lt;value&gt;hdfs://xxxx:8020/spark-assembly.jar&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>2.2.0以上版本后, 把$SPARK_HOME/jars中的所有jar都上传到hdfs指定文件夹中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;spark.yarn.jars&lt;/name&gt;</div><div class="line">  &lt;value&gt;hdfs://xxxx:8020/spark-jars/*&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<h4 id="配置spark"><a href="#配置spark" class="headerlink" title="配置spark"></a>配置spark</h4><table>
<thead>
<tr>
<th>属性</th>
<th>推荐设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.executor.cores</td>
<td>Between 5-7, See tuning details section</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores) </td>
</tr>
<tr>
<td>spark.yarn.executor.memoryOverhead</td>
<td>15-20% of spark.executor.memory</td>
</tr>
<tr>
<td>spark.executor.instances</td>
<td>Depends on spark.executor.memory + spark.yarn.executor.memoryOverhead, see tuning details section.</td>
</tr>
</tbody>
</table>
<h4 id="推荐配置"><a href="#推荐配置" class="headerlink" title="推荐配置"></a>推荐配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">mapreduce.input.fileinputformat.split.maxsize=750000000</div><div class="line">hive.vectorized.execution.enabled=true</div><div class="line"></div><div class="line">hive.cbo.enable=true</div><div class="line">hive.optimize.reducededuplication.min.reducer=4</div><div class="line">hive.optimize.reducededuplication=true</div><div class="line">hive.orc.splits.include.file.footer=false</div><div class="line">hive.merge.mapfiles=true</div><div class="line">hive.merge.sparkfiles=false</div><div class="line">hive.merge.smallfiles.avgsize=16000000</div><div class="line">hive.merge.size.per.task=256000000</div><div class="line">hive.merge.orcfile.stripe.level=true</div><div class="line">hive.auto.convert.join=true</div><div class="line">hive.auto.convert.join.noconditionaltask=true</div><div class="line">hive.auto.convert.join.noconditionaltask.size=894435328</div><div class="line">hive.optimize.bucketmapjoin.sortedmerge=false</div><div class="line">hive.map.aggr.hash.percentmemory=0.5</div><div class="line">hive.map.aggr=true</div><div class="line">hive.optimize.sort.dynamic.partition=false</div><div class="line">hive.stats.autogather=true</div><div class="line">hive.stats.fetch.column.stats=true</div><div class="line">hive.vectorized.execution.reduce.enabled=false</div><div class="line">hive.vectorized.groupby.checkinterval=4096</div><div class="line">hive.vectorized.groupby.flush.percent=0.1</div><div class="line">hive.compute.query.using.stats=true</div><div class="line">hive.limit.pushdown.memory.usage=0.4</div><div class="line">hive.optimize.index.filter=true</div><div class="line">hive.exec.reducers.bytes.per.reducer=67108864</div><div class="line">hive.smbjoin.cache.rows=10000</div><div class="line">hive.exec.orc.default.stripe.size=67108864</div><div class="line">hive.fetch.task.conversion=more</div><div class="line">hive.fetch.task.conversion.threshold=1073741824</div><div class="line">hive.fetch.task.aggr=false</div><div class="line">mapreduce.input.fileinputformat.list-status.num-threads=5</div><div class="line">spark.kryo.referenceTracking=false</div><div class="line">spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch</div></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/12/docker容器的时间同步/" title="docker容器的时间同步" itemprop="url">docker容器的时间同步</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-12T08:02:33.000Z" itemprop="datePublished"> 发表于 2017-12-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>最近在测试环境上上了一个新的组件，发现docker容器内的时间是有问题的，就是总比真正的时间慢4分钟左右。然后就exec进去安装了ntpdate工具进行时间同步工作。</p>
<p>然后收到了如下信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@1462539f1dc2:/# ntpdate -u cn.pool.ntp.org</div><div class="line">12 Dec 15:57:35 ntpdate[502]: step-systime: Operation not permitted</div></pre></td></tr></table></figure></p>
<p>看到是这个操作被禁止了，而且在docker里是用root执行的，竟然被禁止了。所以考虑到应该是docker对于苏初级的保护。返回宿主机，查看时间…果然与docker容器内一致，然后到宿主机安装ntpdate，进行时间同步之后，docker内时间自然就正常了。</p>
<p>回想一下docker原理：Docker本质上是宿主机（Linux）上的进程，通过namespace实现资源隔离，通过cgroups实现资源限制，通过写时复用机制(copy-on-write)实现高效的文件操作。归根到底还是调用的宿主机的东西，并不像虚拟机自己有一套完整的机制。</p>
<p><a href="https://kasheemlew.github.io/2017/05/18/docker-linux/" target="_blank" rel="external">https://kasheemlew.github.io/2017/05/18/docker-linux/</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/tez-UI配置/" title="tez-UI配置" itemprop="url">tez-UI配置</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T10:43:28.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>将tez根目录下的war包copy到tomcat的webapps目录中（0.8.5版本有两个ui包,但是没有感觉到有什么区别）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp ~/apache-tez-0.8.5-bin/tez-ui-0.8.5.war /server/tomcat9/webapps</div></pre></td></tr></table></figure></p>
<p>重启tomcat，war包会自动发布。修改配置文件里的timeline-service和resourcemanager的位置信息后，重启tomcat。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">timeline: &quot;http://datanode04.will.com:8188&quot;,</div><div class="line">rm: &quot;http://datanode02.will.com:8088&quot;,</div></pre></td></tr></table></figure></p>
<h4 id="配置tez-site-xml"><a href="#配置tez-site-xml" class="headerlink" title="配置tez-site.xml"></a>配置tez-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;tez.tez-ui.history-url.base&lt;/name&gt;</div><div class="line">  &lt;value&gt;http://schedule.will.com:8181/tez-ui-0.8.5&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;  </div><div class="line">    &lt;name&gt;tez.history.logging.service.class&lt;/name&gt;  </div><div class="line">    &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt;  </div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>设置yarn-site.xml中timeline-service的cros属性为true。之后重启timeline-service</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yarn.timeline-service.http-cross-origin.enabled = true</div></pre></td></tr></table></figure>
<p>其实tez ui就是把所有tez类型的app历史拉出来，按照tez的特性进行定制化的完美展示。</p>
<p>参考：</p>
<ul>
<li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=61331897" target="_blank" rel="external">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=61331897</a></li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html" target="_blank" rel="external">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/hive2-on-tez问题/" title="hive2-on-tez问题" itemprop="url">hive2-on-tez问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T09:16:51.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>hive cli端<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hive&gt; set hive.execution.engine;</div><div class="line">hive.execution.engine=tez</div><div class="line">hive&gt; select count(1) as num, year(create_time) as time from money_record group by year(create_time);</div><div class="line">Query ID = root_20171211171347_57cea87e-f54a-457d-93cf-b312063a822e</div><div class="line">Total jobs = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask</div></pre></td></tr></table></figure></p>
<p>yarn app的log信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">............</div><div class="line">............</div><div class="line"># container文件内容</div><div class="line"></div><div class="line">7749233 1292 -r-xr-xr-x   1 yarn     hadoop    1319613 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/tez-dag-0.8.5.jar</div><div class="line">37749246    8 -r-xr-xr-x   1 yarn     hadoop       7734 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/tez-yarn-timeline-history-with-acls-0.8.5.jar</div><div class="line">37749242  144 -r-xr-xr-x   1 yarn     hadoop     146572 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/tez-tests-0.8.5.jar</div><div class="line">37749230    4 drwxr-xr-x   2 yarn     hadoop       4096 Dec 11 17:13 ./tezlib/apache-tez-0.8.5-bin/share</div><div class="line">37749247 41916 -r-xr-xr-x   1 yarn     hadoop   42921455 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/share/tez.tar.gz</div><div class="line"></div><div class="line">......</div><div class="line"># 启动脚本</div><div class="line">LogType:launch_container.sh</div><div class="line">Log Upload Time:Mon Dec 11 17:13:49 +0800 2017</div><div class="line">LogLength:5084</div><div class="line">Log Contents:</div><div class="line">#!/bin/bash</div><div class="line">....</div><div class="line">....</div><div class="line"></div><div class="line"># 指定类路径</div><div class="line">export CLASSPATH=&quot;/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure:$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*:$HADOOP_CONF_DIR:&quot;</div><div class="line">........</div><div class="line"></div><div class="line">LogType:stderr</div><div class="line">Log Upload Time:Mon Dec 11 17:13:50 +0800 2017</div><div class="line">LogLength:77</div><div class="line">Log Contents:</div><div class="line">Error: Could not find or load main class org.apache.tez.dag.app.DAGAppMaster</div><div class="line">End of LogType:stderr</div></pre></td></tr></table></figure></p>
<p>可以看到报错信息是找不到tez的主类，这个一般就是类路径的问题了。</p>
<p>找一个hive 1.2.1的tez的正常的程序，看一下container文件内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">....</div><div class="line">12322304  164 -r-xr-xr-x   1 yarn     hadoop     164267 Apr 25  2016 ./tezlib/tez-runtime-internals-0.7.0.2.4.2.0-258.jar</div><div class="line">12322432   64 -r-xr-xr-x   1 yarn     hadoop      62943 Apr 25  2016 ./tezlib/tez-history-parser-0.7.0.2.4.2.0-258.jar</div><div class="line">12322283   68 -r-xr-xr-x   1 yarn     hadoop      67195 Apr 25  2016 ./tezlib/tez-common-0.7.0.2.4.2.0-258.jar</div><div class="line">12322430   28 -r-xr-xr-x   1 yarn     hadoop      25622 Apr 25  2016 ./tezlib/tez-yarn-timeline-history-0.7.0.2.4.2.0-258.jar</div><div class="line">12322306  532 -r-xr-xr-x   1 yarn     hadoop     542434 Apr 25  2016 ./tezlib/tez-runtime-library-0.7.0.2.4.2.0-258.jar</div><div class="line">....</div><div class="line"></div><div class="line">LogType:launch_container.sh</div><div class="line">Log Upload Time:Mon Dec 11 15:47:42 +0800 2017</div><div class="line">LogLength:5740</div><div class="line">Log Contents:</div><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">....</div><div class="line">export CLASSPATH=&quot;/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure:$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*:$HADOOP_CONF_DIR:&quot;</div><div class="line"></div><div class="line">....</div></pre></td></tr></table></figure></p>
<p>可以看到都是用的默认的<code>$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*</code>, 上面的执行错误的container里的tez文件多一个文件夹路径。</p>
<p>对于<code>tez.lib.uris</code>的注释里也说到：会解压开，然后直接作为classpath。</p>
<p>tez对于classpath<a href="https://tez.apache.org/releases/0.8.5/tez-api-javadocs/configs/TezConfiguration.html" target="_blank" rel="external">配置参考</a>.</p>
<p>我们目前的配置是<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  </div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hdp/apps/$&#123;hdp.version&#125;/tez/apache-tez-0.8.5-bin.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>可以看出是这个包的问题，那么就有两个方式解决了。</p>
<ul>
<li>把apache-tez-0.8.5-bin.tar.gz自己处理一下，把多余的文件夹去掉，重新上传</li>
<li>添加个classpath的前缀，也就是把文件夹名称加上。<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>....<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>看一下tez的目录，发现share目录下有个tez.tar.gz, 这个文件就是专门做这个事情的。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/hive命令行使用tez问题/" title="hive命令行使用tez问题" itemprop="url">hive命令行使用tez问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T07:48:25.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li>使用ambari2.2.2.0搭建的集群环境</li>
<li>使用了自定义的hive UDF包</li>
</ul>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>通过hue在hiveserver2上使用tez进行查询，正常运行。</p>
<p>使用hive cli进行tez查询的时候，报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2017-12-11 15:06:30,745 INFO  [main]: ql.Context (Context.java:getMRScratchDir(330)) - New scratch dir is hdfs://dd/tmp/hive/root/c6354673-afc9-4515-8c44-ff7c7f75edcb/hive_2017-12-11_15-06-30_517_4074051706046309823-1</div><div class="line">2017-12-11 15:06:30,750 INFO  [main]: exec.Task (TezTask.java:updateSession(270)) - Tez session hasn&apos;t been created yet. Opening session</div><div class="line">2017-12-11 15:06:30,750 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(130)) - Opening the session with id 97240ae8-cce2-4000-83d6-f6729146ebab for thread main log trace id -  query id - root_20171211150630_4fbc2d95-5613-438c-9499-126da1ce1f49</div><div class="line">2017-12-11 15:06:30,750 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(146)) - User of session id 97240ae8-cce2-4000-83d6-f6729146ebab is root</div><div class="line">2017-12-11 15:06:30,763 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530270</div><div class="line">2017-12-11 15:06:30,766 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530408</div><div class="line">2017-12-11 15:06:30,770 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530453</div><div class="line">2017-12-11 15:06:30,773 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530487</div><div class="line">2017-12-11 15:06:30,775 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530408</div><div class="line">2017-12-11 15:06:30,777 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/server/app/hive/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div><div class="line">2017-12-11 15:06:30,780 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(958)) - Looks like another thread is writing the same file will wait.</div><div class="line">2017-12-11 15:06:30,780 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(965)) - Number of wait attempts: 5. Wait interval: 5000</div><div class="line">2017-12-11 15:06:55,790 ERROR [main]: tez.DagUtils (DagUtils.java:localizeResource(981)) - Could not find the jar that was being uploaded</div><div class="line">2017-12-11 15:06:55,790 ERROR [main]: exec.Task (TezTask.java:execute(212)) - Failed to execute tez graph.</div><div class="line">java.io.IOException: Previous writer likely failed to write hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar. Failing because I am unlikely to write too.</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862)</div></pre></td></tr></table></figure></p>
<h4 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h4><p>上传udf到hdfs中tez session目录的时候，出现错误。</p>
<h4 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h4><p>一遍遍地检查HDFS上的目录，发现尽管报错，也已经有了这个UDF jar文件了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@servicenode02 livy]# hdfs dfs -ls /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   3 root hdfs     258861 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/hive-hcatalog-core.jar</div><div class="line">-rw-r--r--   3 root hdfs       3809 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_hive_udf.jar</div><div class="line">-rw-r--r--   3 root hdfs      83977 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_json_serde_1.3.8.jar</div><div class="line">-rw-r--r--   3 root hdfs      24630 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div></pre></td></tr></table></figure></p>
<p>来来回回看了八百遍，到源码里也看了一下<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// 尝试将本地文件copy到远程的目录中</span></div><div class="line">    destFS.copyFromLocalFile(<span class="keyword">false</span>, <span class="keyword">false</span>, src, dest);</div><div class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">  <span class="comment">// 如果copy失败，一般就是文件已经存在了，或者有其他人在进行同样的操作</span></div><div class="line">  LOG.info(<span class="string">"Looks like another thread is writing the same file will wait."</span>);</div><div class="line">  <span class="keyword">int</span> waitAttempts =</div><div class="line">      conf.getInt(HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.varname,</div><div class="line">          HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.defaultIntVal);</div><div class="line">  <span class="keyword">long</span> sleepInterval = HiveConf.getTimeVar(</div><div class="line">      conf, HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_WAIT_INTERVAL,</div><div class="line">      TimeUnit.MILLISECONDS);</div><div class="line">  LOG.info(<span class="string">"Number of wait attempts: "</span> + waitAttempts + <span class="string">". Wait interval: "</span></div><div class="line">      + sleepInterval);</div><div class="line">      </div><div class="line">  <span class="comment">// 隔一段时间检查一下，这个文件是不是已经有了</span></div><div class="line">  <span class="keyword">boolean</span> found = <span class="keyword">false</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; waitAttempts; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (!checkPreExisting(src, dest, conf)) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        Thread.sleep(sleepInterval);</div><div class="line">      &#125; <span class="keyword">catch</span> (InterruptedException interruptedException) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(interruptedException);</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      found = <span class="keyword">true</span>;</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 如果一直都么有，就报上面的错误了</span></div><div class="line">  <span class="keyword">if</span> (!found) &#123;</div><div class="line">    LOG.error(<span class="string">"Could not find the jar that was being uploaded"</span>);</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Previous writer likely failed to write "</span> + dest +</div><div class="line">        <span class="string">". Failing because I am unlikely to write too."</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>对于<code>checkPreExisting</code>方法，也要看一下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 检查目标目录下是不是已经有了同名文件，</span></div><div class="line"><span class="comment">// 如果文件已经有了，那么要检查文件是否一致，这里简单用len验证</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">checkPreExisting</span><span class="params">(Path src, Path dest, Configuration conf)</span></span></div><div class="line">    <span class="keyword">throws</span> IOException &#123;</div><div class="line">    FileSystem destFS = dest.getFileSystem(conf);</div><div class="line">    FileSystem sourceFS = src.getFileSystem(conf);</div><div class="line">    FileStatus destStatus = FileUtils.getFileStatusOrNull(destFS, dest);</div><div class="line">    <span class="keyword">if</span> (destStatus != <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="comment">// 我挂到了这一步，就是udf的版本不一致</span></div><div class="line">      <span class="keyword">return</span> (sourceFS.getFileStatus(src).getLen() == destStatus.getLen());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>好了。那么看一下我的问题。为了使用udf，我们的数据研发在hive cli的初始化文件<code>bin/.hiverc</code>中添加了下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">add jar /server/app/hive/will_hive_udf.jar;</div><div class="line">add jar /usr/hdp/current/hive-client/auxlib/tinyv_hive_udf.jar;</div><div class="line">create temporary function  strstart as &apos;com.will.common.hive.StrStart&apos;;  </div><div class="line">create temporary function  strDateFormat as &apos;com.will.common.hive.StrDateFormat&apos;;</div><div class="line">create temporary function  strContain as &apos;com.will.common.hive.StrContain&apos;; </div><div class="line">create temporary function  parse_uri as &apos;com.will.common.hive.ParseUri&apos;;</div><div class="line">create temporary function  strEnd as &apos;com.will.common.hive.StrEnd&apos;;</div><div class="line">create temporary function  split_by_index as &apos;com.will.common.hive.SplitByIndex&apos;;</div><div class="line">create temporary function birthday as &apos;com.will.common.hive.BirthdayByIdcard&apos;;</div><div class="line">create temporary function gender as &apos;com.will.common.hive.GenderByIdcard&apos;;  </div><div class="line">create temporary function usernamesen as &apos;com.will.common.hive.UserNameSen&apos;;</div><div class="line">create temporary function strTrim as &apos;com.will.common.hive.StrTrim&apos;;</div><div class="line">create temporary function channel as &apos;com.will.common.hive.Channel&apos;;</div><div class="line">create temporary function iptonum as &apos;com.will.common.hive.IpToNumber&apos;;</div><div class="line">create temporary function geturl as &apos;com.will.common.hive.ParseUrls&apos;;</div><div class="line">create temporary function money_record_type as &apos;com.will.common.hive.MoneyRecordType&apos;;</div><div class="line">create temporary function xv_encode as &apos;com.will.tinyv.Encode&apos;;</div><div class="line">create temporary function xv_decode as &apos;com.will.tinyv.Decode&apos;;</div><div class="line">set hive.mapred.mode=nostrict;</div></pre></td></tr></table></figure></p>
<p>重新启动hive cli，报错的日志，发现上传了来自两个不同地方的will_hive_udf.jar文件：一个来自<code>hive/auxlib</code>目录，另一个来自<code>/server/app</code>…..而<strong>只有第一次报错的时候才能看到第一个udf的上传过程，因为同一个session中，已经上传过的，验证没问题就跳过了。。。。只打印了一个时间戳，也没有指明是哪个文件</strong>…..导致定位问题的时候，有些恍惚，还以为是tez与UDF功能之间的问题<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">2017-12-11 15:09:30,604 INFO  [main]: ql.Context (Context.java:getMRScratchDir(330)) - New scratch dir is hdfs://dd/tmp/hive/root/c6354673-afc9-4515-8c44-ff7</div><div class="line">c7f75edcb/hive_2017-12-11_15-09-30_406_4902171368286077085-1</div><div class="line">2017-12-11 15:09:30,608 INFO  [main]: exec.Task (TezTask.java:updateSession(270)) - Tez session hasn&apos;t been created yet. Opening session</div><div class="line">2017-12-11 15:09:30,608 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(130)) - Opening the session with id 97240ae8-cce2-4000-83d6-f6729146ebab</div><div class="line"> for thread main log trace id -  query id - root_20171211150930_d928c00e-62da-4c0e-95c2-23a28594a7c0</div><div class="line">2017-12-11 15:09:30,608 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(146)) - User of session id 97240ae8-cce2-4000-83d6-f6729146ebab is root</div><div class="line">2017-12-11 15:09:30,613 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/curre</div><div class="line">nt/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/hive-hcatalog-co</div><div class="line">re.jar</div><div class="line">2017-12-11 15:09:30,655 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170667</div><div class="line">2017-12-11 15:09:30,656 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2</div><div class="line">.0-258/hive/auxlib/tinyv_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_hive_udf.jar</div><div class="line">2017-12-11 15:09:30,683 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170691</div><div class="line">2017-12-11 15:09:30,684 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2</div><div class="line">.0-258/hive/auxlib/tinyv_json_serde_1.3.8.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_json_serde_1.3.8.j</div><div class="line">ar</div><div class="line">2017-12-11 15:09:30,707 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170720</div><div class="line">2017-12-11 15:09:30,709 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2.0-258/hive/auxlib/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div><div class="line">2017-12-11 15:09:30,729 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170742</div><div class="line">2017-12-11 15:09:30,732 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170691</div><div class="line">2017-12-11 15:09:30,733 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/server/app/hive/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div><div class="line">2017-12-11 15:09:30,735 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(958)) - Looks like another thread is writing the same file will wait.</div><div class="line">2017-12-11 15:09:30,736 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(965)) - Number of wait attempts: 5. Wait interval: 5000</div><div class="line">2017-12-11 15:09:55,745 ERROR [main]: tez.DagUtils (DagUtils.java:localizeResource(981)) - Could not find the jar that was being uploaded</div><div class="line">2017-12-11 15:09:55,745 ERROR [main]: exec.Task (TezTask.java:execute(212)) - Failed to execute tez graph.</div><div class="line">java.io.IOException: Previous writer likely failed to write hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar. Failing because I am unlikely to write too.</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFilesFromConf(DagUtils.java:805)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.refreshLocalResourcesFromConf(TezSessionState.java:233)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:158)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezTask.updateSession(TezTask.java:271)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:151)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)</div></pre></td></tr></table></figure></p>
<h4 id="总结原因"><a href="#总结原因" class="headerlink" title="总结原因"></a>总结原因</h4><ul>
<li>udf版本不一致，导致后面jar包上传失败</li>
</ul>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>把.hiverc中的命令暂时去掉，重试tez引擎。就能成功调用了。</p>
<h4 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h4><p>为什么已经设定了<code>hive.aux.jars.path</code>了，还要在.hiverc额外处理呢。</p>
<p>先在hivecli中确认一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@schedule ~]#hive -e &quot;set hive.aux.jars.path&quot;</div><div class="line">WARNING: Use &quot;yarn jar&quot; to launch YARN applications.</div><div class="line"></div><div class="line">Logging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.properties</div><div class="line">Putting the global hiverc in $HIVE_HOME/bin/.hiverc is deprecated. Please use $HIVE_CONF_DIR/.hiverc instead.</div><div class="line">hive.aux.jars.path=file:///usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_hive_udf.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_json_serde_1.3.8.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/will_hive_udf.jar</div></pre></td></tr></table></figure></p>
<p>但是测试了一下，没有.hiverc里的初始化，就不能执行udf函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hive&gt; select iptonum(&apos;10.2.19.32&apos;);</div><div class="line">FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function &apos;iptonum&apos;</div></pre></td></tr></table></figure></p>
<p>可是hue里，也就是hiveserver2里却可以执行udf函数。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/canal同步的TimeStamp问题/" title="canal同步的TimeStamp问题" itemprop="url">canal同步的TimeStamp问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T05:30:37.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>部门接了新的实时方面的需求，选定使用canal进行mysql binlog方面的处理。</p>
<p>上线一段时间后都比较稳定，但是有一天，实时数据开发的同事找到我说有个别字段是有问题的。大概表结构如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`bb`</span> (</div><div class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</div><div class="line">  <span class="string">`lastUpdateTime`</span> <span class="keyword">timestamp</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">CURRENT_TIMESTAMP</span>,</div><div class="line">  <span class="string">`firstTimeBindTime`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</div><div class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</div><div class="line">) <span class="keyword">ENGINE</span>=MyISAM AUTO_INCREMENT=<span class="number">1526640</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8</div></pre></td></tr></table></figure></p>
<p>有问题的字段为<code>lastUpdateTime</code>，主库为<code>2017-12-11 11:22:40</code>的时候，canal client中消费到的却是<code>2017-12-10 22:22:40</code>，而且所有这个字段都是相差同样的13个小时。但是有一个地方比较值得注意，就是<code>firstTimeBindTime</code>的值都是正常的。</p>
<h4 id="排查流程"><a href="#排查流程" class="headerlink" title="排查流程"></a>排查流程</h4><ul>
<li><p>查看canal server的系统时区，进行修正确认</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime </div><div class="line">date # 发现时间已经正确了</div></pre></td></tr></table></figure>
</li>
<li><p>重启canal server</p>
</li>
</ul>
<p>发现还是有问题，去mysql主机上翻看binlog，找到对应的记录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysqlbinlog -vv /server/mysql_data/mysql-bin.000141</div></pre></td></tr></table></figure></p>
<p>发现这个字段对应的时间戳是正确的<code>1512962560</code>。那么问题看来还是在canal server这块了。</p>
<h4 id="源码走读"><a href="#源码走读" class="headerlink" title="源码走读"></a>源码走读</h4><p>顺便看下源码吧。对于binlog进行解析的主类：<code>MysqlEventParser.start()</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">...</div><div class="line"><span class="comment">// event处理与解析</span></div><div class="line">CanalEntry.Entry entry = parseAndProfilingIfNecessary(event);</div><div class="line">...</div></pre></td></tr></table></figure>
<p>进入<code>parseAndProfilingIfNecessary</code>方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="comment">// 使用parser解析</span></div><div class="line">CanalEntry.Entry event = binlogParser.parse(bod);</div><div class="line">...</div></pre></td></tr></table></figure>
<p>找到对应的parser类为LogEventConvert，可以看到<code>LogEventConvert</code>是继承了<code>BinlogParser</code>的。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> BinlogParser <span class="title">buildParser</span><span class="params">()</span> </span>&#123;</div><div class="line">    LogEventConvert convert = <span class="keyword">new</span> LogEventConvert();</div><div class="line">    ......</div><div class="line">    <span class="keyword">return</span> convert;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>再去<code>LogEventConvert</code>的parse方法<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="comment">// 对于修改数据的event的处理</span></div><div class="line"><span class="keyword">case</span> LogEvent.WRITE_ROWS_EVENT:</div><div class="line">    <span class="keyword">return</span> parseRowsEvent((WriteRowsLogEvent) logEvent);</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>继续找<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (buffer.nextOneRow(columns)) &#123;</div><div class="line">    <span class="comment">// 处理row记录</span></div><div class="line">    RowData.Builder rowDataBuilder = RowData.newBuilder();</div><div class="line">    <span class="keyword">if</span> (EventType.INSERT == eventType) &#123;</div><div class="line">        <span class="comment">// insert的记录放在before字段中</span></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, <span class="keyword">true</span>, tableMeta);</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (EventType.DELETE == eventType) &#123;</div><div class="line">        <span class="comment">// delete的记录放在before字段中</span></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, <span class="keyword">false</span>, tableMeta);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// update需要处理before/after</span></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, <span class="keyword">false</span>, tableMeta);</div><div class="line">        <span class="keyword">if</span> (!buffer.nextOneRow(changeColumns)) &#123;</div><div class="line">            rowChangeBuider.addRowDatas(rowDataBuilder.build());</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, changeColumns, <span class="keyword">true</span>, tableMeta);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    rowChangeBuider.addRowDatas(rowDataBuilder.build());</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>对于行数据的每个字段的处理，是先获取到value，然后再针对地做一些额外处理。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">....</div><div class="line"><span class="keyword">final</span> Serializable value = buffer.getValue();</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>通过<code>RowsLogBuffer</code>进行value的获取，我们找到给value赋值的地方<code>fetchValue</code><br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">final</span> Serializable <span class="title">fetchValue</span><span class="params">(<span class="keyword">int</span> type, <span class="keyword">final</span> <span class="keyword">int</span> meta, <span class="keyword">boolean</span> isBinary)</span> </span>&#123;</div><div class="line">    .....</div><div class="line">    <span class="keyword">switch</span> (type) &#123;</div><div class="line">	 <span class="keyword">case</span> LogEvent.MYSQL_TYPE_TIMESTAMP: &#123;</div><div class="line">        <span class="keyword">final</span> <span class="keyword">long</span> i32 = buffer.getUint32();</div><div class="line">        <span class="keyword">if</span> (i32 == <span class="number">0</span>) &#123;</div><div class="line">            value = <span class="string">"0000-00-00 00:00:00"</span>;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            String v = <span class="keyword">new</span> Timestamp(i32 * <span class="number">1000</span>).toString();</div><div class="line">            value = v.substring(<span class="number">0</span>, v.length() - <span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line">        javaType = Types.TIMESTAMP;</div><div class="line">        length = <span class="number">4</span>;</div><div class="line">        <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">    .....</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>把这部分弄出去，使用上面binlog文件中的timestamp单独测试.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> java.sql.Timestamp;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by root on 17-12-11.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Will</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] p )</span></span>&#123;</div><div class="line">        String value = <span class="string">""</span>;</div><div class="line">        <span class="keyword">final</span> <span class="keyword">long</span> i32 = <span class="number">1512962560l</span>;</div><div class="line">        System.out.println(TimeZone.getDefault());</div><div class="line">        System.out.println(i32);</div><div class="line">        <span class="keyword">if</span> (i32 == <span class="number">0</span>) &#123;</div><div class="line">            value = <span class="string">"0000-00-00 00:00:00"</span>;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            String v = <span class="keyword">new</span> Timestamp(i32 * <span class="number">1000</span>).toString();</div><div class="line">            value = v.substring(<span class="number">0</span>, v.length() - <span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line">        System.out.println(value);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在测试机是正常的, 但是在线上环境是不正常的，最后在java里输出了一下<code>Timezone.getDefault()</code>，发现线上是”America/New_York”，正好就是差13个小时的。可是在系统里运行date命令，返回的是上海时区。上网搜了一下，有人建议使用下面命令确定时区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@etl01 ~]# ls -l /etc/localtime</div><div class="line">lrwxrwxrwx. 1 root root 38 Oct 21 00:25 /etc/localtime -&gt; ../usr/share/zoneinfo/America/New_York</div></pre></td></tr></table></figure></p>
<p>这里看到虽然我前面使用cp已经覆盖了/etc/localtime文件，而且并没有报错。但是查看的时候，还是显示为到纽约的链接。</p>
<p>删掉这个文件，重新cp。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@etl01 ~]# rm -vf /etc/localtime</div><div class="line">removed \u2018/etc/localtime\u2019</div><div class="line">[root@etl01 ~]# cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime </div><div class="line">[root@etl01 ~]# ls -l /etc/localtime</div><div class="line">-rw-r--r--. 1 root root 405 Dec 11 13:14 /etc/localtime</div><div class="line">[root@etl01 ~]# java Will</div><div class="line">sun.util.calendar.ZoneInfo[id=&quot;America/New_York&quot;,offset=-18000000,dstSavings=3600000,useDaylight=true,transitions=235,lastRule=java.util.SimpleTimeZone[id=America/New_York,offset=-18000000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]</div><div class="line">America/New_York</div></pre></td></tr></table></figure></p>
<p>还是不对，无奈，再次删掉。按照系统的样子，创建软链，成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@etl01 ~]# rm /etc/localtime </div><div class="line">rm: remove regular file \u2018/etc/localtime\u2019? y</div><div class="line">[root@etl01 ~]# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</div><div class="line">[root@etl01 ~]# ls -l /etc/localtime</div><div class="line">lrwxrwxrwx. 1 root root 33 Dec 11 13:15 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai</div><div class="line">[root@etl01 ~]# java Will</div><div class="line">sun.util.calendar.ZoneInfo[id=&quot;Asia/Shanghai&quot;,offset=28800000,dstSavings=0,useDaylight=false,transitions=19,lastRule=null]</div><div class="line">Asia/Shanghai</div></pre></td></tr></table></figure></p>
<p>CentOS Linux release 7.0.1406 (Core)系统……….比较无语</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/27/">27</a><a class="extend next" rel="next" href="/page/3/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  


  

  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/youdaonote/" title="youdaonote">youdaonote<sup>209</sup></a></li>
			
		
			
				<li><a href="/tags/源码/" title="源码">源码<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/akka/" title="akka">akka<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/flume/" title="flume">flume<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/ETL/" title="ETL">ETL<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/solr/" title="solr">solr<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/spring/" title="spring">spring<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/调度平台/" title="调度平台">调度平台<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/azkaban/" title="azkaban">azkaban<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/scala/" title="scala">scala<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ambari/" title="ambari">ambari<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/quartz/" title="quartz">quartz<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/python/" title="python">python<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ember/" title="ember">ember<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/nodejs/" title="nodejs">nodejs<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/R/" title="R">R<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/guava/" title="guava">guava<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/heroku/" title="heroku">heroku<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hdfs/" title="hdfs">hdfs<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://github.com/willcup" target="_blank" title=" 我自己的github">github</a>
            
          </li>
        
          <li>
            
            	<a href="http://thisding.com" target="_blank" title="朋友的主页">Steven&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=null&verifier=&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Will Chen in MeiTuan. <br/>
			元 亨 利 贞.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
		<a href="mailto:chenxin15@meituan.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2017 
		
		<a href="/about" target="_blank" title="Will Chen">Will Chen</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fe6d1f421bbc9962127a50488f9ed37d1' type='text/javascript'%3E%3C/script%3E"));
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
