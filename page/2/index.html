
 <!DOCTYPE HTML>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
  <script type="text/javascript">
    (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
    (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
    e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
    })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');
    
    _st('install','yNiKTKaAnwd1uuxVMfiE','2.0.0');
  </script>
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5b99dfd487346155d274c0c49c3fb869";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>

  
    <title>Will&#39;s Blog</title>
    <meta name="viewport" content="width=device-width, initial-scale=1,user-scalable=no">
    
    <meta name="author" content="Will Chen">
    

    
    <meta name="description" content="左右水色 右手天光">
<meta property="og:type" content="website">
<meta property="og:title" content="Will's Blog">
<meta property="og:url" content="https://runningdata.github.io/page/2/index.html">
<meta property="og:site_name" content="Will's Blog">
<meta property="og:description" content="左右水色 右手天光">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Will's Blog">
<meta name="twitter:description" content="左右水色 右手天光">

    
    <link rel="alternative" href="/atom.xml" title="Will&#39;s Blog" type="application/atom+xml">
    
    
    <link rel="icon" href="/img/favicon.ico">
    
    
    <link rel="apple-touch-icon" href="/img/jacman.jpg">
    <link rel="apple-touch-icon-precomposed" href="/img/jacman.jpg">
    
    <link rel="stylesheet" href="/css/style.css">
</head>

  <body>
    <header>
      
<div>
		
			<div id="imglogo">
				<a href="/"><img src="/img/logo.png" alt="Will&#39;s Blog" title="Will&#39;s Blog"/></a>
			</div>
			
			<div id="textlogo">
				<h1 class="site-name"><a href="/" title="Will&#39;s Blog">Will&#39;s Blog</a></h1>
				<h2 class="blog-motto">简易 变易 不易</h2>
			</div>
			<div class="navbar"><a class="navbutton navmobile" href="#" title="菜单">
			</a></div>
			<nav class="animated">
				<ul>
					<ul>
					 
						<li><a href="/">首页</a></li>
					
						<li><a href="/archives">归档</a></li>
					
						<li><a href="/about">关于</a></li>
					
					<li>
 					
                                                <form class="search" action="/search/index.html" method="get" accept-charset="utf-8" target="_blank">
                                                        <label>搜索</label>
                                                <input name="s" type="hidden" value= null ><input type="text" name="q" size="30" placeholder="搜索"><br>
                                                </form>
					
					</li>
				</ul>
			</nav>			
</div>

    </header>
    <div id="container">
      <div id="main">

   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/15/yarn的节点label/" title="yarn的节点label" itemprop="url">yarn的节点label</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-15T02:59:13.000Z" itemprop="datePublished"> 发表于 2017-12-15</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h4><p>nodel label是用来把一些共同特征的节点进行归组的手段，然后任务可以指定label要求，运行到指定label的节点上。</p>
<p>现在只支持node partition：</p>
<p>一个node只有一个node partition，所以一个集群被分为多个子集，这些子集互不相交。默认情况下，node是属于DEFAULT partition的。用户可以配置每个partition对于queue中可用多少资源。具体需要看下一节。</p>
<p>有两种不同的node partition：</p>
<ul>
<li>exclusive： container会分配在node label完全匹配的node上。(例如指定了partition=”x”，那么就被分配到partition=“x”的node上，默认的就分配到默认的node上)</li>
<li>non-exclusive: 如果一个partition是non-exclusive的，它就可以把空闲资源分配给请求DEFAULT partition的。</li>
</ul>
<p>用户可以为每个queue指定可以访问的多个node label，app只能使用这个queue能使用的node label的node。</p>
<h4 id="Features"><a href="#Features" class="headerlink" title="Features"></a>Features</h4><ul>
<li>Partition cluster - 每个node都可以有一个label，这样cluster就会被划分为包含多个不相交的子集partition.</li>
<li>ACL of node-labels on queues - 用户可以指定每个queue可以访问的label的node。</li>
<li>指定一个queue可以访问某个label的比例 - 例如queue A可以使用label=hbase的30%的node resource。</li>
<li>在resource request中指定nodel label的要求, 那么就只会分配这个label属性的节点资源。如果什么都没有指定的话，就默认为DEFAULT partition。</li>
<li>运维相关<ul>
<li>nodel label和node label的映射关系可以通过RM restart进行恢复</li>
<li>更新node label - admin可在RM运行态更新node和queue的label</li>
</ul>
</li>
</ul>
<h4 id="Configuration"><a href="#Configuration" class="headerlink" title="Configuration"></a>Configuration</h4><h5 id="设置ResourceManager启用Node-Labels"><a href="#设置ResourceManager启用Node-Labels" class="headerlink" title="设置ResourceManager启用Node Labels:"></a>设置ResourceManager启用Node Labels:</h5><p>yarn-site.xml</p>
<table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>yarn.node-labels.fs-store.root-dir</td>
<td>hdfs://namenode:port/path/to/store/node-labels/</td>
</tr>
<tr>
<td>yarn.node-labels.enabled</td>
<td>true</td>
</tr>
</tbody>
</table>
<p>注意:</p>
<p>确保<code>yarn.node-labels.fs-store.root-dir</code>已经存在，而且RM有权限访问(一般是yarn用户访问)</p>
<p>如果用户想把node label存储在RM的本地文件系统，就是用<code>file:///home/yarn/node-label</code></p>
<h6 id="添加-修改-node-labels-list和node-to-labels-mapping-to-YARN"><a href="#添加-修改-node-labels-list和node-to-labels-mapping-to-YARN" class="headerlink" title="添加/修改 node labels list和node-to-labels mapping to YARN"></a>添加/修改 node labels list和node-to-labels mapping to YARN</h6><p>这是两个连续的步骤。</p>
<ul>
<li><p>添加Node labels池到cluster中，后面给node添加的label只能是这里面的label，并不是给集群所有节点默认label，这个要注意一下:</p>
<ul>
<li>执行<code>yarn rmadmin -addToClusterNodeLabels &quot;label_1(exclusive=true/false),label_2(exclusive=true/false)</code> 添加 node label.</li>
<li>如果用户没有指定 “(exclusive=…)”, execlusive默认为true.</li>
<li>运行<code>yarn cluster --list-node-labels</code>，检查新加的node label是否已生效</li>
</ul>
</li>
<li><p>给node添加label</p>
<ul>
<li>执行<code>yarn rmadmin -replaceLabelsOnNode “node1[:port]=label1 node2=label2”</code>. 给node1添加label1,给node2添加label2. 如果用户没有指定port, 就给这个node上运行的所有的NodeManagers都添加.</li>
</ul>
</li>
</ul>
<h5 id="给node-labels配置Schedulers"><a href="#给node-labels配置Schedulers" class="headerlink" title="给node labels配置Schedulers"></a>给node labels配置Schedulers</h5><h6 id="Capacity-Scheduler-配置"><a href="#Capacity-Scheduler-配置" class="headerlink" title="Capacity Scheduler 配置"></a>Capacity Scheduler 配置</h6><table>
<thead>
<tr>
<th>Property</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.capacity</queue-path></td>
<td>Set the percentage of the queue can access to nodes belong to DEFAULT partition. The sum of DEFAULT capacities for direct children under each parent, must be equal to 100.</td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.accessible-node-labels</queue-path></td>
<td>Admin need specify labels can be accessible by each queue, split by comma, like “hbase,storm” means queue can access label hbase and storm. All queues can access to nodes without label, user don’t have to specify that. If user don’t specify this field, it will inherit from its parent. If user want to explicitly specify a queue can only access nodes without labels, just put a space as the value.</td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.accessible-node-labels.<label>.capacity</label></queue-path></td>
<td>Set the percentage of the queue can access to nodes belong to <label> partition . The sum of <label> capacities for direct children under each parent, must be equal to 100. By default, it’s 0.</label></label></td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.accessible-node-labels.<label>.maximum-capacity</label></queue-path></td>
<td>Similar to yarn.scheduler.capacity.<queue-path>.maximum-capacity, it is for maximum-capacity for labels of each queue. By default, it’s 100.</queue-path></td>
</tr>
<tr>
<td>yarn.scheduler.capacity.<queue-path>.default-node-label-expression</queue-path></td>
<td>Value like “hbase”, which means: if applications submitted to the queue without specifying node label in their resource requests, it will use “hbase” as default-node-label-expression. By default, this is empty, so application will get containers from nodes without label.</td>
</tr>
</tbody>
</table>
<p>node label配置示例:</p>
<p>假设我们的queue是这样的</p>
<pre><code>           root
       /     |    \
engineer    sales  marketing
</code></pre><p>We have 5 nodes (hostname=h1..h5) in the cluster, each of them has 24G memory, 24 vcores. 1 among the 5 nodes has GPU (assume it’s h5). So admin added GPU label to h5.</p>
<p>我们集群中有5个node，每个node有24G内存，24个vcore。其中一个node h5有GPU，那么admin想给很h5添加一个GPU的label。</p>
<p>用户就可以这样配置：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">yarn.scheduler.capacity.root.queues=engineering,marketing,sales</div><div class="line">yarn.scheduler.capacity.root.engineering.capacity=33</div><div class="line">yarn.scheduler.capacity.root.marketing.capacity=34</div><div class="line">yarn.scheduler.capacity.root.sales.capacity=33</div><div class="line"></div><div class="line">yarn.scheduler.capacity.root.engineering.accessible-node-labels=GPU</div><div class="line">yarn.scheduler.capacity.root.marketing.accessible-node-labels=GPU</div><div class="line"></div><div class="line">yarn.scheduler.capacity.root.engineering.accessible-node-labels.GPU.capacity=50</div><div class="line">yarn.scheduler.capacity.root.marketing.accessible-node-labels.GPU.capacity=50</div><div class="line"></div><div class="line">yarn.scheduler.capacity.root.engineering.default-node-label-expression=GPU</div></pre></td></tr></table></figure></p>
<p>我们设置了<code>root.engineering/marketing/sales.capacity=33</code>, 这样每个队列都保证有33%的资源,也就是 24 <em> 4 </em> (1/3) = (32G mem, 32 v-cores).</p>
<p>只有engineering/marketing queue 有权限使用GPU partition (因为 root.<queue-name>.accessible-node-labels).</queue-name></p>
<p>engineering/marketing queue 都可以使用partition=GPU的 1/2资源，也就是h5的1/2的资源，也就是24 * 0.5 = (12G mem, 12 v-cores).</p>
<p>注意:</p>
<p>配置完后，要执行<code>yarn rmadmin -refreshQueues</code>来使配置生效, 然后去RM的web UI检查一下。</p>
<h4 id="为APP指定node-label"><a href="#为APP指定node-label" class="headerlink" title="为APP指定node label"></a>为APP指定node label</h4><p>可以通过java API指定node label的要求。</p>
<ul>
<li>ApplicationSubmissionContext.setNodeLabelExpression(..) 设置node label表达式</li>
<li>ResourceRequest.setNodeLabelExpression(..) 设置单独resource request的node label表达式, 这个可以overwrite ApplicationSubmissionContext的表达式设置</li>
<li>在ApplicationSubmissionContext中的setAMContainerResourceRequest.setNodeLabelExpressio用来定位app master container要求的node label</li>
</ul>
<h4 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h4><h5 id="通过UI监控"><a href="#通过UI监控" class="headerlink" title="通过UI监控"></a>通过UI监控</h5><p>web UI提供的指标:</p>
<p>Nodes page: <a href="http://RM-Address:port/cluster/nodes" target="_blank" rel="external">http://RM-Address:port/cluster/nodes</a><br>Node labels page: <a href="http://RM-Address:port/cluster/nodelabels,可以获取到类型" target="_blank" rel="external">http://RM-Address:port/cluster/nodelabels,可以获取到类型</a> (exclusive/non-exclusive), active node managers的数量, 每个partition的总资源<br>Scheduler page: <a href="http://RM-Address:port/cluster/scheduler" target="_blank" rel="external">http://RM-Address:port/cluster/scheduler</a>, 每个queue关于label的配置</p>
<h5 id="通过命令行监控"><a href="#通过命令行监控" class="headerlink" title="通过命令行监控"></a>通过命令行监控</h5><ul>
<li>yarn cluster –list-node-labels</li>
<li>yarn node -status <nodeid> </nodeid></li>
</ul>
<h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><p>YARN Capacity Scheduler, if you need more understanding about how to configure Capacity Scheduler<br>Write YARN application using node labels, you can see following two links as examples: YARN distributed shell, Hadoop MapReduce</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/12/hive-on-spark配置/" title="hive-on-spark配置" itemprop="url">hive-on-spark配置</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-12T10:15:55.000Z" itemprop="datePublished"> 发表于 2017-12-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="安装spark"><a href="#安装spark" class="headerlink" title="安装spark"></a>安装spark</h4><p>hive on spark默认也是支持spark on yarn的模式的。</p>
<h4 id="配置YARN"><a href="#配置YARN" class="headerlink" title="配置YARN"></a>配置YARN</h4><p>不能使用capacity scheduler，必须使用fair scheduler【….看到这里就想放弃了】</p>
<p>yarn.resourcemanager.scheduler.class=org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</p>
<h4 id="配置Hive"><a href="#配置Hive" class="headerlink" title="配置Hive"></a>配置Hive</h4><h5 id="添加spark依赖："><a href="#添加spark依赖：" class="headerlink" title="添加spark依赖："></a>添加spark依赖：</h5><ul>
<li>hive2.2.0之前，把spark-assembly jar链接到HIVE_HOME/lib中</li>
<li>2.2.0之后，hive on spark需要运行在spark 2.0.0以上的版本，没有assembly jar包 <ul>
<li>要运行YARN模式的话，添加下面的包到HIVE_HOME/lib<ul>
<li>scala-library</li>
<li>spark-core</li>
<li>spark-network-common</li>
</ul>
</li>
<li>运行local模式的话，添加下面包的链接到hive_home/lib中<ul>
<li>chill-java  chill  jackson-module-paranamer  jackson-module-scala  jersey-container-servlet-core</li>
<li>jersey-server  json4s-ast  kryo-shaded  minlog  scala-xml  spark-launcher</li>
<li>spark-network-shuffle  spark-unsafe  xbean-asm5-shaded</li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="配置hive的执行引擎"><a href="#配置hive的执行引擎" class="headerlink" title="配置hive的执行引擎"></a>配置hive的执行引擎</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">set hive.execution.engine=spark;</div></pre></td></tr></table></figure>
<h5 id="配置spark相关的参数"><a href="#配置spark相关的参数" class="headerlink" title="配置spark相关的参数"></a>配置spark相关的参数</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">set spark.master=&lt;Spark Master URL&gt;</div><div class="line">set spark.eventLog.enabled=true;</div><div class="line">set spark.eventLog.dir=&lt;Spark event log folder (must exist)&gt;</div><div class="line">set spark.executor.memory=512m;             </div><div class="line">set spark.serializer=org.apache.spark.serializer.KryoSerializer;</div></pre></td></tr></table></figure>
<p>hive 2.2.0以前，要把spark-assembly jar到HDS中<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;spark.yarn.jar&lt;/name&gt;</div><div class="line">  &lt;value&gt;hdfs://xxxx:8020/spark-assembly.jar&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<p>2.2.0以上版本后, 把$SPARK_HOME/jars中的所有jar都上传到hdfs指定文件夹中：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;spark.yarn.jars&lt;/name&gt;</div><div class="line">  &lt;value&gt;hdfs://xxxx:8020/spark-jars/*&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure></p>
<h4 id="配置spark"><a href="#配置spark" class="headerlink" title="配置spark"></a>配置spark</h4><table>
<thead>
<tr>
<th>属性</th>
<th>推荐设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>spark.executor.cores</td>
<td>Between 5-7, See tuning details section</td>
</tr>
<tr>
<td>spark.executor.memory</td>
<td>yarn.nodemanager.resource.memory-mb * (spark.executor.cores / yarn.nodemanager.resource.cpu-vcores) </td>
</tr>
<tr>
<td>spark.yarn.executor.memoryOverhead</td>
<td>15-20% of spark.executor.memory</td>
</tr>
<tr>
<td>spark.executor.instances</td>
<td>Depends on spark.executor.memory + spark.yarn.executor.memoryOverhead, see tuning details section.</td>
</tr>
</tbody>
</table>
<h4 id="推荐配置"><a href="#推荐配置" class="headerlink" title="推荐配置"></a>推荐配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">mapreduce.input.fileinputformat.split.maxsize=750000000</div><div class="line">hive.vectorized.execution.enabled=true</div><div class="line"></div><div class="line">hive.cbo.enable=true</div><div class="line">hive.optimize.reducededuplication.min.reducer=4</div><div class="line">hive.optimize.reducededuplication=true</div><div class="line">hive.orc.splits.include.file.footer=false</div><div class="line">hive.merge.mapfiles=true</div><div class="line">hive.merge.sparkfiles=false</div><div class="line">hive.merge.smallfiles.avgsize=16000000</div><div class="line">hive.merge.size.per.task=256000000</div><div class="line">hive.merge.orcfile.stripe.level=true</div><div class="line">hive.auto.convert.join=true</div><div class="line">hive.auto.convert.join.noconditionaltask=true</div><div class="line">hive.auto.convert.join.noconditionaltask.size=894435328</div><div class="line">hive.optimize.bucketmapjoin.sortedmerge=false</div><div class="line">hive.map.aggr.hash.percentmemory=0.5</div><div class="line">hive.map.aggr=true</div><div class="line">hive.optimize.sort.dynamic.partition=false</div><div class="line">hive.stats.autogather=true</div><div class="line">hive.stats.fetch.column.stats=true</div><div class="line">hive.vectorized.execution.reduce.enabled=false</div><div class="line">hive.vectorized.groupby.checkinterval=4096</div><div class="line">hive.vectorized.groupby.flush.percent=0.1</div><div class="line">hive.compute.query.using.stats=true</div><div class="line">hive.limit.pushdown.memory.usage=0.4</div><div class="line">hive.optimize.index.filter=true</div><div class="line">hive.exec.reducers.bytes.per.reducer=67108864</div><div class="line">hive.smbjoin.cache.rows=10000</div><div class="line">hive.exec.orc.default.stripe.size=67108864</div><div class="line">hive.fetch.task.conversion=more</div><div class="line">hive.fetch.task.conversion.threshold=1073741824</div><div class="line">hive.fetch.task.aggr=false</div><div class="line">mapreduce.input.fileinputformat.list-status.num-threads=5</div><div class="line">spark.kryo.referenceTracking=false</div><div class="line">spark.kryo.classesToRegister=org.apache.hadoop.hive.ql.io.HiveKey,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch</div></pre></td></tr></table></figure>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/12/docker容器的时间同步/" title="docker容器的时间同步" itemprop="url">docker容器的时间同步</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-12T08:02:33.000Z" itemprop="datePublished"> 发表于 2017-12-12</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>最近在测试环境上上了一个新的组件，发现docker容器内的时间是有问题的，就是总比真正的时间慢4分钟左右。然后就exec进去安装了ntpdate工具进行时间同步工作。</p>
<p>然后收到了如下信息：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">root@1462539f1dc2:/# ntpdate -u cn.pool.ntp.org</div><div class="line">12 Dec 15:57:35 ntpdate[502]: step-systime: Operation not permitted</div></pre></td></tr></table></figure></p>
<p>看到是这个操作被禁止了，而且在docker里是用root执行的，竟然被禁止了。所以考虑到应该是docker对于苏初级的保护。返回宿主机，查看时间…果然与docker容器内一致，然后到宿主机安装ntpdate，进行时间同步之后，docker内时间自然就正常了。</p>
<p>回想一下docker原理：Docker本质上是宿主机（Linux）上的进程，通过namespace实现资源隔离，通过cgroups实现资源限制，通过写时复用机制(copy-on-write)实现高效的文件操作。归根到底还是调用的宿主机的东西，并不像虚拟机自己有一套完整的机制。</p>
<p><a href="https://kasheemlew.github.io/2017/05/18/docker-linux/" target="_blank" rel="external">https://kasheemlew.github.io/2017/05/18/docker-linux/</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/tez-UI配置/" title="tez-UI配置" itemprop="url">tez-UI配置</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T10:43:28.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>将tez根目录下的war包copy到tomcat的webapps目录中（0.8.5版本有两个ui包,但是没有感觉到有什么区别）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cp ~/apache-tez-0.8.5-bin/tez-ui-0.8.5.war /server/tomcat9/webapps</div></pre></td></tr></table></figure></p>
<p>重启tomcat，war包会自动发布。修改配置文件里的timeline-service和resourcemanager的位置信息后，重启tomcat。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">timeline: &quot;http://datanode04.will.com:8188&quot;,</div><div class="line">rm: &quot;http://datanode02.will.com:8088&quot;,</div></pre></td></tr></table></figure></p>
<h4 id="配置tez-site-xml"><a href="#配置tez-site-xml" class="headerlink" title="配置tez-site.xml"></a>配置tez-site.xml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&lt;property&gt;</div><div class="line">  &lt;name&gt;tez.tez-ui.history-url.base&lt;/name&gt;</div><div class="line">  &lt;value&gt;http://schedule.will.com:8181/tez-ui-0.8.5&lt;/value&gt;</div><div class="line">&lt;/property&gt;</div><div class="line"></div><div class="line">&lt;property&gt;  </div><div class="line">    &lt;name&gt;tez.history.logging.service.class&lt;/name&gt;  </div><div class="line">    &lt;value&gt;org.apache.tez.dag.history.logging.ats.ATSHistoryLoggingService&lt;/value&gt;  </div><div class="line">&lt;/property&gt;</div></pre></td></tr></table></figure>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>设置yarn-site.xml中timeline-service的cros属性为true。之后重启timeline-service</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yarn.timeline-service.http-cross-origin.enabled = true</div></pre></td></tr></table></figure>
<p>其实tez ui就是把所有tez类型的app历史拉出来，按照tez的特性进行定制化的完美展示。</p>
<p>参考：</p>
<ul>
<li><a href="https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=61331897" target="_blank" rel="external">https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=61331897</a></li>
<li><a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html" target="_blank" rel="external">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/TimelineServer.html</a></li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/hive2-on-tez问题/" title="hive2-on-tez问题" itemprop="url">hive2-on-tez问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T09:16:51.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>hive cli端<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">hive&gt; set hive.execution.engine;</div><div class="line">hive.execution.engine=tez</div><div class="line">hive&gt; select count(1) as num, year(create_time) as time from money_record group by year(create_time);</div><div class="line">Query ID = root_20171211171347_57cea87e-f54a-457d-93cf-b312063a822e</div><div class="line">Total jobs = 1</div><div class="line">Launching Job 1 out of 1</div><div class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask</div></pre></td></tr></table></figure></p>
<p>yarn app的log信息<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">............</div><div class="line">............</div><div class="line"># container文件内容</div><div class="line"></div><div class="line">7749233 1292 -r-xr-xr-x   1 yarn     hadoop    1319613 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/tez-dag-0.8.5.jar</div><div class="line">37749246    8 -r-xr-xr-x   1 yarn     hadoop       7734 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/tez-yarn-timeline-history-with-acls-0.8.5.jar</div><div class="line">37749242  144 -r-xr-xr-x   1 yarn     hadoop     146572 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/tez-tests-0.8.5.jar</div><div class="line">37749230    4 drwxr-xr-x   2 yarn     hadoop       4096 Dec 11 17:13 ./tezlib/apache-tez-0.8.5-bin/share</div><div class="line">37749247 41916 -r-xr-xr-x   1 yarn     hadoop   42921455 Mar  8  2017 ./tezlib/apache-tez-0.8.5-bin/share/tez.tar.gz</div><div class="line"></div><div class="line">......</div><div class="line"># 启动脚本</div><div class="line">LogType:launch_container.sh</div><div class="line">Log Upload Time:Mon Dec 11 17:13:49 +0800 2017</div><div class="line">LogLength:5084</div><div class="line">Log Contents:</div><div class="line">#!/bin/bash</div><div class="line">....</div><div class="line">....</div><div class="line"></div><div class="line"># 指定类路径</div><div class="line">export CLASSPATH=&quot;/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure:$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*:$HADOOP_CONF_DIR:&quot;</div><div class="line">........</div><div class="line"></div><div class="line">LogType:stderr</div><div class="line">Log Upload Time:Mon Dec 11 17:13:50 +0800 2017</div><div class="line">LogLength:77</div><div class="line">Log Contents:</div><div class="line">Error: Could not find or load main class org.apache.tez.dag.app.DAGAppMaster</div><div class="line">End of LogType:stderr</div></pre></td></tr></table></figure></p>
<p>可以看到报错信息是找不到tez的主类，这个一般就是类路径的问题了。</p>
<p>找一个hive 1.2.1的tez的正常的程序，看一下container文件内容：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">....</div><div class="line">12322304  164 -r-xr-xr-x   1 yarn     hadoop     164267 Apr 25  2016 ./tezlib/tez-runtime-internals-0.7.0.2.4.2.0-258.jar</div><div class="line">12322432   64 -r-xr-xr-x   1 yarn     hadoop      62943 Apr 25  2016 ./tezlib/tez-history-parser-0.7.0.2.4.2.0-258.jar</div><div class="line">12322283   68 -r-xr-xr-x   1 yarn     hadoop      67195 Apr 25  2016 ./tezlib/tez-common-0.7.0.2.4.2.0-258.jar</div><div class="line">12322430   28 -r-xr-xr-x   1 yarn     hadoop      25622 Apr 25  2016 ./tezlib/tez-yarn-timeline-history-0.7.0.2.4.2.0-258.jar</div><div class="line">12322306  532 -r-xr-xr-x   1 yarn     hadoop     542434 Apr 25  2016 ./tezlib/tez-runtime-library-0.7.0.2.4.2.0-258.jar</div><div class="line">....</div><div class="line"></div><div class="line">LogType:launch_container.sh</div><div class="line">Log Upload Time:Mon Dec 11 15:47:42 +0800 2017</div><div class="line">LogLength:5740</div><div class="line">Log Contents:</div><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">....</div><div class="line">export CLASSPATH=&quot;/usr/hdp/2.4.2.0-258/hadoop/lib/hadoop-lzo-0.6.0.2.4.2.0-258.jar:/etc/hadoop/conf/secure:$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*:$HADOOP_CONF_DIR:&quot;</div><div class="line"></div><div class="line">....</div></pre></td></tr></table></figure></p>
<p>可以看到都是用的默认的<code>$PWD:$PWD/*:$PWD/tezlib/*:$PWD/tezlib/lib/*</code>, 上面的执行错误的container里的tez文件多一个文件夹路径。</p>
<p>对于<code>tez.lib.uris</code>的注释里也说到：会解压开，然后直接作为classpath。</p>
<p>tez对于classpath<a href="https://tez.apache.org/releases/0.8.5/tez-api-javadocs/configs/TezConfiguration.html" target="_blank" rel="external">配置参考</a>.</p>
<p>我们目前的配置是<br><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">  </div><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hdp/apps/$&#123;hdp.version&#125;/tez/apache-tez-0.8.5-bin.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure></p>
<p>可以看出是这个包的问题，那么就有两个方式解决了。</p>
<ul>
<li>把apache-tez-0.8.5-bin.tar.gz自己处理一下，把多余的文件夹去掉，重新上传</li>
<li>添加个classpath的前缀，也就是把文件夹名称加上。<figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>....<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
</li>
</ul>
<p>看一下tez的目录，发现share目录下有个tez.tar.gz, 这个文件就是专门做这个事情的。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/hive命令行使用tez问题/" title="hive命令行使用tez问题" itemprop="url">hive命令行使用tez问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T07:48:25.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><ul>
<li>使用ambari2.2.2.0搭建的集群环境</li>
<li>使用了自定义的hive UDF包</li>
</ul>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><p>通过hue在hiveserver2上使用tez进行查询，正常运行。</p>
<p>使用hive cli进行tez查询的时候，报错：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">2017-12-11 15:06:30,745 INFO  [main]: ql.Context (Context.java:getMRScratchDir(330)) - New scratch dir is hdfs://dd/tmp/hive/root/c6354673-afc9-4515-8c44-ff7c7f75edcb/hive_2017-12-11_15-06-30_517_4074051706046309823-1</div><div class="line">2017-12-11 15:06:30,750 INFO  [main]: exec.Task (TezTask.java:updateSession(270)) - Tez session hasn&apos;t been created yet. Opening session</div><div class="line">2017-12-11 15:06:30,750 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(130)) - Opening the session with id 97240ae8-cce2-4000-83d6-f6729146ebab for thread main log trace id -  query id - root_20171211150630_4fbc2d95-5613-438c-9499-126da1ce1f49</div><div class="line">2017-12-11 15:06:30,750 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(146)) - User of session id 97240ae8-cce2-4000-83d6-f6729146ebab is root</div><div class="line">2017-12-11 15:06:30,763 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530270</div><div class="line">2017-12-11 15:06:30,766 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530408</div><div class="line">2017-12-11 15:06:30,770 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530453</div><div class="line">2017-12-11 15:06:30,773 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530487</div><div class="line">2017-12-11 15:06:30,775 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512974530408</div><div class="line">2017-12-11 15:06:30,777 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/server/app/hive/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div><div class="line">2017-12-11 15:06:30,780 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(958)) - Looks like another thread is writing the same file will wait.</div><div class="line">2017-12-11 15:06:30,780 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(965)) - Number of wait attempts: 5. Wait interval: 5000</div><div class="line">2017-12-11 15:06:55,790 ERROR [main]: tez.DagUtils (DagUtils.java:localizeResource(981)) - Could not find the jar that was being uploaded</div><div class="line">2017-12-11 15:06:55,790 ERROR [main]: exec.Task (TezTask.java:execute(212)) - Failed to execute tez graph.</div><div class="line">java.io.IOException: Previous writer likely failed to write hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar. Failing because I am unlikely to write too.</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862)</div></pre></td></tr></table></figure></p>
<h4 id="问题定位"><a href="#问题定位" class="headerlink" title="问题定位"></a>问题定位</h4><p>上传udf到hdfs中tez session目录的时候，出现错误。</p>
<h4 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h4><p>一遍遍地检查HDFS上的目录，发现尽管报错，也已经有了这个UDF jar文件了。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@servicenode02 livy]# hdfs dfs -ls /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab</div><div class="line">Found 4 items</div><div class="line">-rw-r--r--   3 root hdfs     258861 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/hive-hcatalog-core.jar</div><div class="line">-rw-r--r--   3 root hdfs       3809 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_hive_udf.jar</div><div class="line">-rw-r--r--   3 root hdfs      83977 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_json_serde_1.3.8.jar</div><div class="line">-rw-r--r--   3 root hdfs      24630 2017-12-11 15:09 /tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div></pre></td></tr></table></figure></p>
<p>来来回回看了八百遍，到源码里也看了一下<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">try</span> &#123;</div><div class="line">    <span class="comment">// 尝试将本地文件copy到远程的目录中</span></div><div class="line">    destFS.copyFromLocalFile(<span class="keyword">false</span>, <span class="keyword">false</span>, src, dest);</div><div class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</div><div class="line">  <span class="comment">// 如果copy失败，一般就是文件已经存在了，或者有其他人在进行同样的操作</span></div><div class="line">  LOG.info(<span class="string">"Looks like another thread is writing the same file will wait."</span>);</div><div class="line">  <span class="keyword">int</span> waitAttempts =</div><div class="line">      conf.getInt(HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.varname,</div><div class="line">          HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_NUM_WAIT_ATTEMPTS.defaultIntVal);</div><div class="line">  <span class="keyword">long</span> sleepInterval = HiveConf.getTimeVar(</div><div class="line">      conf, HiveConf.ConfVars.HIVE_LOCALIZE_RESOURCE_WAIT_INTERVAL,</div><div class="line">      TimeUnit.MILLISECONDS);</div><div class="line">  LOG.info(<span class="string">"Number of wait attempts: "</span> + waitAttempts + <span class="string">". Wait interval: "</span></div><div class="line">      + sleepInterval);</div><div class="line">      </div><div class="line">  <span class="comment">// 隔一段时间检查一下，这个文件是不是已经有了</span></div><div class="line">  <span class="keyword">boolean</span> found = <span class="keyword">false</span>;</div><div class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; waitAttempts; i++) &#123;</div><div class="line">    <span class="keyword">if</span> (!checkPreExisting(src, dest, conf)) &#123;</div><div class="line">      <span class="keyword">try</span> &#123;</div><div class="line">        Thread.sleep(sleepInterval);</div><div class="line">      &#125; <span class="keyword">catch</span> (InterruptedException interruptedException) &#123;</div><div class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> IOException(interruptedException);</div><div class="line">      &#125;</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">      found = <span class="keyword">true</span>;</div><div class="line">      <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">  &#125;</div><div class="line">  <span class="comment">// 如果一直都么有，就报上面的错误了</span></div><div class="line">  <span class="keyword">if</span> (!found) &#123;</div><div class="line">    LOG.error(<span class="string">"Could not find the jar that was being uploaded"</span>);</div><div class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> IOException(<span class="string">"Previous writer likely failed to write "</span> + dest +</div><div class="line">        <span class="string">". Failing because I am unlikely to write too."</span>);</div><div class="line">  &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>对于<code>checkPreExisting</code>方法，也要看一下：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 检查目标目录下是不是已经有了同名文件，</span></div><div class="line"><span class="comment">// 如果文件已经有了，那么要检查文件是否一致，这里简单用len验证</span></div><div class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">checkPreExisting</span><span class="params">(Path src, Path dest, Configuration conf)</span></span></div><div class="line">    <span class="keyword">throws</span> IOException &#123;</div><div class="line">    FileSystem destFS = dest.getFileSystem(conf);</div><div class="line">    FileSystem sourceFS = src.getFileSystem(conf);</div><div class="line">    FileStatus destStatus = FileUtils.getFileStatusOrNull(destFS, dest);</div><div class="line">    <span class="keyword">if</span> (destStatus != <span class="keyword">null</span>) &#123;</div><div class="line">      <span class="comment">// 我挂到了这一步，就是udf的版本不一致</span></div><div class="line">      <span class="keyword">return</span> (sourceFS.getFileStatus(src).getLen() == destStatus.getLen());</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>好了。那么看一下我的问题。为了使用udf，我们的数据研发在hive cli的初始化文件<code>bin/.hiverc</code>中添加了下面的命令：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">add jar /server/app/hive/will_hive_udf.jar;</div><div class="line">add jar /usr/hdp/current/hive-client/auxlib/tinyv_hive_udf.jar;</div><div class="line">create temporary function  strstart as &apos;com.will.common.hive.StrStart&apos;;  </div><div class="line">create temporary function  strDateFormat as &apos;com.will.common.hive.StrDateFormat&apos;;</div><div class="line">create temporary function  strContain as &apos;com.will.common.hive.StrContain&apos;; </div><div class="line">create temporary function  parse_uri as &apos;com.will.common.hive.ParseUri&apos;;</div><div class="line">create temporary function  strEnd as &apos;com.will.common.hive.StrEnd&apos;;</div><div class="line">create temporary function  split_by_index as &apos;com.will.common.hive.SplitByIndex&apos;;</div><div class="line">create temporary function birthday as &apos;com.will.common.hive.BirthdayByIdcard&apos;;</div><div class="line">create temporary function gender as &apos;com.will.common.hive.GenderByIdcard&apos;;  </div><div class="line">create temporary function usernamesen as &apos;com.will.common.hive.UserNameSen&apos;;</div><div class="line">create temporary function strTrim as &apos;com.will.common.hive.StrTrim&apos;;</div><div class="line">create temporary function channel as &apos;com.will.common.hive.Channel&apos;;</div><div class="line">create temporary function iptonum as &apos;com.will.common.hive.IpToNumber&apos;;</div><div class="line">create temporary function geturl as &apos;com.will.common.hive.ParseUrls&apos;;</div><div class="line">create temporary function money_record_type as &apos;com.will.common.hive.MoneyRecordType&apos;;</div><div class="line">create temporary function xv_encode as &apos;com.will.tinyv.Encode&apos;;</div><div class="line">create temporary function xv_decode as &apos;com.will.tinyv.Decode&apos;;</div><div class="line">set hive.mapred.mode=nostrict;</div></pre></td></tr></table></figure></p>
<p>重新启动hive cli，报错的日志，发现上传了来自两个不同地方的will_hive_udf.jar文件：一个来自<code>hive/auxlib</code>目录，另一个来自<code>/server/app</code>…..而<strong>只有第一次报错的时候才能看到第一个udf的上传过程，因为同一个session中，已经上传过的，验证没问题就跳过了。。。。只打印了一个时间戳，也没有指明是哪个文件</strong>…..导致定位问题的时候，有些恍惚，还以为是tez与UDF功能之间的问题<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">2017-12-11 15:09:30,604 INFO  [main]: ql.Context (Context.java:getMRScratchDir(330)) - New scratch dir is hdfs://dd/tmp/hive/root/c6354673-afc9-4515-8c44-ff7</div><div class="line">c7f75edcb/hive_2017-12-11_15-09-30_406_4902171368286077085-1</div><div class="line">2017-12-11 15:09:30,608 INFO  [main]: exec.Task (TezTask.java:updateSession(270)) - Tez session hasn&apos;t been created yet. Opening session</div><div class="line">2017-12-11 15:09:30,608 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(130)) - Opening the session with id 97240ae8-cce2-4000-83d6-f6729146ebab</div><div class="line"> for thread main log trace id -  query id - root_20171211150930_d928c00e-62da-4c0e-95c2-23a28594a7c0</div><div class="line">2017-12-11 15:09:30,608 INFO  [main]: tez.TezSessionState (TezSessionState.java:open(146)) - User of session id 97240ae8-cce2-4000-83d6-f6729146ebab is root</div><div class="line">2017-12-11 15:09:30,613 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/curre</div><div class="line">nt/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/hive-hcatalog-co</div><div class="line">re.jar</div><div class="line">2017-12-11 15:09:30,655 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170667</div><div class="line">2017-12-11 15:09:30,656 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2</div><div class="line">.0-258/hive/auxlib/tinyv_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_hive_udf.jar</div><div class="line">2017-12-11 15:09:30,683 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170691</div><div class="line">2017-12-11 15:09:30,684 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2</div><div class="line">.0-258/hive/auxlib/tinyv_json_serde_1.3.8.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/tinyv_json_serde_1.3.8.j</div><div class="line">ar</div><div class="line">2017-12-11 15:09:30,707 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170720</div><div class="line">2017-12-11 15:09:30,709 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/usr/hdp/2.4.2.0-258/hive/auxlib/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div><div class="line">2017-12-11 15:09:30,729 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170742</div><div class="line">2017-12-11 15:09:30,732 INFO  [main]: tez.DagUtils (DagUtils.java:createLocalResource(720)) - Resource modification time: 1512976170691</div><div class="line">2017-12-11 15:09:30,733 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(954)) - Localizing resource because it does not exist: file:/server/app/hive/will_hive_udf.jar to dest: hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar</div><div class="line">2017-12-11 15:09:30,735 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(958)) - Looks like another thread is writing the same file will wait.</div><div class="line">2017-12-11 15:09:30,736 INFO  [main]: tez.DagUtils (DagUtils.java:localizeResource(965)) - Number of wait attempts: 5. Wait interval: 5000</div><div class="line">2017-12-11 15:09:55,745 ERROR [main]: tez.DagUtils (DagUtils.java:localizeResource(981)) - Could not find the jar that was being uploaded</div><div class="line">2017-12-11 15:09:55,745 ERROR [main]: exec.Task (TezTask.java:execute(212)) - Failed to execute tez graph.</div><div class="line">java.io.IOException: Previous writer likely failed to write hdfs://dd/tmp/hive/root/_tez_session_dir/97240ae8-cce2-4000-83d6-f6729146ebab/will_hive_udf.jar. Failing because I am unlikely to write too.</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeResource(DagUtils.java:982)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.addTempResources(DagUtils.java:862)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.DagUtils.localizeTempFilesFromConf(DagUtils.java:805)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.refreshLocalResourcesFromConf(TezSessionState.java:233)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:158)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezTask.updateSession(TezTask.java:271)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:151)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)</div><div class="line">        at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)</div></pre></td></tr></table></figure></p>
<h4 id="总结原因"><a href="#总结原因" class="headerlink" title="总结原因"></a>总结原因</h4><ul>
<li>udf版本不一致，导致后面jar包上传失败</li>
</ul>
<h4 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h4><p>把.hiverc中的命令暂时去掉，重试tez引擎。就能成功调用了。</p>
<h4 id="其他思考"><a href="#其他思考" class="headerlink" title="其他思考"></a>其他思考</h4><p>为什么已经设定了<code>hive.aux.jars.path</code>了，还要在.hiverc额外处理呢。</p>
<p>先在hivecli中确认一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@schedule ~]#hive -e &quot;set hive.aux.jars.path&quot;</div><div class="line">WARNING: Use &quot;yarn jar&quot; to launch YARN applications.</div><div class="line"></div><div class="line">Logging initialized using configuration in file:/etc/hive/2.4.2.0-258/0/hive-log4j.properties</div><div class="line">Putting the global hiverc in $HIVE_HOME/bin/.hiverc is deprecated. Please use $HIVE_CONF_DIR/.hiverc instead.</div><div class="line">hive.aux.jars.path=file:///usr/hdp/current/hive-webhcat/share/hcatalog/hive-hcatalog-core.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_hive_udf.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/tinyv_json_serde_1.3.8.jar,file:///usr/hdp/2.4.2.0-258/hive/auxlib/will_hive_udf.jar</div></pre></td></tr></table></figure></p>
<p>但是测试了一下，没有.hiverc里的初始化，就不能执行udf函数。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">hive&gt; select iptonum(&apos;10.2.19.32&apos;);</div><div class="line">FAILED: SemanticException [Error 10011]: Line 1:7 Invalid function &apos;iptonum&apos;</div></pre></td></tr></table></figure></p>
<p>可是hue里，也就是hiveserver2里却可以执行udf函数。</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/11/canal同步的TimeStamp问题/" title="canal同步的TimeStamp问题" itemprop="url">canal同步的TimeStamp问题</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-11T05:30:37.000Z" itemprop="datePublished"> 发表于 2017-12-11</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>部门接了新的实时方面的需求，选定使用canal进行mysql binlog方面的处理。</p>
<p>上线一段时间后都比较稳定，但是有一天，实时数据开发的同事找到我说有个别字段是有问题的。大概表结构如下：<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> <span class="string">`bb`</span> (</div><div class="line">  <span class="string">`id`</span> <span class="built_in">int</span>(<span class="number">11</span>) <span class="keyword">NOT</span> <span class="literal">NULL</span> AUTO_INCREMENT,</div><div class="line">  <span class="string">`lastUpdateTime`</span> <span class="keyword">timestamp</span> <span class="literal">NULL</span> <span class="keyword">DEFAULT</span> <span class="keyword">CURRENT_TIMESTAMP</span>,</div><div class="line">  <span class="string">`firstTimeBindTime`</span> datetime <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</div><div class="line">  <span class="string">`name`</span> <span class="built_in">varchar</span>(<span class="number">255</span>) <span class="keyword">DEFAULT</span> <span class="literal">NULL</span>,</div><div class="line">  PRIMARY <span class="keyword">KEY</span> (<span class="string">`id`</span>)</div><div class="line">) <span class="keyword">ENGINE</span>=MyISAM AUTO_INCREMENT=<span class="number">1526640</span> <span class="keyword">DEFAULT</span> <span class="keyword">CHARSET</span>=utf8</div></pre></td></tr></table></figure></p>
<p>有问题的字段为<code>lastUpdateTime</code>，主库为<code>2017-12-11 11:22:40</code>的时候，canal client中消费到的却是<code>2017-12-10 22:22:40</code>，而且所有这个字段都是相差同样的13个小时。但是有一个地方比较值得注意，就是<code>firstTimeBindTime</code>的值都是正常的。</p>
<h4 id="排查流程"><a href="#排查流程" class="headerlink" title="排查流程"></a>排查流程</h4><ul>
<li><p>查看canal server的系统时区，进行修正确认</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime </div><div class="line">date # 发现时间已经正确了</div></pre></td></tr></table></figure>
</li>
<li><p>重启canal server</p>
</li>
</ul>
<p>发现还是有问题，去mysql主机上翻看binlog，找到对应的记录。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mysqlbinlog -vv /server/mysql_data/mysql-bin.000141</div></pre></td></tr></table></figure></p>
<p>发现这个字段对应的时间戳是正确的<code>1512962560</code>。那么问题看来还是在canal server这块了。</p>
<h4 id="源码走读"><a href="#源码走读" class="headerlink" title="源码走读"></a>源码走读</h4><p>顺便看下源码吧。对于binlog进行解析的主类：<code>MysqlEventParser.start()</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">...</div><div class="line"><span class="comment">// event处理与解析</span></div><div class="line">CanalEntry.Entry entry = parseAndProfilingIfNecessary(event);</div><div class="line">...</div></pre></td></tr></table></figure>
<p>进入<code>parseAndProfilingIfNecessary</code>方法</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="comment">// 使用parser解析</span></div><div class="line">CanalEntry.Entry event = binlogParser.parse(bod);</div><div class="line">...</div></pre></td></tr></table></figure>
<p>找到对应的parser类为LogEventConvert，可以看到<code>LogEventConvert</code>是继承了<code>BinlogParser</code>的。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">protected</span> BinlogParser <span class="title">buildParser</span><span class="params">()</span> </span>&#123;</div><div class="line">    LogEventConvert convert = <span class="keyword">new</span> LogEventConvert();</div><div class="line">    ......</div><div class="line">    <span class="keyword">return</span> convert;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>再去<code>LogEventConvert</code>的parse方法<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">...</div><div class="line"><span class="comment">// 对于修改数据的event的处理</span></div><div class="line"><span class="keyword">case</span> LogEvent.WRITE_ROWS_EVENT:</div><div class="line">    <span class="keyword">return</span> parseRowsEvent((WriteRowsLogEvent) logEvent);</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>继续找<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> (buffer.nextOneRow(columns)) &#123;</div><div class="line">    <span class="comment">// 处理row记录</span></div><div class="line">    RowData.Builder rowDataBuilder = RowData.newBuilder();</div><div class="line">    <span class="keyword">if</span> (EventType.INSERT == eventType) &#123;</div><div class="line">        <span class="comment">// insert的记录放在before字段中</span></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, <span class="keyword">true</span>, tableMeta);</div><div class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (EventType.DELETE == eventType) &#123;</div><div class="line">        <span class="comment">// delete的记录放在before字段中</span></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, <span class="keyword">false</span>, tableMeta);</div><div class="line">    &#125; <span class="keyword">else</span> &#123;</div><div class="line">        <span class="comment">// update需要处理before/after</span></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, columns, <span class="keyword">false</span>, tableMeta);</div><div class="line">        <span class="keyword">if</span> (!buffer.nextOneRow(changeColumns)) &#123;</div><div class="line">            rowChangeBuider.addRowDatas(rowDataBuilder.build());</div><div class="line">            <span class="keyword">break</span>;</div><div class="line">        &#125;</div><div class="line"></div><div class="line">        tableError |= parseOneRow(rowDataBuilder, event, buffer, changeColumns, <span class="keyword">true</span>, tableMeta);</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    rowChangeBuider.addRowDatas(rowDataBuilder.build());</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>对于行数据的每个字段的处理，是先获取到value，然后再针对地做一些额外处理。<br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">....</div><div class="line"><span class="keyword">final</span> Serializable value = buffer.getValue();</div><div class="line">...</div></pre></td></tr></table></figure></p>
<p>通过<code>RowsLogBuffer</code>进行value的获取，我们找到给value赋值的地方<code>fetchValue</code><br><figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">final</span> Serializable <span class="title">fetchValue</span><span class="params">(<span class="keyword">int</span> type, <span class="keyword">final</span> <span class="keyword">int</span> meta, <span class="keyword">boolean</span> isBinary)</span> </span>&#123;</div><div class="line">    .....</div><div class="line">    <span class="keyword">switch</span> (type) &#123;</div><div class="line">	 <span class="keyword">case</span> LogEvent.MYSQL_TYPE_TIMESTAMP: &#123;</div><div class="line">        <span class="keyword">final</span> <span class="keyword">long</span> i32 = buffer.getUint32();</div><div class="line">        <span class="keyword">if</span> (i32 == <span class="number">0</span>) &#123;</div><div class="line">            value = <span class="string">"0000-00-00 00:00:00"</span>;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            String v = <span class="keyword">new</span> Timestamp(i32 * <span class="number">1000</span>).toString();</div><div class="line">            value = v.substring(<span class="number">0</span>, v.length() - <span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line">        javaType = Types.TIMESTAMP;</div><div class="line">        length = <span class="number">4</span>;</div><div class="line">        <span class="keyword">break</span>;</div><div class="line">    &#125;</div><div class="line">    .....</div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>把这部分弄出去，使用上面binlog文件中的timestamp单独测试.</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"><span class="keyword">import</span> java.sql.Timestamp;</div><div class="line"></div><div class="line"><span class="comment">/**</span></div><div class="line"> * Created by root on 17-12-11.</div><div class="line"> */</div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Will</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] p )</span></span>&#123;</div><div class="line">        String value = <span class="string">""</span>;</div><div class="line">        <span class="keyword">final</span> <span class="keyword">long</span> i32 = <span class="number">1512962560l</span>;</div><div class="line">        System.out.println(TimeZone.getDefault());</div><div class="line">        System.out.println(i32);</div><div class="line">        <span class="keyword">if</span> (i32 == <span class="number">0</span>) &#123;</div><div class="line">            value = <span class="string">"0000-00-00 00:00:00"</span>;</div><div class="line">        &#125; <span class="keyword">else</span> &#123;</div><div class="line">            String v = <span class="keyword">new</span> Timestamp(i32 * <span class="number">1000</span>).toString();</div><div class="line">            value = v.substring(<span class="number">0</span>, v.length() - <span class="number">2</span>);</div><div class="line">        &#125;</div><div class="line">        System.out.println(value);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>在测试机是正常的, 但是在线上环境是不正常的，最后在java里输出了一下<code>Timezone.getDefault()</code>，发现线上是”America/New_York”，正好就是差13个小时的。可是在系统里运行date命令，返回的是上海时区。上网搜了一下，有人建议使用下面命令确定时区<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@etl01 ~]# ls -l /etc/localtime</div><div class="line">lrwxrwxrwx. 1 root root 38 Oct 21 00:25 /etc/localtime -&gt; ../usr/share/zoneinfo/America/New_York</div></pre></td></tr></table></figure></p>
<p>这里看到虽然我前面使用cp已经覆盖了/etc/localtime文件，而且并没有报错。但是查看的时候，还是显示为到纽约的链接。</p>
<p>删掉这个文件，重新cp。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@etl01 ~]# rm -vf /etc/localtime</div><div class="line">removed \u2018/etc/localtime\u2019</div><div class="line">[root@etl01 ~]# cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime </div><div class="line">[root@etl01 ~]# ls -l /etc/localtime</div><div class="line">-rw-r--r--. 1 root root 405 Dec 11 13:14 /etc/localtime</div><div class="line">[root@etl01 ~]# java Will</div><div class="line">sun.util.calendar.ZoneInfo[id=&quot;America/New_York&quot;,offset=-18000000,dstSavings=3600000,useDaylight=true,transitions=235,lastRule=java.util.SimpleTimeZone[id=America/New_York,offset=-18000000,dstSavings=3600000,useDaylight=true,startYear=0,startMode=3,startMonth=2,startDay=8,startDayOfWeek=1,startTime=7200000,startTimeMode=0,endMode=3,endMonth=10,endDay=1,endDayOfWeek=1,endTime=7200000,endTimeMode=0]]</div><div class="line">America/New_York</div></pre></td></tr></table></figure></p>
<p>还是不对，无奈，再次删掉。按照系统的样子，创建软链，成功。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@etl01 ~]# rm /etc/localtime </div><div class="line">rm: remove regular file \u2018/etc/localtime\u2019? y</div><div class="line">[root@etl01 ~]# ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</div><div class="line">[root@etl01 ~]# ls -l /etc/localtime</div><div class="line">lrwxrwxrwx. 1 root root 33 Dec 11 13:15 /etc/localtime -&gt; /usr/share/zoneinfo/Asia/Shanghai</div><div class="line">[root@etl01 ~]# java Will</div><div class="line">sun.util.calendar.ZoneInfo[id=&quot;Asia/Shanghai&quot;,offset=28800000,dstSavings=0,useDaylight=false,transitions=19,lastRule=null]</div><div class="line">Asia/Shanghai</div></pre></td></tr></table></figure></p>
<p>CentOS Linux release 7.0.1406 (Core)系统……….比较无语</p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/07/hive-llap启动/" title="hive-llap启动" itemprop="url">hive-llap启动</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-07T09:22:12.000Z" itemprop="datePublished"> 发表于 2017-12-07</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h4><ul>
<li>tez</li>
<li>slider</li>
</ul>
<h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><h5 id="conf-hive-env-sh"><a href="#conf-hive-env-sh" class="headerlink" title="conf/hive-env.sh"></a>conf/hive-env.sh</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 到tez的配置目录里找tez-site.xml</div><div class="line">export TEZ_HOME=/usr/hdp/current/tez-client</div><div class="line">export TEZ_CONF_DIR=$TEZ_HOME/conf</div><div class="line"></div><div class="line">HADOOP_HOME=/usr/hdp/current/hadoop-client/</div></pre></td></tr></table></figure>
<h5 id="hive-site-xml"><a href="#hive-site-xml" class="headerlink" title="hive-site.xml"></a>hive-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.mode<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>llap<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">description</span>&gt;</span></div><div class="line">     Expects one of [container, llap].</div><div class="line">     Chooses whether query fragments will run in container or in llap</div><div class="line">   <span class="tag">&lt;/<span class="name">description</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h5 id="tez-site-xml"><a href="#tez-site-xml" class="headerlink" title="tez-site.xml"></a>tez-site.xml</h5><figure class="highlight xml"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>tez.lib.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></div><div class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>/hdp/apps/$&#123;hdp.version&#125;/tez/apache-tez-0.8.5-bin.tar.gz<span class="tag">&lt;/<span class="name">value</span>&gt;</span></div><div class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></div></pre></td></tr></table></figure>
<h4 id="slider查看"><a href="#slider查看" class="headerlink" title="slider查看"></a>slider查看</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@servicenode04 apache-hive-2.3.2-bin]# slider list</div><div class="line">2017-12-07 18:10:01,421 [main] INFO  impl.TimelineClientImpl - Timeline service address: http://datanode04.will.com:8188/ws/v1/timeline/</div><div class="line">2017-12-07 18:10:02,387 [main] WARN  shortcircuit.DomainSocketFactory - The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.</div><div class="line">2017-12-07 18:10:02,403 [main] INFO  client.RMProxy - Connecting to ResourceManager at datanode02.will.com/10.2.19.83:8050</div><div class="line">llap_service                       RUNNING  application_1511064572746_27005             http://datanode02.will.com:8088/proxy/application_1511064572746_27005/</div></pre></td></tr></table></figure>
<h4 id="报错"><a href="#报错" class="headerlink" title="报错"></a>报错</h4><h5 id="container大小"><a href="#container大小" class="headerlink" title="container大小"></a>container大小</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@servicenode04 apache-hive-2.3.2-bin]# ./bin/hive --service llap --instances 5 </div><div class="line">llap</div><div class="line">INFO cli.LlapServiceDriver: LLAP service driver invoked with arguments=-directory</div><div class="line">INFO conf.HiveConf: Found configuration file file:/server/apache-hive-2.3.2-bin/conf/hive-site.xml</div><div class="line">Failed: Container size (-1B) should be greater than minimum allocation(2.00GB)</div><div class="line">java.lang.IllegalArgumentException: Container size (-1B) should be greater than minimum allocation(2.00GB)</div><div class="line">	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:92)</div><div class="line">	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.run(LlapServiceDriver.java:313)</div><div class="line">	at org.apache.hadoop.hive.llap.cli.LlapServiceDriver.main(LlapServiceDriver.java:116)</div><div class="line">INFO cli.LlapServiceDriver: LLAP service driver finished</div></pre></td></tr></table></figure>
<p>提示是container大小还没有最小的大。加个参数就行了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./bin/hive --service llap --size 2147483648 --instances 3</div></pre></td></tr></table></figure></p>
<h5 id="配置漏项"><a href="#配置漏项" class="headerlink" title="配置漏项"></a>配置漏项</h5><p>生成slider项目目录之后，启动llap失败。到yarn的日志里翻看，发现异常：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">yarn logs --applicationId application_1511064572746_25101 |less</div></pre></td></tr></table></figure></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">017-12-06T11:56:39,771 WARN  [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exception</div><div class="line">java.lang.NullPointerException</div><div class="line">        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.&lt;init&gt;(LlapDaemon.java:139) ~[hive-llap-server-2.3.2.jar:2.3.2]</div><div class="line">        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.main(LlapDaemon.java:521) [hive-llap-server-2.3.2.jar:2.3.2]</div><div class="line">End of LogType:llap-daemon-root-datanode02.will.com.log</div></pre></td></tr></table></figure>
<p>找到对应版本的源码里，发现是少了配置项<code>String hosts = HiveConf.getTrimmedVar(daemonConf, ConfVars.LLAP_DAEMON_SERVICE_HOSTS);</code>,也就是<code>hive.llap.daemon.service.hosts</code>，应该配置为@llap_service.</p>
<h5 id="tez版本问题"><a href="#tez版本问题" class="headerlink" title="tez版本问题"></a>tez版本问题</h5><p>重新生产slider项目，再次，运行，还是异常退出了，继续看yarn的log。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">ue</div><div class="line">2017-12-07T17:12:20,582 WARN  [main ()] org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon: Failed to start LLAP Daemon with exception</div><div class="line">java.lang.NoClassDefFoundError: org/apache/tez/hadoop/shim/HadoopShimsLoader</div><div class="line">        at org.apache.hadoop.hive.llap.daemon.impl.ContainerRunnerImpl.&lt;init&gt;(ContainerRunnerImpl.java:157) ~[hive-llap-server-2.3.2.jar:2.3.2]</div><div class="line">        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.&lt;init&gt;(LlapDaemon.java:283) ~[hive-llap-server-2.3.2.jar:2.3.2]</div><div class="line">        at org.apache.hadoop.hive.llap.daemon.impl.LlapDaemon.main(LlapDaemon.java:521) [hive-llap-server-2.3.2.jar:2.3.2]</div><div class="line">Caused by: java.lang.ClassNotFoundException: org.apache.tez.hadoop.shim.HadoopShimsLoader</div><div class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381) ~[?:1.8.0_60]</div><div class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424) ~[?:1.8.0_60]</div><div class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) ~[?:1.8.0_60]</div><div class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ~[?:1.8.0_60]</div><div class="line">        ... 3 more</div><div class="line">End of LogType:llap-daemon-root-datanode05.will.com.log</div></pre></td></tr></table></figure></p>
<p>查了一下，这个类是tez的。我当前使用的是0.7版本的，到github上试了一下，这个版本还没有这个类。那就自己下载个有这个类的tez，上传到hdfs，重新修改tez-site.xml，指定下位置。</p>
<h5 id="log4j版本问题？"><a href="#log4j版本问题？" class="headerlink" title="log4j版本问题？"></a>log4j版本问题？</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">Exception in thread &quot;main&quot; java.lang.NoClassDefFoundError: org/apache/commons/logging/LogFactory</div><div class="line">        at org.apache.hadoop.service.AbstractService.&lt;clinit&gt;(AbstractService.java:43)</div><div class="line">Caused by: java.lang.ClassNotFoundException: org.apache.commons.logging.LogFactory</div><div class="line">        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)</div><div class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)</div><div class="line">        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)</div><div class="line">        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)</div><div class="line">        ... 1 more</div><div class="line">End of LogType:llap-daemon-root-datanode11.will.com.out</div></pre></td></tr></table></figure>
<p>这次是hadoop的AbstractService初始化的时候，找不到commons-logging的类了…好累</p>
<p>看这行日志上面输出了类路径之类的东西：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">+ exec /server/java/jdk1.8.0_60/bin/java -Dproc_llapdaemon -Xms4096m -Xmx4096m -XX:+UseG1GC -XX:+ResizeTLAB -XX:+UseNUMA -XX:-ResizePLAB -server -Djava.net.p</div><div class="line">referIPv4Stack=true -XX:NewRatio=8 -XX:+UseNUMA -XX:+PrintGCDetails -verbose:gc -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=4 -XX:GCLogFileSize=100M -XX</div><div class="line">:+PrintGCDateStamps -Xloggc:/server/hadoop/yarn/log/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006//gc.log -Djava.io.tmpdir=/ser</div><div class="line">ver/hadoop/yarn/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/tmp/ -Dlog4j.configurationFile=llap</div><div class="line">-daemon-log4j2.properties -Dllap.daemon.log.dir=/server/hadoop/yarn/log/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/ -Dllap.d</div><div class="line">aemon.log.file=llap-daemon-root-datanode02.will.com.log -Dllap.daemon.root.logger=query-routing -Dllap.daemon.log.level=INFO -classpath &apos;/server/hadoop/yar</div><div class="line">n/local/usercache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//conf/:/server/hadoop/yarn/local/user</div><div class="line">cache/root/appcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//lib/*:/server/hadoop/yarn/local/usercache/root/a</div><div class="line">ppcache/application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//lib/tez/*:/server/hadoop/yarn/local/usercache/root/appcache/</div><div class="line">application_1511064572746_27003/container_e51_1511064572746_27003_01_000006/app/install//lib/udfs/*:.:&apos;</div></pre></td></tr></table></figure></p>
<p>明显发现这个classpath是不包含hadoop相关的类的。一番查找，找到hive根目录下的<code>scripts/llap/bin/runLlapDaemon.sh</code>文件，添加一下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">CLASSPATH=$&#123;LLAP_DAEMON_CONF_DIR&#125;:$&#123;LLAP_DAEMON_HOME&#125;/lib/*:$&#123;LLAP_DAEMON_HOME&#125;/lib/tez/*:$&#123;LLAP_DAEMON_HOME&#125;/lib/udfs/*:.`hadoop classpath`</div></pre></td></tr></table></figure></p>
<p>再重新生成slider项目。</p>
<p>参考：</p>
<p><a href="http://housong.github.io/2017/hive-llap/" target="_blank" rel="external">http://housong.github.io/2017/hive-llap/</a></p>
<p><a href="http://blog.csdn.net/qingzhenli/article/details/72723018" target="_blank" rel="external">http://blog.csdn.net/qingzhenli/article/details/72723018</a></p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/06/开源年会/" title="开源年会" itemprop="url">开源年会</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-06T08:17:00.000Z" itemprop="datePublished"> 发表于 2017-12-06</time>
    
  </p>
</header>
    <div class="article-content">
        
        <h4 id="霍泰稳-本土社区运营的思考"><a href="#霍泰稳-本土社区运营的思考" class="headerlink" title="霍泰稳  本土社区运营的思考"></a>霍泰稳  本土社区运营的思考</h4><h5 id="运营"><a href="#运营" class="headerlink" title="运营"></a>运营</h5><ul>
<li>社区开始需要灵魂人物，后面不应该需要，而应该是品牌效应<br>  开始的时候，要频繁出去跑，与各种专家交往，成为交际花。当品牌已经有了的时候，就要重点突出品牌，而不是个人了。</li>
<li>架构<br>  一线人员参与内容，因为内容专业性很强。使用架构组织内容的生产，总编辑 -&gt; 6个专题首席编辑(java, ruby，每个专题有一个首席编辑，负责组建对应主题的编辑运营团队) -&gt; 若干社区编辑。</li>
<li><p>付费</p>
<ul>
<li>不要被人当成公益组织，不带道德光环。费用是按照翻译字数/写作字数，或者专家约稿收取。多多少少有些补贴。</li>
<li><p>大会门票理所当然也不会是免费的。</p>
<p>这样才能让人们知道自己的支持是有价值、有价格的。</p>
</li>
</ul>
</li>
<li>开源和免费完全没关系</li>
</ul>
<h5 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h5><ul>
<li>raven。自动展示infoQ的英文站文章，然后社区编辑过来领取任务，进行翻译。有些众包平台的意思，以前都是专人整理Excel，每天发送一次，统计一次。</li>
<li>wiki。</li>
<li>stuQ. 流程</li>
</ul>
<h5 id="经验"><a href="#经验" class="headerlink" title="经验"></a>经验</h5><ul>
<li>运营公司的心态去做社区，与时俱进调整价值观</li>
<li>青出于蓝，找到可以帮助自己的黑马</li>
<li>坚持创新</li>
</ul>
<h4 id="kylin-成长之路"><a href="#kylin-成长之路" class="headerlink" title="kylin 成长之路"></a>kylin 成长之路</h4><p>teradata太昂贵，数据都放在HDFS，原来需要将HDFS的数据先load到teardata才能查询。调研之后，就决定自己做。luke负责这个东西，找到蒋旭(了解SQL分析与支持，了解Hadoop)。13年开始做，14年10月1日，ebay内部上了生产环境，同时放到github上进行开源，很快收获了很多的好的评价。</p>
<p>14年11月成为apache孵化器项目，接受了很多mentor的指导，代码迁移到apache代码库，成员加入apache，各种license的处理与剥离。解决大量用户的各种问题，快速升级。获得了国外知名技术社区的关注，并给以奖项。</p>
<p>kylin与apache建立私人关系，开始在国内美团、京东、bat等开始铺展，进一步推动孵化器向顶级项目转化。apache社区的人有人说：中国人喜欢用QQ群等工具尽享项目讨论，而不符合apache社区的邮件讨论的风格。</p>
<p>也有人投反对票，前端有google的不明确license前端字体。随后，kylin快速处理，发布新的release，以供审核。</p>
<p>后来成立了kylingence，专门搞kylin。开源版知识产权属于apache社区。</p>
<p>kylin2.0主要变化是支持雪花模型，支持spark进行构建工作，2.1支持查询下压(下压给hive或者spark sql进行查询)。</p>
<p>核心代码人员有5个，活跃的commiter不到10个，contributter有70个左右。</p>
<ul>
<li>对用户友好，好用。kylin最开始就很完善，包括前端、后端、性能</li>
<li>构建生态。合适地利用其他技术，不要重复造轮子</li>
<li>轻量级构建。对管理员友好，易于扩展、容灾等</li>
<li>倾听社区用户。解决用户痛点，才能得民心</li>
</ul>
<p>贡献到kylin</p>
<ul>
<li>加入社区讨论，知道大家在做什么，难点在哪儿</li>
<li>贡献patch，或者github提PR，量大了就直接提为committer</li>
</ul>
<h4 id="carbonData"><a href="#carbonData" class="headerlink" title="carbonData"></a>carbonData</h4><p>保证代码质量：充分的review，大牛多不能保证质量</p>
<p>把apache 文档研究了一个月，把项目关键路径规划了一下</p>
<p>去美国找了基金会的导师，这个比较重要</p>
<p>有人会说你的代码跟我们的代码相似，就懵了….就跟他说受了你的鼓舞~~~~然后成了导师</p>
<p>apache way</p>
<p>只要有人用起来，就会被鞭子抽着跑</p>
<h4 id="apache-weex"><a href="#apache-weex" class="headerlink" title="apache weex"></a>apache weex</h4><p>使用HTML + css描述app结构、样式，逻辑写在script中。可以远程加载成为native 界面。</p>
<p><a href="http://weex.apache.org/cn/" target="_blank" rel="external">http://weex.apache.org/cn/</a></p>
<ul>
<li>内部事业部推销</li>
<li>外部邀请使用者参与进来</li>
<li>跟内部apache项目团队取经</li>
</ul>
<p>开源的目的：</p>
<ul>
<li>加大曝光率，增强内部竞争优势：阿里内部有很多类似weex的东西</li>
<li>国际化</li>
<li>去掉商业背景，业界共享</li>
</ul>
<p>问题：</p>
<p>目前committer只有7个，而且都是阿里的。近一年内，只release了一次。</p>
<p>原因：</p>
<p>团队业务属性比较重，这种开发方式与apache这种社区的方式有些区别，JIRA上基本都是内部业务的，并没有跟apache社区的结合同步起来。文档更新不及时、版本划分不明晰等。</p>
<ul>
<li>单元测试质量与覆盖率</li>
</ul>
<h4 id="RocketMQ"><a href="#RocketMQ" class="headerlink" title="RocketMQ"></a>RocketMQ</h4><p>所有成功的开源项目，背后都是一个商业化公司在运营。</p>
<p>社区：reader、user，还有writer，负责宣传工具的好处。</p>
<p>有很多同质的项目，比如kafka。</p>
<p>社区要大、要建生态、定规范(方便同质产品的互操作)、商业化解决方案。</p>
<p>捐献项目(宽进严出)</p>
<ul>
<li>找导师，3位以上，其中一个是主席(activeMQ的相关人)，找对了champion，就成功了一半</li>
<li>找mentor</li>
<li>找proposer，项目的介绍，项目的承诺：不能孤立，人员多元化</li>
</ul>
<p>apache协议</p>
<ul>
<li>可以用或者改我的软件，告诉我改了哪儿？ 在license中说明</li>
<li>扩展协议要与apache协议兼容</li>
<li>用我的软件，出问题我不负责</li>
</ul>
<p>apache way（价值主张）</p>
<ul>
<li>社区大于产品</li>
<li>writer，就是社区运营</li>
<li>社区中每个人都是个体，不受任何人雇佣，没有任何上下级关系</li>
<li>社区的表彰是基于对社区的共享，持续关注并提交PR，就会被邀请成为commiter，然后负责更多的事情，一直到成为PMC</li>
</ul>
<p>流程</p>
<ul>
<li>apache license的扫描等</li>
<li>单元测试必须在3分钟之内做完。</li>
<li>集成测试，maven</li>
</ul>
<p>PR的模板</p>
<ul>
<li>简单告诉改动了什么</li>
<li>怎样验证此次改动</li>
</ul>
<h4 id="阿里类docker工具-pouch"><a href="#阿里类docker工具-pouch" class="headerlink" title="阿里类docker工具: pouch"></a>阿里类docker工具: pouch</h4><h5 id="隔离性"><a href="#隔离性" class="headerlink" title="隔离性"></a>隔离性</h5><p>docker容器内free -g，其实是宿主机的内存，但是java应用的内存分配是根据这个自动分配的，所以会有些问题。</p>
<p>基于磁盘和网络的隔离与限制</p>
<h5 id="仓库的p2p分发"><a href="#仓库的p2p分发" class="headerlink" title="仓库的p2p分发"></a>仓库的p2p分发</h5><p>降低各种客户端同时对registry进行pull或push等。P2P就是类迅雷的东西了，每个客户端也是一个服务端</p>
<p>##### </p>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>





   
    
    <article class="post-expand post" itemprop="articleBody"> 
        <header class="article-info clearfix">
  <h1 itemprop="name">
    
      <a href="/2017/12/05/slider对于app的要求/" title="slider对于app的要求" itemprop="url">slider对于app的要求</a>
  </h1>
  <p class="article-author">By
       
		<a href="/about" title="Will Chen" target="_blank" itemprop="author">Will Chen</a>
		
  <p class="article-time">
    <time datetime="2017-12-05T09:49:35.000Z" itemprop="datePublished"> 发表于 2017-12-05</time>
    
  </p>
</header>
    <div class="article-content">
        
        <p>slider把app安装并运行在yarn上，但是app并不一定要是针对yarn定制的。</p>
<p>app应该是被slider发布，也就是yarn负责安装，slider负责配置，最后yarn负责执行。yarn会在销毁container的时候kill掉执行的进程，所以被部署的app应该知道这个，并能够在没有人工参与的前提下自动启动一个新的container运行自己的component。</p>
<p>app的component还应该能够自己动态发现其他的component，在启动和执行的时候都要能发现，因为后面server或者进程失败的话，也会导致component位置变化。</p>
<h4 id="must"><a href="#must" class="headerlink" title="must"></a>must</h4><ul>
<li>使用tar进行安装运行，而且运行用户不能是root</li>
<li>自包含或者所有的依赖都已经预装了</li>
<li>支持节点的动态发现，比如通过ZK</li>
<li>节点能够动态rebind自己，就是说如果即便节点被删除，app仍能继续运行</li>
<li>把kill作为通用机制处理</li>
<li>支持多个实例运行在同个集群中，不同app实例可以共享同一个server</li>
<li>当多个role实例被分配到同一个物理节点的时候，要能够正确操作。</li>
<li>安全可靠。yarn不会再sandbox中运行代码的</li>
<li>如果与HDFS交互，注意兼容性</li>
<li>持久化数据到HDFS，配置好文件位置。slider中每个app实例都需要有一个目录。</li>
<li>配置目录要可配置，最好不要是绝对路径</li>
<li>log输出不要是一个固定的位置</li>
<li>不明确让它停止，就一直跑下去。slider吧app的终止作为失败处理，会自动重启container。</li>
</ul>
<h4 id="must-not"><a href="#must-not" class="headerlink" title="must not"></a>must not</h4><ul>
<li>人工的启动和关闭</li>
</ul>
<h4 id="should"><a href="#should" class="headerlink" title="should"></a>should</h4><ul>
<li>发布一个RPC或者HTTP端口，可以通过zk或者admin API。slider会提供一个发布机制</li>
<li>可通过标准的hadoop机制进行配置：text、xml文件。不然，就需要自定义的解析器</li>
<li>支持明确的参数来指定配置目录</li>
<li>允许通过-D类似方式指定运行参数</li>
<li>不要运行复杂脚本</li>
<li>提供给slider获取集群节点和状态的方式，这样slider才能探测失败的worker节点并处理</li>
<li>启动的时候能够感知位置。</li>
<li>支持简单的探活，可以是http get</li>
<li>退出的时候返回一个可读的内容，方便针对性处理</li>
<li>支持集群大小调整</li>
<li>支持ambari类似的平台管理</li>
</ul>
<h4 id="may"><a href="#may" class="headerlink" title="may"></a>may</h4><ul>
<li>动态分配RPC或者web端口</li>
<li>包含一个单独的进程，运行在指定位置，这个进程一挂，就引起app终止。这样的进程也能运行在slider am的container中。如果一个活着的cluster不能处理这个进程的restart或者migration，这个slider app就不能处理slider am的restart</li>
<li>汇报load/cost</li>
</ul>
<h4 id="may-not"><a href="#may-not" class="headerlink" title="may not"></a>may not</h4><ul>
<li>yarn可写</li>
<li>纯java</li>
<li>支持动态配置</li>
</ul>

        
        
        <p class="article-more-link">
          
       </p>
    </div>
    

    <footer class="article-footer clearfix">
<div class="article-catetags">


  <div class="article-tags">
  
  <span></span> <a href="/tags/youdaonote/">youdaonote</a>
  </div>

</div>




<div class="comments-count">
	
</div>

</footer>


    </article>






  <nav id="page-nav" class="clearfix">
    <a class="extend prev" rel="prev" href="/"><span></span>Prev</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/24/">24</a><a class="extend next" rel="next" href="/page/3/">Next<span></span></a>
  </nav>

</div>
      <div class="openaside"><a class="navbutton" href="#" title="显示侧边栏"></a></div>

<div id="asidepart">
<div class="closeaside"><a class="closebutton" href="#" title="隐藏侧边栏"></a></div>
<aside class="clearfix">

  


  

  
<div class="tagslist">
	<p class="asidetitle">标签</p>
		<ul class="clearfix">
		
			
				<li><a href="/tags/youdaonote/" title="youdaonote">youdaonote<sup>187</sup></a></li>
			
		
			
				<li><a href="/tags/源码/" title="源码">源码<sup>10</sup></a></li>
			
		
			
				<li><a href="/tags/akka/" title="akka">akka<sup>9</sup></a></li>
			
		
			
				<li><a href="/tags/flume/" title="flume">flume<sup>5</sup></a></li>
			
		
			
				<li><a href="/tags/ETL/" title="ETL">ETL<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/solr/" title="solr">solr<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/spring/" title="spring">spring<sup>4</sup></a></li>
			
		
			
				<li><a href="/tags/调度平台/" title="调度平台">调度平台<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/azkaban/" title="azkaban">azkaban<sup>3</sup></a></li>
			
		
			
				<li><a href="/tags/mysql/" title="mysql">mysql<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/scala/" title="scala">scala<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/ambari/" title="ambari">ambari<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/quartz/" title="quartz">quartz<sup>2</sup></a></li>
			
		
			
				<li><a href="/tags/nodejs/" title="nodejs">nodejs<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/Solr/" title="Solr">Solr<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/guava/" title="guava">guava<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/heroku/" title="heroku">heroku<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hdfs/" title="hdfs">hdfs<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/hue/" title="hue">hue<sup>1</sup></a></li>
			
		
			
				<li><a href="/tags/ElasticSearch/" title="ElasticSearch">ElasticSearch<sup>1</sup></a></li>
			
		
		</ul>
</div>


  <div class="linkslist">
  <p class="asidetitle">友情链接</p>
    <ul>
        
          <li>
            
            	<a href="https://github.com/willcup" target="_blank" title=" 我自己的github">github</a>
            
          </li>
        
          <li>
            
            	<a href="http://thisding.com" target="_blank" title="朋友的主页">Steven&#39;s Blog</a>
            
          </li>
        
    </ul>
</div>

  


  <div class="rsspart">
	<a href="/atom.xml" target="_blank" title="rss">RSS 订阅</a>
</div>

  <div class="weiboshow">
  <p class="asidetitle">新浪微博</p>
    <iframe width="100%" height="119" class="share_self"  frameborder="0" scrolling="no" src="http://widget.weibo.com/weiboshow/index.php?language=&width=0&height=119&fansRow=2&ptype=1&speed=0&skin=9&isTitle=1&noborder=1&isWeibo=0&isFans=0&uid=null&verifier=&dpc=1"></iframe>
</div>


</aside>
</div>
    </div>
    <footer><div id="footer" >
	
	<div class="line">
		<span></span>
		<div class="author"></div>
	</div>
	
	
	<section class="info">
		<p> Hello ,I&#39;m Will Chen in MeiTuan. <br/>
			元 亨 利 贞.</p>
	</section>
	 
	<div class="social-font" class="clearfix">
		
		
		
		
		
		
		
		
		
		
		<a href="mailto:chenxin15@meituan.com" target="_blank" class="icon-email" title="Email Me"></a>
		
	</div>
			
		

		<p class="copyright">
		Powered by <a href="http://hexo.io" target="_blank" title="hexo">hexo</a> and Theme by <a href="https://github.com/wuchong/jacman" target="_blank" title="Jacman">Jacman</a> © 2017 
		
		<a href="/about" target="_blank" title="Will Chen">Will Chen</a>
		
		
		</p>
</div>
</footer>
    <script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/jquery.imagesloaded.min.js"></script>
<script src="/js/gallery.js"></script>
<script src="/js/jquery.qrcode-0.12.0.min.js"></script>

<script type="text/javascript">
$(document).ready(function(){ 
  $('.navbar').click(function(){
    $('header nav').toggleClass('shownav');
  });
  var myWidth = 0;
  function getSize(){
    if( typeof( window.innerWidth ) == 'number' ) {
      myWidth = window.innerWidth;
    } else if( document.documentElement && document.documentElement.clientWidth) {
      myWidth = document.documentElement.clientWidth;
    };
  };
  var m = $('#main'),
      a = $('#asidepart'),
      c = $('.closeaside'),
      o = $('.openaside');
  c.click(function(){
    a.addClass('fadeOut').css('display', 'none');
    o.css('display', 'block').addClass('fadeIn');
    m.addClass('moveMain');
  });
  o.click(function(){
    o.css('display', 'none').removeClass('beforeFadeIn');
    a.css('display', 'block').removeClass('fadeOut').addClass('fadeIn');      
    m.removeClass('moveMain');
  });
  $(window).scroll(function(){
    o.css("top",Math.max(80,260-$(this).scrollTop()));
  });
  
  $(window).resize(function(){
    getSize(); 
    if (myWidth >= 1024) {
      $('header nav').removeClass('shownav');
    }else{
      m.removeClass('moveMain');
      a.css('display', 'block').removeClass('fadeOut');
      o.css('display', 'none');
        
    }
  });
});
</script>










<link rel="stylesheet" href="/fancybox/jquery.fancybox.css" media="screen" type="text/css">
<script src="/fancybox/jquery.fancybox.pack.js"></script>
<script type="text/javascript">
$(document).ready(function(){ 
  $('.article-content').each(function(i){
    $(this).find('img').each(function(){
      if ($(this).parent().hasClass('fancybox')) return;
      var alt = this.alt;
      if (alt) $(this).after('<span class="caption">' + alt + '</span>');
      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox"></a>');
    });
    $(this).find('.fancybox').each(function(){
      $(this).attr('rel', 'article' + i);
    });
  });
  if($.fancybox){
    $('.fancybox').fancybox();
  }
}); 
</script>



<!-- Analytics Begin -->



<script type="text/javascript">
var _bdhmProtocol = (("https:" == document.location.protocol) ? " https://" : " http://");
document.write(unescape("%3Cscript src='" + _bdhmProtocol + "hm.baidu.com/h.js%3Fe6d1f421bbc9962127a50488f9ed37d1' type='text/javascript'%3E%3C/script%3E"));
</script>



<!-- Analytics End -->

<!-- Totop Begin -->

	<div id="totop">
	<a title="返回顶部"><img src="/img/scrollup.png"/></a>
	</div>
	<script src="/js/totop.js"></script>

<!-- Totop End -->

<!-- MathJax Begin -->
<!-- mathjax config similar to math.stackexchange -->


<!-- MathJax End -->

<!-- Tiny_search Begin -->

<!-- Tiny_search End -->

  </body>
 </html>
